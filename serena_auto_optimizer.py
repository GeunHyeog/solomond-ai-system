#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸš€ Serena Auto Optimizer
SOLOMOND AI ì‹œìŠ¤í…œ ìë™ ìµœì í™” ì—”ì§„

ì´ ëª¨ë“ˆì€ Serena ì—ì´ì „íŠ¸ì˜ í•µì‹¬ ê¸°ëŠ¥ì¸ ìë™ ì´ìŠˆ íƒì§€ì™€ ìµœì í™”ë¥¼
Claude Code í™˜ê²½ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•œ í˜•íƒœë¡œ êµ¬í˜„í•©ë‹ˆë‹¤.

í•µì‹¬ ê¸°ëŠ¥:
1. ì‹¤ì‹œê°„ ì½”ë“œ ë¶„ì„ ë° ì´ìŠˆ íƒì§€
2. ìë™ ìˆ˜ì • ìŠ¤í¬ë¦½íŠ¸ ìƒì„± ë° ì‹¤í–‰
3. ì„±ëŠ¥ ìµœì í™” ì œì•ˆ ë° ì ìš©
4. SOLOMOND AI ì‹œìŠ¤í…œ ê±´ê°•ë„ ëª¨ë‹ˆí„°ë§

Author: Serena & SOLOMOND AI Team
Version: 1.0.0
Created: 2025-08-17
"""

import re
import json
import shutil
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
import logging

@dataclass
class AutoFixResult:
    """ìë™ ìˆ˜ì • ê²°ê³¼"""
    file_path: str
    fixes_applied: int
    backup_created: bool
    success: bool
    error_message: Optional[str] = None
    modified_lines: List[int] = None

class SerenaAutoOptimizer:
    """Serena ìë™ ìµœì í™” ì—”ì§„"""
    
    def __init__(self, project_root: str = None):
        self.project_root = Path(project_root) if project_root else Path.cwd()
        self.backup_dir = self.project_root / "serena_backups"
        self.backup_dir.mkdir(exist_ok=True)
        
        # ë¡œê¹… ì„¤ì •
        self.logger = self._setup_logger()
        
        # ìë™ ìˆ˜ì • íŒ¨í„´ë“¤
        self.auto_fix_patterns = {
            "threadpool_fix": {
                "description": "ThreadPoolExecutorë¥¼ context managerë¡œ ë³€ê²½",
                "pattern": r"(\s*)(executor\s*=\s*ThreadPoolExecutor\([^)]*\))\s*\n",
                "replacement": r"\1with ThreadPoolExecutor() as executor:\n\1    # Serena ìë™ ìˆ˜ì •: with ë¬¸ìœ¼ë¡œ ë¦¬ì†ŒìŠ¤ ì•ˆì „ ê´€ë¦¬\n",
                "confidence": 0.95,
                "impact": "critical"
            },
            
            "file_io_fix": {
                "description": "íŒŒì¼ ì—´ê¸°ë¥¼ context managerë¡œ ë³€ê²½",
                "pattern": r"(\s*)((?:file|f)\s*=\s*open\([^)]+\))\s*\n",
                "replacement": r"\1with open(...) as file:\n\1    # Serena ìë™ ìˆ˜ì •: with ë¬¸ìœ¼ë¡œ íŒŒì¼ ì•ˆì „ ê´€ë¦¬\n",
                "confidence": 0.85,
                "impact": "medium"
            },
            
            "gpu_memory_cleanup": {
                "description": "CUDA ì—°ì‚° í›„ ë©”ëª¨ë¦¬ ì •ë¦¬ ì¶”ê°€",
                "pattern": r"(.*torch\.cuda\..*)\n(?!\s*torch\.cuda\.empty_cache)",
                "replacement": r"\1\n    torch.cuda.empty_cache()  # Serena ìë™ ì¶”ê°€: GPU ë©”ëª¨ë¦¬ ì •ë¦¬\n",
                "confidence": 0.90,
                "impact": "high"
            },
            
            "streamlit_cache_add": {
                "description": "ë¬´ê±°ìš´ í•¨ìˆ˜ì— Streamlit ìºì‹œ ì¶”ê°€",
                "pattern": r"(def\s+\w+.*heavy.*\([^)]*\):\s*\n)(?!\s*@st\.cache)",
                "replacement": r"@st.cache_data  # Serena ìë™ ì¶”ê°€: ì„±ëŠ¥ ìµœì í™”\n\1",
                "confidence": 0.80,
                "impact": "medium"
            }
        }
        
        # ì„í¬íŠ¸ ìë™ ì¶”ê°€ íŒ¨í„´
        self.import_patterns = {
            "threadpool": "from concurrent.futures import ThreadPoolExecutor",
            "torch_cuda": "import torch",
            "streamlit_cache": "import streamlit as st"
        }

    def _setup_logger(self) -> logging.Logger:
        """ë¡œê±° ì„¤ì •"""
        logger = logging.getLogger("SerenaAutoOptimizer")
        if not logger.handlers:
            logger.setLevel(logging.INFO)
            handler = logging.FileHandler(self.project_root / "serena_auto_fixer.log")
            formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        return logger

    def analyze_and_fix_project(self, target_files: List[str] = None, auto_apply: bool = False) -> Dict[str, Any]:
        """
        í”„ë¡œì íŠ¸ ë¶„ì„ ë° ìë™ ìˆ˜ì •
        
        Args:
            target_files: ëŒ€ìƒ íŒŒì¼ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ìë™ ì„ íƒ)
            auto_apply: Trueë©´ ìë™ìœ¼ë¡œ ìˆ˜ì • ì ìš©, Falseë©´ ë¶„ì„ë§Œ
        """
        results = {
            "serena_optimizer": {
                "name": "Serena Auto Optimizer",
                "version": "1.0.0",
                "timestamp": datetime.now().isoformat()
            },
            "analysis_summary": {
                "files_analyzed": 0,
                "fixable_issues": 0,
                "critical_fixes": 0,
                "auto_fixes_applied": 0 if not auto_apply else 0
            },
            "file_results": [],
            "optimization_report": [],
            "errors": []
        }
        
        try:
            # ëŒ€ìƒ íŒŒì¼ ê²°ì •
            if not target_files:
                target_files = self._get_priority_files()
            
            # ê° íŒŒì¼ ë¶„ì„ ë° ìˆ˜ì •
            for file_path in target_files:
                if not Path(file_path).exists():
                    continue
                    
                file_result = self._analyze_and_fix_file(file_path, auto_apply)
                results["file_results"].append(file_result)
                results["analysis_summary"]["files_analyzed"] += 1
                
                if file_result["fixable_issues"] > 0:
                    results["analysis_summary"]["fixable_issues"] += file_result["fixable_issues"]
                    
                if file_result["critical_fixes"] > 0:
                    results["analysis_summary"]["critical_fixes"] += file_result["critical_fixes"]
                    
                if auto_apply and file_result["fixes_applied"] > 0:
                    results["analysis_summary"]["auto_fixes_applied"] += file_result["fixes_applied"]
            
            # ìµœì í™” ë³´ê³ ì„œ ìƒì„±
            results["optimization_report"] = self._generate_optimization_report(results)
            
        except Exception as e:
            error_msg = f"í”„ë¡œì íŠ¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            self.logger.error(error_msg)
            results["errors"].append(error_msg)
        
        return results

    def _get_priority_files(self) -> List[str]:
        """ìš°ì„ ìˆœìœ„ íŒŒì¼ ëª©ë¡ ë°˜í™˜"""
        priority_files = [
            "conference_analysis_COMPLETE_WORKING.py",
            "solomond_ai_main_dashboard.py",
            "hybrid_compute_manager.py",
            "dual_brain_integration.py",
            "ai_insights_engine.py"
        ]
        
        existing_files = []
        for file_name in priority_files:
            file_path = self.project_root / file_name
            if file_path.exists():
                existing_files.append(str(file_path))
        
        # core ë””ë ‰í† ë¦¬ì˜ ì¤‘ìš” íŒŒì¼ë“¤ ì¶”ê°€
        core_dir = self.project_root / "core"
        if core_dir.exists():
            core_files = [
                "multimodal_pipeline.py",
                "batch_processing_engine.py",
                "memory_optimizer.py"
            ]
            
            for file_name in core_files:
                file_path = core_dir / file_name
                if file_path.exists():
                    existing_files.append(str(file_path))
        
        return existing_files

    def _analyze_and_fix_file(self, file_path: str, auto_apply: bool) -> Dict[str, Any]:
        """ë‹¨ì¼ íŒŒì¼ ë¶„ì„ ë° ìˆ˜ì •"""
        file_result = {
            "file_path": str(Path(file_path).name),
            "full_path": file_path,
            "fixable_issues": 0,
            "critical_fixes": 0,
            "fixes_applied": 0,
            "backup_created": False,
            "issues_found": [],
            "modifications": []
        }
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                original_content = f.read()
                
            modified_content = original_content
            lines = original_content.split('\n')
            
            # ê° íŒ¨í„´ë³„ë¡œ ë¶„ì„ ë° ìˆ˜ì •
            for pattern_name, pattern_info in self.auto_fix_patterns.items():
                matches = list(re.finditer(
                    pattern_info["pattern"],
                    modified_content,
                    re.MULTILINE
                ))
                
                for match in matches:
                    line_num = original_content[:match.start()].count('\n') + 1
                    
                    issue = {
                        "line": line_num,
                        "pattern": pattern_name,
                        "description": pattern_info["description"],
                        "impact": pattern_info["impact"],
                        "confidence": pattern_info["confidence"],
                        "code_snippet": lines[line_num - 1].strip() if line_num <= len(lines) else ""
                    }
                    
                    file_result["issues_found"].append(issue)
                    file_result["fixable_issues"] += 1
                    
                    if pattern_info["impact"] == "critical":
                        file_result["critical_fixes"] += 1
                    
                    # ìë™ ìˆ˜ì • ì ìš©
                    if auto_apply:
                        if not file_result["backup_created"]:
                            self._create_backup(file_path)
                            file_result["backup_created"] = True
                        
                        # íŒ¨í„´ ìˆ˜ì • ì ìš©
                        modified_content = re.sub(
                            pattern_info["pattern"],
                            pattern_info["replacement"],
                            modified_content,
                            count=1  # í•œ ë²ˆì— í•˜ë‚˜ì”© ìˆ˜ì •
                        )
                        
                        file_result["fixes_applied"] += 1
                        file_result["modifications"].append({
                            "line": line_num,
                            "pattern": pattern_name,
                            "description": pattern_info["description"]
                        })
            
            # í•„ìš”í•œ ì„í¬íŠ¸ ì¶”ê°€
            if auto_apply and file_result["fixes_applied"] > 0:
                modified_content = self._add_required_imports(modified_content, file_result["modifications"])
                
                # ìˆ˜ì •ëœ ë‚´ìš© ì €ì¥
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(modified_content)
                
                self.logger.info(f"íŒŒì¼ ìˆ˜ì • ì™„ë£Œ: {file_path} ({file_result['fixes_applied']}ê°œ ìˆ˜ì •)")
                
        except Exception as e:
            error_msg = f"íŒŒì¼ ë¶„ì„/ìˆ˜ì • ì‹¤íŒ¨ {file_path}: {str(e)}"
            self.logger.error(error_msg)
            file_result["error"] = error_msg
        
        return file_result

    def _create_backup(self, file_path: str) -> str:
        """íŒŒì¼ ë°±ì—… ìƒì„±"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        file_name = Path(file_path).name
        backup_name = f"{file_name}.backup_{timestamp}"
        backup_path = self.backup_dir / backup_name
        
        shutil.copy2(file_path, backup_path)
        self.logger.info(f"ë°±ì—… ìƒì„±: {backup_path}")
        
        return str(backup_path)

    def _add_required_imports(self, content: str, modifications: List[Dict]) -> str:
        """í•„ìš”í•œ ì„í¬íŠ¸ ìë™ ì¶”ê°€"""
        lines = content.split('\n')
        imports_to_add = set()
        
        # ìˆ˜ì •ì‚¬í•­ì— ë”°ë¼ í•„ìš”í•œ ì„í¬íŠ¸ ê²°ì •
        for mod in modifications:
            pattern = mod["pattern"]
            if "threadpool" in pattern and "ThreadPoolExecutor" in content:
                imports_to_add.add(self.import_patterns["threadpool"])
            elif "gpu_memory" in pattern and "torch.cuda" in content:
                imports_to_add.add(self.import_patterns["torch_cuda"])
            elif "streamlit_cache" in pattern and "@st.cache" in content:
                imports_to_add.add(self.import_patterns["streamlit_cache"])
        
        # ê¸°ì¡´ ì„í¬íŠ¸ê°€ ì—†ëŠ” ê²½ìš°ì—ë§Œ ì¶”ê°€
        for import_statement in imports_to_add:
            if import_statement not in content:
                # ë‹¤ë¥¸ ì„í¬íŠ¸ ë¬¸ ë’¤ì— ì¶”ê°€
                import_added = False
                for i, line in enumerate(lines):
                    if line.startswith(("import ", "from ")) and not import_added:
                        continue
                    elif not line.startswith(("import ", "from ")) and i > 0 and not import_added:
                        lines.insert(i, import_statement)
                        import_added = True
                        break
                
                if not import_added:
                    lines.insert(0, import_statement)
        
        return '\n'.join(lines)

    def _generate_optimization_report(self, results: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ìµœì í™” ë³´ê³ ì„œ ìƒì„±"""
        report = []
        
        summary = results["analysis_summary"]
        
        # ì „ì²´ ìš”ì•½
        report.append({
            "category": "summary",
            "title": "Serena ìë™ ìµœì í™” ìš”ì•½",
            "content": f"ì´ {summary['files_analyzed']}ê°œ íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ "
                      f"{summary['fixable_issues']}ê°œì˜ ìˆ˜ì • ê°€ëŠ¥í•œ ì´ìŠˆë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.",
            "metrics": {
                "files_analyzed": summary["files_analyzed"],
                "total_issues": summary["fixable_issues"],
                "critical_issues": summary["critical_fixes"],
                "fixes_applied": summary["auto_fixes_applied"]
            }
        })
        
        # í¬ë¦¬í‹°ì»¬ ì´ìŠˆ ë³´ê³ 
        if summary["critical_fixes"] > 0:
            report.append({
                "category": "critical",
                "title": "í¬ë¦¬í‹°ì»¬ ì´ìŠˆ í•´ê²°",
                "content": f"{summary['critical_fixes']}ê°œì˜ í¬ë¦¬í‹°ì»¬ ì´ìŠˆê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. "
                          "ì´ëŸ¬í•œ ì´ìŠˆë“¤ì€ ì‹œìŠ¤í…œ ì•ˆì •ì„±ì— ì§ì ‘ì ì¸ ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤.",
                "recommendation": "ì¦‰ì‹œ ìˆ˜ì •ì„ ì ìš©í•˜ì—¬ SOLOMOND AI ì‹œìŠ¤í…œì˜ ì•ˆì •ì„±ì„ í™•ë³´í•˜ì„¸ìš”.",
                "impact": "ì‹œìŠ¤í…œ í¬ë˜ì‹œ ë°©ì§€, ë¦¬ì†ŒìŠ¤ ëˆ„ìˆ˜ í•´ê²°"
            })
        
        # ì„±ëŠ¥ ìµœì í™” ë³´ê³ 
        performance_fixes = sum(
            1 for file_result in results["file_results"]
            for issue in file_result.get("issues_found", [])
            if issue.get("impact") in ["medium", "high"]
        )
        
        if performance_fixes > 0:
            report.append({
                "category": "performance",
                "title": "ì„±ëŠ¥ ìµœì í™” ê¸°íšŒ",
                "content": f"{performance_fixes}ê°œì˜ ì„±ëŠ¥ ìµœì í™” ê¸°íšŒê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.",
                "recommendation": "Streamlit ìºì‹±ê³¼ ë©”ëª¨ë¦¬ ê´€ë¦¬ ìµœì í™”ë¥¼ ì ìš©í•˜ì„¸ìš”.",
                "impact": "ì‘ë‹µ ì†ë„ í–¥ìƒ, ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ê°ì†Œ"
            })
        
        # ì½”ë“œ í’ˆì§ˆ ë³´ê³ 
        if summary["fixable_issues"] > 0:
            quality_score = max(100 - (summary["critical_fixes"] * 20 + performance_fixes * 5), 0)
            report.append({
                "category": "quality",
                "title": "ì½”ë“œ í’ˆì§ˆ í‰ê°€",
                "content": f"í˜„ì¬ ì½”ë“œ í’ˆì§ˆ ì ìˆ˜: {quality_score}/100",
                "recommendation": "ë°œê²¬ëœ ì´ìŠˆë“¤ì„ ìˆ˜ì •í•˜ì—¬ ì½”ë“œ í’ˆì§ˆì„ í–¥ìƒì‹œí‚¤ì„¸ìš”.",
                "next_steps": [
                    "í¬ë¦¬í‹°ì»¬ ì´ìŠˆ ìš°ì„  ìˆ˜ì •",
                    "ì„±ëŠ¥ ìµœì í™” íŒ¨í„´ ì ìš©",
                    "ì •ê¸°ì ì¸ ì½”ë“œ ë¦¬ë·° ìˆ˜í–‰"
                ]
            })
        
        return report

    def generate_auto_fix_script(self, analysis_results: Dict[str, Any]) -> str:
        """ì‹¤í–‰ ê°€ëŠ¥í•œ ìë™ ìˆ˜ì • ìŠ¤í¬ë¦½íŠ¸ ìƒì„±"""
        script_lines = [
            "#!/usr/bin/env python3",
            "# -*- coding: utf-8 -*-",
            "\"\"\"",
            "ğŸ”§ Serena ìë™ ìƒì„± ìˆ˜ì • ìŠ¤í¬ë¦½íŠ¸",
            "SOLOMOND AI ì‹œìŠ¤í…œ ìë™ ìµœì í™”",
            f"ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "\"\"\"",
            "",
            "import re",
            "import shutil",
            "from pathlib import Path",
            "from datetime import datetime",
            "",
            "def backup_file(file_path):",
            "    \"\"\"íŒŒì¼ ë°±ì—… ìƒì„±\"\"\"",
            "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')",
            "    backup_path = f'{file_path}.serena_backup_{timestamp}'",
            "    shutil.copy2(file_path, backup_path)",
            "    print(f'âœ… ë°±ì—… ìƒì„±: {backup_path}')",
            "    return backup_path",
            "",
        ]
        
        # ìˆ˜ì •í•  íŒŒì¼ë“¤ ì¶”ì¶œ
        files_with_critical_issues = [
            file_result for file_result in analysis_results["file_results"]
            if file_result["critical_fixes"] > 0
        ]
        
        if files_with_critical_issues:
            script_lines.extend([
                "def fix_critical_issues():",
                "    \"\"\"í¬ë¦¬í‹°ì»¬ ì´ìŠˆ ìë™ ìˆ˜ì •\"\"\"",
                "    fixes_applied = 0",
                "    files_modified = []",
                ""
            ])
            
            for file_result in files_with_critical_issues:
                file_path = file_result["full_path"]
                script_lines.extend([
                    f"    # {file_result['file_path']} ìˆ˜ì •",
                    f"    file_path = '{file_path}'",
                    "    if Path(file_path).exists():",
                    "        backup_file(file_path)",
                    "        ",
                    "        with open(file_path, 'r', encoding='utf-8') as f:",
                    "            content = f.read()",
                    "        ",
                    "        original_content = content",
                    ""
                ])
                
                # ê° ì´ìŠˆë³„ ìˆ˜ì • ë¡œì§ ì¶”ê°€
                for issue in file_result["issues_found"]:
                    if issue["impact"] == "critical":
                        if "threadpool" in issue["pattern"]:
                            script_lines.extend([
                                "        # ThreadPool ë¦¬ì†ŒìŠ¤ ê´€ë¦¬ ìˆ˜ì •",
                                "        pattern = r'(\\s*)(executor\\s*=\\s*ThreadPoolExecutor\\([^)]*\\))\\s*\\n'",
                                "        replacement = r'\\1with ThreadPoolExecutor() as executor:\\n\\1    # Serena ìˆ˜ì •: ì•ˆì „í•œ ë¦¬ì†ŒìŠ¤ ê´€ë¦¬\\n'",
                                "        content = re.sub(pattern, replacement, content, flags=re.MULTILINE)",
                                ""
                            ])
                
                script_lines.extend([
                    "        if content != original_content:",
                    "            with open(file_path, 'w', encoding='utf-8') as f:",
                    "                f.write(content)",
                    "            fixes_applied += 1",
                    "            files_modified.append(file_path)",
                    f"            print(f'ğŸ”§ ìˆ˜ì • ì™„ë£Œ: {file_result['file_path']}')",
                    "    ",
                    ""
                ])
            
            script_lines.extend([
                "    return fixes_applied, files_modified",
                ""
            ])
        
        # ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„
        script_lines.extend([
            "def main():",
            "    \"\"\"ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜\"\"\"",
            "    print('ğŸ¤– Serena ìë™ ìˆ˜ì • ì‹œìŠ¤í…œ ì‹œì‘')",
            "    print('ğŸ¯ SOLOMOND AI ì‹œìŠ¤í…œ ìµœì í™”')",
            "    print('=' * 50)",
            "    ",
            "    total_fixes = 0",
            "    modified_files = []",
            "    ",
        ])
        
        if files_with_critical_issues:
            script_lines.extend([
                "    # í¬ë¦¬í‹°ì»¬ ì´ìŠˆ ìˆ˜ì •",
                "    critical_fixes, critical_files = fix_critical_issues()",
                "    total_fixes += critical_fixes",
                "    modified_files.extend(critical_files)",
                "    ",
            ])
        
        script_lines.extend([
            "    print(f'\\nâœ… ìˆ˜ì • ì™„ë£Œ!')",
            "    print(f'ğŸ“Š ì´ {total_fixes}ê°œ íŒŒì¼ ìˆ˜ì •ë¨')",
            "    print(f'ğŸ“ ìˆ˜ì •ëœ íŒŒì¼: {len(modified_files)}ê°œ')",
            "    ",
            "    if total_fixes > 0:",
            "        print('\\nğŸ’¡ ê¶Œì¥ì‚¬í•­:')",
            "        print('1. ì‹œìŠ¤í…œì„ ì¬ì‹œì‘í•˜ì—¬ ë³€ê²½ì‚¬í•­ í™•ì¸')",
            "        print('2. ë°±ì—… íŒŒì¼ë“¤ì´ ìƒì„±ë˜ì—ˆìœ¼ë¯€ë¡œ í•„ìš”ì‹œ ë³µì› ê°€ëŠ¥')",
            "        print('3. ìˆ˜ì • í›„ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ìˆ˜í–‰')",
            "    else:",
            "        print('â„¹ï¸  ìˆ˜ì •ì´ í•„ìš”í•œ í¬ë¦¬í‹°ì»¬ ì´ìŠˆê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.')",
            "",
            "if __name__ == '__main__':",
            "    main()"
        ])
        
        return "\n".join(script_lines)

def main():
    """Serena Auto Optimizer ì‹¤í–‰"""
    print("ğŸš€ Serena Auto Optimizer")
    print("ğŸ¯ SOLOMOND AI ì‹œìŠ¤í…œ ìë™ ìµœì í™” ì—”ì§„")
    print("=" * 60)
    
    optimizer = SerenaAutoOptimizer()
    
    # ë¶„ì„ë§Œ ì‹¤í–‰ (ìë™ ìˆ˜ì •ì€ ì‚¬ìš©ì í™•ì¸ í›„)
    print("ğŸ“Š í”„ë¡œì íŠ¸ ë¶„ì„ ì¤‘...")
    results = optimizer.analyze_and_fix_project(auto_apply=False)
    
    # ê²°ê³¼ ì¶œë ¥
    summary = results["analysis_summary"]
    print(f"âœ… ë¶„ì„ ì™„ë£Œ!")
    print(f"ğŸ“ ë¶„ì„ëœ íŒŒì¼: {summary['files_analyzed']}ê°œ")
    print(f"ğŸ” ìˆ˜ì • ê°€ëŠ¥í•œ ì´ìŠˆ: {summary['fixable_issues']}ê°œ")
    
    if summary["critical_fixes"] > 0:
        print(f"ğŸš¨ í¬ë¦¬í‹°ì»¬ ì´ìŠˆ: {summary['critical_fixes']}ê°œ")
    
    # ìµœì í™” ë³´ê³ ì„œ í‘œì‹œ
    if results["optimization_report"]:
        print(f"\nğŸ“‹ Serena ìµœì í™” ë³´ê³ ì„œ:")
        for report in results["optimization_report"]:
            if report["category"] == "summary":
                continue
                
            emoji = {"critical": "ğŸš¨", "performance": "âš¡", "quality": "ğŸ“ˆ"}
            print(f"\n{emoji.get(report['category'], 'ğŸ’¡')} {report['title']}")
            print(f"   {report['content']}")
            
            if "recommendation" in report:
                print(f"   ğŸ’¡ ê¶Œì¥ì‚¬í•­: {report['recommendation']}")
    
    # ìë™ ìˆ˜ì • ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
    if summary["critical_fixes"] > 0:
        print(f"\nğŸ”§ ìë™ ìˆ˜ì • ìŠ¤í¬ë¦½íŠ¸ ìƒì„± ì¤‘...")
        fix_script = optimizer.generate_auto_fix_script(results)
        
        script_path = Path("serena_auto_fix_generated.py")
        with open(script_path, 'w', encoding='utf-8') as f:
            f.write(fix_script)
        
        print(f"âœ… ìë™ ìˆ˜ì • ìŠ¤í¬ë¦½íŠ¸ ìƒì„±: {script_path}")
        print(f"ğŸ’¡ ì‹¤í–‰ ë°©ë²•: python {script_path}")
        print(f"âš ï¸  ì£¼ì˜: ì‹¤í–‰ ì „ ì¤‘ìš” íŒŒì¼ì„ ë°±ì—…í•˜ì„¸ìš”!")

if __name__ == "__main__":
    main()