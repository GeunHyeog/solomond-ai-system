"""
ì†”ë¡œëª¬ë“œ AI ì‹œìŠ¤í…œ - API ë¼ìš°íŠ¸
FastAPI ê¸°ë°˜ REST API ì—”ë“œí¬ì¸íŠ¸
"""

from fastapi import APIRouter, File, UploadFile, HTTPException
from fastapi.responses import JSONResponse
import time
from typing import Optional

# Import ì²˜ë¦¬ (ê°œë°œ í™˜ê²½ í˜¸í™˜)
try:
    # ìƒëŒ€ import ì‹œë„
    from ..core.analyzer import get_analyzer, check_whisper_status
except ImportError:
    try:
        # ì ˆëŒ€ import ì‹œë„
        import sys
        from pathlib import Path
        sys.path.append(str(Path(__file__).parent.parent))
        from core.analyzer import get_analyzer, check_whisper_status
    except ImportError:
        # ìµœí›„ì˜ ìˆ˜ë‹¨: í•¨ìˆ˜ ì •ì˜
        print("âš ï¸ core.analyzerë¥¼ importí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëŒ€ì²´ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        
        def get_analyzer(model_size="base"):
            class DummyAnalyzer:
                def get_model_info(self):
                    return {"model_size": "base", "model_loaded": False, "whisper_available": False, "supported_formats": [".mp3", ".wav", ".m4a"]}
                async def analyze_uploaded_file(self, file_content, filename, language="ko"):
                    return {"success": False, "error": "Analyzer not available"}
            return DummyAnalyzer()
        
        def check_whisper_status():
            return {"whisper_available": False, "import_error": "Module not found"}

# API ë¼ìš°í„° ìƒì„±
router = APIRouter()

@router.post("/process_audio")
async def process_audio(audio_file: UploadFile = File(...)):
    """
    ìŒì„± íŒŒì¼ ì²˜ë¦¬ API
    
    ì—…ë¡œë“œëœ ìŒì„± íŒŒì¼ì„ STT ë¶„ì„í•˜ì—¬ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    """
    start_time = time.time()
    
    try:
        # íŒŒì¼ ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘
        filename = audio_file.filename
        file_content = await audio_file.read()
        file_size_mb = round(len(file_content) / (1024 * 1024), 2)
        
        print(f"ğŸ“ íŒŒì¼ ìˆ˜ì‹ : {filename} ({file_size_mb} MB)")
        
        # Whisper ìƒíƒœ í™•ì¸
        whisper_status = check_whisper_status()
        if not whisper_status["whisper_available"]:
            return JSONResponse({
                "success": False,
                "error": "Whisper ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install openai-whisper' ì‹¤í–‰í•˜ì„¸ìš”."
            })
        
        # ë¶„ì„ê¸° ê°€ì ¸ì˜¤ê¸°
        analyzer = get_analyzer("base")
        
        # ìŒì„± ë¶„ì„ ì‹¤í–‰
        result = await analyzer.analyze_uploaded_file(
            file_content=file_content,
            filename=filename,
            language="ko"
        )
        
        # ì‘ë‹µ í˜•ì‹ì„ ê¸°ì¡´ minimal_stt_test.pyì™€ ë™ì¼í•˜ê²Œ ë§ì¶¤
        if result["success"]:
            return JSONResponse({
                "success": True,
                "filename": result["filename"],
                "file_size": result["file_size"],
                "transcribed_text": result["transcribed_text"],
                "processing_time": result["processing_time"],
                "detected_language": result["detected_language"]
            })
        else:
            return JSONResponse({
                "success": False,
                "error": result["error"],
                "processing_time": result.get("processing_time", round(time.time() - start_time, 2))
            })
            
    except Exception as e:
        processing_time = round(time.time() - start_time, 2)
        error_msg = str(e)
        
        print(f"âŒ API ì˜¤ë¥˜: {error_msg}")
        
        return JSONResponse({
            "success": False,
            "error": error_msg,
            "processing_time": processing_time
        })

@router.get("/test")
async def system_test():
    """
    ì‹œìŠ¤í…œ ìƒíƒœ í…ŒìŠ¤íŠ¸ API
    
    ì‹œìŠ¤í…œì˜ í˜„ì¬ ìƒíƒœì™€ ì‚¬ìš© ê°€ëŠ¥í•œ ê¸°ëŠ¥ë“¤ì„ í™•ì¸
    """
    whisper_status = check_whisper_status()
    analyzer = get_analyzer()
    model_info = analyzer.get_model_info()
    
    return JSONResponse({
        "status": "OK",
        "version": "3.0",
        "python_version": "3.13+",
        "whisper_available": whisper_status["whisper_available"],
        "model_info": model_info,
        "supported_formats": model_info["supported_formats"],
        "features": {
            "stt": whisper_status["whisper_available"],
            "translation": True,
            "file_upload": True,
            "modular_architecture": True
        }
    })

@router.get("/health")
async def health_check():
    """
    í—¬ìŠ¤ ì²´í¬ API
    
    ì„œë²„ê°€ ì •ìƒì ìœ¼ë¡œ ë™ì‘í•˜ëŠ”ì§€ í™•ì¸
    """
    return {"status": "healthy", "timestamp": time.time()}

@router.post("/analyze_batch")
async def analyze_batch_files(
    files: list[UploadFile] = File(...),
    language: Optional[str] = "ko"
):
    """
    ë‹¤ì¤‘ íŒŒì¼ ë°°ì¹˜ ë¶„ì„ API (í™•ì¥ ê¸°ëŠ¥)
    
    ì—¬ëŸ¬ ìŒì„± íŒŒì¼ì„ í•œ ë²ˆì— ì²˜ë¦¬
    """
    results = []
    analyzer = get_analyzer()
    
    for file in files:
        try:
            file_content = await file.read()
            result = await analyzer.analyze_uploaded_file(
                file_content=file_content,
                filename=file.filename,
                language=language
            )
            results.append({
                "filename": file.filename,
                "result": result
            })
        except Exception as e:
            results.append({
                "filename": file.filename,
                "result": {
                    "success": False,
                    "error": str(e)
                }
            })
    
    successful_count = sum(1 for r in results if r["result"]["success"])
    
    return JSONResponse({
        "batch_success": True,
        "total_files": len(files),
        "successful_files": successful_count,
        "failed_files": len(files) - successful_count,
        "results": results
    })

@router.get("/models")
async def list_available_models():
    """
    ì‚¬ìš© ê°€ëŠ¥í•œ Whisper ëª¨ë¸ ëª©ë¡ API
    """
    models = [
        {"name": "tiny", "size": "~39 MB", "speed": "ë§¤ìš° ë¹ ë¦„", "accuracy": "ë‚®ìŒ"},
        {"name": "base", "size": "~74 MB", "speed": "ë¹ ë¦„", "accuracy": "ì¤‘ê°„"},
        {"name": "small", "size": "~244 MB", "speed": "ë³´í†µ", "accuracy": "ì¢‹ìŒ"},
        {"name": "medium", "size": "~769 MB", "speed": "ëŠë¦¼", "accuracy": "ë§¤ìš° ì¢‹ìŒ"},
        {"name": "large", "size": "~1550 MB", "speed": "ë§¤ìš° ëŠë¦¼", "accuracy": "ìµœê³ "}
    ]
    
    return JSONResponse({
        "available_models": models,
        "current_model": get_analyzer().get_model_info().get("model_size", "base"),
        "recommendation": "base ëª¨ë¸ì´ ì†ë„ì™€ ì •í™•ë„ì˜ ê· í˜•ì´ ì¢‹ìŠµë‹ˆë‹¤."
    })

# ì—ëŸ¬ í•¸ë“¤ëŸ¬
@router.exception_handler(HTTPException)
async def http_exception_handler(request, exc):
    return JSONResponse(
        status_code=exc.status_code,
        content={"success": False, "error": exc.detail}
    )

@router.exception_handler(Exception)
async def general_exception_handler(request, exc):
    return JSONResponse(
        status_code=500,
        content={"success": False, "error": "ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}
    )
