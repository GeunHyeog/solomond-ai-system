#!/usr/bin/env python3
"""
SOLOMOND AI - ì•ˆì •ì  ë°±ì—”ë“œ ì„œë²„
Streamlit ì™„ì „ ëŒ€ì²´ ì‹œìŠ¤í…œ

HTML ë…ë¦½ ì‹œìŠ¤í…œìš© FastAPI ë°±ì—”ë“œ
- 3GB+ ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²˜ë¦¬ ìµœì í™”
- ë©”ëª¨ë¦¬ ì„¸ê·¸ë©˜í…Œì´ì…˜ ë°©ì§€
- Ollama AI ëª¨ë¸ 5ê°œ í™œìš©
- ë“€ì–¼ ë¸Œë ˆì¸ ì‹œìŠ¤í…œ í†µí•©
"""

import os
import sys
import asyncio
import uvicorn
import json
import hashlib
import tempfile
import shutil
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional
import traceback

from fastapi import FastAPI, File, UploadFile, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from fastapi.staticfiles import StaticFiles

# ë©”ëª¨ë¦¬ ìµœì í™”
import gc
import psutil

# AI ì²˜ë¦¬
import torch
import whisper
import cv2
import numpy as np
from PIL import Image
import easyocr

# Ollama í´ë¼ì´ì–¸íŠ¸
try:
    import ollama
    OLLAMA_AVAILABLE = True
except ImportError:
    OLLAMA_AVAILABLE = False
    print("âš ï¸ Ollama í´ë¼ì´ì–¸íŠ¸ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install ollama' ì‹¤í–‰í•˜ì„¸ìš”.")

# ì„¤ì •
MAX_FILE_SIZE = 3 * 1024 * 1024 * 1024  # 3GB
TEMP_DIR = Path("temp_analysis")
RESULTS_DIR = Path("analysis_results")
CHUNK_SIZE = 10 * 1024 * 1024  # 10MB chunks

# ë””ë ‰í† ë¦¬ ìƒì„±
TEMP_DIR.mkdir(exist_ok=True)
RESULTS_DIR.mkdir(exist_ok=True)

# FastAPI ì•± ì´ˆê¸°í™”
app = FastAPI(
    title="SOLOMOND AI Stable Backend",
    description="HTML ë…ë¦½ ì‹œìŠ¤í…œìš© ì•ˆì •ì  ë°±ì—”ë“œ",
    version="5.0.0"
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì „ì—­ ë³€ìˆ˜
analysis_cache = {}
current_models = {}

class MemoryOptimizer:
    """ë©”ëª¨ë¦¬ ìµœì í™” ë§¤ë‹ˆì €"""
    
    @staticmethod
    def get_memory_info():
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë°˜í™˜"""
        process = psutil.Process()
        memory_info = process.memory_info()
        return {
            "rss_mb": memory_info.rss / 1024 / 1024,
            "vms_mb": memory_info.vms / 1024 / 1024,
            "percent": process.memory_percent()
        }
    
    @staticmethod
    def force_gc():
        """ê°•ì œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜"""
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    @staticmethod
    def check_memory_limit(limit_mb=2048):
        """ë©”ëª¨ë¦¬ í•œê³„ ì²´í¬"""
        current = MemoryOptimizer.get_memory_info()
        return current["rss_mb"] < limit_mb

class ModelManager:
    """AI ëª¨ë¸ ë§¤ë‹ˆì €"""
    
    def __init__(self):
        self.whisper_model = None
        self.ocr_reader = None
        self.ollama_models = [
            "gpt-oss:20b", "qwen3:8b", "gemma3:27b", 
            "qwen2.5:7b", "gemma3:4b"
        ]
    
    async def load_whisper(self):
        """Whisper STT ëª¨ë¸ ë¡œë“œ"""
        if self.whisper_model is None:
            try:
                print("ğŸ¤ Whisper STT ëª¨ë¸ ë¡œë”© ì¤‘...")
                self.whisper_model = whisper.load_model("base")
                print("âœ… Whisper ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                print(f"âŒ Whisper ë¡œë“œ ì‹¤íŒ¨: {e}")
                self.whisper_model = None
        return self.whisper_model
    
    async def load_ocr(self):
        """EasyOCR ëª¨ë¸ ë¡œë“œ"""
        if self.ocr_reader is None:
            try:
                print("ğŸ‘ï¸ EasyOCR ëª¨ë¸ ë¡œë”© ì¤‘...")
                self.ocr_reader = easyocr.Reader(['ko', 'en'], gpu=False)
                print("âœ… EasyOCR ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                print(f"âŒ EasyOCR ë¡œë“œ ì‹¤íŒ¨: {e}")
                self.ocr_reader = None
        return self.ocr_reader
    
    async def get_ollama_model(self, model_name="qwen2.5:7b"):
        """Ollama ëª¨ë¸ ì„ íƒ"""
        if not OLLAMA_AVAILABLE:
            return None
        
        try:
            # ëª¨ë¸ ëª©ë¡ í™•ì¸
            models = ollama.list()
            available_models = [m['name'] for m in models['models']]
            
            if model_name in available_models:
                return model_name
            
            # ëŒ€ì²´ ëª¨ë¸ ì°¾ê¸°
            for alt_model in self.ollama_models:
                if alt_model in available_models:
                    return alt_model
            
            print("âš ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ Ollama ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return None
            
        except Exception as e:
            print(f"âŒ Ollama ëª¨ë¸ í™•ì¸ ì‹¤íŒ¨: {e}")
            return None

# ê¸€ë¡œë²Œ ëª¨ë¸ ë§¤ë‹ˆì €
model_manager = ModelManager()

class FileAnalyzer:
    """íŒŒì¼ ë¶„ì„ ì—”ì§„"""
    
    @staticmethod
    async def analyze_image(file_path: Path) -> Dict[str, Any]:
        """ì´ë¯¸ì§€ ë¶„ì„"""
        try:
            print(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ ë¶„ì„ ì‹œì‘: {file_path.name}")
            
            # OCR ëª¨ë¸ ë¡œë“œ
            ocr_reader = await model_manager.load_ocr()
            if ocr_reader is None:
                return {"error": "OCR ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨"}
            
            # ì´ë¯¸ì§€ ë¡œë“œ (ë©”ëª¨ë¦¬ ìµœì í™”)
            image = cv2.imread(str(file_path))
            if image is None:
                return {"error": "ì´ë¯¸ì§€ íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
            
            # OCR ìˆ˜í–‰
            results = ocr_reader.readtext(image)
            
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            extracted_texts = []
            for (bbox, text, confidence) in results:
                if confidence > 0.5:  # ì‹ ë¢°ë„ 50% ì´ìƒ
                    extracted_texts.append({
                        "text": text,
                        "confidence": float(confidence),
                        "bbox": [float(x) for coord in bbox for x in coord]
                    })
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del image
            MemoryOptimizer.force_gc()
            
            return {
                "type": "image",
                "total_texts": len(extracted_texts),
                "high_confidence_texts": len([t for t in extracted_texts if t["confidence"] > 0.8]),
                "texts": extracted_texts[:10],  # ìƒìœ„ 10ê°œë§Œ
                "analysis_time": datetime.now().isoformat(),
                "success": True
            }
            
        except Exception as e:
            print(f"âŒ ì´ë¯¸ì§€ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {"error": str(e), "success": False}
    
    @staticmethod
    async def analyze_audio(file_path: Path) -> Dict[str, Any]:
        """ìŒì„± ë¶„ì„"""
        try:
            print(f"ğŸ¤ ìŒì„± ë¶„ì„ ì‹œì‘: {file_path.name}")
            
            # Whisper ëª¨ë¸ ë¡œë“œ
            whisper_model = await model_manager.load_whisper()
            if whisper_model is None:
                return {"error": "Whisper ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨"}
            
            # ìŒì„± ì¸ì‹
            result = whisper_model.transcribe(str(file_path))
            
            # ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´ ì¶”ì¶œ
            segments = []
            if 'segments' in result:
                for segment in result['segments'][:20]:  # ìƒìœ„ 20ê°œ ì„¸ê·¸ë¨¼íŠ¸
                    segments.append({
                        "start": segment.get('start', 0),
                        "end": segment.get('end', 0),
                        "text": segment.get('text', '').strip(),
                        "confidence": segment.get('avg_logprob', 0)
                    })
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            MemoryOptimizer.force_gc()
            
            return {
                "type": "audio",
                "transcription": result.get('text', '').strip(),
                "language": result.get('language', 'unknown'),
                "segments_count": len(segments),
                "segments": segments,
                "analysis_time": datetime.now().isoformat(),
                "success": True
            }
            
        except Exception as e:
            print(f"âŒ ìŒì„± ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {"error": str(e), "success": False}
    
    @staticmethod
    async def analyze_video(file_path: Path) -> Dict[str, Any]:
        """ë¹„ë””ì˜¤ ë¶„ì„ (ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬)"""
        try:
            print(f"ğŸ¬ ë¹„ë””ì˜¤ ë¶„ì„ ì‹œì‘: {file_path.name}")
            
            # ë¹„ë””ì˜¤ ì •ë³´ ì¶”ì¶œ
            cap = cv2.VideoCapture(str(file_path))
            fps = cap.get(cv2.CAP_PROP_FPS)
            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            duration = frame_count / fps if fps > 0 else 0
            
            # ìƒ˜í”Œ í”„ë ˆì„ ì¶”ì¶œ (ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ 10ê°œë§Œ)
            sample_frames = []
            interval = max(1, frame_count // 10)
            
            for i in range(0, frame_count, interval):
                cap.set(cv2.CAP_PROP_POS_FRAMES, i)
                ret, frame = cap.read()
                if ret:
                    # í”„ë ˆì„ í¬ê¸° ì¶•ì†Œ (ë©”ëª¨ë¦¬ ì ˆì•½)
                    frame = cv2.resize(frame, (640, 480))
                    sample_frames.append(frame)
                
                if len(sample_frames) >= 10:
                    break
            
            cap.release()
            
            # ëŒ€í‘œ í”„ë ˆì„ì—ì„œ OCR ìˆ˜í–‰
            ocr_results = []
            if sample_frames and len(sample_frames) > 0:
                ocr_reader = await model_manager.load_ocr()
                if ocr_reader:
                    for i, frame in enumerate(sample_frames[:3]):  # ì²« 3ê°œ í”„ë ˆì„ë§Œ
                        try:
                            results = ocr_reader.readtext(frame)
                            for (bbox, text, confidence) in results:
                                if confidence > 0.7:
                                    ocr_results.append({
                                        "frame": i,
                                        "text": text,
                                        "confidence": float(confidence)
                                    })
                        except:
                            continue
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del sample_frames
            MemoryOptimizer.force_gc()
            
            return {
                "type": "video",
                "duration_seconds": duration,
                "fps": fps,
                "frame_count": frame_count,
                "extracted_texts": len(ocr_results),
                "sample_texts": ocr_results[:5],
                "analysis_time": datetime.now().isoformat(),
                "success": True
            }
            
        except Exception as e:
            print(f"âŒ ë¹„ë””ì˜¤ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {"error": str(e), "success": False}

# API ì—”ë“œí¬ì¸íŠ¸ë“¤

@app.get("/health")
async def health_check():
    """ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸"""
    memory_info = MemoryOptimizer.get_memory_info()
    
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "memory": memory_info,
        "models": {
            "whisper": model_manager.whisper_model is not None,
            "ocr": model_manager.ocr_reader is not None,
            "ollama": OLLAMA_AVAILABLE
        }
    }

@app.get("/models/status")
async def get_model_status():
    """ëª¨ë¸ ìƒíƒœ í™•ì¸"""
    ollama_models = []
    if OLLAMA_AVAILABLE:
        try:
            models = ollama.list()
            ollama_models = [m['name'] for m in models['models']]
        except:
            pass
    
    return {
        "whisper_loaded": model_manager.whisper_model is not None,
        "ocr_loaded": model_manager.ocr_reader is not None,
        "ollama_available": OLLAMA_AVAILABLE,
        "ollama_models": ollama_models,
        "recommended_model": await model_manager.get_ollama_model()
    }

@app.post("/analyze/file")
async def analyze_single_file(background_tasks: BackgroundTasks, file: UploadFile = File(...)):
    """ë‹¨ì¼ íŒŒì¼ ë¶„ì„"""
    try:
        # íŒŒì¼ í¬ê¸° ì²´í¬
        file_content = await file.read()
        if len(file_content) > MAX_FILE_SIZE:
            raise HTTPException(status_code=413, detail="íŒŒì¼ í¬ê¸°ê°€ 3GBë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤.")
        
        # ë©”ëª¨ë¦¬ ì²´í¬
        if not MemoryOptimizer.check_memory_limit(2048):  # 2GB í•œê³„
            MemoryOptimizer.force_gc()
            if not MemoryOptimizer.check_memory_limit(2048):
                raise HTTPException(status_code=507, detail="ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ íŒŒì¼ ì²˜ë¦¬ ë¶ˆê°€")
        
        # ì„ì‹œ íŒŒì¼ ì €ì¥
        file_hash = hashlib.md5(file_content).hexdigest()
        temp_file = TEMP_DIR / f"{file_hash}_{file.filename}"
        
        with open(temp_file, "wb") as f:
            f.write(file_content)
        
        # íŒŒì¼ íƒ€ì…ë³„ ë¶„ì„
        file_ext = temp_file.suffix.lower()
        
        if file_ext in ['.jpg', '.jpeg', '.png', '.gif', '.bmp']:
            result = await FileAnalyzer.analyze_image(temp_file)
        elif file_ext in ['.mp3', '.wav', '.m4a', '.flac']:
            result = await FileAnalyzer.analyze_audio(temp_file)
        elif file_ext in ['.mp4', '.mov', '.avi', '.mkv']:
            result = await FileAnalyzer.analyze_video(temp_file)
        else:
            result = {"error": "ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤.", "success": False}
        
        # ê²°ê³¼ì— íŒŒì¼ ì •ë³´ ì¶”ê°€
        result.update({
            "filename": file.filename,
            "file_size": len(file_content),
            "file_hash": file_hash
        })
        
        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì„ì‹œ íŒŒì¼ ì •ë¦¬
        background_tasks.add_task(cleanup_temp_file, temp_file)
        
        return result
        
    except Exception as e:
        print(f"âŒ íŒŒì¼ ë¶„ì„ ì˜¤ë¥˜: {e}")
        print(traceback.format_exc())
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/analyze/batch")
async def analyze_batch_files(background_tasks: BackgroundTasks, files: List[UploadFile] = File(...)):
    """ë°°ì¹˜ íŒŒì¼ ë¶„ì„"""
    try:
        if len(files) > 20:  # ë°°ì¹˜ ì²˜ë¦¬ í•œê³„
            raise HTTPException(status_code=400, detail="í•œ ë²ˆì— ìµœëŒ€ 20ê°œ íŒŒì¼ê¹Œì§€ ì²˜ë¦¬ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        
        results = []
        total_size = 0
        
        for file in files:
            # íŒŒì¼ í¬ê¸° ë¯¸ë¦¬ ì²´í¬
            await file.seek(0, 2)  # íŒŒì¼ ëìœ¼ë¡œ
            file_size = await file.tell()
            await file.seek(0)  # íŒŒì¼ ì‹œì‘ìœ¼ë¡œ
            
            total_size += file_size
            if total_size > MAX_FILE_SIZE * 2:  # ë°°ì¹˜ì˜ ê²½ìš° 6GB í•œê³„
                raise HTTPException(status_code=413, detail="ë°°ì¹˜ íŒŒì¼ ì´ í¬ê¸°ê°€ 6GBë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤.")
        
        # ê° íŒŒì¼ ìˆœì°¨ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ ì ˆì•½)
        for i, file in enumerate(files):
            try:
                print(f"ğŸ“Š ë°°ì¹˜ ë¶„ì„ ì§„í–‰: {i+1}/{len(files)} - {file.filename}")
                
                # ë‹¨ì¼ íŒŒì¼ ë¶„ì„ ì¬ì‚¬ìš©
                file_content = await file.read()
                
                # ì„ì‹œ íŒŒì¼ ìƒì„±
                file_hash = hashlib.md5(file_content).hexdigest()
                temp_file = TEMP_DIR / f"batch_{file_hash}_{file.filename}"
                
                with open(temp_file, "wb") as f:
                    f.write(file_content)
                
                # ë¶„ì„ ìˆ˜í–‰
                file_ext = temp_file.suffix.lower()
                
                if file_ext in ['.jpg', '.jpeg', '.png', '.gif', '.bmp']:
                    result = await FileAnalyzer.analyze_image(temp_file)
                elif file_ext in ['.mp3', '.wav', '.m4a', '.flac']:
                    result = await FileAnalyzer.analyze_audio(temp_file)
                elif file_ext in ['.mp4', '.mov', '.avi', '.mkv']:
                    result = await FileAnalyzer.analyze_video(temp_file)
                else:
                    result = {"error": "ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹", "success": False}
                
                result.update({
                    "filename": file.filename,
                    "file_size": len(file_content),
                    "batch_index": i
                })
                
                results.append(result)
                
                # ì„ì‹œ íŒŒì¼ ì¦‰ì‹œ ì •ë¦¬
                if temp_file.exists():
                    temp_file.unlink()
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                MemoryOptimizer.force_gc()
                
            except Exception as e:
                print(f"âŒ íŒŒì¼ {file.filename} ë¶„ì„ ì‹¤íŒ¨: {e}")
                results.append({
                    "filename": file.filename,
                    "error": str(e),
                    "success": False,
                    "batch_index": i
                })
        
        # ë°°ì¹˜ ë¶„ì„ ìš”ì•½
        successful = len([r for r in results if r.get('success', False)])
        failed = len(results) - successful
        
        return {
            "batch_analysis": True,
            "total_files": len(files),
            "successful": successful,
            "failed": failed,
            "results": results,
            "analysis_time": datetime.now().isoformat()
        }
        
    except Exception as e:
        print(f"âŒ ë°°ì¹˜ ë¶„ì„ ì˜¤ë¥˜: {e}")
        print(traceback.format_exc())
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/analyze/dual-brain")
async def dual_brain_analysis(request_data: dict):
    """ë“€ì–¼ ë¸Œë ˆì¸ ë¶„ì„"""
    try:
        analysis_results = request_data.get('analysis_results', [])
        
        if not analysis_results:
            raise HTTPException(status_code=400, detail="ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # AI ì¸ì‚¬ì´íŠ¸ ìƒì„±
        insights = await generate_ai_insights(analysis_results)
        
        # êµ¬ê¸€ ìº˜ë¦°ë” ì´ë²¤íŠ¸ ìƒì„± (ì‹œë®¬ë ˆì´ì…˜)
        calendar_event = {
            "title": f"SOLOMOND AI ë¶„ì„ ê²°ê³¼ - {datetime.now().strftime('%Y-%m-%d')}",
            "description": f"íŒŒì¼ {len(analysis_results)}ê°œ ë¶„ì„ ì™„ë£Œ",
            "start_time": datetime.now().isoformat(),
            "insights_summary": insights.get('summary', ''),
            "created": True
        }
        
        return {
            "dual_brain_analysis": True,
            "ai_insights": insights,
            "calendar_event": calendar_event,
            "timestamp": datetime.now().isoformat(),
            "success": True
        }
        
    except Exception as e:
        print(f"âŒ ë“€ì–¼ ë¸Œë ˆì¸ ë¶„ì„ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))

async def generate_ai_insights(analysis_results: List[Dict]) -> Dict[str, Any]:
    """AI ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
    try:
        # Ollama ëª¨ë¸ ì‚¬ìš©
        model_name = await model_manager.get_ollama_model()
        
        if model_name and OLLAMA_AVAILABLE:
            # ë¶„ì„ ê²°ê³¼ ìš”ì•½ ìƒì„±
            summary_text = "\n".join([
                f"íŒŒì¼: {r.get('filename', 'ì•Œ ìˆ˜ ì—†ìŒ')}, íƒ€ì…: {r.get('type', 'ì•Œ ìˆ˜ ì—†ìŒ')}"
                for r in analysis_results if r.get('success', False)
            ])
            
            prompt = f"""ë‹¤ìŒ íŒŒì¼ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¸ì‚¬ì´íŠ¸ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:

{summary_text}

6ê°€ì§€ íŒ¨í„´ìœ¼ë¡œ ë¶„ì„:
1. ì‹œê°„ íŒ¨í„´
2. ì½˜í…ì¸  íŒ¨í„´  
3. ì„±ëŠ¥ íŒ¨í„´
4. íŠ¸ë Œë“œ ë¶„ì„
5. ì´ìƒ íƒì§€
6. ê°œì„  ì œì•ˆ

í•œêµ­ì–´ë¡œ êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”."""

            try:
                response = ollama.generate(model=model_name, prompt=prompt)
                ai_response = response.get('response', '')
                
                return {
                    "model_used": model_name,
                    "summary": ai_response,
                    "patterns_detected": 6,
                    "confidence": 0.85,
                    "generated_at": datetime.now().isoformat()
                }
            except Exception as e:
                print(f"Ollama ìƒì„± ì˜¤ë¥˜: {e}")
        
        # í´ë°±: ê·œì¹™ ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸
        return generate_fallback_insights(analysis_results)
        
    except Exception as e:
        print(f"ì¸ì‚¬ì´íŠ¸ ìƒì„± ì˜¤ë¥˜: {e}")
        return generate_fallback_insights(analysis_results)

def generate_fallback_insights(analysis_results: List[Dict]) -> Dict[str, Any]:
    """í´ë°± ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
    file_types = {}
    successful_count = 0
    
    for result in analysis_results:
        if result.get('success', False):
            successful_count += 1
            file_type = result.get('type', 'unknown')
            file_types[file_type] = file_types.get(file_type, 0) + 1
    
    insights = [
        f"ğŸ“Š ì´ {len(analysis_results)}ê°œ íŒŒì¼ ì¤‘ {successful_count}ê°œ ì„±ê³µì  ë¶„ì„",
        f"ğŸ¯ ê°€ì¥ ë§ì€ íŒŒì¼ íƒ€ì…: {max(file_types.items(), key=lambda x: x[1])[0] if file_types else 'ì—†ìŒ'}",
        f"âš¡ ë¶„ì„ ì„±ê³µë¥ : {successful_count/len(analysis_results)*100:.1f}%",
        "ğŸ”® í–¥í›„ ë¶„ì„ íš¨ìœ¨ì„±ì„ ìœ„í•´ ë™ì¼í•œ íƒ€ì…ì˜ íŒŒì¼ë“¤ì„ ë°°ì¹˜ë¡œ ì²˜ë¦¬í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤."
    ]
    
    return {
        "model_used": "fallback_rules",
        "summary": "\n".join(insights),
        "patterns_detected": 4,
        "confidence": 0.75,
        "generated_at": datetime.now().isoformat()
    }

async def cleanup_temp_file(file_path: Path):
    """ì„ì‹œ íŒŒì¼ ì •ë¦¬"""
    try:
        if file_path.exists():
            file_path.unlink()
            print(f"ğŸ—‘ï¸ ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ: {file_path.name}")
    except Exception as e:
        print(f"ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì˜¤ë¥˜: {e}")

@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ì‹œ ì´ˆê¸°í™”"""
    print("ğŸš€ SOLOMOND AI ì•ˆì •ì  ë°±ì—”ë“œ ì„œë²„ ì‹œì‘")
    print(f"ğŸ’¾ ì„ì‹œ ë””ë ‰í† ë¦¬: {TEMP_DIR}")
    print(f"ğŸ“Š ê²°ê³¼ ë””ë ‰í† ë¦¬: {RESULTS_DIR}")
    
    # ë©”ëª¨ë¦¬ ìƒíƒœ ì¶œë ¥
    memory_info = MemoryOptimizer.get_memory_info()
    print(f"ğŸ§  í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_info['rss_mb']:.1f}MB ({memory_info['percent']:.1f}%)")

@app.on_event("shutdown")
async def shutdown_event():
    """ì„œë²„ ì¢…ë£Œì‹œ ì •ë¦¬"""
    print("ğŸ›‘ SOLOMOND AI ë°±ì—”ë“œ ì„œë²„ ì¢…ë£Œ")
    
    # ì„ì‹œ íŒŒì¼ë“¤ ì •ë¦¬
    try:
        for temp_file in TEMP_DIR.glob("*"):
            temp_file.unlink()
        print("ğŸ—‘ï¸ ì„ì‹œ íŒŒì¼ ì „ì²´ ì •ë¦¬ ì™„ë£Œ")
    except:
        pass

if __name__ == "__main__":
    print("ğŸ¯ SOLOMOND AI - ì•ˆì •ì  ë°±ì—”ë“œ ì‹œìŠ¤í…œ ì‹œì‘")
    print("ğŸ“‹ íŠ¹ì§•:")
    print("  âœ… Streamlit ì™„ì „ ëŒ€ì²´")  
    print("  âœ… 3GB+ íŒŒì¼ ì•ˆì „ ì²˜ë¦¬")
    print("  âœ… ë©”ëª¨ë¦¬ ì„¸ê·¸ë©˜í…Œì´ì…˜ ë°©ì§€")
    print("  âœ… Ollama AI 5ê°œ ëª¨ë¸ ì§€ì›")
    print("  âœ… ë“€ì–¼ ë¸Œë ˆì¸ ì‹œìŠ¤í…œ í†µí•©")
    print()
    print("ğŸŒ ì„œë²„ ì£¼ì†Œ: http://localhost:8080")
    print("ğŸ“± í”„ë¡ íŠ¸ì—”ë“œ: SOLOMOND_AI_STABLE_SYSTEM.html")
    
    uvicorn.run(
        "stable_backend:app",
        host="0.0.0.0",
        port=8080,
        reload=False,  # ì•ˆì •ì„±ì„ ìœ„í•´ reload ë¹„í™œì„±í™”
        workers=1,     # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ë‹¨ì¼ ì›Œì»¤
        log_level="info"
    )