#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ§  AI ì¸ì‚¬ì´íŠ¸ ì—”ì§„ - íŒ¨í„´ ì¸ì‹ ë° ë¯¸ë˜ ì „ë§ ì‹œìŠ¤í…œ
AI Insights Engine for SOLOMOND AI Dual Brain System

í•µì‹¬ ê¸°ëŠ¥:
1. ë¶„ì„ íŒ¨í„´ íƒì§€ (ì‹œê°„, ì£¼ì œ, ì„±ê³µë¥  ë“±)
2. íŠ¸ë Œë“œ ì˜ˆì¸¡ ë° ì¸ì‚¬ì´íŠ¸ ìƒì„±
3. ê°œì¸í™”ëœ ì¶”ì²œ ì‹œìŠ¤í…œ
4. ë¯¸ë˜ ê³„íš ì œì•ˆ
"""

import json
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
import streamlit as st
from collections import Counter, defaultdict
import re
from dataclasses import dataclass

@dataclass
class InsightPattern:
    """ì¸ì‚¬ì´íŠ¸ íŒ¨í„´ ë°ì´í„° êµ¬ì¡°"""
    pattern_type: str
    confidence: float
    description: str
    evidence: List[str]
    recommendation: str

class AIInsightsEngine:
    """AI ì¸ì‚¬ì´íŠ¸ ì—”ì§„ - ë“€ì–¼ ë¸Œë ˆì¸ì˜ í•µì‹¬"""
    
    def __init__(self):
        self.history_dir = Path("analysis_history")
        self.insights_cache = {}
        self.load_analysis_data()
    
    def load_analysis_data(self) -> List[Dict]:
        """ëª¨ë“  ë¶„ì„ ë°ì´í„° ë¡œë“œ"""
        self.all_analyses = []
        
        if not self.history_dir.exists():
            return []
        
        # ë©”íƒ€ë°ì´í„° ë¡œë“œ
        metadata_file = self.history_dir / "analysis_metadata.json"
        if metadata_file.exists():
            try:
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                    
                # ê° ë¶„ì„ì˜ ìƒì„¸ ë°ì´í„° ë¡œë“œ
                for analysis_meta in metadata.get("analyses", []):
                    analysis_file = self.history_dir / f"{analysis_meta['id']}_analysis.json"
                    if analysis_file.exists():
                        with open(analysis_file, 'r', encoding='utf-8') as f:
                            full_analysis = json.load(f)
                            self.all_analyses.append(full_analysis)
                            
            except Exception as e:
                st.warning(f"âš ï¸ ë¶„ì„ ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return self.all_analyses
    
    def detect_temporal_patterns(self) -> List[InsightPattern]:
        """ì‹œê°„ ê¸°ë°˜ íŒ¨í„´ íƒì§€"""
        patterns = []
        
        # ìµœì í™”: ë°ì´í„° ë¶€ì¡± ì‹œì—ë„ ê¸°ë³¸ íŒ¨í„´ íƒì§€ (ì„ê³„ê°’ ì¡°ì •)
        if len(self.all_analyses) < 1:
            return patterns
        
        # ì‹œê°„ë³„ ë¶„ì„ í™œë™ íŒ¨í„´
        hours = []
        weekdays = []
        success_by_hour = defaultdict(list)
        
        for analysis in self.all_analyses:
            try:
                timestamp = datetime.fromisoformat(analysis["timestamp"])
                hour = timestamp.hour
                weekday = timestamp.strftime("%A")
                
                hours.append(hour)
                weekdays.append(weekday)
                
                success_rate = analysis["success_count"] / analysis["total_files"]
                success_by_hour[hour].append(success_rate)
                
            except Exception as e:
                continue
        
        # ìµœê³  í™œë™ ì‹œê°„ëŒ€ íƒì§€ (ìµœì í™”: ë‹¨ì¼ ë°ì´í„°ë„ íŒ¨í„´ìœ¼ë¡œ ì¸ì‹)
        if hours:
            most_active_hour = max(set(hours), key=hours.count)
            activity_count = hours.count(most_active_hour)
            
            if activity_count >= 1:  # ìµœì í™”: ì„ê³„ê°’ 2â†’1ë¡œ ë‚®ì¶¤
                confidence = min(activity_count / len(hours), 0.9)
                
                # í•´ë‹¹ ì‹œê°„ëŒ€ ì„±ê³µë¥  ê³„ì‚°
                avg_success = np.mean(success_by_hour[most_active_hour]) if success_by_hour[most_active_hour] else 0
                
                pattern = InsightPattern(
                    pattern_type="temporal_peak",
                    confidence=confidence,
                    description=f"ì£¼ë¡œ {most_active_hour}ì‹œê²½ì— ë¶„ì„ í™œë™ì´ ê°€ì¥ í™œë°œí•©ë‹ˆë‹¤ (ì„±ê³µë¥ : {avg_success*100:.1f}%)",
                    evidence=[f"{activity_count}/{len(hours)} ë¶„ì„ì´ {most_active_hour}ì‹œì— ìˆ˜í–‰ë¨"],
                    recommendation=f"ğŸ• {most_active_hour}ì‹œëŠ” ë‹¹ì‹ ì˜ ìµœê³  ì§‘ì¤‘ ì‹œê°„ëŒ€ì…ë‹ˆë‹¤. ì¤‘ìš”í•œ ë¶„ì„ì„ ì´ ì‹œê°„ì— ë°°ì¹˜í•˜ì„¸ìš”."
                )
                patterns.append(pattern)
        
        # ì£¼ê°„ íŒ¨í„´ íƒì§€
        if weekdays:
            most_active_day = max(set(weekdays), key=weekdays.count)
            day_count = weekdays.count(most_active_day)
            
            if day_count >= 2:
                confidence = min(day_count / len(weekdays), 0.85)
                
                pattern = InsightPattern(
                    pattern_type="weekly_pattern",
                    confidence=confidence,
                    description=f"{most_active_day}ì— ë¶„ì„ í™œë™ì´ ì§‘ì¤‘ë©ë‹ˆë‹¤",
                    evidence=[f"{day_count}/{len(weekdays)} ë¶„ì„ì´ {most_active_day}ì— ìˆ˜í–‰ë¨"],
                    recommendation=f"ğŸ“… {most_active_day}ëŠ” ë‹¹ì‹ ì˜ ì£¼ê°„ ë¶„ì„ ë°ì´ ì…ë‹ˆë‹¤. ì •ê¸° ìŠ¤ì¼€ì¤„ë¡œ í™œìš©í•˜ì„¸ìš”."
                )
                patterns.append(pattern)
        
        return patterns
    
    def detect_content_patterns(self) -> List[InsightPattern]:
        """ì½˜í…ì¸  ë° ì£¼ì œ íŒ¨í„´ íƒì§€"""
        patterns = []
        
        if len(self.all_analyses) < 2:
            return patterns
        
        # ì»¨í¼ëŸ°ìŠ¤ëª…ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = []
        industries = []
        success_by_industry = defaultdict(list)
        
        for analysis in self.all_analyses:
            try:
                pre_info = analysis.get("pre_info", {})
                conference_name = pre_info.get("conference_name", "").lower()
                industry = pre_info.get("industry_field", "").lower()
                
                # í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ ë°©ì‹)
                if conference_name:
                    words = re.findall(r'\b\w+\b', conference_name)
                    keywords.extend([w for w in words if len(w) > 2])
                
                if industry:
                    industries.append(industry)
                    success_rate = analysis["success_count"] / analysis["total_files"]
                    success_by_industry[industry].append(success_rate)
                    
            except Exception as e:
                continue
        
        # ì£¼ìš” ê´€ì‹¬ ë¶„ì•¼ íƒì§€
        if industries:
            top_industry = max(set(industries), key=industries.count)
            industry_count = industries.count(top_industry)
            
            if industry_count >= 2:
                confidence = min(industry_count / len(industries), 0.9)
                avg_success = np.mean(success_by_industry[top_industry])
                
                pattern = InsightPattern(
                    pattern_type="industry_focus",
                    confidence=confidence,
                    description=f"'{top_industry}' ë¶„ì•¼ì— ì§€ì†ì ì¸ ê´€ì‹¬ì„ ë³´ì…ë‹ˆë‹¤ (í‰ê·  ì„±ê³µë¥ : {avg_success*100:.1f}%)",
                    evidence=[f"{industry_count}/{len(industries)} ë¶„ì„ì´ {top_industry} ë¶„ì•¼"],
                    recommendation=f"ğŸ¯ {top_industry} ë¶„ì•¼ ì „ë¬¸ì„±ì´ ë†’ì•„ì§€ê³  ìˆìŠµë‹ˆë‹¤. ê´€ë ¨ ì‹¬í™” ë¶„ì„ì„ ê³„íší•´ë³´ì„¸ìš”."
                )
                patterns.append(pattern)
        
        # ìì£¼ ë“±ì¥í•˜ëŠ” í‚¤ì›Œë“œ íŒ¨í„´
        if keywords:
            keyword_freq = Counter(keywords)
            top_keywords = keyword_freq.most_common(3)
            
            if top_keywords and top_keywords[0][1] >= 2:
                top_keyword, freq = top_keywords[0]
                confidence = min(freq / len(self.all_analyses), 0.8)
                
                pattern = InsightPattern(
                    pattern_type="keyword_trend",
                    confidence=confidence,
                    description=f"'{top_keyword}' í‚¤ì›Œë“œê°€ ìì£¼ ë“±ì¥í•©ë‹ˆë‹¤ ({freq}íšŒ)",
                    evidence=[f"'{kw}': {count}íšŒ" for kw, count in top_keywords[:3]],
                    recommendation=f"ğŸ” '{top_keyword}' ê´€ë ¨ íŠ¸ë Œë“œë¥¼ ë” ê¹Šì´ ë¶„ì„í•´ë³´ì‹œëŠ” ê²ƒì´ ì¢‹ê² ìŠµë‹ˆë‹¤."
                )
                patterns.append(pattern)
        
        return patterns
    
    def detect_performance_patterns(self) -> List[InsightPattern]:
        """ì„±ëŠ¥ ë° í’ˆì§ˆ íŒ¨í„´ íƒì§€"""
        patterns = []
        
        if len(self.all_analyses) < 3:
            return patterns
        
        # ì„±ê³µë¥  íŠ¸ë Œë“œ ë¶„ì„
        success_rates = []
        file_counts = []
        timestamps = []
        
        for analysis in self.all_analyses:
            try:
                success_rate = analysis["success_count"] / analysis["total_files"]
                success_rates.append(success_rate)
                file_counts.append(analysis["total_files"])
                timestamps.append(datetime.fromisoformat(analysis["timestamp"]))
                
            except Exception as e:
                continue
        
        if len(success_rates) >= 3:
            # ì„±ê³µë¥  ê°œì„  íŠ¸ë Œë“œ
            recent_rates = success_rates[-3:]
            earlier_rates = success_rates[:-3] if len(success_rates) > 3 else success_rates[:3]
            
            recent_avg = np.mean(recent_rates)
            earlier_avg = np.mean(earlier_rates)
            
            if recent_avg > earlier_avg + 0.1:  # 10% ì´ìƒ ê°œì„ 
                improvement = (recent_avg - earlier_avg) * 100
                confidence = min(improvement / 50, 0.95)  # ìµœëŒ€ 95%
                
                pattern = InsightPattern(
                    pattern_type="performance_improvement",
                    confidence=confidence,
                    description=f"ë¶„ì„ ì„±ê³µë¥ ì´ {improvement:.1f}% í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤ ({earlier_avg*100:.1f}% â†’ {recent_avg*100:.1f}%)",
                    evidence=[f"ìµœê·¼ 3íšŒ í‰ê· : {recent_avg*100:.1f}%", f"ì´ì „ í‰ê· : {earlier_avg*100:.1f}%"],
                    recommendation="ğŸš€ ë¶„ì„ í’ˆì§ˆì´ ì§€ì†ì ìœ¼ë¡œ í–¥ìƒë˜ê³  ìˆìŠµë‹ˆë‹¤. í˜„ì¬ ì ‘ê·¼ ë°©ì‹ì„ ìœ ì§€í•˜ì„¸ìš”!"
                )
                patterns.append(pattern)
            
            # íŒŒì¼ ìˆ˜ì™€ ì„±ê³µë¥  ê´€ê³„ ë¶„ì„
            if len(file_counts) >= 5:
                correlation = np.corrcoef(file_counts, success_rates)[0, 1]
                
                if abs(correlation) > 0.5:
                    conf = min(abs(correlation), 0.9)
                    trend = "ë†’ì„ìˆ˜ë¡" if correlation > 0 else "ë‚®ì„ìˆ˜ë¡"
                    
                    pattern = InsightPattern(
                        pattern_type="volume_quality_relation",
                        confidence=conf,
                        description=f"íŒŒì¼ ìˆ˜ê°€ {trend} ì„±ê³µë¥ ì´ ë†’ì•„ì§€ëŠ” ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤ (ìƒê´€ê´€ê³„: {correlation:.2f})",
                        evidence=[f"íŒŒì¼ ìˆ˜ í‰ê· : {np.mean(file_counts):.1f}ê°œ", f"ì„±ê³µë¥  í‰ê· : {np.mean(success_rates)*100:.1f}%"],
                        recommendation="ğŸ“Š ìµœì ì˜ íŒŒì¼ ìˆ˜ë¥¼ ì°¾ì•„ ë¶„ì„ íš¨ìœ¨ì„±ì„ ë†’ì—¬ë³´ì„¸ìš”." if correlation < 0 else "ğŸ“ˆ ëŒ€ìš©ëŸ‰ ë¶„ì„ì—ì„œ ë” ì¢‹ì€ ê²°ê³¼ë¥¼ ì–»ê³  ìˆìŠµë‹ˆë‹¤."
                    )
                    patterns.append(pattern)
        
        return patterns
    
    def predict_future_trends(self) -> List[Dict[str, Any]]:
        """ë¯¸ë˜ íŠ¸ë Œë“œ ì˜ˆì¸¡"""
        predictions = []
        
        if len(self.all_analyses) < 4:
            return [{
                "type": "insufficient_data",
                "title": "ë°ì´í„° ì¶•ì  ì¤‘",
                "description": "ë” ë§ì€ ë¶„ì„ ë°ì´í„°ê°€ ì¶•ì ë˜ë©´ ì •í™•í•œ íŠ¸ë Œë“œ ì˜ˆì¸¡ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.",
                "confidence": 0.3,
                "timeline": "2-3íšŒ ë¶„ì„ í›„"
            }]
        
        # ì‹œê°„ ê¸°ë°˜ ì˜ˆì¸¡
        timestamps = []
        success_rates = []
        
        for analysis in self.all_analyses:
            try:
                timestamps.append(datetime.fromisoformat(analysis["timestamp"]))
                success_rates.append(analysis["success_count"] / analysis["total_files"])
            except:
                continue
        
        if len(timestamps) >= 4:
            # ë¶„ì„ ë¹ˆë„ ì˜ˆì¸¡
            time_diffs = [(timestamps[i] - timestamps[i-1]).days for i in range(1, len(timestamps))]
            avg_interval = np.mean(time_diffs)
            
            next_analysis_date = timestamps[-1] + timedelta(days=avg_interval)
            
            predictions.append({
                "type": "next_analysis",
                "title": "ë‹¤ìŒ ë¶„ì„ ì˜ˆìƒ ì‹œì ",
                "description": f"{next_analysis_date.strftime('%Yë…„ %mì›” %dì¼')} ê²½ì— ìƒˆë¡œìš´ ë¶„ì„ì„ ìˆ˜í–‰í•  ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤",
                "confidence": min(0.7, len(timestamps) / 10),
                "timeline": f"{avg_interval:.0f}ì¼ í›„",
                "suggestion": "ì •ê¸°ì ì¸ ë¶„ì„ ìŠ¤ì¼€ì¤„ì„ ìº˜ë¦°ë”ì— ë“±ë¡í•´ë³´ì„¸ìš”"
            })
            
            # ì„±ê³µë¥  íŠ¸ë Œë“œ ì˜ˆì¸¡
            if len(success_rates) >= 4:
                recent_trend = np.polyfit(range(len(success_rates)), success_rates, 1)[0]
                
                if abs(recent_trend) > 0.01:  # 1% ì´ìƒ ë³€í™”
                    direction = "ìƒìŠ¹" if recent_trend > 0 else "í•˜ë½"
                    future_rate = success_rates[-1] + recent_trend * 2  # 2íšŒ ë¶„ì„ í›„ ì˜ˆìƒ
                    
                    predictions.append({
                        "type": "success_trend",
                        "title": f"ì„±ê³µë¥  {direction} íŠ¸ë Œë“œ",
                        "description": f"í˜„ì¬ íŠ¸ë Œë“œê°€ ì§€ì†ë˜ë©´ ì„±ê³µë¥ ì´ {future_rate*100:.1f}%ê¹Œì§€ {direction}í•  ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤",
                        "confidence": min(abs(recent_trend) * 50, 0.8),
                        "timeline": "ë‹¤ìŒ 2íšŒ ë¶„ì„",
                        "suggestion": "í–¥ìƒ" if recent_trend > 0 else "ê°œì„ ì±…ì„ ê³ ë ¤í•´ë³´ì„¸ìš”"
                    })
        
        return predictions
    
    def generate_personalized_recommendations(self) -> List[Dict[str, Any]]:
        """ê°œì¸í™”ëœ ì¶”ì²œ ì‹œìŠ¤í…œ"""
        recommendations = []
        
        if len(self.all_analyses) == 0:
            return [{
                "category": "ì‹œì‘í•˜ê¸°",
                "priority": "ë†’ìŒ",
                "title": "ì²« ë²ˆì§¸ ì»¨í¼ëŸ°ìŠ¤ ë¶„ì„",
                "description": "AI ë“€ì–¼ ë¸Œë ˆì¸ì„ í™œì„±í™”í•˜ê¸° ìœ„í•´ ì²« ë¶„ì„ì„ ì‹œì‘í•´ë³´ì„¸ìš”",
                "action": "ëª¨ë“ˆ1ì—ì„œ íŒŒì¼ ì—…ë¡œë“œ í›„ ë¶„ì„ ì‹¤í–‰",
                "expected_benefit": "ê°œì¸í™”ëœ ì¸ì‚¬ì´íŠ¸ ì‹œì‘"
            }]
        
        # ë¶„ì„ ë¹ˆë„ ê¸°ë°˜ ì¶”ì²œ
        total_analyses = len(self.all_analyses)
        
        if total_analyses < 3:
            recommendations.append({
                "category": "ë°ì´í„° ì¶•ì ",
                "priority": "ë†’ìŒ",
                "title": "ë” ë§ì€ ë¶„ì„ ìˆ˜í–‰",
                "description": f"í˜„ì¬ {total_analyses}íšŒ ë¶„ì„ ì™„ë£Œ. 5íšŒ ì´ìƒ ë¶„ì„í•˜ë©´ ê³ ê¸‰ íŒ¨í„´ íƒì§€ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤",
                "action": "ë‹¤ì–‘í•œ ì»¨í¼ëŸ°ìŠ¤ë¥¼ ì •ê¸°ì ìœ¼ë¡œ ë¶„ì„",
                "expected_benefit": "íŒ¨í„´ ì¸ì‹ ì •í™•ë„ ëŒ€í­ í–¥ìƒ"
            })
        
        # ìµœê·¼ í™œë™ ê¸°ë°˜ ì¶”ì²œ
        if self.all_analyses:
            last_analysis = max(self.all_analyses, key=lambda x: x["timestamp"])
            last_time = datetime.fromisoformat(last_analysis["timestamp"])
            days_since = (datetime.now() - last_time).days
            
            if days_since > 7:
                recommendations.append({
                    "category": "í™œë™ ì¬ê°œ",
                    "priority": "ì¤‘ê°„",
                    "title": "ì •ê¸° ë¶„ì„ ì¬ê°œ",
                    "description": f"ë§ˆì§€ë§‰ ë¶„ì„ í›„ {days_since}ì¼ì´ ê²½ê³¼í–ˆìŠµë‹ˆë‹¤",
                    "action": "ìƒˆë¡œìš´ ì»¨í¼ëŸ°ìŠ¤ ë¶„ì„ ì‹œì‘",
                    "expected_benefit": "ì§€ì†ì ì¸ ì¸ì‚¬ì´íŠ¸ ì¶•ì "
                })
        
        # ì„±ê³µë¥  ê¸°ë°˜ ì¶”ì²œ
        success_rates = [a["success_count"] / a["total_files"] for a in self.all_analyses]
        avg_success = np.mean(success_rates)
        
        if avg_success < 0.8:
            recommendations.append({
                "category": "í’ˆì§ˆ ê°œì„ ",
                "priority": "ë†’ìŒ",
                "title": "ë¶„ì„ í’ˆì§ˆ í–¥ìƒ",
                "description": f"í‰ê·  ì„±ê³µë¥  {avg_success*100:.1f}%. íŒŒì¼ ì „ì²˜ë¦¬ë‚˜ í’ˆì§ˆ í™•ì¸ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤",
                "action": "íŒŒì¼ í˜•ì‹ê³¼ í’ˆì§ˆì„ ì‚¬ì „ì— í™•ì¸",
                "expected_benefit": "85% ì´ìƒ ì„±ê³µë¥  ë‹¬ì„±"
            })
        elif avg_success > 0.95:
            recommendations.append({
                "category": "ë„ì „",
                "priority": "ë‚®ìŒ", 
                "title": "ê³ ê¸‰ ë¶„ì„ ë„ì „",
                "description": f"ë›°ì–´ë‚œ ì„±ê³µë¥  {avg_success*100:.1f}%! ë” ë³µì¡í•œ ë¶„ì„ì— ë„ì „í•´ë³´ì„¸ìš”",
                "action": "ëŒ€ìš©ëŸ‰ íŒŒì¼ì´ë‚˜ ë‹¤ì–‘í•œ í˜•ì‹ ë¶„ì„ ì‹œë„",
                "expected_benefit": "ë¶„ì„ ì—­ëŸ‰ í™•ì¥"
            })
        
        # êµ¬ê¸€ ìº˜ë¦°ë” ì—°ë™ ì¶”ì²œ
        calendar_enabled = any("google_calendar_enabled" in str(a) for a in self.all_analyses)
        if total_analyses >= 3 and not calendar_enabled:
            recommendations.append({
                "category": "ì‹œìŠ¤í…œ í†µí•©",
                "priority": "ì¤‘ê°„",
                "title": "êµ¬ê¸€ ìº˜ë¦°ë” ì—°ë™ í™œì„±í™”",
                "description": "ë¶„ì„ ì´ë ¥ì„ ìº˜ë¦°ë”ì— ìë™ ì €ì¥í•˜ì—¬ ë” ì²´ê³„ì ì¸ ê´€ë¦¬ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤",
                "action": "google_calendar_connector.py ì‹¤í–‰ í›„ ì„¤ì •",
                "expected_benefit": "ìŠ¤ì¼€ì¤„ ìµœì í™” ë° íŒ¨í„´ ì‹œê°í™”"
            })
        
        return recommendations
    
    def generate_comprehensive_insights(self) -> Dict[str, Any]:
        """ì¢…í•© ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        temporal_patterns = self.detect_temporal_patterns()
        content_patterns = self.detect_content_patterns() 
        performance_patterns = self.detect_performance_patterns()
        predictions = self.predict_future_trends()
        recommendations = self.generate_personalized_recommendations()
        
        # ì¸ì‚¬ì´íŠ¸ ì ìˆ˜ ê³„ì‚°
        total_analyses = len(self.all_analyses)
        insight_maturity = min(total_analyses / 10, 1.0)  # 10íšŒ ë¶„ì„ì´ë©´ ì™„ì „ ì„±ìˆ™
        
        avg_success = np.mean([a["success_count"] / a["total_files"] for a in self.all_analyses]) if self.all_analyses else 0
        quality_score = avg_success
        
        pattern_count = len(temporal_patterns) + len(content_patterns) + len(performance_patterns)
        pattern_richness = min(pattern_count / 5, 1.0)  # 5ê°œ íŒ¨í„´ì´ë©´ í’ë¶€í•¨
        
        overall_score = (insight_maturity * 0.4 + quality_score * 0.3 + pattern_richness * 0.3) * 100
        
        return {
            "metadata": {
                "total_analyses": total_analyses,
                "analysis_period": self._get_analysis_period(),
                "last_update": datetime.now().isoformat(),
                "insight_maturity": insight_maturity,
                "overall_score": overall_score
            },
            "patterns": {
                "temporal": temporal_patterns,
                "content": content_patterns,
                "performance": performance_patterns
            },
            "predictions": predictions,
            "recommendations": recommendations,
            "summary": self._generate_summary(temporal_patterns, content_patterns, performance_patterns, predictions)
        }
    
    def _get_analysis_period(self) -> str:
        """ë¶„ì„ ê¸°ê°„ ê³„ì‚°"""
        if not self.all_analyses:
            return "ë¶„ì„ ì—†ìŒ"
        
        timestamps = [datetime.fromisoformat(a["timestamp"]) for a in self.all_analyses]
        start_date = min(timestamps)
        end_date = max(timestamps)
        
        period_days = (end_date - start_date).days
        
        if period_days == 0:
            return "1ì¼"
        elif period_days < 7:
            return f"{period_days}ì¼"
        elif period_days < 30:
            return f"{period_days // 7}ì£¼"
        else:
            return f"{period_days // 30}ê°œì›”"
    
    def _generate_summary(self, temporal, content, performance, predictions) -> str:
        """ì¸ì‚¬ì´íŠ¸ ìš”ì•½ ìƒì„±"""
        if not self.all_analyses:
            return "ì•„ì§ ë¶„ì„ ë°ì´í„°ê°€ ì—†ì–´ íŒ¨í„´ì„ íŒŒì•…í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì²« ë¶„ì„ì„ ì‹œì‘í•´ë³´ì„¸ìš”!"
        
        summary_parts = []
        
        # í™œë™ íŒ¨í„´
        if temporal:
            time_pattern = temporal[0]
            summary_parts.append(f"â° {time_pattern.description}")
        
        # ì£¼ì œ íŒ¨í„´  
        if content:
            content_pattern = content[0]
            summary_parts.append(f"ğŸ¯ {content_pattern.description}")
        
        # ì„±ëŠ¥ íŒ¨í„´
        if performance:
            perf_pattern = performance[0]
            summary_parts.append(f"ğŸ“ˆ {perf_pattern.description}")
        
        # ì˜ˆì¸¡
        if predictions and predictions[0]["type"] != "insufficient_data":
            pred = predictions[0]
            summary_parts.append(f"ğŸ”® {pred['description']}")
        
        if summary_parts:
            return " | ".join(summary_parts)
        else:
            return f"ì´ {len(self.all_analyses)}íšŒ ë¶„ì„ ì™„ë£Œ. ë” ë§ì€ ë°ì´í„°ê°€ ì¶•ì ë˜ë©´ ìƒì„¸í•œ íŒ¨í„´ì„ íƒì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."

def render_insights_dashboard():
    """ì¸ì‚¬ì´íŠ¸ ëŒ€ì‹œë³´ë“œ ë Œë”ë§"""
    st.title("ğŸ§  AI ì¸ì‚¬ì´íŠ¸ ëŒ€ì‹œë³´ë“œ")
    st.markdown("**ë“€ì–¼ ë¸Œë ˆì¸ì˜ í•µì‹¬ - íŒ¨í„´ ì¸ì‹ ë° ë¯¸ë˜ ì „ë§**")
    
    # ì¸ì‚¬ì´íŠ¸ ì—”ì§„ ì´ˆê¸°í™”
    engine = AIInsightsEngine()
    insights = engine.generate_comprehensive_insights()
    
    # ì „ì²´ ì ìˆ˜ ë° ìƒíƒœ
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("AI ë¸Œë ˆì¸ ì„±ìˆ™ë„", f"{insights['metadata']['insight_maturity']*100:.0f}%")
    
    with col2:
        st.metric("ì´ ë¶„ì„ ìˆ˜", insights['metadata']['total_analyses'])
    
    with col3:
        st.metric("ë¶„ì„ ê¸°ê°„", insights['metadata']['analysis_period'])
    
    with col4:
        st.metric("ì¢…í•© ì ìˆ˜", f"{insights['metadata']['overall_score']:.0f}/100")
    
    # ìš”ì•½
    st.subheader("ğŸ“‹ AI ì¸ì‚¬ì´íŠ¸ ìš”ì•½")
    st.info(insights['summary'])
    
    # íƒ­ìœ¼ë¡œ êµ¬ì„±
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ” íŒ¨í„´ ë¶„ì„", "ğŸ”® ë¯¸ë˜ ì˜ˆì¸¡", "ğŸ’¡ ê°œì¸í™” ì¶”ì²œ", "ğŸ“Š ìƒì„¸ ë°ì´í„°"])
    
    with tab1:
        st.header("ğŸ” ë°œê²¬ëœ íŒ¨í„´ë“¤")
        
        # ì‹œê°„ íŒ¨í„´
        if insights['patterns']['temporal']:
            st.subheader("â° ì‹œê°„ íŒ¨í„´")
            for pattern in insights['patterns']['temporal']:
                with st.expander(f"{'â­' * int(pattern.confidence * 5)} {pattern.description}"):
                    st.write(f"**ì‹ ë¢°ë„**: {pattern.confidence*100:.0f}%")
                    st.write(f"**ê·¼ê±°**: {', '.join(pattern.evidence)}")
                    st.success(f"ğŸ’¡ {pattern.recommendation}")
        
        # ì½˜í…ì¸  íŒ¨í„´
        if insights['patterns']['content']:
            st.subheader("ğŸ¯ ì½˜í…ì¸  íŒ¨í„´") 
            for pattern in insights['patterns']['content']:
                with st.expander(f"{'â­' * int(pattern.confidence * 5)} {pattern.description}"):
                    st.write(f"**ì‹ ë¢°ë„**: {pattern.confidence*100:.0f}%")
                    st.write(f"**ê·¼ê±°**: {', '.join(pattern.evidence)}")
                    st.success(f"ğŸ’¡ {pattern.recommendation}")
        
        # ì„±ëŠ¥ íŒ¨í„´
        if insights['patterns']['performance']:
            st.subheader("ğŸ“ˆ ì„±ëŠ¥ íŒ¨í„´")
            for pattern in insights['patterns']['performance']:
                with st.expander(f"{'â­' * int(pattern.confidence * 5)} {pattern.description}"):
                    st.write(f"**ì‹ ë¢°ë„**: {pattern.confidence*100:.0f}%")
                    st.write(f"**ê·¼ê±°**: {', '.join(pattern.evidence)}")
                    st.success(f"ğŸ’¡ {pattern.recommendation}")
    
    with tab2:
        st.header("ğŸ”® ë¯¸ë˜ íŠ¸ë Œë“œ ì˜ˆì¸¡")
        
        for prediction in insights['predictions']:
            confidence_stars = 'â­' * int(prediction['confidence'] * 5)
            
            with st.expander(f"{confidence_stars} {prediction['title']}"):
                st.write(prediction['description'])
                st.write(f"**ì‹ ë¢°ë„**: {prediction['confidence']*100:.0f}%")
                st.write(f"**ì˜ˆìƒ ì‹œê¸°**: {prediction['timeline']}")
                
                if 'suggestion' in prediction:
                    st.info(f"ğŸ’¡ ì œì•ˆ: {prediction['suggestion']}")
    
    with tab3:
        st.header("ğŸ’¡ ê°œì¸í™”ëœ ì¶”ì²œ")
        
        priority_order = {"ë†’ìŒ": 0, "ì¤‘ê°„": 1, "ë‚®ìŒ": 2}
        sorted_recommendations = sorted(insights['recommendations'], key=lambda x: priority_order.get(x['priority'], 3))
        
        for rec in sorted_recommendations:
            priority_color = {"ë†’ìŒ": "ğŸ”´", "ì¤‘ê°„": "ğŸŸ¡", "ë‚®ìŒ": "ğŸŸ¢"}.get(rec['priority'], "âšª")
            
            with st.expander(f"{priority_color} {rec['title']} ({rec['category']})"):
                st.write(rec['description'])
                st.write(f"**ì‹¤í–‰ ë°©ë²•**: {rec['action']}")
                st.success(f"ğŸ¯ ê¸°ëŒ€ íš¨ê³¼: {rec['expected_benefit']}")
    
    with tab4:
        st.header("ğŸ“Š ìƒì„¸ ë°ì´í„°")
        
        if engine.all_analyses:
            # ë¶„ì„ ë°ì´í„°í”„ë ˆì„ ìƒì„±
            df_data = []
            for analysis in engine.all_analyses:
                df_data.append({
                    "ë¶„ì„ì¼": datetime.fromisoformat(analysis["timestamp"]).strftime("%Y-%m-%d %H:%M"),
                    "ì»¨í¼ëŸ°ìŠ¤ëª…": analysis["pre_info"].get("conference_name", "Unknown"),
                    "ì—…ê³„": analysis["pre_info"].get("industry_field", "Unknown"),
                    "íŒŒì¼ìˆ˜": analysis["total_files"],
                    "ì„±ê³µìˆ˜": analysis["success_count"],
                    "ì„±ê³µë¥ ": f"{analysis['success_count']/analysis['total_files']*100:.1f}%",
                    "íŒŒì¼ìœ í˜•": ", ".join(analysis["file_types"])
                })
            
            df = pd.DataFrame(df_data)
            st.dataframe(df, use_container_width=True)
            
            # ì›ì‹œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ
            if st.button("ğŸ“¥ ì¸ì‚¬ì´íŠ¸ ë°ì´í„° ë‹¤ìš´ë¡œë“œ"):
                insights_json = json.dumps(insights, ensure_ascii=False, indent=2, default=str)
                st.download_button(
                    "ğŸ’¾ JSON íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
                    insights_json,
                    file_name=f"ai_insights_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                    mime="application/json"
                )
        else:
            st.info("ğŸ“Š ë¶„ì„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì»¨í¼ëŸ°ìŠ¤ ë¶„ì„ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”.")

if __name__ == "__main__":
    st.set_page_config(
        page_title="AI ì¸ì‚¬ì´íŠ¸ ì—”ì§„",
        page_icon="ğŸ§ ",
        layout="wide"
    )
    
    render_insights_dashboard()