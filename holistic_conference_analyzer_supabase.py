#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ—ƒï¸ í™€ë¦¬ìŠ¤í‹± ì»¨í¼ëŸ°ìŠ¤ ë¶„ì„ê¸° (Supabase ì§€ì›)
Holistic Conference Analyzer with Supabase Support
"""

import json
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
import streamlit as st
from dataclasses import dataclass, asdict
import re
from collections import defaultdict, Counter

# ë°ì´í„°ë² ì´ìŠ¤ ì–´ëŒ‘í„° ì„í¬íŠ¸
from database_adapter import DatabaseFactory, DatabaseInterface

try:
    from sentence_transformers import SentenceTransformer
    import spacy
    # ğŸ›¡ï¸ ì•ˆì „í•œ ëª¨ë¸ ë¡œë”© ì‹œìŠ¤í…œ
    from defensive_model_loader import safe_sentence_transformer_load, enable_defensive_mode
    enable_defensive_mode()
    ADVANCED_NLP_AVAILABLE = True
except ImportError:
    ADVANCED_NLP_AVAILABLE = False
    safe_sentence_transformer_load = None

@dataclass
class ConferenceFragment:
    """ì»¨í¼ëŸ°ìŠ¤ ì¡°ê°"""
    fragment_id: str
    file_source: str
    file_type: str
    timestamp: str
    speaker: Optional[str]
    content: str
    confidence: float
    keywords: List[str]
    embedding: Optional[np.ndarray] = None

@dataclass
class ConferenceEntity:
    """ì»¨í¼ëŸ°ìŠ¤ ê°œì²´"""
    entity_id: str
    entity_type: str  # person, topic, decision, action_item
    name: str
    mentions: List[str]  # fragment_ids
    importance_score: float
    relationships: List[str]  # ì—°ê´€ entity_ids

class HolisticConferenceAnalyzerSupabase:
    """Supabase ì§€ì› í™€ë¦¬ìŠ¤í‹± ì»¨í¼ëŸ°ìŠ¤ ë¶„ì„ê¸°"""
    
    def __init__(self, conference_name: str = "default", db_type: str = "auto"):
        self.conference_name = conference_name
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
        self.db: DatabaseInterface = DatabaseFactory.create_database(db_type, conference_name)
        
        # NLP ëª¨ë¸ ì´ˆê¸°í™”
        if ADVANCED_NLP_AVAILABLE:
            try:
                # ğŸ›¡ï¸ ì•ˆì „í•œ ëª¨ë¸ ë¡œë”©ìœ¼ë¡œ meta tensor ë¬¸ì œ ì™„ì „ ë°©ì§€
                self.embedder = safe_sentence_transformer_load('paraphrase-multilingual-MiniLM-L12-v2')
                self.use_advanced_nlp = True
            except Exception as e:
                st.warning(f"ê³ ê¸‰ NLP ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.use_advanced_nlp = False
        else:
            self.use_advanced_nlp = False
        
        # ë¶„ì„ ë°ì´í„°
        self.fragments: List[ConferenceFragment] = []
        self.entities: List[ConferenceEntity] = []
        self.topics: List[Dict[str, Any]] = []
    
    def check_database_connection(self) -> Dict[str, Any]:
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìƒíƒœ í™•ì¸"""
        db_type = type(self.db).__name__
        is_connected = self.db.is_connected()
        
        if is_connected:
            count = self.db.get_fragment_count(self.conference_name)
            return {
                "connected": True,
                "database_type": db_type,
                "fragment_count": count,
                "message": f"{db_type}ì—ì„œ {count}ê°œ ì¡°ê° ë°œê²¬"
            }
        else:
            return {
                "connected": False,
                "database_type": db_type,
                "fragment_count": 0,
                "message": f"{db_type} ì—°ê²° ì‹¤íŒ¨ ë˜ëŠ” ë°ì´í„° ì—†ìŒ"
            }
    
    def load_fragments_from_database(self) -> bool:
        """ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¡°ê° ë¡œë“œ"""
        try:
            fragment_data = self.db.get_fragments(self.conference_name)
            
            self.fragments = []
            for data in fragment_data:
                fragment = ConferenceFragment(
                    fragment_id=data['fragment_id'],
                    file_source=data['file_source'],
                    file_type=data['file_type'],
                    timestamp=data['timestamp'],
                    speaker=data['speaker'],
                    content=data['content'],
                    confidence=data['confidence'],
                    keywords=data['keywords'] if isinstance(data['keywords'], list) else []
                )
                self.fragments.append(fragment)
            
            return len(self.fragments) > 0
            
        except Exception as e:
            st.error(f"ì¡°ê° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def create_sample_data_if_empty(self) -> bool:
        """ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ìƒ˜í”Œ ë°ì´í„° ìƒì„±"""
        if self.db.get_fragment_count(self.conference_name) > 0:
            return True
        
        # í…Œì´ë¸” ìƒì„±
        self.db.create_fragments_table()
        
        # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
        sample_fragments = [
            {
                'fragment_id': f'{self.conference_name}_001',
                'file_source': 'presentation_intro.jpg',
                'file_type': 'image',
                'timestamp': datetime.now().isoformat(),
                'speaker': 'ê¹€ëŒ€í‘œ',
                'content': 'AI ê¸°ìˆ  ë™í–¥ ì»¨í¼ëŸ°ìŠ¤ì— ì˜¤ì‹  ì—¬ëŸ¬ë¶„ì„ í™˜ì˜í•©ë‹ˆë‹¤. ì˜¤ëŠ˜ì€ ì¸ê³µì§€ëŠ¥ì˜ ìµœì‹  ë°œì „ì‚¬í•­ê³¼ ë¹„ì¦ˆë‹ˆìŠ¤ ì ìš© ë°©ì•ˆì— ëŒ€í•´ ë…¼ì˜í•˜ê² ìŠµë‹ˆë‹¤.',
                'confidence': 0.92,
                'keywords': ["AI", "ê¸°ìˆ ", "ë™í–¥", "ì»¨í¼ëŸ°ìŠ¤", "ì¸ê³µì§€ëŠ¥", "ë¹„ì¦ˆë‹ˆìŠ¤"]
            },
            {
                'fragment_id': f'{self.conference_name}_002',
                'file_source': 'discussion_audio.m4a',
                'file_type': 'audio',
                'timestamp': datetime.now().isoformat(),
                'speaker': 'ë°•ì—°êµ¬ì›',
                'content': 'ChatGPTì™€ GPT-4ì˜ ë“±ì¥ìœ¼ë¡œ ìì—°ì–´ ì²˜ë¦¬ ë¶„ì•¼ê°€ í˜ì‹ ë˜ê³  ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ ëŒ€í™”í˜• AIì˜ ì„±ëŠ¥ì´ ë†€ë¼ìš¸ ì •ë„ë¡œ í–¥ìƒë˜ì—ˆì£ .',
                'confidence': 0.88,
                'keywords': ["ChatGPT", "GPT-4", "ìì—°ì–´", "ì²˜ë¦¬", "ëŒ€í™”í˜•", "AI", "ì„±ëŠ¥"]
            },
            {
                'fragment_id': f'{self.conference_name}_003',
                'file_source': 'technical_slide.png',
                'file_type': 'image',
                'timestamp': datetime.now().isoformat(),
                'speaker': 'ì´ê°œë°œì',
                'content': 'ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ê°œë°œì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ê²ƒì€ ë°ì´í„° í’ˆì§ˆì…ë‹ˆë‹¤. ì¢‹ì€ ë°ì´í„° ì—†ì´ëŠ” ì¢‹ì€ ëª¨ë¸ì„ ë§Œë“¤ ìˆ˜ ì—†ì–´ìš”.',
                'confidence': 0.85,
                'keywords': ["ë¨¸ì‹ ëŸ¬ë‹", "ëª¨ë¸", "ê°œë°œ", "ë°ì´í„°", "í’ˆì§ˆ"]
            },
            {
                'fragment_id': f'{self.conference_name}_004',
                'file_source': 'qa_session.wav',
                'file_type': 'audio',
                'timestamp': datetime.now().isoformat(),
                'speaker': 'ìµœì§ˆë¬¸ì',
                'content': 'ì‹¤ì œ ì„œë¹„ìŠ¤ì— AIë¥¼ ë„ì…í•  ë•Œ ì–´ë–¤ ì ë“¤ì„ ì£¼ì˜í•´ì•¼ í• ê¹Œìš”? íŠ¹íˆ í™•ì¥ì„±ê³¼ ë¹„ìš© ì¸¡ë©´ì—ì„œ ê³ ë ¤ì‚¬í•­ì´ ê¶ê¸ˆí•©ë‹ˆë‹¤.',
                'confidence': 0.90,
                'keywords': ["ì„œë¹„ìŠ¤", "AI", "ë„ì…", "í™•ì¥ì„±", "ë¹„ìš©", "ê³ ë ¤ì‚¬í•­"]
            },
            {
                'fragment_id': f'{self.conference_name}_005',
                'file_source': 'business_discussion.mp4',
                'file_type': 'video',
                'timestamp': datetime.now().isoformat(),
                'speaker': 'ì •ë§¤ë‹ˆì €',
                'content': 'AI ë„ì… ROI ë¶„ì„ ê²°ê³¼, ì´ˆê¸° ë¹„ìš©ì€ ë†’ì§€ë§Œ ì¥ê¸°ì ìœ¼ë¡œëŠ” 30% ì´ìƒì˜ íš¨ìœ¨ì„± ì¦ëŒ€ë¥¼ ê¸°ëŒ€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.',
                'confidence': 0.87,
                'keywords': ["AI", "ë„ì…", "ROI", "ë¶„ì„", "ë¹„ìš©", "íš¨ìœ¨ì„±"]
            }
        ]
        
        if self.db.insert_fragments_batch(sample_fragments):
            st.success(f"âœ… {len(sample_fragments)}ê°œ ìƒ˜í”Œ ì¡°ê° ìƒì„± ì™„ë£Œ")
            return True
        else:
            st.error("âŒ ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì‹¤íŒ¨")
            return False
    
    def analyze_conference_holistically(self) -> Dict[str, Any]:
        """í™€ë¦¬ìŠ¤í‹± ì»¨í¼ëŸ°ìŠ¤ ë¶„ì„ ì‹¤í–‰"""
        try:
            # 1. ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¡°ê° ë¡œë“œ
            if not self.load_fragments_from_database():
                # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ìƒ˜í”Œ ë°ì´í„° ìƒì„±
                if not self.create_sample_data_if_empty():
                    return {"error": "ë°ì´í„° ë¡œë“œ ë° ìƒì„± ì‹¤íŒ¨"}
                # ë‹¤ì‹œ ë¡œë“œ
                if not self.load_fragments_from_database():
                    return {"error": "ìƒ˜í”Œ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨"}
            
            # 2. ê°œì²´ ì¶”ì¶œ
            self._extract_entities()
            
            # 3. ì£¼ì œ í´ëŸ¬ìŠ¤í„°ë§
            self._perform_topic_clustering()
            
            # 4. ì „ì²´ í†µê³„ ê³„ì‚°
            total_fragments = len(self.fragments)
            total_entities = len(self.entities)
            total_topics = len(self.topics)
            
            # 5. ë°œí‘œìë³„ ë¶„ì„
            speaker_analysis = self._analyze_speakers()
            
            # 6. ì‹ ë¢°ë„ ê³„ì‚°
            avg_confidence = np.mean([f.confidence for f in self.fragments]) if self.fragments else 0
            
            # 7. ì‹œê°„ëŒ€ë³„ ë¶„ì„
            temporal_analysis = self._analyze_temporal_patterns()
            
            results = {
                "conference_name": self.conference_name,
                "analysis_timestamp": datetime.now().isoformat(),
                "database_type": type(self.db).__name__,
                "total_fragments": total_fragments,
                "total_entities": total_entities,
                "total_topics": total_topics,
                "average_confidence": avg_confidence,
                "speaker_analysis": speaker_analysis,
                "temporal_analysis": temporal_analysis,
                "topic_distribution": self.topics,
                "key_insights": self._generate_key_insights()
            }
            
            return results
            
        except Exception as e:
            return {"error": f"í™€ë¦¬ìŠ¤í‹± ë¶„ì„ ì‹¤íŒ¨: {e}"}
    
    def _extract_entities(self):
        """ê°œì²´ ì¶”ì¶œ"""
        # ë°œí‘œì ê°œì²´
        speakers = {}
        for fragment in self.fragments:
            if fragment.speaker:
                if fragment.speaker not in speakers:
                    speakers[fragment.speaker] = []
                speakers[fragment.speaker].append(fragment.fragment_id)
        
        # ë°œí‘œìë¥¼ ê°œì²´ë¡œ ë³€í™˜
        for speaker, mentions in speakers.items():
            entity = ConferenceEntity(
                entity_id=f"speaker_{speaker}",
                entity_type="person",
                name=speaker,
                mentions=mentions,
                importance_score=len(mentions) / len(self.fragments),
                relationships=[]
            )
            self.entities.append(entity)
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ì£¼ì œ ê°œì²´
        all_keywords = []
        for fragment in self.fragments:
            all_keywords.extend(fragment.keywords)
        
        keyword_freq = Counter(all_keywords)
        for keyword, freq in keyword_freq.most_common(10):
            if freq >= 2:  # 2ë²ˆ ì´ìƒ ì–¸ê¸‰ëœ í‚¤ì›Œë“œë§Œ
                mentions = []
                for fragment in self.fragments:
                    if keyword in fragment.keywords:
                        mentions.append(fragment.fragment_id)
                
                entity = ConferenceEntity(
                    entity_id=f"topic_{keyword}",
                    entity_type="topic",
                    name=keyword,
                    mentions=mentions,
                    importance_score=freq / len(self.fragments),
                    relationships=[]
                )
                self.entities.append(entity)
    
    def _perform_topic_clustering(self):
        """ì£¼ì œ í´ëŸ¬ìŠ¤í„°ë§"""
        # í‚¤ì›Œë“œ ê¸°ë°˜ ê°„ë‹¨í•œ í´ëŸ¬ìŠ¤í„°ë§
        all_keywords = []
        for fragment in self.fragments:
            all_keywords.extend(fragment.keywords)
        
        keyword_freq = Counter(all_keywords)
        
        for keyword, freq in keyword_freq.most_common(8):
            if freq >= 2:
                cluster_fragments = []
                for fragment in self.fragments:
                    if keyword in fragment.keywords:
                        cluster_fragments.append({
                            'fragment_id': fragment.fragment_id,
                            'content': fragment.content[:100] + "..." if len(fragment.content) > 100 else fragment.content,
                            'speaker': fragment.speaker,
                            'confidence': fragment.confidence
                        })
                
                topic = {
                    'topic_name': keyword,
                    'frequency': freq,
                    'importance': freq / len(self.fragments),
                    'fragments': cluster_fragments,
                    'related_speakers': list(set([f['speaker'] for f in cluster_fragments if f['speaker']]))
                }
                self.topics.append(topic)
    
    def _analyze_speakers(self) -> Dict[str, Any]:
        """ë°œí‘œì ë¶„ì„"""
        speaker_stats = {}
        
        for fragment in self.fragments:
            if fragment.speaker:
                if fragment.speaker not in speaker_stats:
                    speaker_stats[fragment.speaker] = {
                        'fragment_count': 0,
                        'total_confidence': 0,
                        'keywords': [],
                        'content_length': 0
                    }
                
                speaker_stats[fragment.speaker]['fragment_count'] += 1
                speaker_stats[fragment.speaker]['total_confidence'] += fragment.confidence
                speaker_stats[fragment.speaker]['keywords'].extend(fragment.keywords)
                speaker_stats[fragment.speaker]['content_length'] += len(fragment.content)
        
        # í†µê³„ ê³„ì‚°
        for speaker, stats in speaker_stats.items():
            stats['avg_confidence'] = stats['total_confidence'] / stats['fragment_count']
            stats['top_keywords'] = [kw for kw, _ in Counter(stats['keywords']).most_common(5)]
            stats['avg_content_length'] = stats['content_length'] / stats['fragment_count']
            stats['engagement_score'] = stats['fragment_count'] / len(self.fragments)
        
        return speaker_stats
    
    def _analyze_temporal_patterns(self) -> Dict[str, Any]:
        """ì‹œê°„ëŒ€ë³„ íŒ¨í„´ ë¶„ì„"""
        if not self.fragments:
            return {}
        
        # ë‹¨ìˆœ ìˆœì„œ ê¸°ë°˜ ë¶„ì„ (ì‹¤ì œ íƒ€ì„ìŠ¤íƒ¬í”„ íŒŒì‹± ëŒ€ì‹ )
        total_fragments = len(self.fragments)
        
        # ì‹œì‘, ì¤‘ê°„, ë êµ¬ê°„ ë¶„ì„
        start_section = self.fragments[:total_fragments//3] if total_fragments >= 3 else self.fragments[:1]
        middle_section = self.fragments[total_fragments//3:2*total_fragments//3] if total_fragments >= 3 else []
        end_section = self.fragments[2*total_fragments//3:] if total_fragments >= 3 else []
        
        def analyze_section(fragments, section_name):
            if not fragments:
                return None
            
            keywords = []
            speakers = []
            avg_confidence = 0
            
            for f in fragments:
                keywords.extend(f.keywords)
                if f.speaker:
                    speakers.append(f.speaker)
                avg_confidence += f.confidence
            
            return {
                'section': section_name,
                'fragment_count': len(fragments),
                'top_keywords': [kw for kw, _ in Counter(keywords).most_common(3)],
                'active_speakers': list(set(speakers)),
                'avg_confidence': avg_confidence / len(fragments) if fragments else 0
            }
        
        return {
            'total_duration_fragments': total_fragments,
            'start_section': analyze_section(start_section, 'ì‹œì‘'),
            'middle_section': analyze_section(middle_section, 'ì¤‘ê°„'),
            'end_section': analyze_section(end_section, 'ë')
        }
    
    def _generate_key_insights(self) -> List[str]:
        """í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        insights = []
        
        if not self.fragments:
            return ["ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."]
        
        # ê°€ì¥ í™œë°œí•œ ë°œí‘œì
        speaker_counts = Counter([f.speaker for f in self.fragments if f.speaker])
        if speaker_counts:
            most_active = speaker_counts.most_common(1)[0]
            insights.append(f"ê°€ì¥ í™œë°œí•œ ë°œí‘œìëŠ” '{most_active[0]}'ë¡œ {most_active[1]}ë²ˆ ë°œì–¸í–ˆìŠµë‹ˆë‹¤.")
        
        # ì£¼ìš” ì£¼ì œ
        if self.topics:
            top_topic = max(self.topics, key=lambda t: t['importance'])
            insights.append(f"í•µì‹¬ ì£¼ì œëŠ” '{top_topic['topic_name']}'ë¡œ ì „ì²´ì˜ {top_topic['importance']:.1%}ë¥¼ ì°¨ì§€í•©ë‹ˆë‹¤.")
        
        # ì „ì²´ ì‹ ë¢°ë„
        avg_confidence = np.mean([f.confidence for f in self.fragments])
        insights.append(f"ì „ì²´ ë¶„ì„ ì‹ ë¢°ë„ëŠ” {avg_confidence:.1%}ë¡œ {'ë†’ì€' if avg_confidence > 0.8 else 'ë³´í†µ' if avg_confidence > 0.6 else 'ë‚®ì€'} ìˆ˜ì¤€ì…ë‹ˆë‹¤.")
        
        # ë‹¤ì–‘ì„± ë¶„ì„
        unique_speakers = len(set([f.speaker for f in self.fragments if f.speaker]))
        if unique_speakers > 1:
            insights.append(f"{unique_speakers}ëª…ì˜ ë‹¤ì–‘í•œ ì°¸ì—¬ìê°€ ê· í˜•ìˆê²Œ ì°¸ì—¬í–ˆìŠµë‹ˆë‹¤.")
        
        return insights

# Streamlit UI
def main():
    st.title("ğŸ—ƒï¸ í™€ë¦¬ìŠ¤í‹± ì»¨í¼ëŸ°ìŠ¤ ë¶„ì„ê¸° (Supabase ì§€ì›)")
    st.markdown("**ë©€í‹°ëª¨ë‹¬ ë°ì´í„°ë¥¼ ì™„ì „íˆ í†µí•©í•˜ì—¬ ì „ì²´ì ì¸ ì»¨í¼ëŸ°ìŠ¤ ì´í•´ë¥¼ ì œê³µí•©ë‹ˆë‹¤**")
    
    # ì‚¬ì´ë“œë°” ì„¤ì •
    st.sidebar.header("âš™ï¸ ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •")
    conference_name = st.sidebar.text_input("ì»¨í¼ëŸ°ìŠ¤ ì´ë¦„", "my_conference")
    db_type = st.sidebar.selectbox("ë°ì´í„°ë² ì´ìŠ¤ íƒ€ì…", ["auto", "sqlite", "supabase"])
    
    # ë¶„ì„ê¸° ì´ˆê¸°í™”
    analyzer = HolisticConferenceAnalyzerSupabase(conference_name, db_type)
    
    # ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ í™•ì¸
    db_status = analyzer.check_database_connection()
    
    if db_status["connected"]:
        st.success(f"âœ… {db_status['message']}")
    else:
        st.warning(f"âš ï¸ {db_status['message']}")
        if db_type == "supabase":
            st.info("ğŸ’¡ Supabase í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ê±°ë‚˜ SQLite ëª¨ë“œë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
    
    # í™€ë¦¬ìŠ¤í‹± ë¶„ì„ ì‹¤í–‰
    if st.button("ğŸ—ƒï¸ í™€ë¦¬ìŠ¤í‹± ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„ ì‹œì‘", type="primary"):
        with st.spinner("í™€ë¦¬ìŠ¤í‹± ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            results = analyzer.analyze_conference_holistically()
            
            if "error" in results:
                st.error(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {results['error']}")
                return
            
            st.success("âœ… í™€ë¦¬ìŠ¤í‹± ë¶„ì„ ì™„ë£Œ!")
            
            # ê²°ê³¼ í‘œì‹œ
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("ì´ ì¡°ê° ìˆ˜", results["total_fragments"])
            
            with col2:
                st.metric("ë°œê²¬ëœ ê°œì²´", results["total_entities"])
            
            with col3:
                st.metric("ì£¼ìš” ì£¼ì œ", results["total_topics"])
            
            with col4:
                st.metric("í‰ê·  ì‹ ë¢°ë„", f"{results['average_confidence']:.1%}")
            
            # ìƒì„¸ ê²°ê³¼ íƒ­
            tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“Š ì „ì²´ ê°œìš”", "ğŸ‘¥ ë°œí‘œì ë¶„ì„", "ğŸ“‹ ì£¼ì œ ë¶„ì„", "ğŸ• ì‹œê°„ëŒ€ ë¶„ì„"])
            
            with tab1:
                st.markdown("### ğŸ¯ í•µì‹¬ ì¸ì‚¬ì´íŠ¸")
                for insight in results["key_insights"]:
                    st.markdown(f"- {insight}")
                
                st.markdown("### ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ ì •ë³´")
                st.markdown(f"**ë°ì´í„°ë² ì´ìŠ¤**: {results['database_type']}")
                st.markdown(f"**ë¶„ì„ ì‹œê°„**: {results['analysis_timestamp']}")
            
            with tab2:
                st.markdown("### ğŸ‘¥ ë°œí‘œìë³„ ìƒì„¸ ë¶„ì„")
                speaker_analysis = results.get("speaker_analysis", {})
                
                for speaker, stats in speaker_analysis.items():
                    with st.expander(f"ğŸ¤ {speaker}"):
                        col1, col2 = st.columns(2)
                        
                        with col1:
                            st.metric("ë°œì–¸ íšŸìˆ˜", stats["fragment_count"])
                            st.metric("í‰ê·  ì‹ ë¢°ë„", f"{stats['avg_confidence']:.1%}")
                        
                        with col2:
                            st.metric("ì°¸ì—¬ë„", f"{stats['engagement_score']:.1%}")
                            st.metric("í‰ê·  ë°œì–¸ ê¸¸ì´", f"{stats['avg_content_length']:.0f}ì")
                        
                        st.markdown("**ì£¼ìš” í‚¤ì›Œë“œ**")
                        st.write(", ".join(stats["top_keywords"]))
            
            with tab3:
                st.markdown("### ğŸ“‹ ì£¼ì œë³„ ìƒì„¸ ë¶„ì„")
                topics = results.get("topic_distribution", [])
                
                for topic in topics:
                    with st.expander(f"ğŸ“Œ {topic['topic_name']} (ì¤‘ìš”ë„: {topic['importance']:.1%})"):
                        st.markdown(f"**ì–¸ê¸‰ ë¹ˆë„**: {topic['frequency']}íšŒ")
                        st.markdown(f"**ê´€ë ¨ ë°œí‘œì**: {', '.join(topic['related_speakers'])}")
                        
                        st.markdown("**ê´€ë ¨ ë‚´ìš©**")
                        for fragment in topic['fragments'][:3]:  # ìƒìœ„ 3ê°œë§Œ í‘œì‹œ
                            st.markdown(f"- *{fragment['speaker']}*: {fragment['content']}")
            
            with tab4:
                st.markdown("### ğŸ• ì‹œê°„ëŒ€ë³„ íŒ¨í„´ ë¶„ì„")
                temporal = results.get("temporal_analysis", {})
                
                if temporal:
                    sections = [temporal.get('start_section'), temporal.get('middle_section'), temporal.get('end_section')]
                    
                    for section in sections:
                        if section:
                            st.markdown(f"#### {section['section']} êµ¬ê°„")
                            
                            col1, col2 = st.columns(2)
                            with col1:
                                st.metric("ì¡°ê° ìˆ˜", section['fragment_count'])
                                st.metric("í‰ê·  ì‹ ë¢°ë„", f"{section['avg_confidence']:.1%}")
                            
                            with col2:
                                st.markdown("**ì£¼ìš” í‚¤ì›Œë“œ**")
                                st.write(", ".join(section['top_keywords']))
                                st.markdown("**í™œë°œí•œ ë°œí‘œì**")
                                st.write(", ".join(section['active_speakers']))
            
            # ìƒì„¸ ì •ë³´
            with st.expander("ğŸ“Š ì „ì²´ ë¶„ì„ ê²°ê³¼ (JSON)"):
                st.json(results)

if __name__ == "__main__":
    main()