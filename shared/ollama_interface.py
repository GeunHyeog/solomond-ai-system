#!/usr/bin/env python3
"""
ğŸ¤– ê³µí†µ Ollama AI ì¸í„°í˜ì´ìŠ¤
ëª¨ë“  ì†”ë¡œëª¬ë“œ AI ëª¨ë“ˆì—ì„œ ê³µí†µìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” Ollama í†µí•© ì‹œìŠ¤í…œ

ì£¼ìš” ê¸°ëŠ¥:
- ëª¨ë¸ë³„ ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ ê´€ë¦¬
- ì£¼ì–¼ë¦¬ ì—…ê³„ íŠ¹í™” ì»¨í…ìŠ¤íŠ¸
- ë‹¤êµ­ì–´ ì§€ì› (í•œêµ­ì–´/ì˜ì–´)
- ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì§€ì›
- ì—ëŸ¬ ì²˜ë¦¬ ë° í´ë°± ì‹œìŠ¤í…œ
"""

import requests
import json
import time
from typing import Dict, List, Optional, Generator
from datetime import datetime
import logging

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

class OllamaInterface:
    """Ollama AI ëª¨ë¸ í†µí•© ì¸í„°í˜ì´ìŠ¤"""
    
    def __init__(self, base_url: str = "http://localhost:11434"):
        self.base_url = base_url
        self.available_models = self._get_available_models()
        
        # ëª¨ë“ˆë³„ ì¶”ì²œ ëª¨ë¸ ì„¤ì • (ì†ë„ ìš°ì„  ìµœì í™”)
        self.module_models = {
            "conference_analysis": "qwen2.5:7b",      # ë¹ ë¥¸ íšŒì˜ ë¶„ì„
            "web_crawler": "qwen2.5:7b",             # ë‰´ìŠ¤ ìš”ì•½ ë° ë²ˆì—­
            "gemstone_analysis": "qwen2.5:7b",       # ë³´ì„ ì „ë¬¸ ë¶„ì„
            "cad_conversion": "llama3.2:3b",         # ë¹ ë¥¸ ì´ë¯¸ì§€ ì²˜ë¦¬
            "general": "qwen2.5:7b"                  # ì¼ë°˜ ìš©ë„
        }
        
        # ì£¼ì–¼ë¦¬ ì—…ê³„ íŠ¹í™” í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        self.jewelry_prompts = {
            "news_summary": """
ë‹¹ì‹ ì€ ì£¼ì–¼ë¦¬ ì—…ê³„ ì „ë¬¸ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ë‹¤ìŒ ë‰´ìŠ¤ë¥¼ ë¹ ë¥´ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”.

ìš”ì•½ í¬ë§·:
- ğŸ“° í•µì‹¬ ë‚´ìš©: (2-3ë¬¸ì¥)
- ğŸ’ ì—…ê³„ ì˜í–¥: (ìƒ/ì¤‘/í•˜)
- ğŸ”‘ í‚¤ì›Œë“œ: (ë³´ì„ëª…, ë¸Œëœë“œ, íŠ¸ë Œë“œ)
- â­ ì¤‘ìš”ë„: (ìƒ/ì¤‘/í•˜)

ê¸°ì‚¬:
{content}

**ìš”ì•½:**""",
            
            "conference_analysis": """
ë‹¹ì‹ ì€ ì£¼ì–¼ë¦¬ ì—…ê³„ ë©€í‹°ëª¨ë‹¬ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì´ë¯¸ì§€, ìŒì„±, í…ìŠ¤íŠ¸ë¥¼ í†µí•©ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ê¹Šì´ ìˆëŠ” ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

ğŸ¯ **ë©€í‹°ëª¨ë‹¬ í†µí•© ë¶„ì„ í”„ë ˆì„ì›Œí¬**:

1. **ì‹œê·¸ë„ ì¶”ì¶œ ë° ë…¸ì´ì¦ˆ í•„í„°ë§**:
   - í•µì‹¬ ì‹œê·¸ë„: ë°˜ë³µ ì–¸ê¸‰, ê°•ì¡° í‘œí˜„, ì‹œê°ì  ê°•ì¡°ì 
   - ë…¸ì´ì¦ˆ ì œê±°: ì¼íšŒì„± ì–¸ê¸‰, ë¶€ìˆ˜ì  ë‚´ìš©, ê¸°ìˆ ì  ë¬¸ì œ
   - í¬ë¡œìŠ¤ëª¨ë‹¬ ê²€ì¦: ì—¬ëŸ¬ ëª¨ë‹¬ì—ì„œ í™•ì¸ë˜ëŠ” ë‚´ìš© ìš°ì„ ìˆœìœ„

2. **ìƒí™©ì  ì»¨í…ìŠ¤íŠ¸ ì´í•´**:
   - ë°œí™”ìë³„ í•µì‹¬ ë©”ì‹œì§€ êµ¬ë¶„
   - ì‹œê°„ íë¦„ì— ë”°ë¥¸ ì£¼ì œ ë³€í™” ì¶”ì 
   - ì‹œê°ì  ìë£Œì™€ ìŒì„± ë‚´ìš©ì˜ ì¼ì¹˜ë„ ë¶„ì„

3. **ì—…ê³„ ì „ë¬¸ì„± ì ìš©**:
   - ì£¼ì–¼ë¦¬ ì‹œì¥ íŠ¸ë Œë“œ ì»¨í…ìŠ¤íŠ¸
   - ê¸°ìˆ  í˜ì‹  ë° ì§€ì†ê°€ëŠ¥ì„± ì´ìŠˆ
   - ì†Œë¹„ì í–‰ë™ ë³€í™” íŒ¨í„´

4. **ì‹¤í–‰ ê°€ëŠ¥í•œ ì¸ì‚¬ì´íŠ¸ ìƒì„±**:
   - ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ ì•¡ì…˜ ì•„ì´í…œ
   - ì¤‘ê¸° ì „ëµì  ê³ ë ¤ì‚¬í•­
   - ì¥ê¸° ì—…ê³„ ë³€í™” ì „ë§

**ë¶„ì„ ëŒ€ìƒ ë‚´ìš©**:
{content}

**í†µí•© ë¶„ì„ ê²°ê³¼** (ê° ì„¹ì…˜ 3-4ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ):

ğŸ” **í•µì‹¬ ì‹œê·¸ë„ ë¶„ì„**:
- 

ğŸ’¡ **ìƒí™©ì  ì¸ì‚¬ì´íŠ¸**:
- 

ğŸ¯ **ì—…ê³„ í•¨ì˜**:
- 

ğŸš€ **ì‹¤í–‰ ë°©ì•ˆ**:
- 

âš ï¸ **ì£¼ì˜ ì‚¬í•­**:
- 

ğŸ“ˆ **ë¯¸ë˜ ì „ë§**:
- """,
            
            "gemstone_identification": """
ë‹¹ì‹ ì€ ë³´ì„í•™ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ë³´ì„ ì´ë¯¸ì§€ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚°ì§€ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.

ë¶„ì„ ê¸°ì¤€:
- ìƒ‰ìƒ íŠ¹ì„±
- ë‚´í¬ë¬¼ íŒ¨í„´
- ê´‘í•™ì  íŠ¹ì„±
- ì§€ì§ˆí•™ì  ë°°ê²½

ì´ë¯¸ì§€ ì •ë³´:
{content}

ì‚°ì§€ ë¶„ì„:""",
            
            "translation": """
ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ {target_language}ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”. ì£¼ì–¼ë¦¬ ì—…ê³„ ì „ë¬¸ ìš©ì–´ëŠ” ì •í™•íˆ ë²ˆì—­í•˜ê³ , ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„ì„ ì‚¬ìš©í•´ì£¼ì„¸ìš”.

ì›ë¬¸:
{content}

ë²ˆì—­:"""
        }
    
    def _get_available_models(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ Ollama ëª¨ë¸ ëª©ë¡ ì¡°íšŒ"""
        try:
            response = requests.get(f"{self.base_url}/api/tags")
            if response.status_code == 200:
                models = response.json()
                return [model["name"] for model in models.get("models", [])]
            return []
        except Exception as e:
            logger.error(f"ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def select_model(self, module_type: str = "general") -> str:
        """ëª¨ë“ˆ íƒ€ì…ì— ë”°ë¥¸ ìµœì  ëª¨ë¸ ì„ íƒ"""
        recommended = self.module_models.get(module_type, "qwen2.5:7b")
        
        # ì¶”ì²œ ëª¨ë¸ì´ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸
        if recommended in self.available_models:
            return recommended
        
        # í´ë°± ìˆœì„œ
        fallback_models = ["qwen2.5:7b", "llama3.2:3b", "gemma2:2b"]
        for model in fallback_models:
            if model in self.available_models:
                return model
        
        # ë§ˆì§€ë§‰ í´ë°±: ì²« ë²ˆì§¸ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸
        return self.available_models[0] if self.available_models else "llama3.2:3b"
    
    def generate_response(
        self, 
        prompt: str, 
        model: Optional[str] = None,
        stream: bool = False,
        max_tokens: int = 2000,
        temperature: float = 0.7
    ) -> str:
        """í…ìŠ¤íŠ¸ ìƒì„± (ê¸°ë³¸ ëª¨ë“œ)"""
        
        if model is None:
            model = self.select_model()
        
        payload = {
            "model": model,
            "prompt": prompt,
            "stream": stream,
            "options": {
                "num_predict": max_tokens,
                "temperature": temperature,
                "top_k": 40,
                "top_p": 0.9
            }
        }
        
        try:
            response = requests.post(
                f"{self.base_url}/api/generate",
                json=payload,
                stream=stream,
                timeout=300  # 5ë¶„ìœ¼ë¡œ ì—°ì¥
            )
            
            if stream:
                return self._handle_stream_response(response)
            else:
                if response.status_code == 200:
                    result = response.json()
                    return result.get("response", "")
                else:
                    logger.error(f"API ì˜¤ë¥˜: {response.status_code}")
                    return f"ì˜¤ë¥˜ ë°œìƒ: {response.status_code}"
                    
        except Exception as e:
            logger.error(f"ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            return f"AI ëª¨ë¸ ì˜¤ë¥˜: {str(e)}"
    
    def _handle_stream_response(self, response) -> Generator[str, None, None]:
        """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬"""
        try:
            for line in response.iter_lines():
                if line:
                    data = json.loads(line.decode('utf-8'))
                    if 'response' in data:
                        yield data['response']
                    if data.get('done', False):
                        break
        except Exception as e:
            logger.error(f"ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            yield f"ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {str(e)}"
    
    def chat_complete(
        self,
        messages: List[Dict[str, str]],
        model: Optional[str] = None,
        stream: bool = False
    ) -> str:
        """ì±„íŒ… ì™„ì„± (ëŒ€í™”í˜•)"""
        
        if model is None:
            model = self.select_model()
        
        payload = {
            "model": model,
            "messages": messages,
            "stream": stream
        }
        
        try:
            response = requests.post(
                f"{self.base_url}/api/chat",
                json=payload,
                stream=stream,
                timeout=300  # 5ë¶„ìœ¼ë¡œ ì—°ì¥
            )
            
            if stream:
                return self._handle_stream_response(response)
            else:
                if response.status_code == 200:
                    result = response.json()
                    return result.get("message", {}).get("content", "")
                else:
                    return f"ì±„íŒ… ì˜¤ë¥˜: {response.status_code}"
                    
        except Exception as e:
            logger.error(f"ì±„íŒ… ì™„ì„± ì‹¤íŒ¨: {e}")
            return f"ì±„íŒ… ì˜¤ë¥˜: {str(e)}"
    
    def summarize_jewelry_news(self, content: str, model: Optional[str] = None) -> str:
        """ì£¼ì–¼ë¦¬ ë‰´ìŠ¤ ìš”ì•½ íŠ¹í™” í•¨ìˆ˜"""
        if model is None:
            model = self.select_model("web_crawler")
        
        prompt = self.jewelry_prompts["news_summary"].format(content=content)
        return self.generate_response(prompt, model=model, temperature=0.3)
    
    def analyze_conference(self, content: str, model: Optional[str] = None) -> str:
        """ì»¨í¼ëŸ°ìŠ¤ ë¶„ì„ íŠ¹í™” í•¨ìˆ˜ (ì²­í¬ ë¶„í•  ì²˜ë¦¬)"""
        if model is None:
            model = self.select_model("conference_analysis")
        
        # ê¸´ í…ìŠ¤íŠ¸ëŠ” ì²­í¬ë¡œ ë¶„í• 
        if len(content) > 8000:  # 8000ì ì´ìƒì´ë©´ ë¶„í• 
            return self._analyze_long_content(content, model)
        
        prompt = self.jewelry_prompts["conference_analysis"].format(content=content)
        return self.generate_response(prompt, model=model, temperature=0.5)
    
    def _analyze_long_content(self, content: str, model: str) -> str:
        """ê¸´ í…ìŠ¤íŠ¸ ì²­í¬ ë¶„í•  ë¶„ì„"""
        try:
            # í…ìŠ¤íŠ¸ë¥¼ 6000ì ë‹¨ìœ„ë¡œ ë¶„í• 
            chunk_size = 6000
            chunks = [content[i:i+chunk_size] for i in range(0, len(content), chunk_size)]
            
            chunk_analyses = []
            
            for i, chunk in enumerate(chunks):
                prompt = f"""
ë‹¤ìŒì€ ì»¨í¼ëŸ°ìŠ¤ ë‚´ìš©ì˜ {i+1}/{len(chunks)} ë¶€ë¶„ì…ë‹ˆë‹¤. ì´ ë¶€ë¶„ì„ ê°„ë‹¨íˆ ìš”ì•½í•´ì£¼ì„¸ìš”:

{chunk}

ê°„ë‹¨ ìš”ì•½:"""
                
                chunk_result = self.generate_response(
                    prompt, 
                    model=model, 
                    temperature=0.3,
                    max_tokens=500  # ì²­í¬ë‹¹ ì§§ê²Œ
                )
                chunk_analyses.append(chunk_result)
            
            # ì „ì²´ ì¢…í•© ë¶„ì„
            combined_summary = "\n\n".join(chunk_analyses)
            final_prompt = self.jewelry_prompts["conference_analysis"].format(
                content=f"ë‹¤ìŒì€ ì»¨í¼ëŸ°ìŠ¤ ê° ë¶€ë¶„ë³„ ìš”ì•½ì…ë‹ˆë‹¤:\n{combined_summary}"
            )
            
            return self.generate_response(final_prompt, model=model, temperature=0.5)
            
        except Exception as e:
            logger.error(f"ê¸´ ì»¨í…ì¸  ë¶„ì„ ì‹¤íŒ¨: {e}")
            return f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
    
    def identify_gemstone(self, image_info: str, model: Optional[str] = None) -> str:
        """ë³´ì„ ì‹ë³„ íŠ¹í™” í•¨ìˆ˜"""
        if model is None:
            model = self.select_model("gemstone_analysis")
        
        prompt = self.jewelry_prompts["gemstone_identification"].format(content=image_info)
        return self.generate_response(prompt, model=model, temperature=0.2)
    
    def translate_text(
        self, 
        content: str, 
        target_language: str = "í•œêµ­ì–´",
        model: Optional[str] = None
    ) -> str:
        """ë²ˆì—­ íŠ¹í™” í•¨ìˆ˜"""
        if model is None:
            model = self.select_model("general")
        
        prompt = self.jewelry_prompts["translation"].format(
            content=content, 
            target_language=target_language
        )
        return self.generate_response(prompt, model=model, temperature=0.1)
    
    def get_model_info(self) -> Dict[str, any]:
        """í˜„ì¬ ì„¤ì • ë° ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return {
            "available_models": self.available_models,
            "module_models": self.module_models,
            "base_url": self.base_url,
            "status": "ì—°ê²°ë¨" if self.available_models else "ì—°ê²° ì‹¤íŒ¨"
        }
    
    def health_check(self) -> bool:
        """Ollama ì„œë²„ ìƒíƒœ í™•ì¸"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=10)
            return response.status_code == 200
        except:
            return False
    
    def get_server_status(self) -> Dict[str, any]:
        """Ollama ì„œë²„ ìƒì„¸ ìƒíƒœ ì •ë³´"""
        try:
            # ê¸°ë³¸ ì—°ê²° í™•ì¸
            health = self.health_check()
            
            if not health:
                return {
                    "status": "disconnected",
                    "message": "Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "suggestion": "í„°ë¯¸ë„ì—ì„œ 'ollama serve' ëª…ë ¹ì„ ì‹¤í–‰í•˜ì„¸ìš”."
                }
            
            # ëª¨ë¸ ëª©ë¡ í™•ì¸
            models = self._get_available_models()
            
            return {
                "status": "connected",
                "available_models": models,
                "recommended_model": self.select_model("conference_analysis"),
                "message": f"ì •ìƒ ì—°ê²°ë¨ ({len(models)}ê°œ ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥)"
            }
            
        except Exception as e:
            return {
                "status": "error",
                "message": f"ìƒíƒœ í™•ì¸ ì˜¤ë¥˜: {str(e)}",
                "suggestion": "Ollama ì„œë²„ë¥¼ ì¬ì‹œì‘í•´ë³´ì„¸ìš”."
            }

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
global_ollama = OllamaInterface()

# í¸ì˜ í•¨ìˆ˜ë“¤
def quick_summary(text: str, model: str = None) -> str:
    """ë¹ ë¥¸ ìš”ì•½"""
    return global_ollama.summarize_jewelry_news(text, model)

def quick_translate(text: str, target: str = "í•œêµ­ì–´") -> str:
    """ë¹ ë¥¸ ë²ˆì—­"""
    return global_ollama.translate_text(text, target)

def quick_analysis(text: str, model: str = None) -> str:
    """ë¹ ë¥¸ ë¶„ì„"""
    return global_ollama.analyze_conference(text, model)

def get_ollama_status() -> Dict[str, any]:
    """Ollama ìƒíƒœ ì •ë³´"""
    return global_ollama.get_server_status()

def get_ollama_models() -> Dict[str, any]:
    """Ollama ëª¨ë¸ ì •ë³´"""
    return global_ollama.get_model_info()

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    ollama = OllamaInterface()
    
    print("Ollama Interface Test")
    print("=" * 50)
    
    # ìƒíƒœ í™•ì¸
    print(f"Connection Status: {ollama.health_check()}")
    print(f"Available Models: {ollama.available_models}")
    
    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
    if ollama.health_check():
        test_prompt = "ì£¼ì–¼ë¦¬ ì—…ê³„ì˜ ë¯¸ë˜ íŠ¸ë Œë“œì— ëŒ€í•´ ê°„ë‹¨íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”."
        print(f"\nTest Prompt: {test_prompt}")
        print("=" * 50)
        
        response = ollama.generate_response(test_prompt)
        print(f"Response: {response[:200]}...")
    else:
        print("Cannot connect to Ollama server.")