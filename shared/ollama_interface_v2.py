#!/usr/bin/env python3
"""
ğŸ† SOLOMOND AI v3.0 - ê³ ë„í™”ëœ Ollama ì¸í„°í˜ì´ìŠ¤
ìµœì í™”ëœ 5ê°œ ëª¨ë¸ì„ í™œìš©í•œ ì°¨ì„¸ëŒ€ AI í†µí•© ì‹œìŠ¤í…œ

ëª¨ë¸ ë¼ì¸ì—…:
- gpt-oss:20b (OpenAI o3-mini ìˆ˜ì¤€) - ì°¨ì„¸ëŒ€ ì¶”ë¡  ì—”ì§„
- gemma3:27b (êµ¬ê¸€ ìµœê°•) - ë³µì¡í•œ ë¶„ì„
- qwen3:8b (ìµœì‹  ì¶”ë¡ ) - ë©”ì¸ ì›Œí¬í˜¸ìŠ¤  
- qwen2.5:7b (ì•ˆì •ì„±) - ë°±ì—… ì‹œìŠ¤í…œ
- gemma3:4b (ê²½ëŸ‰) - ë¹ ë¥¸ ì‘ì—…
"""

import requests
import json
import time
from typing import Dict, List, Optional, Generator, Tuple
from datetime import datetime
import logging
from enum import Enum
import asyncio

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

class ModelTier(Enum):
    """ëª¨ë¸ ì„±ëŠ¥ ë“±ê¸‰"""
    ULTIMATE = "ultimate"      # GPT-OSS-20B
    PREMIUM = "premium"        # Gemma3-27B  
    STANDARD = "standard"      # Qwen3-8B
    STABLE = "stable"         # Qwen2.5-7B
    FAST = "fast"             # Gemma3-4B

class TaskComplexity(Enum):
    """ì‘ì—… ë³µì¡ë„"""
    SIMPLE = "simple"         # ê°„ë‹¨í•œ ìš”ì•½, ë²ˆì—­
    MODERATE = "moderate"     # ì¼ë°˜ì ì¸ ë¶„ì„
    COMPLEX = "complex"       # ì‹¬í™” ì¶”ë¡ , ë³µì¡í•œ ë¶„ì„
    CRITICAL = "critical"     # ìµœê³  í’ˆì§ˆ ìš”êµ¬

class AdvancedOllamaInterface:
    """ì°¨ì„¸ëŒ€ Ollama í†µí•© ì¸í„°í˜ì´ìŠ¤"""
    
    def __init__(self, base_url: str = "http://localhost:11434"):
        self.base_url = base_url
        self.available_models = self._get_available_models()
        
        # ğŸ† ìµœì í™”ëœ ëª¨ë¸ ë¼ì¸ì—…
        self.model_lineup = {
            ModelTier.ULTIMATE: "gpt-oss:20b",     # OpenAI ë ˆë²¨ ì¶”ë¡ 
            ModelTier.PREMIUM: "gemma3:27b",       # êµ¬ê¸€ ìµœê°•
            ModelTier.STANDARD: "qwen3:8b",        # ë©”ì¸ ì›Œí¬í˜¸ìŠ¤
            ModelTier.STABLE: "qwen2.5:7b",        # ì•ˆì •ì„± ì¤‘ì‹œ
            ModelTier.FAST: "gemma3:4b"            # ë¹ ë¥¸ ì²˜ë¦¬
        }
        
        # ğŸ¯ ì‘ì—…ë³„ ëª¨ë¸ ìë™ ì„ íƒ ì „ëµ
        self.task_model_mapping = {
            # ì»¨í¼ëŸ°ìŠ¤ ë¶„ì„
            "conference_simple": ModelTier.FAST,      # ë¹ ë¥¸ ìš”ì•½
            "conference_standard": ModelTier.STANDARD, # ì¼ë°˜ ë¶„ì„
            "conference_deep": ModelTier.ULTIMATE,    # ì‹¬í™” ì¶”ë¡ 
            
            # ì›¹ í¬ë¡¤ë§ & ë‰´ìŠ¤
            "news_summary": ModelTier.FAST,           # ë¹ ë¥¸ ìš”ì•½
            "news_analysis": ModelTier.STANDARD,      # íŠ¸ë Œë“œ ë¶„ì„
            "news_insight": ModelTier.PREMIUM,        # ì‹¬í™” ì¸ì‚¬ì´íŠ¸
            
            # ë³´ì„ ë¶„ì„
            "gemstone_basic": ModelTier.STANDARD,     # ê¸°ë³¸ ì‹ë³„
            "gemstone_expert": ModelTier.PREMIUM,     # ì „ë¬¸ê°€ ë¶„ì„
            "gemstone_research": ModelTier.ULTIMATE,  # ì—°êµ¬ ìˆ˜ì¤€
            
            # 3D CAD
            "cad_simple": ModelTier.FAST,            # ë¹ ë¥¸ ë³€í™˜
            "cad_complex": ModelTier.STANDARD,       # ë³µì¡í•œ ì²˜ë¦¬
            
            # ì¼ë°˜ ì‘ì—…
            "translation": ModelTier.FAST,           # ë²ˆì—­
            "coding": ModelTier.STANDARD,            # ì½”ë”© ì§€ì›
            "reasoning": ModelTier.ULTIMATE,         # ë³µì¡í•œ ì¶”ë¡ 
            "creative": ModelTier.PREMIUM            # ì°½ì‘ ì‘ì—…
        }
        
        # ğŸ§  ê³ ë„í™”ëœ í”„ë¡¬í”„íŠ¸ ì‹œìŠ¤í…œ
        self.advanced_prompts = {
            "ultimate_analysis": """
ğŸ† ë‹¹ì‹ ì€ ì„¸ê³„ ìµœê³  ìˆ˜ì¤€ì˜ AI ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. OpenAI o3-mini ìˆ˜ì¤€ì˜ ì¶”ë¡  ëŠ¥ë ¥ìœ¼ë¡œ ë‹¤ìŒì„ ë¶„ì„í•´ì£¼ì„¸ìš”.

ğŸ¯ **ë¶„ì„ ëª©í‘œ**: {task_goal}

ğŸ“‹ **ë¶„ì„ í”„ë ˆì„ì›Œí¬**:
1. ğŸ” **ì‹¬ì¸µ ì´í•´**: ë‚´ìš©ì˜ í•µì‹¬ ì˜ë„ì™€ ìˆ¨ê²¨ì§„ íŒ¨í„´ íŒŒì•…
2. ğŸ’¡ **í†µì°° ë„ì¶œ**: í‘œë©´ì  ë¶„ì„ì„ ë„˜ì–´ì„  ë…ì°½ì  ì¸ì‚¬ì´íŠ¸
3. ğŸ¯ **ì‹¤í–‰ ë°©ì•ˆ**: êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì•¡ì…˜ í”Œëœ
4. ğŸ”® **ë¯¸ë˜ ì˜ˆì¸¡**: íŠ¸ë Œë“œ ì „ë§ ë° ì˜í–¥ ë¶„ì„

ğŸ“Š **ì…ë ¥ ë°ì´í„°**:
{content}

ğŸš€ **ìµœê³  í’ˆì§ˆ ë¶„ì„ ê²°ê³¼**:""",

            "premium_synthesis": """
ğŸ”¥ ë‹¹ì‹ ì€ êµ¬ê¸€ ìµœê°• AI ëª¨ë¸ ìˆ˜ì¤€ì˜ ì¢…í•© ë¶„ì„ê°€ì…ë‹ˆë‹¤. ë³µì¡í•œ ì •ë³´ë¥¼ ì™„ë²½íˆ í†µí•©í•´ì£¼ì„¸ìš”.

ğŸª **í†µí•© ë¶„ì„ ë¯¸ì…˜**: {task_goal}

âš¡ **ë¶„ì„ ì°¨ì›**:
â€¢ ğŸ“ˆ **ì •ëŸ‰ì  ë¶„ì„**: ë°ì´í„°, ìˆ˜ì¹˜, í†µê³„ì  íŒ¨í„´
â€¢ ğŸ“ **ì •ì„±ì  ë¶„ì„**: ì˜ë¯¸, ë§¥ë½, ê°ì •ì  ë‰˜ì•™ìŠ¤  
â€¢ ğŸŒ **ì‹œìŠ¤í…œì  ë¶„ì„**: ì „ì²´ì  êµ¬ì¡°ì™€ ìƒí˜¸ê´€ê³„
â€¢ âš¡ **ì‹œê°„ì  ë¶„ì„**: ê³¼ê±°-í˜„ì¬-ë¯¸ë˜ ì—°ê²°ê³ ë¦¬

ğŸ“š **ë¶„ì„ ëŒ€ìƒ**:
{content}

ğŸ’ **í†µí•© ë¶„ì„ ë³´ê³ ì„œ**:""",

            "standard_processing": """
âš¡ ë‹¹ì‹ ì€ Qwen3 ìµœì‹  ì¶”ë¡  ì—”ì§„ì…ë‹ˆë‹¤. ë¹ ë¥´ê³  ì •í™•í•˜ê²Œ ì²˜ë¦¬í•´ì£¼ì„¸ìš”.

ğŸ¯ **ì‘ì—…**: {task_goal}

ğŸ“‹ **ì²˜ë¦¬ ë°©ì‹**:
âœ… í•µì‹¬ í¬ì¸íŠ¸ ìœ„ì£¼ ë¶„ì„
âœ… ëª…í™•í•˜ê³  êµ¬ì¡°í™”ëœ ê²°ê³¼  
âœ… ì‹¤ìš©ì  ê´€ì  ìœ ì§€

ğŸ“„ **ì…ë ¥**:
{content}

ğŸš€ **ì²˜ë¦¬ ê²°ê³¼**:""",

            "fast_summary": """
ğŸš€ ë¹ ë¥¸ ì²˜ë¦¬ ëª¨ë“œì…ë‹ˆë‹¤. íš¨ìœ¨ì ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.

ëª©ì : {task_goal}
ë‚´ìš©: {content}

âš¡ **ê°„ê²°í•œ ê²°ê³¼**:"""
        }
    
    def _get_available_models(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ í™•ì¸"""
        try:
            response = requests.get(f"{self.base_url}/api/tags")
            if response.status_code == 200:
                models = response.json()
                return [model["name"] for model in models.get("models", [])]
            return []
        except Exception as e:
            logger.error(f"ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def smart_model_selection(
        self, 
        task_type: str, 
        content_length: int = 0,
        quality_priority: bool = False,
        speed_priority: bool = False
    ) -> Tuple[str, str]:
        """ğŸ§  ìŠ¤ë§ˆíŠ¸ ëª¨ë¸ ì„ íƒ ì‹œìŠ¤í…œ"""
        
        # 1ë‹¨ê³„: ê¸°ë³¸ ì‘ì—… íƒ€ì…ë³„ ëª¨ë¸ ì„ íƒ
        base_tier = self.task_model_mapping.get(task_type, ModelTier.STANDARD)
        
        # 2ë‹¨ê³„: ì»¨í…ì¸  ê¸¸ì´ ê³ ë ¤
        if content_length > 10000:  # ê¸´ í…ìŠ¤íŠ¸
            if base_tier == ModelTier.FAST:
                base_tier = ModelTier.STANDARD
        elif content_length > 50000:  # ë§¤ìš° ê¸´ í…ìŠ¤íŠ¸
            base_tier = ModelTier.PREMIUM
        
        # 3ë‹¨ê³„: ì‚¬ìš©ì ìš°ì„ ìˆœìœ„ ê³ ë ¤
        if quality_priority:
            if base_tier in [ModelTier.FAST, ModelTier.STANDARD]:
                base_tier = ModelTier.PREMIUM
            elif base_tier == ModelTier.STABLE:
                base_tier = ModelTier.ULTIMATE
        
        if speed_priority:
            if base_tier in [ModelTier.PREMIUM, ModelTier.ULTIMATE]:
                base_tier = ModelTier.STANDARD
        
        # 4ë‹¨ê³„: ì‹¤ì œ ëª¨ë¸ëª… ë°˜í™˜ + í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„ íƒ
        model_name = self.model_lineup[base_tier]
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„ íƒ
        if base_tier == ModelTier.ULTIMATE:
            prompt_template = "ultimate_analysis"
        elif base_tier == ModelTier.PREMIUM:
            prompt_template = "premium_synthesis"
        elif base_tier == ModelTier.STANDARD:
            prompt_template = "standard_processing"
        else:
            prompt_template = "fast_summary"
        
        # ëª¨ë¸ ì‚¬ìš© ë¶ˆê°€ì‹œ í´ë°±
        if model_name not in self.available_models:
            fallback_models = [
                self.model_lineup[ModelTier.STANDARD],
                self.model_lineup[ModelTier.STABLE], 
                self.model_lineup[ModelTier.FAST]
            ]
            for fallback in fallback_models:
                if fallback in self.available_models:
                    return fallback, "standard_processing"
            
            return self.available_models[0] if self.available_models else "qwen2.5:7b", "fast_summary"
        
        return model_name, prompt_template
    
    def advanced_generate(
        self,
        task_type: str,
        content: str,
        task_goal: str = "",
        quality_priority: bool = False,
        speed_priority: bool = False,
        stream: bool = False,
        max_tokens: int = 3000,
        temperature: float = 0.7
    ) -> str:
        """ğŸš€ ê³ ë„í™”ëœ ìƒì„± í•¨ìˆ˜"""
        
        # ìŠ¤ë§ˆíŠ¸ ëª¨ë¸ ì„ íƒ
        model, prompt_template = self.smart_model_selection(
            task_type=task_type,
            content_length=len(content),
            quality_priority=quality_priority,
            speed_priority=speed_priority
        )
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = self.advanced_prompts[prompt_template].format(
            task_goal=task_goal or f"{task_type} ì‘ì—…",
            content=content
        )
        
        # ëª¨ë¸ë³„ ìµœì í™”ëœ íŒŒë¼ë¯¸í„°
        model_params = self._get_optimized_params(model, task_type)
        
        payload = {
            "model": model,
            "prompt": prompt,
            "stream": stream,
            "options": {
                "num_predict": max_tokens,
                "temperature": model_params["temperature"],
                "top_k": model_params["top_k"],
                "top_p": model_params["top_p"]
            }
        }
        
        try:
            logger.info(f"ì‚¬ìš© ëª¨ë¸: {model} | ì‘ì—…: {task_type} | í…œí”Œë¦¿: {prompt_template}")
            
            response = requests.post(
                f"{self.base_url}/api/generate",
                json=payload,
                stream=stream,
                timeout=600  # GPT-OSSëŠ” ì‹œê°„ì´ ë” í•„ìš”
            )
            
            if stream:
                return self._handle_stream_response(response)
            else:
                if response.status_code == 200:
                    result = response.json()
                    return result.get("response", "")
                else:
                    logger.error(f"API ì˜¤ë¥˜: {response.status_code}")
                    return f"ì˜¤ë¥˜ ë°œìƒ: {response.status_code}"
                    
        except Exception as e:
            logger.error(f"ìƒì„± ì‹¤íŒ¨: {e}")
            return f"AI ëª¨ë¸ ì˜¤ë¥˜: {str(e)}"
    
    def _get_optimized_params(self, model: str, task_type: str) -> Dict:
        """ëª¨ë¸ë³„ ìµœì í™” íŒŒë¼ë¯¸í„°"""
        base_params = {
            "temperature": 0.7,
            "top_k": 40, 
            "top_p": 0.9
        }
        
        # GPT-OSS ìµœì í™”
        if "gpt-oss" in model:
            if "reasoning" in task_type or "deep" in task_type:
                base_params["temperature"] = 0.3  # ì¶”ë¡  ì‘ì—…ì€ ë‚®ì€ ì˜¨ë„
            else:
                base_params["temperature"] = 0.5
            base_params["top_k"] = 50
        
        # Gemma3 ìµœì í™”
        elif "gemma3" in model:
            base_params["temperature"] = 0.6
            base_params["top_k"] = 30
        
        # Qwen ìµœì í™”  
        elif "qwen" in model:
            base_params["temperature"] = 0.7
            base_params["top_k"] = 40
        
        return base_params
    
    def _handle_stream_response(self, response) -> Generator[str, None, None]:
        """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬"""
        try:
            for line in response.iter_lines():
                if line:
                    data = json.loads(line.decode('utf-8'))
                    if 'response' in data:
                        yield data['response']
                    if data.get('done', False):
                        break
        except Exception as e:
            logger.error(f"ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            yield f"ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {str(e)}"
    
    # ğŸ¯ íŠ¹í™” í•¨ìˆ˜ë“¤
    def ultimate_conference_analysis(self, content: str, context: str = "") -> str:
        """ğŸ† ê¶ê·¹ ì»¨í¼ëŸ°ìŠ¤ ë¶„ì„ (GPT-OSS í™œìš©)"""
        return self.advanced_generate(
            task_type="conference_deep",
            content=content,
            task_goal=f"ì£¼ì–¼ë¦¬ ì—…ê³„ ì»¨í¼ëŸ°ìŠ¤ ì‹¬ì¸µ ë¶„ì„{' - ' + context if context else ''}",
            quality_priority=True
        )
    
    def premium_market_insight(self, content: str) -> str:
        """ğŸ’ í”„ë¦¬ë¯¸ì—„ ì‹œì¥ ì¸ì‚¬ì´íŠ¸ (Gemma3-27B í™œìš©)"""
        return self.advanced_generate(
            task_type="news_insight", 
            content=content,
            task_goal="ì£¼ì–¼ë¦¬ ì‹œì¥ ë™í–¥ ë° ë¯¸ë˜ ì „ë§ ë¶„ì„",
            quality_priority=True
        )
    
    def fast_news_summary(self, content: str) -> str:
        """âš¡ ë¹ ë¥¸ ë‰´ìŠ¤ ìš”ì•½ (Gemma3-4B í™œìš©)"""
        return self.advanced_generate(
            task_type="news_summary",
            content=content,
            task_goal="ì£¼ì–¼ë¦¬ ë‰´ìŠ¤ í•µì‹¬ ìš”ì•½",
            speed_priority=True
        )
    
    def expert_gemstone_analysis(self, image_info: str, analysis_level: str = "standard") -> str:
        """ğŸ’ ì „ë¬¸ê°€ê¸‰ ë³´ì„ ë¶„ì„"""
        task_mapping = {
            "basic": "gemstone_basic",
            "standard": "gemstone_expert", 
            "research": "gemstone_research"
        }
        
        return self.advanced_generate(
            task_type=task_mapping.get(analysis_level, "gemstone_expert"),
            content=image_info,
            task_goal=f"ë³´ì„ ì „ë¬¸ ë¶„ì„ ({analysis_level} ìˆ˜ì¤€)",
            quality_priority=(analysis_level in ["standard", "research"])
        )
    
    def intelligent_translation(self, content: str, target_lang: str = "í•œêµ­ì–´") -> str:
        """ğŸŒ ì§€ëŠ¥í˜• ë²ˆì—­"""
        return self.advanced_generate(
            task_type="translation",
            content=content,
            task_goal=f"ì£¼ì–¼ë¦¬ ì „ë¬¸ ìš©ì–´ ì •í™• ë²ˆì—­ ({target_lang})",
            speed_priority=True
        )
    
    def get_system_status(self) -> Dict[str, any]:
        """ğŸ” ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸"""
        status = {
            "timestamp": datetime.now().isoformat(),
            "server_status": self.health_check(),
            "available_models": self.available_models,
            "model_lineup": {},
            "recommendations": {}
        }
        
        # ëª¨ë¸ ë¼ì¸ì—… ìƒíƒœ
        for tier, model_name in self.model_lineup.items():
            is_available = model_name in self.available_models
            status["model_lineup"][tier.value] = {
                "model": model_name,
                "available": is_available,
                "status": "OK Available" if is_available else "NEED Install"
            }
        
        # ì¶”ì²œ ì‚¬í•­
        missing_models = [model for model in self.model_lineup.values() 
                         if model not in self.available_models]
        
        if missing_models:
            status["recommendations"]["install"] = f"Install recommended: {', '.join(missing_models)}"
        else:
            status["recommendations"]["status"] = "All models are perfectly configured!"
        
        return status
    
    def health_check(self) -> bool:
        """ì„œë²„ ìƒíƒœ í™•ì¸"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=10)
            return response.status_code == 200
        except:
            return False
    
    def benchmark_models(self, test_prompt: str = "ì£¼ì–¼ë¦¬ ì—…ê³„ì˜ ë¯¸ë˜ íŠ¸ë Œë“œë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”.") -> Dict[str, any]:
        """ğŸ ëª¨ë¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        results = {}
        
        for tier, model_name in self.model_lineup.items():
            if model_name not in self.available_models:
                results[tier.value] = {"status": "ëª¨ë¸ ì—†ìŒ", "time": 0, "response": ""}
                continue
            
            start_time = time.time()
            try:
                response = self.advanced_generate(
                    task_type="standard_processing",
                    content=test_prompt,
                    task_goal="ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸"
                )
                end_time = time.time()
                
                results[tier.value] = {
                    "model": model_name,
                    "time": round(end_time - start_time, 2),
                    "response_length": len(response),
                    "status": "ì„±ê³µ",
                    "preview": response[:100] + "..." if len(response) > 100 else response
                }
            except Exception as e:
                results[tier.value] = {
                    "model": model_name,
                    "time": 0,
                    "status": f"ì˜¤ë¥˜: {str(e)}",
                    "response": ""
                }
        
        return results

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ 
advanced_ollama = AdvancedOllamaInterface()

# ğŸš€ í¸ì˜ í•¨ìˆ˜ë“¤
def ultimate_analysis(content: str, context: str = "") -> str:
    """ê¶ê·¹ ë¶„ì„ (GPT-OSS)"""
    return advanced_ollama.ultimate_conference_analysis(content, context)

def premium_insight(content: str) -> str:
    """í”„ë¦¬ë¯¸ì—„ ì¸ì‚¬ì´íŠ¸ (Gemma3-27B)"""
    return advanced_ollama.premium_market_insight(content)

def quick_summary(content: str) -> str:
    """ë¹ ë¥¸ ìš”ì•½ (Gemma3-4B)"""
    return advanced_ollama.fast_news_summary(content)

def smart_translate(content: str, target: str = "í•œêµ­ì–´") -> str:
    """ìŠ¤ë§ˆíŠ¸ ë²ˆì—­"""
    return advanced_ollama.intelligent_translation(content, target)

def expert_gemstone(image_info: str, level: str = "standard") -> str:
    """ì „ë¬¸ê°€ ë³´ì„ ë¶„ì„"""
    return advanced_ollama.expert_gemstone_analysis(image_info, level)

def get_system_info() -> Dict[str, any]:
    """ì‹œìŠ¤í…œ ì •ë³´"""
    return advanced_ollama.get_system_status()

def benchmark_all() -> Dict[str, any]:
    """ì „ì²´ ë²¤ì¹˜ë§ˆí¬"""
    return advanced_ollama.benchmark_models()

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ë° ë²¤ì¹˜ë§ˆí¬
    print("SOLOMOND AI v3.0 - Advanced Ollama Interface")
    print("=" * 60)
    
    # ì‹œìŠ¤í…œ ìƒíƒœ
    status = get_system_info()
    print(f"ì„œë²„ ìƒíƒœ: {status['server_status']}")
    print(f"ì‚¬ìš© ê°€ëŠ¥ ëª¨ë¸: {len(status['available_models'])}ê°œ")
    
    for tier, info in status['model_lineup'].items():
        print(f"{tier.upper()}: {info['model']} - {info['status']}")
    
    # ê°„ë‹¨í•œ ë²¤ì¹˜ë§ˆí¬ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
    if status['server_status']:
        print("\nRunning benchmark tests...")
        results = benchmark_all()
        
        for tier, result in results.items():
            if result['status'] == 'ì„±ê³µ':
                print(f"{tier.upper()}: {result['time']}s | {result['response_length']} chars")
            else:
                print(f"{tier.upper()}: {result['status']}")