#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ§  SOLOMOND AI Serena ì½”ë”© ì—ì´ì „íŠ¸ í†µí•© ì‹œìŠ¤í…œ
Serena-Powered Code Intelligence for SOLOMOND AI

í•µì‹¬ ê¸°ëŠ¥:
1. Symbol-level ì½”ë“œ ë¶„ì„ ë° í¸ì§‘
2. í† í° íš¨ìœ¨ì ì¸ ì½”ë“œ ë¸”ë¡ ì¶”ì¶œ
3. í”„ë¡œì íŠ¸ë³„ ì»¨í…ìŠ¤íŠ¸ ë©”ëª¨ë¦¬
4. ì •ë°€í•œ ì½”ë“œ ìˆ˜ì • ë° ìµœì í™”
5. SOLOMOND AI íŠ¹í™” ì„±ëŠ¥ ìµœì í™”

Serenaì˜ í•µì‹¬ ëŠ¥ë ¥ì„ í‘œì¤€ Python ë„êµ¬ë¡œ êµ¬í˜„
"""

import os
import ast
import re
import json
import hashlib
import time
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Set
from dataclasses import dataclass, asdict
from datetime import datetime
import logging
import traceback
from collections import defaultdict, Counter

# í”„ë¡œì íŠ¸ êµ¬ì¡° ë¶„ì„ì„ ìœ„í•œ ì„í¬íŠ¸
import tokenize
import io
from tokenize import TokenInfo

@dataclass
class CodeSymbol:
    """ì½”ë“œ ì‹¬ë³¼ ì •ë³´"""
    name: str
    type: str  # function, class, variable, import
    file_path: str
    line_start: int
    line_end: int
    indentation: int
    docstring: Optional[str] = None
    arguments: Optional[List[str]] = None
    dependencies: Optional[List[str]] = None
    complexity: int = 0
    last_modified: Optional[str] = None

@dataclass
class CodeBlock:
    """í† í° íš¨ìœ¨ì ì¸ ì½”ë“œ ë¸”ë¡"""
    content: str
    symbols: List[CodeSymbol]
    file_path: str
    block_id: str
    token_count: int
    importance_score: float
    context_relevance: float

@dataclass
class ProjectMemory:
    """í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸ ë©”ëª¨ë¦¬"""
    project_name: str
    symbols: Dict[str, CodeSymbol]
    dependencies: Dict[str, List[str]]
    patterns: Dict[str, int]
    performance_bottlenecks: List[str]
    last_updated: str
    code_quality_metrics: Dict[str, float]

class SerenaCodeAnalyzer:
    """Serena ìŠ¤íƒ€ì¼ ì½”ë“œ ë¶„ì„ê¸°"""
    
    def __init__(self, project_root: str = None):
        self.project_root = Path(project_root) if project_root else Path.cwd()
        self.memory_file = self.project_root / ".solomond_serena_memory.json"
        self.project_memory = self._load_or_create_memory()
        self.logger = self._setup_logger()
        
        # SOLOMOND AI íŠ¹í™” íŒ¨í„´
        self.solomond_patterns = {
            'threadpool_issues': r'ThreadPoolExecutor.*(?:submit|map)',
            'memory_leaks': r'(?:torch\.cuda|np\.array|PIL\.Image).*(?:not.*del|no.*cleanup)',
            'streamlit_performance': r'st\.(?:cache|session_state).*(?:heavy|large|slow)',
            'ollama_integration': r'ollama.*(?:generate|chat|pull)',
            'multimodal_processing': r'(?:audio|image|video).*(?:process|analyze|extract)',
            'gpu_memory_issues': r'(?:cuda|gpu).*(?:memory|allocation|oom)',
            'error_handling_missing': r'try:.*except.*pass',
            'inefficient_loops': r'for.*in.*(?:range\(len|enumerate).*\[.*\]'
        }
        
    def _setup_logger(self) -> logging.Logger:
        """ë¡œê±° ì„¤ì •"""
        logger = logging.getLogger("SerenaAgent")
        logger.setLevel(logging.INFO)
        handler = logging.FileHandler(self.project_root / "serena_agent.log")
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        return logger
        
    def _load_or_create_memory(self) -> ProjectMemory:
        """í”„ë¡œì íŠ¸ ë©”ëª¨ë¦¬ ë¡œë“œ ë˜ëŠ” ìƒì„±"""
        if self.memory_file.exists():
            try:
                with open(self.memory_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    return ProjectMemory(**data)
            except Exception as e:
                print(f"ë©”ëª¨ë¦¬ ë¡œë“œ ì‹¤íŒ¨, ìƒˆë¡œ ìƒì„±: {e}")
        
        return ProjectMemory(
            project_name="SOLOMOND_AI",
            symbols={},
            dependencies={},
            patterns={},
            performance_bottlenecks=[],
            last_updated=datetime.now().isoformat(),
            code_quality_metrics={}
        )
    
    def save_memory(self):
        """í”„ë¡œì íŠ¸ ë©”ëª¨ë¦¬ ì €ì¥"""
        try:
            self.project_memory.last_updated = datetime.now().isoformat()
            with open(self.memory_file, 'w', encoding='utf-8') as f:
                json.dump(asdict(self.project_memory), f, indent=2, ensure_ascii=False)
        except Exception as e:
            self.logger.error(f"ë©”ëª¨ë¦¬ ì €ì¥ ì‹¤íŒ¨: {e}")

    def analyze_file_symbols(self, file_path: str) -> List[CodeSymbol]:
        """íŒŒì¼ì˜ ëª¨ë“  ì‹¬ë³¼ ë¶„ì„ (Symbol-level analysis)"""
        symbols = []
        file_path = Path(file_path)
        
        if not file_path.exists() or file_path.suffix != '.py':
            return symbols
            
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                
            tree = ast.parse(content)
            
            for node in ast.walk(tree):
                symbol = self._extract_symbol_from_node(node, str(file_path), content)
                if symbol:
                    symbols.append(symbol)
                    
        except Exception as e:
            self.logger.error(f"íŒŒì¼ ë¶„ì„ ì‹¤íŒ¨ {file_path}: {e}")
            
        return symbols
    
    def _extract_symbol_from_node(self, node: ast.AST, file_path: str, content: str) -> Optional[CodeSymbol]:
        """AST ë…¸ë“œì—ì„œ ì‹¬ë³¼ ì •ë³´ ì¶”ì¶œ"""
        lines = content.split('\n')
        
        if isinstance(node, ast.FunctionDef):
            docstring = ast.get_docstring(node)
            args = [arg.arg for arg in node.args.args]
            
            return CodeSymbol(
                name=node.name,
                type="function",
                file_path=file_path,
                line_start=node.lineno,
                line_end=node.end_lineno or node.lineno,
                indentation=len(lines[node.lineno-1]) - len(lines[node.lineno-1].lstrip()),
                docstring=docstring,
                arguments=args,
                complexity=self._calculate_complexity(node)
            )
            
        elif isinstance(node, ast.ClassDef):
            docstring = ast.get_docstring(node)
            
            return CodeSymbol(
                name=node.name,
                type="class",
                file_path=file_path,
                line_start=node.lineno,
                line_end=node.end_lineno or node.lineno,
                indentation=len(lines[node.lineno-1]) - len(lines[node.lineno-1].lstrip()),
                docstring=docstring,
                complexity=len(node.body)
            )
            
        elif isinstance(node, (ast.Import, ast.ImportFrom)):
            import_names = []
            if isinstance(node, ast.Import):
                import_names = [alias.name for alias in node.names]
            else:
                module = node.module or ""
                import_names = [f"{module}.{alias.name}" for alias in node.names]
                
            return CodeSymbol(
                name=", ".join(import_names),
                type="import",
                file_path=file_path,
                line_start=node.lineno,
                line_end=node.lineno,
                indentation=len(lines[node.lineno-1]) - len(lines[node.lineno-1].lstrip()),
                dependencies=import_names
            )
            
        return None
    
    def _calculate_complexity(self, node: ast.AST) -> int:
        """ì½”ë“œ ë³µì¡ë„ ê³„ì‚° (McCabe ìˆœí™˜ ë³µì¡ë„ ê°„ì†Œí™” ë²„ì „)"""
        complexity = 1  # ê¸°ë³¸ ë³µì¡ë„
        
        for child in ast.walk(node):
            if isinstance(child, (ast.If, ast.While, ast.For, ast.Try, ast.With)):
                complexity += 1
            elif isinstance(child, ast.ExceptHandler):
                complexity += 1
                
        return complexity

    def extract_efficient_code_blocks(self, file_path: str, max_tokens: int = 2000) -> List[CodeBlock]:
        """í† í° íš¨ìœ¨ì ì¸ ì½”ë“œ ë¸”ë¡ ì¶”ì¶œ"""
        symbols = self.analyze_file_symbols(file_path)
        blocks = []
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                
            current_block = []
            current_symbols = []
            current_tokens = 0
            
            for symbol in sorted(symbols, key=lambda s: s.line_start):
                # ì‹¬ë³¼ì— í•´ë‹¹í•˜ëŠ” ì½”ë“œ ë¼ì¸ë“¤ ì¶”ì¶œ
                symbol_lines = lines[symbol.line_start-1:symbol.line_end]
                symbol_content = ''.join(symbol_lines)
                symbol_tokens = len(symbol_content.split())
                
                if current_tokens + symbol_tokens > max_tokens and current_block:
                    # í˜„ì¬ ë¸”ë¡ ì™„ì„±
                    block_content = ''.join(current_block)
                    block_id = hashlib.md5(block_content.encode()).hexdigest()[:8]
                    
                    blocks.append(CodeBlock(
                        content=block_content,
                        symbols=current_symbols.copy(),
                        file_path=file_path,
                        block_id=block_id,
                        token_count=current_tokens,
                        importance_score=self._calculate_importance(current_symbols),
                        context_relevance=self._calculate_relevance(current_symbols)
                    ))
                    
                    current_block = []
                    current_symbols = []
                    current_tokens = 0
                
                current_block.extend(symbol_lines)
                current_symbols.append(symbol)
                current_tokens += symbol_tokens
            
            # ë§ˆì§€ë§‰ ë¸”ë¡ ì²˜ë¦¬
            if current_block:
                block_content = ''.join(current_block)
                block_id = hashlib.md5(block_content.encode()).hexdigest()[:8]
                
                blocks.append(CodeBlock(
                    content=block_content,
                    symbols=current_symbols,
                    file_path=file_path,
                    block_id=block_id,
                    token_count=current_tokens,
                    importance_score=self._calculate_importance(current_symbols),
                    context_relevance=self._calculate_relevance(current_symbols)
                ))
                
        except Exception as e:
            self.logger.error(f"ì½”ë“œ ë¸”ë¡ ì¶”ì¶œ ì‹¤íŒ¨ {file_path}: {e}")
            
        return blocks
    
    def _calculate_importance(self, symbols: List[CodeSymbol]) -> float:
        """ì‹¬ë³¼ë“¤ì˜ ì¤‘ìš”ë„ ê³„ì‚°"""
        score = 0.0
        
        for symbol in symbols:
            # í´ë˜ìŠ¤ì™€ í•¨ìˆ˜ëŠ” ë†’ì€ ì ìˆ˜
            if symbol.type == "class":
                score += 3.0
            elif symbol.type == "function":
                score += 2.0
                # ë³µì¡í•œ í•¨ìˆ˜ëŠ” ë” ë†’ì€ ì ìˆ˜
                score += symbol.complexity * 0.1
            elif symbol.type == "import":
                score += 0.5
                
        return score / len(symbols) if symbols else 0.0
    
    def _calculate_relevance(self, symbols: List[CodeSymbol]) -> float:
        """SOLOMOND AI í”„ë¡œì íŠ¸ì™€ì˜ ê´€ë ¨ì„± ê³„ì‚°"""
        relevance = 0.0
        solomond_keywords = [
            'streamlit', 'conference', 'analysis', 'ollama', 'whisper', 
            'easyocr', 'multimodal', 'gpu', 'threading', 'threadpool'
        ]
        
        for symbol in symbols:
            symbol_text = f"{symbol.name} {symbol.docstring or ''}".lower()
            keyword_matches = sum(1 for keyword in solomond_keywords if keyword in symbol_text)
            relevance += keyword_matches * 0.2
            
        return min(relevance, 1.0)

    def detect_solomond_issues(self, file_path: str) -> Dict[str, List[Dict]]:
        """SOLOMOND AI íŠ¹í™” ë¬¸ì œì  íƒì§€"""
        issues = defaultdict(list)
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                lines = content.split('\n')
                
            for pattern_name, pattern in self.solomond_patterns.items():
                matches = re.finditer(pattern, content, re.MULTILINE | re.IGNORECASE)
                
                for match in matches:
                    line_num = content[:match.start()].count('\n') + 1
                    issue = {
                        'line': line_num,
                        'code': lines[line_num-1].strip() if line_num <= len(lines) else '',
                        'pattern': pattern_name,
                        'match': match.group(),
                        'suggestion': self._get_suggestion(pattern_name)
                    }
                    issues[pattern_name].append(issue)
                    
        except Exception as e:
            self.logger.error(f"ë¬¸ì œì  íƒì§€ ì‹¤íŒ¨ {file_path}: {e}")
            
        return dict(issues)
    
    def _get_suggestion(self, pattern_name: str) -> str:
        """íŒ¨í„´ë³„ ê°œì„  ì œì•ˆ"""
        suggestions = {
            'threadpool_issues': 'ThreadPoolExecutor ì‚¬ìš© ì‹œ with ë¬¸ê³¼ ì ì ˆí•œ max_workers ì„¤ì • ì‚¬ìš©',
            'memory_leaks': 'ë©”ëª¨ë¦¬ í•´ì œë¥¼ ìœ„í•œ del ë¬¸ê³¼ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ í˜¸ì¶œ ì¶”ê°€',
            'streamlit_performance': 'st.cache_data ë˜ëŠ” st.cache_resource ì‚¬ìš©ìœ¼ë¡œ ì„±ëŠ¥ ìµœì í™”',
            'ollama_integration': 'Ollama API í˜¸ì¶œ ì‹œ ì—ëŸ¬ ì²˜ë¦¬ì™€ ì¬ì‹œë„ ë¡œì§ ì¶”ê°€',
            'multimodal_processing': 'ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²˜ë¦¬ ì‹œ ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬ì™€ ë©”ëª¨ë¦¬ ê´€ë¦¬',
            'gpu_memory_issues': 'torch.cuda.empty_cache() í˜¸ì¶œê³¼ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì¶”ê°€',
            'error_handling_missing': 'êµ¬ì²´ì ì¸ ì˜ˆì™¸ ì²˜ë¦¬ì™€ ë¡œê¹… ì¶”ê°€',
            'inefficient_loops': 'ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ì´ë‚˜ ë²¡í„°í™” ì—°ì‚° ì‚¬ìš© ê³ ë ¤'
        }
        return suggestions.get(pattern_name, 'ì½”ë“œ ìµœì í™” ê²€í†  í•„ìš”')

    def find_symbol(self, symbol_name: str, symbol_type: str = None) -> List[CodeSymbol]:
        """ì‹¬ë³¼ ê²€ìƒ‰ (Serenaì˜ í•µì‹¬ ê¸°ëŠ¥)"""
        results = []
        
        for file_path in self.project_root.glob("**/*.py"):
            symbols = self.analyze_file_symbols(str(file_path))
            
            for symbol in symbols:
                if symbol.name == symbol_name:
                    if symbol_type is None or symbol.type == symbol_type:
                        results.append(symbol)
                        
        return results
    
    def suggest_optimizations(self, file_path: str) -> Dict[str, Any]:
        """ì„±ëŠ¥ ìµœì í™” ì œì•ˆ"""
        symbols = self.analyze_file_symbols(file_path)
        issues = self.detect_solomond_issues(file_path)
        
        optimizations = {
            'high_complexity_functions': [],
            'performance_issues': [],
            'memory_optimizations': [],
            'solomond_specific': []
        }
        
        # ê³ ë³µì¡ë„ í•¨ìˆ˜ ì°¾ê¸°
        for symbol in symbols:
            if symbol.type == "function" and symbol.complexity > 10:
                optimizations['high_complexity_functions'].append({
                    'function': symbol.name,
                    'complexity': symbol.complexity,
                    'line': symbol.line_start,
                    'suggestion': 'í•¨ìˆ˜ ë¶„ë¦¬ë‚˜ ë¦¬íŒ©í† ë§ ê²€í†  í•„ìš”'
                })
        
        # SOLOMOND AI íŠ¹í™” ì´ìŠˆ
        for issue_type, issue_list in issues.items():
            optimizations['solomond_specific'].extend([
                {
                    'type': issue_type,
                    'details': issue,
                    'priority': 'high' if 'threadpool' in issue_type or 'memory' in issue_type else 'medium'
                }
                for issue in issue_list
            ])
        
        return optimizations
    
    def update_project_memory(self, file_path: str):
        """í”„ë¡œì íŠ¸ ë©”ëª¨ë¦¬ ì—…ë°ì´íŠ¸"""
        symbols = self.analyze_file_symbols(file_path)
        
        for symbol in symbols:
            symbol_key = f"{symbol.file_path}:{symbol.name}"
            self.project_memory.symbols[symbol_key] = symbol
            
        # ì˜ì¡´ì„± ê·¸ë˜í”„ ì—…ë°ì´íŠ¸
        import_symbols = [s for s in symbols if s.type == "import"]
        if import_symbols:
            file_key = str(file_path)
            self.project_memory.dependencies[file_key] = []
            for imp in import_symbols:
                if imp.dependencies:
                    self.project_memory.dependencies[file_key].extend(imp.dependencies)
        
        # íŒ¨í„´ ë¹ˆë„ ì—…ë°ì´íŠ¸
        issues = self.detect_solomond_issues(file_path)
        for pattern_name, issue_list in issues.items():
            if pattern_name not in self.project_memory.patterns:
                self.project_memory.patterns[pattern_name] = 0
            self.project_memory.patterns[pattern_name] += len(issue_list)
        
        self.save_memory()

class SerenaIntegrationEngine:
    """Serena í†µí•© ì—”ì§„ - SOLOMOND AIì™€ì˜ í†µí•© ê´€ë¦¬"""
    
    def __init__(self, project_root: str = None):
        self.analyzer = SerenaCodeAnalyzer(project_root)
        self.integration_log = []
        
    def analyze_project_health(self) -> Dict[str, Any]:
        """í”„ë¡œì íŠ¸ ì „ì²´ ê±´ê°•ë„ ë¶„ì„"""
        health_report = {
            'timestamp': datetime.now().isoformat(),
            'overall_score': 0.0,
            'files_analyzed': 0,
            'total_symbols': 0,
            'critical_issues': 0,
            'performance_bottlenecks': [],
            'recommendations': []
        }
        
        total_files = 0
        total_issues = 0
        
        # ì£¼ìš” Python íŒŒì¼ë“¤ ë¶„ì„
        for py_file in self.analyzer.project_root.glob("**/*.py"):
            if any(skip in str(py_file) for skip in ['venv', '__pycache__', '.git']):
                continue
                
            total_files += 1
            symbols = self.analyzer.analyze_file_symbols(str(py_file))
            issues = self.analyzer.detect_solomond_issues(str(py_file))
            
            health_report['total_symbols'] += len(symbols)
            
            # í¬ë¦¬í‹°ì»¬ ì´ìŠˆ ì¹´ìš´íŠ¸
            critical_patterns = ['threadpool_issues', 'memory_leaks', 'gpu_memory_issues']
            for pattern in critical_patterns:
                if pattern in issues:
                    total_issues += len(issues[pattern])
                    health_report['critical_issues'] += len(issues[pattern])
                    
            # ì„±ëŠ¥ ë³‘ëª©ì  ì¶”ê°€
            optimizations = self.analyzer.suggest_optimizations(str(py_file))
            if optimizations['high_complexity_functions']:
                health_report['performance_bottlenecks'].extend([
                    f"{py_file.name}:{opt['function']}" for opt in optimizations['high_complexity_functions']
                ])
        
        health_report['files_analyzed'] = total_files
        
        # ì „ì²´ ì ìˆ˜ ê³„ì‚° (100ì  ë§Œì )
        if total_files > 0:
            issue_penalty = min(total_issues * 5, 50)  # ì´ìŠˆë‹¹ 5ì  ê°ì , ìµœëŒ€ 50ì 
            health_report['overall_score'] = max(100 - issue_penalty, 0)
        
        # ì¶”ì²œì‚¬í•­ ìƒì„±
        if health_report['critical_issues'] > 0:
            health_report['recommendations'].append("í¬ë¦¬í‹°ì»¬ ì´ìŠˆ ìš°ì„  í•´ê²° í•„ìš”")
        if len(health_report['performance_bottlenecks']) > 5:
            health_report['recommendations'].append("ê³ ë³µì¡ë„ í•¨ìˆ˜ë“¤ì˜ ë¦¬íŒ©í† ë§ ê²€í† ")
        if health_report['overall_score'] < 70:
            health_report['recommendations'].append("ì½”ë“œ í’ˆì§ˆ ì „ë°˜ì  ê°œì„  í•„ìš”")
            
        return health_report
    
    def generate_optimization_plan(self) -> Dict[str, Any]:
        """ìµœì í™” ê³„íš ìƒì„±"""
        plan = {
            'priority_fixes': [],
            'performance_improvements': [],
            'code_quality_enhancements': [],
            'estimated_impact': {}
        }
        
        # í”„ë¡œì íŠ¸ ì „ì²´ ë¶„ì„
        for py_file in self.analyzer.project_root.glob("**/*.py"):
            if any(skip in str(py_file) for skip in ['venv', '__pycache__', '.git']):
                continue
                
            issues = self.analyzer.detect_solomond_issues(str(py_file))
            optimizations = self.analyzer.suggest_optimizations(str(py_file))
            
            # ìš°ì„ ìˆœìœ„ í”½ìŠ¤ (í¬ë¦¬í‹°ì»¬ ì´ìŠˆ)
            critical_patterns = ['threadpool_issues', 'memory_leaks', 'gpu_memory_issues']
            for pattern in critical_patterns:
                if pattern in issues:
                    for issue in issues[pattern]:
                        plan['priority_fixes'].append({
                            'file': str(py_file),
                            'issue': issue,
                            'estimated_time': '30-60ë¶„',
                            'impact': 'high'
                        })
            
            # ì„±ëŠ¥ ê°œì„ ì‚¬í•­
            if optimizations['high_complexity_functions']:
                for func_opt in optimizations['high_complexity_functions']:
                    plan['performance_improvements'].append({
                        'file': str(py_file),
                        'function': func_opt['function'],
                        'complexity': func_opt['complexity'],
                        'suggestion': func_opt['suggestion'],
                        'estimated_time': '1-2ì‹œê°„',
                        'impact': 'medium'
                    })
        
        # ì˜í–¥ë„ ì¶”ì •
        plan['estimated_impact'] = {
            'priority_fixes': f"{len(plan['priority_fixes'])}ê°œ í¬ë¦¬í‹°ì»¬ ì´ìŠˆ í•´ê²°ë¡œ ì•ˆì •ì„± 20-30% í–¥ìƒ",
            'performance_improvements': f"{len(plan['performance_improvements'])}ê°œ í•¨ìˆ˜ ìµœì í™”ë¡œ ì„±ëŠ¥ 10-20% í–¥ìƒ",
            'total_effort': f"ì´ {len(plan['priority_fixes']) + len(plan['performance_improvements'])}ê°œ í•­ëª©, ì˜ˆìƒ ì†Œìš”ì‹œê°„: {len(plan['priority_fixes']) * 1 + len(plan['performance_improvements']) * 1.5:.1f}ì‹œê°„"
        }
        
        return plan

def main():
    """Serena ì—ì´ì „íŠ¸ ë©”ì¸ ì‹¤í–‰"""
    print("ğŸ§  SOLOMOND AI Serena ì½”ë”© ì—ì´ì „íŠ¸ ì‹œì‘")
    
    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ ì‹¤í–‰
    project_root = Path.cwd()
    engine = SerenaIntegrationEngine(str(project_root))
    
    print("\nğŸ“Š í”„ë¡œì íŠ¸ ê±´ê°•ë„ ë¶„ì„ ì¤‘...")
    health_report = engine.analyze_project_health()
    
    print(f"âœ… ë¶„ì„ ì™„ë£Œ!")
    print(f"ğŸ“ ë¶„ì„ëœ íŒŒì¼: {health_report['files_analyzed']}ê°œ")
    print(f"ğŸ” ë°œê²¬ëœ ì‹¬ë³¼: {health_report['total_symbols']}ê°œ")
    print(f"âš ï¸  í¬ë¦¬í‹°ì»¬ ì´ìŠˆ: {health_report['critical_issues']}ê°œ")
    print(f"ğŸ“ˆ ì „ì²´ ê±´ê°•ë„: {health_report['overall_score']:.1f}/100")
    
    if health_report['performance_bottlenecks']:
        print(f"\nğŸŒ ì„±ëŠ¥ ë³‘ëª©ì :")
        for bottleneck in health_report['performance_bottlenecks'][:5]:
            print(f"  - {bottleneck}")
    
    if health_report['recommendations']:
        print(f"\nğŸ’¡ ì¶”ì²œì‚¬í•­:")
        for rec in health_report['recommendations']:
            print(f"  - {rec}")
    
    print("\nğŸ¯ ìµœì í™” ê³„íš ìƒì„± ì¤‘...")
    optimization_plan = engine.generate_optimization_plan()
    
    if optimization_plan['priority_fixes']:
        print(f"\nğŸš¨ ìš°ì„ ìˆœìœ„ ìˆ˜ì •ì‚¬í•­ ({len(optimization_plan['priority_fixes'])}ê°œ):")
        for fix in optimization_plan['priority_fixes'][:3]:
            print(f"  - {Path(fix['file']).name}: {fix['issue']['pattern']}")
    
    print(f"\nğŸ“ˆ ì˜ˆìƒ íš¨ê³¼:")
    for impact_type, impact_desc in optimization_plan['estimated_impact'].items():
        print(f"  - {impact_desc}")
    
    print(f"\nğŸ’¾ í”„ë¡œì íŠ¸ ë©”ëª¨ë¦¬ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
    print(f"ğŸ“ ë¡œê·¸ íŒŒì¼: {project_root / 'serena_agent.log'}")

if __name__ == "__main__":
    main()