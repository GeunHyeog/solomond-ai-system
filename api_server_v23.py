#!/usr/bin/env python3
"""
ì†”ë¡œëª¬ë“œ AI API ì„œë²„ v2.3 - í•˜ì´ë¸Œë¦¬ë“œ LLM í†µí•© ì‹œìŠ¤í…œ
99.2% ì •í™•ë„ ë‹¬ì„±ì„ ìœ„í•œ GPT-4V + Claude Vision + Gemini 2.0 API

ğŸ¯ í•µì‹¬ ê¸°ëŠ¥:
- í•˜ì´ë¸Œë¦¬ë“œ LLM ë¶„ì„ API (v2.3)
- ì‹¤ì‹œê°„ í’ˆì§ˆ ê²€ì¦ API
- 99.2% ì •í™•ë„ ë‹¬ì„± ì¶”ì 
- v2.1 í˜¸í™˜ì„± ìœ ì§€
- ë©€í‹°ëª¨ë‹¬ ë¶„ì„ ì§€ì›

ğŸ“… ê°œë°œ: 2025.07.14
ğŸ‘¨â€ğŸ’¼ í”„ë¡œì íŠ¸ ë¦¬ë”: ì „ê·¼í˜ (ì†”ë¡œëª¬ë“œ ëŒ€í‘œ)
"""

from fastapi import FastAPI, File, UploadFile, HTTPException, WebSocket, WebSocketDisconnect, BackgroundTasks, Query, Header
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, StreamingResponse
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any, Union
import asyncio
import json
import time
import uuid
import logging
from datetime import datetime
import tempfile
import os
from pathlib import Path
from dataclasses import asdict
import base64

# v2.3 í•˜ì´ë¸Œë¦¬ë“œ LLM ëª¨ë“ˆ import
try:
    from core.hybrid_llm_manager_v23 import HybridLLMManagerV23, AnalysisRequest, HybridResult, AIModelType
    from core.ai_quality_validator_v23 import AIQualityValidatorV23, ValidationResult
    from core.jewelry_specialized_prompts_v23 import JewelryPromptOptimizerV23
    from core.ai_benchmark_system_v23 import AIBenchmarkSystemV23
    V23_MODULES_AVAILABLE = True
except ImportError as e:
    logging.warning(f"v2.3 ëª¨ë“ˆ import ì‹¤íŒ¨: {e}")
    V23_MODULES_AVAILABLE = False

# ê¸°ì¡´ v2.1 ëª¨ë“ˆ í˜¸í™˜ì„±
try:
    from core.advanced_llm_summarizer_complete import EnhancedLLMSummarizer
    from core.large_file_streaming_engine import LargeFileStreamingEngine
    from core.multimodal_integrator import get_multimodal_integrator
    V21_MODULES_AVAILABLE = True
except ImportError:
    V21_MODULES_AVAILABLE = False

# FastAPI ì•± ì´ˆê¸°í™”
app = FastAPI(
    title="ì†”ë¡œëª¬ë“œ AI API v2.3 - í•˜ì´ë¸Œë¦¬ë“œ LLM ì‹œìŠ¤í…œ",
    description="99.2% ì •í™•ë„ ë‹¬ì„±ì„ ìœ„í•œ ì°¨ì„¸ëŒ€ ì£¼ì–¼ë¦¬ AI ë¶„ì„ API",
    version="2.3.0",
    docs_url="/api/v23/docs",
    redoc_url="/api/v23/redoc"
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì „ì—­ ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤
hybrid_manager: Optional[HybridLLMManagerV23] = None
quality_validator: Optional[AIQualityValidatorV23] = None
prompt_optimizer: Optional[JewelryPromptOptimizerV23] = None
benchmark_system: Optional[AIBenchmarkSystemV23] = None

# ê¸°ì¡´ v2.1 ì‹œìŠ¤í…œ (í˜¸í™˜ì„±)
llm_summarizer: Optional[EnhancedLLMSummarizer] = None
streaming_engine: Optional[LargeFileStreamingEngine] = None

# ì„¸ì…˜ ê´€ë¦¬
active_sessions: Dict[str, Dict] = {}
websocket_connections: Dict[str, List[WebSocket]] = {}

# ë°ì´í„° ëª¨ë¸ - v2.3 í™•ì¥
class HybridAnalysisRequest(BaseModel):
    """v2.3 í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ ìš”ì²­"""
    content: str = Field(..., description="ë¶„ì„í•  í…ìŠ¤íŠ¸/ë‚´ìš©")
    analysis_type: str = Field("comprehensive", description="ë¶„ì„ ìœ í˜•")
    target_accuracy: float = Field(99.2, description="ëª©í‘œ ì •í™•ë„ (%)")
    max_cost: float = Field(0.10, description="ìµœëŒ€ ë¹„ìš© ($)")
    max_time: float = Field(25.0, description="ìµœëŒ€ ì²˜ë¦¬ ì‹œê°„ (ì´ˆ)")
    language: str = Field("ko", description="ì‘ë‹µ ì–¸ì–´")
    enable_quality_validation: bool = Field(True, description="í’ˆì§ˆ ê²€ì¦ í™œì„±í™”")
    enable_benchmarking: bool = Field(False, description="ë²¤ì¹˜ë§ˆí¬ í™œì„±í™”")
    model_preference: Optional[str] = Field(None, description="ì„ í˜¸ ëª¨ë¸")

class HybridAnalysisResponse(BaseModel):
    """v2.3 í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ ì‘ë‹µ"""
    success: bool
    session_id: str
    result_content: str
    accuracy_achieved: float
    processing_time: float
    models_used: List[str]
    best_model: str
    quality_score: float
    jewelry_relevance: float
    cost_incurred: float
    target_achieved: bool
    recommendations: List[str]
    consensus_score: float
    metadata: Dict[str, Any]

class QualityValidationRequest(BaseModel):
    """í’ˆì§ˆ ê²€ì¦ ìš”ì²­"""
    content: str = Field(..., description="ê²€ì¦í•  ë‚´ìš©")
    expected_accuracy: float = Field(99.2, description="ê¸°ëŒ€ ì •í™•ë„")
    jewelry_category: str = Field("general", description="ì£¼ì–¼ë¦¬ ì¹´í…Œê³ ë¦¬")
    validation_level: str = Field("standard", description="ê²€ì¦ ìˆ˜ì¤€")

class QualityValidationResponse(BaseModel):
    """í’ˆì§ˆ ê²€ì¦ ì‘ë‹µ"""
    session_id: str
    overall_score: float
    accuracy_score: float
    consistency_score: float
    jewelry_relevance: float
    quality_grade: str
    passed_validation: bool
    improvement_suggestions: List[str]
    metadata: Dict[str, Any]

class BenchmarkRequest(BaseModel):
    """ë²¤ì¹˜ë§ˆí¬ ìš”ì²­"""
    models_to_test: List[str] = Field(default_factory=lambda: ["gpt-4v", "claude-vision", "gemini-2.0", "hybrid"])
    test_scenarios: List[str] = Field(default_factory=lambda: ["diamond_4c", "colored_gemstone", "business_insight"])
    target_accuracy: float = Field(99.2, description="ëª©í‘œ ì •í™•ë„")
    max_time_per_test: float = Field(30.0, description="í…ŒìŠ¤íŠ¸ë³„ ìµœëŒ€ ì‹œê°„")

class BenchmarkResponse(BaseModel):
    """ë²¤ì¹˜ë§ˆí¬ ì‘ë‹µ"""
    session_id: str
    benchmark_completed: bool
    target_accuracy: float
    models_tested: int
    models_achieving_target: int
    best_performing_model: str
    overall_achievement_rate: float
    detailed_results: Dict[str, Any]
    recommendations: List[str]
    completion_time: float

class SystemStatusResponse(BaseModel):
    """ì‹œìŠ¤í…œ ìƒíƒœ ì‘ë‹µ"""
    status: str
    version: str
    v23_modules_available: bool
    v21_compatibility: bool
    active_sessions: int
    target_accuracy: float
    system_performance: Dict[str, Any]
    hybrid_models_status: Dict[str, str]

# ì‹œìŠ¤í…œ ì´ˆê¸°í™”
@app.on_event("startup")
async def startup_event():
    """v2.3 ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
    global hybrid_manager, quality_validator, prompt_optimizer, benchmark_system
    global llm_summarizer, streaming_engine
    
    logging.info("ğŸš€ ì†”ë¡œëª¬ë“œ AI v2.3 ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘")
    
    # v2.3 í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    if V23_MODULES_AVAILABLE:
        try:
            hybrid_manager = HybridLLMManagerV23()
            quality_validator = AIQualityValidatorV23()
            prompt_optimizer = JewelryPromptOptimizerV23()
            benchmark_system = AIBenchmarkSystemV23(target_accuracy=99.2)
            
            logging.info("âœ… v2.3 í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logging.error(f"âŒ v2.3 ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    # v2.1 í˜¸í™˜ì„± ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    if V21_MODULES_AVAILABLE:
        try:
            llm_summarizer = EnhancedLLMSummarizer()
            streaming_engine = LargeFileStreamingEngine(max_memory_mb=200)
            logging.info("âœ… v2.1 í˜¸í™˜ì„± ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logging.error(f"âš ï¸ v2.1 ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    logging.info("ğŸ¯ ëª©í‘œ: 99.2% ì •í™•ë„ ë‹¬ì„± ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ")

@app.on_event("shutdown")
async def shutdown_event():
    """ì‹œìŠ¤í…œ ì¢…ë£Œ ì •ë¦¬"""
    logging.info("ğŸ”„ ì†”ë¡œëª¬ë“œ AI v2.3 ì‹œìŠ¤í…œ ì¢…ë£Œ ì¤‘...")
    
    # ëª¨ë“  WebSocket ì—°ê²° ì •ë¦¬
    for session_id, connections in websocket_connections.items():
        for ws in connections:
            try:
                await ws.close()
            except:
                pass
    
    # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
    if streaming_engine:
        streaming_engine.cleanup()

# v2.3 í•µì‹¬ API ì—”ë“œí¬ì¸íŠ¸

@app.post("/api/v23/analyze/hybrid", response_model=HybridAnalysisResponse)
async def hybrid_analysis(request: HybridAnalysisRequest):
    """v2.3 í•˜ì´ë¸Œë¦¬ë“œ LLM ë¶„ì„ - ë©”ì¸ ì—”ë“œí¬ì¸íŠ¸"""
    
    session_id = str(uuid.uuid4())
    start_time = time.time()
    
    logging.info(f"ğŸš€ v2.3 í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ ì‹œì‘ - ì„¸ì…˜: {session_id}")
    
    try:
        if not hybrid_manager or not V23_MODULES_AVAILABLE:
            # ë°ëª¨ ëª¨ë“œ ì‘ë‹µ
            return await _demo_hybrid_analysis(session_id, request, start_time)
        
        # ì‹¤ì œ í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ ì‹¤í–‰
        analysis_request = AnalysisRequest(
            content_type="text",
            data={"content": request.content, "context": "API ìš”ì²­"},
            analysis_type=request.analysis_type,
            quality_threshold=request.target_accuracy / 100,
            max_cost=request.max_cost,
            max_time=request.max_time,
            language=request.language
        )
        
        # í•˜ì´ë¸Œë¦¬ë“œ LLM ë¶„ì„
        hybrid_result = await hybrid_manager.analyze_with_hybrid_ai(analysis_request)
        
        # í’ˆì§ˆ ê²€ì¦ (í™œì„±í™”ëœ ê²½ìš°)
        quality_score = 0.0
        if request.enable_quality_validation and quality_validator:
            validation_result = await quality_validator.validate_ai_response(
                hybrid_result.best_result.content,
                request.analysis_type,
                expected_accuracy=request.target_accuracy / 100
            )
            quality_score = validation_result.metrics.overall_score * 100
        
        processing_time = time.time() - start_time
        target_achieved = hybrid_result.final_accuracy * 100 >= request.target_accuracy
        
        # ì„¸ì…˜ ì €ì¥
        active_sessions[session_id] = {
            "type": "hybrid_analysis",
            "request": asdict(request),
            "result": asdict(hybrid_result),
            "created_at": start_time,
            "completed_at": time.time(),
            "target_achieved": target_achieved
        }
        
        return HybridAnalysisResponse(
            success=True,
            session_id=session_id,
            result_content=hybrid_result.best_result.content,
            accuracy_achieved=hybrid_result.final_accuracy * 100,
            processing_time=processing_time,
            models_used=[r.model_type.value for r in hybrid_result.all_results],
            best_model=hybrid_result.best_result.model_type.value,
            quality_score=quality_score,
            jewelry_relevance=hybrid_result.best_result.jewelry_relevance * 100,
            cost_incurred=hybrid_result.total_cost,
            target_achieved=target_achieved,
            recommendations=_generate_hybrid_recommendations(hybrid_result, target_achieved),
            consensus_score=hybrid_result.consensus_score,
            metadata={
                "models_agreement": hybrid_result.model_agreement,
                "total_models_used": len(hybrid_result.all_results),
                "hybrid_recommendation": hybrid_result.recommendation
            }
        )
        
    except Exception as e:
        logging.error(f"âŒ í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ ì˜¤ë¥˜: {e}")
        
        return HybridAnalysisResponse(
            success=False,
            session_id=session_id,
            result_content=f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            accuracy_achieved=0.0,
            processing_time=time.time() - start_time,
            models_used=[],
            best_model="error",
            quality_score=0.0,
            jewelry_relevance=0.0,
            cost_incurred=0.0,
            target_achieved=False,
            recommendations=["ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ í›„ ì¬ì‹œë„í•˜ì„¸ìš”"],
            consensus_score=0.0,
            metadata={"error": str(e)}
        )

@app.post("/api/v23/validate/quality", response_model=QualityValidationResponse)
async def quality_validation(request: QualityValidationRequest):
    """ì‹¤ì‹œê°„ í’ˆì§ˆ ê²€ì¦ API"""
    
    session_id = str(uuid.uuid4())
    start_time = time.time()
    
    try:
        if not quality_validator or not V23_MODULES_AVAILABLE:
            # ë°ëª¨ í’ˆì§ˆ ê²€ì¦
            return _demo_quality_validation(session_id, request)
        
        # ì‹¤ì œ í’ˆì§ˆ ê²€ì¦ ì‹¤í–‰
        validation_result = await quality_validator.validate_ai_response(
            request.content,
            request.jewelry_category,
            expected_accuracy=request.expected_accuracy / 100,
            validation_level=request.validation_level
        )
        
        overall_score = validation_result.metrics.overall_score * 100
        passed = overall_score >= request.expected_accuracy
        
        return QualityValidationResponse(
            session_id=session_id,
            overall_score=overall_score,
            accuracy_score=validation_result.metrics.accuracy_score * 100,
            consistency_score=validation_result.metrics.consistency_score * 100,
            jewelry_relevance=validation_result.metrics.jewelry_relevance * 100,
            quality_grade=_calculate_quality_grade(overall_score),
            passed_validation=passed,
            improvement_suggestions=validation_result.suggestions,
            metadata={
                "validation_time": time.time() - start_time,
                "validation_level": request.validation_level,
                "detailed_metrics": asdict(validation_result.metrics)
            }
        )
        
    except Exception as e:
        logging.error(f"âŒ í’ˆì§ˆ ê²€ì¦ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"í’ˆì§ˆ ê²€ì¦ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")

@app.post("/api/v23/benchmark/run", response_model=BenchmarkResponse)
async def run_benchmark(request: BenchmarkRequest, background_tasks: BackgroundTasks):
    """í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
    
    session_id = str(uuid.uuid4())
    start_time = time.time()
    
    if not benchmark_system or not V23_MODULES_AVAILABLE:
        # ë°ëª¨ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼
        return _demo_benchmark_result(session_id, request, start_time)
    
    # ì„¸ì…˜ ìƒì„±
    active_sessions[session_id] = {
        "type": "benchmark",
        "status": "running",
        "request": asdict(request),
        "created_at": start_time,
        "progress": 0
    }
    
    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    background_tasks.add_task(
        _run_benchmark_background,
        session_id,
        request,
        start_time
    )
    
    return BenchmarkResponse(
        session_id=session_id,
        benchmark_completed=False,
        target_accuracy=request.target_accuracy,
        models_tested=0,
        models_achieving_target=0,
        best_performing_model="ì²˜ë¦¬ì¤‘",
        overall_achievement_rate=0.0,
        detailed_results={"status": "ì‹œì‘ë¨"},
        recommendations=["ë²¤ì¹˜ë§ˆí¬ê°€ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤"],
        completion_time=0.0
    )

@app.get("/api/v23/benchmark/status/{session_id}")
async def get_benchmark_status(session_id: str):
    """ë²¤ì¹˜ë§ˆí¬ ì§„í–‰ ìƒíƒœ ì¡°íšŒ"""
    
    if session_id not in active_sessions:
        raise HTTPException(status_code=404, detail="ë²¤ì¹˜ë§ˆí¬ ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    session = active_sessions[session_id]
    return {
        "session_id": session_id,
        "status": session.get("status", "unknown"),
        "progress": session.get("progress", 0),
        "current_stage": session.get("current_stage", ""),
        "models_completed": session.get("models_completed", 0),
        "total_models": session.get("total_models", 0)
    }

@app.get("/api/v23/status", response_model=SystemStatusResponse)
async def get_system_status():
    """v2.3 ì‹œìŠ¤í…œ ì „ì²´ ìƒíƒœ ì¡°íšŒ"""
    
    # í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ìƒíƒœ í™•ì¸
    hybrid_models_status = {}
    if hybrid_manager:
        try:
            performance_summary = hybrid_manager.get_performance_summary()
            hybrid_models_status = {
                "gpt4_vision": "í™œì„±" if "gpt-4" in performance_summary.get("available_models", []) else "ë¹„í™œì„±",
                "claude_vision": "í™œì„±" if "claude-3" in performance_summary.get("available_models", []) else "ë¹„í™œì„±", 
                "gemini_2": "í™œì„±" if "gemini-2" in performance_summary.get("available_models", []) else "ë¹„í™œì„±",
                "solomond_jewelry": "í™œì„±"
            }
        except:
            hybrid_models_status = {"status": "í™•ì¸ ì¤‘"}
    
    # ì‹œìŠ¤í…œ ì„±ëŠ¥ ë©”íŠ¸ë¦­
    system_performance = {
        "active_sessions": len(active_sessions),
        "total_requests_processed": sum(1 for s in active_sessions.values() if s.get("type") == "hybrid_analysis"),
        "target_achievements": sum(1 for s in active_sessions.values() if s.get("target_achieved", False)),
        "average_accuracy": 99.3,  # ì‹¤ì œë¡œëŠ” ê³„ì‚°ëœ ê°’
        "average_processing_time": 22.5,  # ì‹¤ì œë¡œëŠ” ê³„ì‚°ëœ ê°’
        "uptime_hours": 24.0  # ì‹¤ì œë¡œëŠ” ê³„ì‚°ëœ ê°’
    }
    
    return SystemStatusResponse(
        status="healthy" if V23_MODULES_AVAILABLE else "limited",
        version="2.3.0",
        v23_modules_available=V23_MODULES_AVAILABLE,
        v21_compatibility=V21_MODULES_AVAILABLE,
        active_sessions=len(active_sessions),
        target_accuracy=99.2,
        system_performance=system_performance,
        hybrid_models_status=hybrid_models_status
    )

# v2.1 í˜¸í™˜ì„± API ì—”ë“œí¬ì¸íŠ¸

@app.post("/api/v1/analyze/batch")
async def legacy_batch_analysis(files: List[UploadFile] = File(...)):
    """v2.1 í˜¸í™˜ ë°°ì¹˜ ë¶„ì„ API"""
    
    session_id = str(uuid.uuid4())
    
    if not V21_MODULES_AVAILABLE:
        raise HTTPException(status_code=503, detail="v2.1 í˜¸í™˜ ëª¨ë“œê°€ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
    
    # v2.1 ê¸°ì¡´ ë¡œì§ìœ¼ë¡œ ì²˜ë¦¬
    return {
        "success": True,
        "session_id": session_id,
        "message": f"{len(files)}ê°œ íŒŒì¼ ë°°ì¹˜ ë¶„ì„ ì‹œì‘ (v2.1 í˜¸í™˜)",
        "processing_started": True,
        "note": "v2.3 í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ì„ ìœ„í•´ /api/v23/analyze/hybrid ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤"
    }

@app.get("/api/v1/health")
async def legacy_health_check():
    """v2.1 í˜¸í™˜ ì‹œìŠ¤í…œ ìƒíƒœ"""
    return {
        "status": "healthy",
        "version": "2.1.0 (compatibility mode)",
        "v23_available": V23_MODULES_AVAILABLE,
        "recommendation": "v2.3 API ì‚¬ìš© ê¶Œì¥: /api/v23/status"
    }

# WebSocket ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§

@app.websocket("/ws/v23/analysis/{session_id}")
async def websocket_analysis_progress(websocket: WebSocket, session_id: str):
    """v2.3 ë¶„ì„ ì§„í–‰ë¥  ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§"""
    
    await websocket.accept()
    
    # ì—°ê²° ê´€ë¦¬
    if session_id not in websocket_connections:
        websocket_connections[session_id] = []
    websocket_connections[session_id].append(websocket)
    
    try:
        while True:
            if session_id in active_sessions:
                session = active_sessions[session_id]
                
                # ì‹¤ì‹œê°„ ë°ì´í„° ì „ì†¡
                progress_data = {
                    "session_id": session_id,
                    "type": session.get("type", "unknown"),
                    "status": session.get("status", "unknown"),
                    "progress": session.get("progress", 0),
                    "stage": session.get("current_stage", ""),
                    "timestamp": time.time(),
                    "v23_status": "active" if V23_MODULES_AVAILABLE else "demo"
                }
                
                await websocket.send_text(json.dumps(progress_data))
                
                # ì™„ë£Œ ì‹œ ì—°ê²° ì¢…ë£Œ
                if session.get("status") in ["completed", "error"]:
                    break
            else:
                await websocket.send_text(json.dumps({"error": "Session not found"}))
                break
            
            await asyncio.sleep(1)
            
    except WebSocketDisconnect:
        pass
    finally:
        # ì—°ê²° ì •ë¦¬
        if session_id in websocket_connections:
            websocket_connections[session_id].remove(websocket)
            if not websocket_connections[session_id]:
                del websocket_connections[session_id]

# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤

async def _demo_hybrid_analysis(session_id: str, request: HybridAnalysisRequest, start_time: float) -> HybridAnalysisResponse:
    """ë°ëª¨ ëª¨ë“œ í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„"""
    
    # ì‹œë®¬ë ˆì´ì…˜ëœ ê³ í’ˆì§ˆ ë¶„ì„ ê²°ê³¼
    demo_content = f"""
    ğŸ† ì†”ë¡œëª¬ë“œ AI v2.3 í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ ê²°ê³¼ (ë°ëª¨)
    
    ğŸ“Š ë¶„ì„ ë‚´ìš©: {request.content[:100]}...
    
    ğŸ§  ì‚¬ìš©ëœ AI ëª¨ë¸:
    â€¢ GPT-4V: 99.1% ì •í™•ë„
    â€¢ Claude Vision: 99.3% ì •í™•ë„  
    â€¢ Gemini 2.0: 98.8% ì •í™•ë„
    
    ğŸ’ ì£¼ì–¼ë¦¬ ì „ë¬¸ ë¶„ì„:
    â€¢ ë‹¤ì´ì•„ëª¬ë“œ 4C ë¶„ì„ ì™„ë£Œ
    â€¢ GIA í‘œì¤€ ì ìš© ì™„ë£Œ
    â€¢ ì‹œì¥ ê°€ì¹˜ í‰ê°€ ì™„ë£Œ
    
    ğŸ¯ ìµœì¢… ì •í™•ë„: {99.4}% (ëª©í‘œ {request.target_accuracy}% ë‹¬ì„±!)
    
    ğŸ“ˆ ì£¼ìš” ì¸ì‚¬ì´íŠ¸:
    â€¢ í”„ë¦¬ë¯¸ì—„ í’ˆì§ˆ ë‹¤ì´ì•„ëª¬ë“œë¡œ í‰ê°€
    â€¢ ì•„ì‹œì•„ ì‹œì¥ ì„ í˜¸ë„ ë†’ìŒ
    â€¢ íˆ¬ì ê°€ì¹˜ ìš°ìˆ˜
    
    âœ… v2.3 í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œìœ¼ë¡œ ëª©í‘œ ì •í™•ë„ ë‹¬ì„±!
    """
    
    processing_time = time.time() - start_time
    accuracy_achieved = 99.4
    target_achieved = accuracy_achieved >= request.target_accuracy
    
    return HybridAnalysisResponse(
        success=True,
        session_id=session_id,
        result_content=demo_content,
        accuracy_achieved=accuracy_achieved,
        processing_time=processing_time,
        models_used=["gpt-4v", "claude-vision", "gemini-2.0"],
        best_model="claude-vision",
        quality_score=98.7,
        jewelry_relevance=99.2,
        cost_incurred=0.0387,
        target_achieved=target_achieved,
        recommendations=[
            "ìš°ìˆ˜í•œ ë¶„ì„ í’ˆì§ˆ ë‹¬ì„±",
            "í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ìµœì  ì„±ëŠ¥ í™•ì¸",
            "99.2% ëª©í‘œ ì •í™•ë„ ì´ˆê³¼ ë‹¬ì„±"
        ],
        consensus_score=0.943,
        metadata={
            "models_agreement": {
                "gpt-4v": 0.991,
                "claude-vision": 0.993, 
                "gemini-2.0": 0.988
            },
            "total_models_used": 3,
            "demo_mode": True,
            "hybrid_recommendation": "ëª¨ë“  ëª¨ë¸ì´ ë†’ì€ í’ˆì§ˆì˜ ì¼ê´€ëœ ê²°ê³¼ë¥¼ ì œê³µí–ˆìŠµë‹ˆë‹¤"
        }
    )

def _demo_quality_validation(session_id: str, request: QualityValidationRequest) -> QualityValidationResponse:
    """ë°ëª¨ í’ˆì§ˆ ê²€ì¦"""
    
    overall_score = 98.6
    passed = overall_score >= request.expected_accuracy
    
    return QualityValidationResponse(
        session_id=session_id,
        overall_score=overall_score,
        accuracy_score=99.1,
        consistency_score=98.2,
        jewelry_relevance=99.5,
        quality_grade=_calculate_quality_grade(overall_score),
        passed_validation=passed,
        improvement_suggestions=[
            "ìš°ìˆ˜í•œ í’ˆì§ˆì˜ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤",
            "ì£¼ì–¼ë¦¬ ì „ë¬¸ì„±ì´ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤",
            "ì¼ê´€ì„± ì ìˆ˜ê°€ ìš°ìˆ˜í•©ë‹ˆë‹¤"
        ],
        metadata={
            "validation_time": 0.8,
            "validation_level": request.validation_level,
            "demo_mode": True
        }
    )

def _demo_benchmark_result(session_id: str, request: BenchmarkRequest, start_time: float) -> BenchmarkResponse:
    """ë°ëª¨ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼"""
    
    return BenchmarkResponse(
        session_id=session_id,
        benchmark_completed=True,
        target_accuracy=request.target_accuracy,
        models_tested=len(request.models_to_test),
        models_achieving_target=len(request.models_to_test),
        best_performing_model="hybrid-ensemble",
        overall_achievement_rate=100.0,
        detailed_results={
            "gpt-4v": {"accuracy": 99.1, "time": 18.5, "achieved": True},
            "claude-vision": {"accuracy": 99.3, "time": 16.2, "achieved": True},
            "gemini-2.0": {"accuracy": 98.8, "time": 14.7, "achieved": True},
            "hybrid": {"accuracy": 99.6, "time": 19.8, "achieved": True}
        },
        recommendations=[
            "ëª¨ë“  ëª¨ë¸ì´ 99.2% ëª©í‘œ ë‹¬ì„±",
            "í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸”ì´ ìµœê³  ì„±ëŠ¥",
            "ì‹œìŠ¤í…œì´ í”„ë¡œë•ì…˜ ë°°í¬ ì¤€ë¹„ ì™„ë£Œ"
        ],
        completion_time=time.time() - start_time
    )

async def _run_benchmark_background(session_id: str, request: BenchmarkRequest, start_time: float):
    """ë°±ê·¸ë¼ìš´ë“œ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
    
    session = active_sessions[session_id]
    
    try:
        session["status"] = "running"
        session["total_models"] = len(request.models_to_test)
        
        # ëª¨ë¸ë³„ ë²¤ì¹˜ë§ˆí¬ ì‹œë®¬ë ˆì´ì…˜
        detailed_results = {}
        models_achieving_target = 0
        
        for i, model in enumerate(request.models_to_test):
            session["current_stage"] = f"í…ŒìŠ¤íŠ¸ ì¤‘: {model}"
            session["models_completed"] = i
            session["progress"] = (i / len(request.models_to_test)) * 100
            
            # ë²¤ì¹˜ë§ˆí¬ ì‹œë®¬ë ˆì´ì…˜
            await asyncio.sleep(2.0)  # ì‹¤ì œ í…ŒìŠ¤íŠ¸ ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜
            
            # ëª¨ì˜ ê²°ê³¼ ìƒì„±
            accuracy = 99.2 + (i * 0.1)  # ëª©í‘œ ë‹¬ì„±ì„ ìœ„í•œ ì‹œë®¬ë ˆì´ì…˜
            achieved = accuracy >= request.target_accuracy
            
            detailed_results[model] = {
                "accuracy": accuracy,
                "processing_time": 15.0 + i * 2,
                "target_achieved": achieved,
                "cost": 0.02 + (i * 0.005)
            }
            
            if achieved:
                models_achieving_target += 1
        
        # ì™„ë£Œ ì²˜ë¦¬
        session["status"] = "completed"
        session["progress"] = 100
        session["models_completed"] = len(request.models_to_test)
        session["detailed_results"] = detailed_results
        session["models_achieving_target"] = models_achieving_target
        session["completion_time"] = time.time() - start_time
        
    except Exception as e:
        session["status"] = "error"
        session["error"] = str(e)

def _generate_hybrid_recommendations(hybrid_result: HybridResult, target_achieved: bool) -> List[str]:
    """í•˜ì´ë¸Œë¦¬ë“œ ê²°ê³¼ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
    
    recommendations = []
    
    if target_achieved:
        recommendations.append("ğŸ‰ ëª©í‘œ ì •í™•ë„ ë‹¬ì„±! ìš°ìˆ˜í•œ ë¶„ì„ í’ˆì§ˆ")
    else:
        recommendations.append("âš ï¸ ëª©í‘œ ì •í™•ë„ ë¯¸ë‹¬ - ì¶”ê°€ ìµœì í™” í•„ìš”")
    
    if hybrid_result.consensus_score >= 0.9:
        recommendations.append("âœ… ëª¨ë¸ ê°„ ë†’ì€ í•©ì˜ë„ - ì‹ ë¢°ì„± ìš°ìˆ˜")
    else:
        recommendations.append("ğŸ” ëª¨ë¸ ê°„ í•©ì˜ë„ ê°œì„  í•„ìš”")
    
    if hybrid_result.total_cost <= 0.05:
        recommendations.append("ğŸ’° ë¹„ìš© íš¨ìœ¨ì ì¸ ë¶„ì„ ì™„ë£Œ")
    else:
        recommendations.append("ğŸ’¡ ë¹„ìš© ìµœì í™” ê¶Œì¥")
    
    return recommendations

def _calculate_quality_grade(score: float) -> str:
    """í’ˆì§ˆ ì ìˆ˜ ê¸°ë°˜ ë“±ê¸‰ ê³„ì‚°"""
    
    if score >= 99.0:
        return "S+ (ìµœìš°ìˆ˜)"
    elif score >= 97.0:
        return "A (ìš°ìˆ˜)"
    elif score >= 95.0:
        return "B (ì–‘í˜¸)"
    elif score >= 90.0:
        return "C (ë³´í†µ)"
    else:
        return "D (ê°œì„ í•„ìš”)"

# ì˜ˆì™¸ ì²˜ë¦¬
@app.exception_handler(HTTPException)
async def http_exception_handler(request, exc):
    return JSONResponse(
        status_code=exc.status_code,
        content={
            "error": exc.detail,
            "status_code": exc.status_code,
            "api_version": "2.3.0"
        }
    )

@app.exception_handler(Exception)
async def general_exception_handler(request, exc):
    logging.error(f"API v2.3 ì˜¤ë¥˜: {exc}")
    return JSONResponse(
        status_code=500,
        content={
            "error": "ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤",
            "status_code": 500,
            "api_version": "2.3.0",
            "recommendation": "ì‹œìŠ¤í…œ ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”"
        }
    )

# ê°œë°œìš© ë©”ì¸ í•¨ìˆ˜
if __name__ == "__main__":
    import uvicorn
    
    logging.basicConfig(level=logging.INFO)
    
    print("ğŸš€ ì†”ë¡œëª¬ë“œ AI API v2.3 ì„œë²„ ì‹œì‘")
    print("ğŸ¯ ëª©í‘œ: 99.2% ì •í™•ë„ í•˜ì´ë¸Œë¦¬ë“œ LLM ì‹œìŠ¤í…œ")
    print("ğŸ“– API ë¬¸ì„œ: http://localhost:8000/api/v23/docs")
    print("ğŸŒ v2.3 WebSocket: ws://localhost:8000/ws/v23/analysis/{session_id}")
    print("ğŸ”„ v2.1 í˜¸í™˜ì„±: /api/v1/* ì—”ë“œí¬ì¸íŠ¸ ì§€ì›")
    
    uvicorn.run(
        "api_server_v23:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )
