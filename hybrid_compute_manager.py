#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ”„ SOLOMOND AI í•˜ì´ë¸Œë¦¬ë“œ ì»´í“¨íŒ… ë§¤ë‹ˆì €
Hybrid Compute Manager - CPU/GPU ë™ì  ì „í™˜ ì‹œìŠ¤í…œ

í•µì‹¬ ê¸°ëŠ¥:
1. ì‹¤ì‹œê°„ GPU/CPU ìƒíƒœ ëª¨ë‹ˆí„°ë§
2. ì‘ì—… ìœ í˜•ë³„ ìµœì  ë¦¬ì†ŒìŠ¤ ìë™ ì„ íƒ
3. ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ìë™ í´ë°±
4. ì„±ëŠ¥ ê¸°ë°˜ ë™ì  ìŠ¤ì¼€ì¤„ë§
"""

import os
import gc
import time
import psutil
import logging
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from enum import Enum
import threading
from pathlib import Path

# GPU ê´€ë ¨ ì„í¬íŠ¸ (ì„ íƒì )
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

try:
    import nvidia_ml_py3 as nvml
    nvml.nvmlInit()
    NVIDIA_ML_AVAILABLE = True
except (ImportError, Exception):
    NVIDIA_ML_AVAILABLE = False

class ComputeMode(Enum):
    """ì»´í“¨íŒ… ëª¨ë“œ"""
    AUTO = "auto"           # ìë™ ì„ íƒ (ê¶Œì¥)
    GPU_PREFERRED = "gpu_preferred"  # GPU ìš°ì„ , CPU í´ë°±
    CPU_PREFERRED = "cpu_preferred"  # CPU ìš°ì„ , GPU ë³´ì¡°
    GPU_ONLY = "gpu_only"   # GPU ì „ìš©
    CPU_ONLY = "cpu_only"   # CPU ì „ìš©

class TaskType(Enum):
    """ì‘ì—… ìœ í˜•"""
    STT_SMALL = "stt_small"         # ì‘ì€ ìŒì„± íŒŒì¼ (<5ë¶„)
    STT_LARGE = "stt_large"         # í° ìŒì„± íŒŒì¼ (>5ë¶„)
    OCR_BATCH = "ocr_batch"         # ë°°ì¹˜ ì´ë¯¸ì§€ OCR
    OCR_REALTIME = "ocr_realtime"   # ì‹¤ì‹œê°„ OCR
    LLM_INFERENCE = "llm_inference" # LLM ì¶”ë¡ 
    VIDEO_PROCESSING = "video_processing" # ë¹„ë””ì˜¤ ì²˜ë¦¬

@dataclass
class ResourceStatus:
    """ë¦¬ì†ŒìŠ¤ ìƒíƒœ"""
    device_type: str  # "cpu" or "gpu"
    total_memory_gb: float
    available_memory_gb: float
    usage_percent: float
    temperature: Optional[float] = None
    power_usage: Optional[float] = None

@dataclass
class TaskSchedule:
    """ì‘ì—… ìŠ¤ì¼€ì¤„"""
    task_type: TaskType
    estimated_time: float
    memory_requirement: float
    preferred_device: str
    fallback_device: str

class HybridComputeManager:
    """í•˜ì´ë¸Œë¦¬ë“œ ì»´í“¨íŒ… ê´€ë¦¬ì"""
    
    def __init__(self, default_mode: ComputeMode = ComputeMode.AUTO):
        """ì´ˆê¸°í™”"""
        self.mode = default_mode
        self.gpu_available = self._check_gpu_availability()
        self.cpu_cores = psutil.cpu_count()
        self.total_memory = psutil.virtual_memory().total / (1024**3)
        
        # ì„±ëŠ¥ íˆìŠ¤í† ë¦¬ ì €ì¥
        self.performance_history = {}
        self.current_tasks = {}
        
        # ë™ì  ì„ê³„ê°’
        self.gpu_memory_threshold = 0.8  # 80% ì´ìƒ ì‚¬ìš©ì‹œ CPU í´ë°±
        self.cpu_usage_threshold = 80    # 80% ì´ìƒ ì‚¬ìš©ì‹œ GPU í™œìš©
        
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        self.logger.info(f"HybridComputeManager ì´ˆê¸°í™” ì™„ë£Œ")
        self.logger.info(f"GPU ì‚¬ìš© ê°€ëŠ¥: {self.gpu_available}")
        self.logger.info(f"CPU ì½”ì–´: {self.cpu_cores}ê°œ")
        self.logger.info(f"ì´ ë©”ëª¨ë¦¬: {self.total_memory:.1f}GB")
    
    def _check_gpu_availability(self) -> bool:
        """GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        if not TORCH_AVAILABLE:
            return False
            
        try:
            # CUDA ì‚¬ìš© ê°€ëŠ¥ì„± í™•ì¸
            if torch.cuda.is_available():
                # GPU ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸
                device = torch.device('cuda')
                test_tensor = torch.ones(100, device=device)
                del test_tensor
                torch.cuda.empty_cache()
                return True
        except Exception as e:
            self.logger.warning(f"GPU í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        return False
    
    def get_resource_status(self) -> Dict[str, ResourceStatus]:
        """í˜„ì¬ ë¦¬ì†ŒìŠ¤ ìƒíƒœ ë°˜í™˜"""
        status = {}
        
        # CPU ìƒíƒœ
        cpu_memory = psutil.virtual_memory()
        status['cpu'] = ResourceStatus(
            device_type="cpu",
            total_memory_gb=cpu_memory.total / (1024**3),
            available_memory_gb=cpu_memory.available / (1024**3),
            usage_percent=cpu_memory.percent
        )
        
        # GPU ìƒíƒœ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
        if self.gpu_available and TORCH_AVAILABLE:
            try:
                gpu_memory_total = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                gpu_memory_allocated = torch.cuda.memory_allocated(0) / (1024**3)
                gpu_memory_available = gpu_memory_total - gpu_memory_allocated
                usage_percent = (gpu_memory_allocated / gpu_memory_total) * 100
                
                # NVIDIA MLì„ í†µí•œ ì˜¨ë„ ì •ë³´ (ì„ íƒì )
                temperature = None
                if NVIDIA_ML_AVAILABLE:
                    try:
                        handle = nvml.nvmlDeviceGetHandleByIndex(0)
                        temperature = nvml.nvmlDeviceGetTemperature(handle, nvml.NVML_TEMPERATURE_GPU)
                    except:
                        pass
                
                status['gpu'] = ResourceStatus(
                    device_type="gpu",
                    total_memory_gb=gpu_memory_total,
                    available_memory_gb=gpu_memory_available,
                    usage_percent=usage_percent,
                    temperature=temperature
                )
            except Exception as e:
                self.logger.warning(f"GPU ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
        
        return status
    
    def select_optimal_device(self, task_type: TaskType, memory_requirement: float = 1.0) -> str:
        """ì‘ì—…ì— ìµœì ì¸ ë””ë°”ì´ìŠ¤ ì„ íƒ"""
        
        if self.mode == ComputeMode.CPU_ONLY:
            return "cpu"
        elif self.mode == ComputeMode.GPU_ONLY:
            if self.gpu_available:
                return "gpu"
            else:
                self.logger.warning("GPU ì „ìš© ëª¨ë“œì´ì§€ë§Œ GPU ì‚¬ìš© ë¶ˆê°€, CPUë¡œ í´ë°±")
                return "cpu"
        
        # í˜„ì¬ ë¦¬ì†ŒìŠ¤ ìƒíƒœ í™•ì¸
        resource_status = self.get_resource_status()
        
        # ì‘ì—… ìœ í˜•ë³„ ìµœì í™” ì „ëµ
        task_preferences = {
            TaskType.STT_SMALL: ("cpu", "gpu"),      # ì‘ì€ ìŒì„±ì€ CPUê°€ íš¨ìœ¨ì 
            TaskType.STT_LARGE: ("gpu", "cpu"),      # í° ìŒì„±ì€ GPUê°€ íš¨ìœ¨ì 
            TaskType.OCR_BATCH: ("gpu", "cpu"),      # ë°°ì¹˜ OCRì€ GPU ìµœì 
            TaskType.OCR_REALTIME: ("cpu", "gpu"),   # ì‹¤ì‹œê°„ OCRì€ CPUê°€ ë¹ ë¦„
            TaskType.LLM_INFERENCE: ("gpu", "cpu"),  # LLMì€ GPU ìš°ì„ 
            TaskType.VIDEO_PROCESSING: ("gpu", "cpu") # ë¹„ë””ì˜¤ëŠ” GPU í•„ìˆ˜
        }
        
        preferred, fallback = task_preferences.get(task_type, ("cpu", "gpu"))
        
        # AUTO ëª¨ë“œ ë¡œì§
        if self.mode == ComputeMode.AUTO:
            # GPU ì‚¬ìš© ê°€ëŠ¥í•˜ê³  ë©”ëª¨ë¦¬ ì¶©ë¶„í•œ ê²½ìš°
            if (self.gpu_available and 
                preferred == "gpu" and 
                "gpu" in resource_status and
                resource_status["gpu"].available_memory_gb >= memory_requirement):
                
                # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ì²´í¬
                if resource_status["gpu"].usage_percent < self.gpu_memory_threshold * 100:
                    return "gpu"
            
            # CPU ì‚¬ìš©ë¥  ì²´í¬
            if resource_status["cpu"].usage_percent < self.cpu_usage_threshold:
                return "cpu"
            
            # ë‘˜ ë‹¤ ë°”ìœ ê²½ìš°, ë©”ëª¨ë¦¬ ì—¬ìœ ê°€ ë§ì€ ìª½ ì„ íƒ
            if "gpu" in resource_status:
                gpu_memory_ratio = resource_status["gpu"].available_memory_gb / resource_status["gpu"].total_memory_gb
                cpu_memory_ratio = resource_status["cpu"].available_memory_gb / resource_status["cpu"].total_memory_gb
                
                if gpu_memory_ratio > cpu_memory_ratio and self.gpu_available:
                    return "gpu"
            
            return "cpu"
        
        # GPU_PREFERRED ëª¨ë“œ
        elif self.mode == ComputeMode.GPU_PREFERRED:
            if (self.gpu_available and 
                "gpu" in resource_status and
                resource_status["gpu"].available_memory_gb >= memory_requirement and
                resource_status["gpu"].usage_percent < 90):
                return "gpu"
            return "cpu"
        
        # CPU_PREFERRED ëª¨ë“œ
        elif self.mode == ComputeMode.CPU_PREFERRED:
            if (resource_status["cpu"].usage_percent < 70 and
                resource_status["cpu"].available_memory_gb >= memory_requirement):
                return "cpu"
            elif self.gpu_available and "gpu" in resource_status:
                return "gpu"
            return "cpu"
        
        return "cpu"  # ê¸°ë³¸ê°’
    
    def optimize_for_task(self, task_type: TaskType, **kwargs) -> Dict[str, Any]:
        """ì‘ì—…ë³„ ìµœì í™” ì„¤ì • ë°˜í™˜"""
        
        device = self.select_optimal_device(task_type, kwargs.get('memory_requirement', 1.0))
        
        optimization_configs = {
            TaskType.STT_SMALL: {
                "device": device,
                "batch_size": 1,
                "fp16": device == "gpu",
                "num_workers": 2 if device == "cpu" else 1
            },
            TaskType.STT_LARGE: {
                "device": device,
                "batch_size": 4 if device == "gpu" else 1,
                "fp16": device == "gpu",
                "chunk_size": 30 if device == "gpu" else 15
            },
            TaskType.OCR_BATCH: {
                "device": device,
                "gpu": device == "gpu",
                "batch_size": 8 if device == "gpu" else 4,
                "parallel": device == "cpu"
            },
            TaskType.OCR_REALTIME: {
                "device": device,
                "gpu": False,  # ì‹¤ì‹œê°„ì€ CPUê°€ ë” ì•ˆì •ì 
                "parallel": True
            },
            TaskType.LLM_INFERENCE: {
                "device": device,
                "fp16": device == "gpu",
                "max_length": 2048 if device == "gpu" else 1024
            },
            TaskType.VIDEO_PROCESSING: {
                "device": device,
                "gpu": device == "gpu",
                "batch_size": 2 if device == "gpu" else 1
            }
        }
        
        config = optimization_configs.get(task_type, {"device": device})
        
        # í™˜ê²½ë³€ìˆ˜ ì„¤ì •
        if device == "cpu":
            os.environ["CUDA_VISIBLE_DEVICES"] = ""
        else:
            if "CUDA_VISIBLE_DEVICES" in os.environ and os.environ["CUDA_VISIBLE_DEVICES"] == "":
                del os.environ["CUDA_VISIBLE_DEVICES"]
        
        self.logger.info(f"ì‘ì—… '{task_type.value}'ì— ëŒ€í•´ '{device}' ë””ë°”ì´ìŠ¤ ì„ íƒë¨")
        
        return config
    
    def cleanup_memory(self, device: str = "both"):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        if device in ["both", "cpu"]:
            gc.collect()
        
        if device in ["both", "gpu"] and self.gpu_available and TORCH_AVAILABLE:
            try:
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
            except Exception as e:
                self.logger.warning(f"GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def get_performance_recommendation(self) -> Dict[str, str]:
        """ì„±ëŠ¥ ìµœì í™” ê¶Œì¥ì‚¬í•­"""
        status = self.get_resource_status()
        recommendations = []
        
        # CPU ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        if status["cpu"].usage_percent > 80:
            recommendations.append("CPU ì‚¬ìš©ë¥ ì´ ë†’ìŠµë‹ˆë‹¤. GPU í™œìš©ì„ ê³ ë ¤í•˜ì„¸ìš”.")
        
        if status["cpu"].available_memory_gb < 2:
            recommendations.append("ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ë°°ì¹˜ í¬ê¸°ë¥¼ ì¤„ì´ì„¸ìš”.")
        
        # GPU ê¸°ë°˜ ê¶Œì¥ì‚¬í•­ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
        if "gpu" in status:
            if status["gpu"].usage_percent > 85:
                recommendations.append("GPU ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. CPU í´ë°±ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
            elif status["gpu"].usage_percent < 30 and self.gpu_available:
                recommendations.append("GPU ë¦¬ì†ŒìŠ¤ê°€ ì¶©ë¶„í•©ë‹ˆë‹¤. GPU í™œìš©ì„ ëŠ˜ë ¤ë³´ì„¸ìš”.")
        
        # ëª¨ë“œ ê¶Œì¥ì‚¬í•­
        mode_recommendations = {
            "light_tasks": "ì‹¤ì‹œê°„ ì²˜ë¦¬ì—ëŠ” CPU_PREFERRED ëª¨ë“œë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.",
            "heavy_tasks": "ëŒ€ìš©ëŸ‰ ì²˜ë¦¬ì—ëŠ” GPU_PREFERRED ëª¨ë“œë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.",
            "mixed_tasks": "ë‹¤ì–‘í•œ ì‘ì—…ì—ëŠ” AUTO ëª¨ë“œë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤."
        }
        
        return {
            "general": recommendations,
            "mode": mode_recommendations
        }

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
_global_manager = None

def get_compute_manager() -> HybridComputeManager:
    """ì „ì—­ í•˜ì´ë¸Œë¦¬ë“œ ì»´í“¨íŒ… ë§¤ë‹ˆì € ë°˜í™˜"""
    global _global_manager
    if _global_manager is None:
        _global_manager = HybridComputeManager()
    return _global_manager

def auto_optimize_for_whisper(audio_duration: float = 60) -> Dict[str, Any]:
    """Whisper STT ìë™ ìµœì í™”"""
    manager = get_compute_manager()
    task_type = TaskType.STT_LARGE if audio_duration > 300 else TaskType.STT_SMALL
    return manager.optimize_for_task(task_type, memory_requirement=audio_duration/60)

def auto_optimize_for_ocr(image_count: int = 1, realtime: bool = False) -> Dict[str, Any]:
    """OCR ìë™ ìµœì í™”"""
    manager = get_compute_manager()
    task_type = TaskType.OCR_REALTIME if realtime else TaskType.OCR_BATCH
    return manager.optimize_for_task(task_type, memory_requirement=image_count * 0.1)

def auto_optimize_for_llm(context_length: int = 1024) -> Dict[str, Any]:
    """LLM ì¶”ë¡  ìë™ ìµœì í™”"""
    manager = get_compute_manager()
    return manager.optimize_for_task(TaskType.LLM_INFERENCE, memory_requirement=context_length/1024)

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    manager = HybridComputeManager()
    
    print("=== í•˜ì´ë¸Œë¦¬ë“œ ì»´í“¨íŒ… ë§¤ë‹ˆì € í…ŒìŠ¤íŠ¸ ===")
    
    # ë¦¬ì†ŒìŠ¤ ìƒíƒœ ì¶œë ¥
    status = manager.get_resource_status()
    for device, info in status.items():
        print(f"{device.upper()}: {info.available_memory_gb:.1f}GB ì‚¬ìš© ê°€ëŠ¥ ({info.usage_percent:.1f}% ì‚¬ìš© ì¤‘)")
    
    # ì‘ì—…ë³„ ìµœì í™” í…ŒìŠ¤íŠ¸
    test_tasks = [
        (TaskType.STT_SMALL, "ì§§ì€ ìŒì„± ì²˜ë¦¬"),
        (TaskType.OCR_BATCH, "ë°°ì¹˜ ì´ë¯¸ì§€ OCR"),
        (TaskType.LLM_INFERENCE, "LLM ì¶”ë¡ ")
    ]
    
    print("\n=== ì‘ì—…ë³„ ìµœì í™” ê²°ê³¼ ===")
    for task_type, description in test_tasks:
        config = manager.optimize_for_task(task_type)
        print(f"{description}: {config['device']} ë””ë°”ì´ìŠ¤ ì„ íƒë¨")
    
    # ì„±ëŠ¥ ê¶Œì¥ì‚¬í•­
    recommendations = manager.get_performance_recommendation()
    print("\n=== ì„±ëŠ¥ ê¶Œì¥ì‚¬í•­ ===")
    for rec in recommendations["general"]:
        print(f"- {rec}")