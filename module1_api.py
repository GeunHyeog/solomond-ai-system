#!/usr/bin/env python3
"""
ğŸ¯ Module1 API ì„œë¹„ìŠ¤
ê¸°ì¡´ conference_analysis.pyë¥¼ FastAPIë¡œ ë˜í•‘
"""

from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.responses import JSONResponse
from typing import List
import sys
import os
import tempfile
import json
from datetime import datetime

# í˜„ì¬ í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

app = FastAPI(
    title="Module1: Conference Analysis API",
    description="ì»¨í¼ëŸ°ìŠ¤ ë¶„ì„ ëª¨ë“ˆ API",
    version="1.0.0"
)

# Ollama ì¸í„°í˜ì´ìŠ¤ ë¡œë“œ
try:
    from shared.ollama_interface import OllamaInterface
    ollama = OllamaInterface()
    OLLAMA_AVAILABLE = True
    print(f"âœ… Ollama ì—°ê²° ì„±ê³µ: {len(ollama.available_models)}ê°œ ëª¨ë¸")
except Exception as e:
    OLLAMA_AVAILABLE = False
    print(f"âŒ Ollama ì—°ê²° ì‹¤íŒ¨: {e}")

@app.get("/")
async def root():
    """ì„œë¹„ìŠ¤ ì •ë³´"""
    return {
        "service": "Module1 - Conference Analysis",
        "version": "1.0.0",
        "status": "online",
        "ollama_available": OLLAMA_AVAILABLE,
        "models": ollama.available_models if OLLAMA_AVAILABLE else [],
        "endpoints": [
            "POST /analyze - íŒŒì¼ ë¶„ì„",
            "GET /health - ìƒíƒœ í™•ì¸"
        ]
    }

@app.get("/health")
async def health_check():
    """ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
    return {
        "status": "healthy",
        "ollama": "available" if OLLAMA_AVAILABLE else "unavailable",
        "timestamp": datetime.now().isoformat()
    }

@app.post("/analyze")
async def analyze_files(files: List[UploadFile] = File(...)):
    """íŒŒì¼ ë¶„ì„ - ê¸°ì¡´ ë¡œì§ í™œìš©"""
    
    if not OLLAMA_AVAILABLE:
        raise HTTPException(status_code=503, detail="Ollama AI ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    if not files:
        raise HTTPException(status_code=400, detail="ë¶„ì„í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
    
    try:
        results = {
            "analysis_id": f"analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            "timestamp": datetime.now().isoformat(),
            "files_count": len(files),
            "files_analyzed": [],
            "comprehensive_summary": {},
            "ai_analysis": {}
        }
        
        # ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        with tempfile.TemporaryDirectory() as temp_dir:
            saved_files = []
            
            # íŒŒì¼ ì €ì¥
            for file in files:
                temp_path = os.path.join(temp_dir, file.filename)
                content = await file.read()
                with open(temp_path, 'wb') as f:
                    f.write(content)
                saved_files.append({
                    "filename": file.filename,
                    "size": len(content),
                    "content_type": file.content_type,
                    "path": temp_path
                })
            
            # íŒŒì¼ë³„ ë¶„ì„
            for file_info in saved_files:
                file_result = await analyze_single_file(file_info)
                results["files_analyzed"].append(file_result)
            
            # ì¢…í•© ë¶„ì„ (AI í™œìš©)
            if results["files_analyzed"]:
                comprehensive = await generate_comprehensive_analysis(results["files_analyzed"])
                results["comprehensive_summary"] = comprehensive
                
                # AI ë¶„ì„ ì¶”ê°€
                ai_analysis = await generate_ai_analysis(results["files_analyzed"])
                results["ai_analysis"] = ai_analysis
        
        return JSONResponse(content=results)
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë¶„ì„ ì‹¤íŒ¨: {str(e)}")

async def analyze_single_file(file_info: dict) -> dict:
    """ë‹¨ì¼ íŒŒì¼ ë¶„ì„"""
    filename = file_info["filename"]
    file_path = file_info["path"]
    
    result = {
        "filename": filename,
        "content_type": file_info["content_type"],
        "size": file_info["size"],
        "analysis_type": "unknown",
        "content": "",
        "metadata": {}
    }
    
    try:
        # íŒŒì¼ í™•ì¥ìì— ë”°ë¥¸ ë¶„ì„
        ext = os.path.splitext(filename)[1].lower()
        
        if ext in ['.txt', '.md']:
            # í…ìŠ¤íŠ¸ íŒŒì¼ ë¶„ì„
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            result.update({
                "analysis_type": "text",
                "content": content[:1000] + "..." if len(content) > 1000 else content,
                "character_count": len(content),
                "line_count": len(content.split('\n'))
            })
            
        elif ext in ['.jpg', '.jpeg', '.png', '.bmp']:
            # ì´ë¯¸ì§€ íŒŒì¼ ë¶„ì„ (OCR ì‹œë®¬ë ˆì´ì…˜)
            result.update({
                "analysis_type": "image",
                "content": f"ì´ë¯¸ì§€ íŒŒì¼ ê°ì§€: {filename}",
                "metadata": {"width": "unknown", "height": "unknown"}
            })
            
        elif ext in ['.mp3', '.wav', '.m4a']:
            # ì˜¤ë””ì˜¤ íŒŒì¼ ë¶„ì„ (STT ì‹œë®¬ë ˆì´ì…˜)
            result.update({
                "analysis_type": "audio", 
                "content": f"ì˜¤ë””ì˜¤ íŒŒì¼ ê°ì§€: {filename}",
                "metadata": {"duration": "unknown", "format": ext}
            })
            
        elif ext in ['.mp4', '.avi', '.mov']:
            # ë¹„ë””ì˜¤ íŒŒì¼ ë¶„ì„
            result.update({
                "analysis_type": "video",
                "content": f"ë¹„ë””ì˜¤ íŒŒì¼ ê°ì§€: {filename}",
                "metadata": {"duration": "unknown", "resolution": "unknown"}
            })
        
        else:
            result["content"] = f"ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {ext}"
    
    except Exception as e:
        result["error"] = str(e)
    
    return result

async def generate_comprehensive_analysis(file_results: List[dict]) -> dict:
    """ì¢…í•© ë¶„ì„ ìƒì„±"""
    
    total_files = len(file_results)
    file_types = {}
    total_content = []
    
    for result in file_results:
        analysis_type = result.get("analysis_type", "unknown")
        file_types[analysis_type] = file_types.get(analysis_type, 0) + 1
        
        if result.get("content"):
            total_content.append(f"{result['filename']}: {result['content'][:200]}")
    
    return {
        "total_files": total_files,
        "file_types_distribution": file_types,
        "content_preview": total_content[:5],  # ìµœëŒ€ 5ê°œ íŒŒì¼ ë¯¸ë¦¬ë³´ê¸°
        "analysis_timestamp": datetime.now().isoformat()
    }

async def generate_ai_analysis(file_results: List[dict]) -> dict:
    """AI ê¸°ë°˜ ì¢…í•© ë¶„ì„"""
    
    if not OLLAMA_AVAILABLE:
        return {"error": "AI ë¶„ì„ ë¶ˆê°€ - Ollama ì—°ê²° ì—†ìŒ"}
    
    try:
        # í…ìŠ¤íŠ¸ ë‚´ìš© í†µí•©
        combined_content = ""
        for result in file_results:
            if result.get("content") and result.get("analysis_type") == "text":
                combined_content += f"\n{result['filename']}:\n{result['content']}\n"
        
        if not combined_content.strip():
            return {"message": "í…ìŠ¤íŠ¸ ë‚´ìš©ì´ ì—†ì–´ AI ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}
        
        # AI ë¶„ì„ ì‹¤í–‰
        analysis_prompt = f"""ë‹¤ìŒ íŒŒì¼ ë‚´ìš©ë“¤ì„ ë¶„ì„í•´ì£¼ì„¸ìš”:

{combined_content[:2000]}

ë¶„ì„ í•­ëª©:
1. ì£¼ìš” ì£¼ì œ
2. í•µì‹¬ ë‚´ìš© ìš”ì•½
3. ì¤‘ìš” í‚¤ì›Œë“œ
4. ì „ì²´ì ì¸ ë§¥ë½"""

        ai_response = ollama.generate_response(
            prompt=analysis_prompt,
            model="qwen2.5:7b"
        )
        
        return {
            "ai_summary": ai_response[:500] + "..." if len(ai_response) > 500 else ai_response,
            "model_used": "qwen2.5:7b",
            "analysis_timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        return {"error": f"AI ë¶„ì„ ì‹¤íŒ¨: {str(e)}"}

if __name__ == "__main__":
    import uvicorn
    print("ğŸ¯ Module1 API ì„œë¹„ìŠ¤ ì‹œì‘...")
    print("ğŸ“ ì„œë¹„ìŠ¤: http://localhost:8001")
    print("ğŸ“š API ë¬¸ì„œ: http://localhost:8001/docs")
    
    uvicorn.run(app, host="0.0.0.0", port=8001, reload=True)