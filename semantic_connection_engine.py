#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ§  ì˜ë¯¸ì  ì—°ê²° ì—”ì§„
Semantic Connection Engine for Holistic Conference Analysis

í•µì‹¬ ëª©í‘œ: íŒŒì¼ ê°„ ì˜ë¯¸ì  ì—°ê´€ì„± ìë™ íƒì§€ ë° ì—°ê²°
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Any, Optional, Tuple
import streamlit as st
from dataclasses import dataclass
import sqlite3
import json
from pathlib import Path
import networkx as nx
from collections import defaultdict, Counter
import re

try:
    from sentence_transformers import SentenceTransformer
    import faiss
    from sklearn.cluster import KMeans, DBSCAN
    from sklearn.metrics.pairwise import cosine_similarity
    import spacy
    ADVANCED_NLP_AVAILABLE = True
except ImportError:
    ADVANCED_NLP_AVAILABLE = False
    st.warning("âš ï¸ ê³ ê¸‰ NLP ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. pip install sentence-transformers faiss-cpu spacy scikit-learn")

@dataclass
class SemanticConnection:
    """ì˜ë¯¸ì  ì—°ê²° ë°ì´í„° êµ¬ì¡°"""
    connection_id: str
    source_fragment: str
    target_fragment: str
    connection_type: str  # semantic, speaker, temporal, topical
    confidence: float
    evidence: str
    relationship: str  # similar, continues, contradicts, supports

@dataclass
class SpeakerProfile:
    """ë°œí‘œì í”„ë¡œí•„"""
    speaker_id: str
    name: str
    fragments: List[str]
    key_topics: List[str]
    speaking_style: Dict[str, Any]
    total_mentions: int

@dataclass
class TopicCluster:
    """ì£¼ì œ í´ëŸ¬ìŠ¤í„°"""
    cluster_id: str
    cluster_name: str
    fragments: List[str]
    key_keywords: List[str]
    centrality_score: float
    internal_coherence: float

class SemanticConnectionEngine:
    """ì˜ë¯¸ì  ì—°ê²° ì—”ì§„"""
    
    def __init__(self, conference_name: str = "default"):
        self.conference_name = conference_name
        self.db_path = f"conference_analysis_{conference_name}.db"
        
        # ê³ ê¸‰ NLP ëª¨ë¸ ì´ˆê¸°í™”
        if ADVANCED_NLP_AVAILABLE:
            try:
                # ë¬¸ì¥ ì„ë² ë”© ëª¨ë¸ (ë‹¤êµ­ì–´ ì§€ì›)
                self.embedder = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
                
                # FAISS ë²¡í„° ì¸ë±ìŠ¤
                self.vector_index = None
                self.fragment_embeddings = {}
                
                # SpaCy NLP íŒŒì´í”„ë¼ì¸ (í•œêµ­ì–´/ì˜ì–´)
                try:
                    # í•œêµ­ì–´ ëª¨ë¸ ì‹œë„
                    self.nlp = spacy.load("ko_core_news_sm")
                except OSError:
                    try:
                        # ì˜ì–´ ëª¨ë¸ ì‹œë„
                        self.nlp = spacy.load("en_core_web_sm")
                    except OSError:
                        # ê¸°ë³¸ ë¹ˆ ëª¨ë¸
                        self.nlp = spacy.blank("ko")
                        st.warning("âš ï¸ SpaCy ì–¸ì–´ ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ë³¸ ê¸°ëŠ¥ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤.")
                
                self.use_advanced_nlp = True
                
            except Exception as e:
                st.error(f"ê³ ê¸‰ NLP ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.use_advanced_nlp = False
        else:
            self.use_advanced_nlp = False
        
        # ì—°ê²° ê·¸ë˜í”„
        self.connection_graph = nx.Graph()
        self.connections: List[SemanticConnection] = []
        self.speaker_profiles: Dict[str, SpeakerProfile] = {}
        self.topic_clusters: List[TopicCluster] = []
    
    def load_fragments_from_db(self) -> List[Dict]:
        """ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¡°ê°ë“¤ ë¡œë“œ"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            cursor.execute('SELECT * FROM fragments ORDER BY created_at')
            rows = cursor.fetchall()
            
            fragments = []
            for row in rows:
                fragment = {
                    'fragment_id': row[0],
                    'file_source': row[1],
                    'file_type': row[2],
                    'timestamp': row[3],
                    'speaker': row[4],
                    'content': row[5],
                    'confidence': row[6],
                    'keywords': json.loads(row[7]) if row[7] else [],
                    'embedding': json.loads(row[8]) if row[8] else None
                }
                fragments.append(fragment)
            
            return fragments
            
        except sqlite3.OperationalError:
            st.warning("âš ï¸ ë¶„ì„ëœ ì»¨í¼ëŸ°ìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì»¨í¼ëŸ°ìŠ¤ ë¶„ì„ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
            return []
        finally:
            conn.close()
    
    def analyze_semantic_connections(self) -> Dict[str, Any]:
        """ì˜ë¯¸ì  ì—°ê²° ë¶„ì„ ì‹¤í–‰"""
        fragments = self.load_fragments_from_db()
        
        if not fragments:
            return {"error": "ë¶„ì„í•  ì¡°ê°ì´ ì—†ìŠµë‹ˆë‹¤."}
        
        if len(fragments) < 2:
            return {"error": "ì—°ê²° ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ìµœì†Œ 2ê°œì˜ ì¡°ê°ì´ í•„ìš”í•©ë‹ˆë‹¤."}
        
        # 1. ì„ë² ë”© ìƒì„± ë° ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶•
        self._build_vector_index(fragments)
        
        # 2. ì˜ë¯¸ì  ìœ ì‚¬ì„± ì—°ê²°
        semantic_connections = self._find_semantic_connections(fragments)
        
        # 3. ë°œí‘œìë³„ ì—°ê²°
        speaker_connections = self._find_speaker_connections(fragments)
        
        # 4. ì£¼ì œë³„ í´ëŸ¬ìŠ¤í„°ë§
        topic_clusters = self._cluster_by_topics(fragments)
        
        # 5. ì‹œê°„ì  ì—°ê²° (ìˆœì„œ ê¸°ë°˜)
        temporal_connections = self._find_temporal_connections(fragments)
        
        # 6. ì—°ê²° ê·¸ë˜í”„ êµ¬ì¶•
        self._build_connection_graph(fragments)
        
        # 7. í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ
        insights = self._extract_connection_insights()
        
        return {
            "total_fragments": len(fragments),
            "semantic_connections": len(semantic_connections),
            "speaker_connections": len(speaker_connections),
            "topic_clusters": len(topic_clusters),
            "temporal_connections": len(temporal_connections),
            "connection_insights": insights,
            "speaker_profiles": {k: {
                "name": v.name,
                "fragments_count": len(v.fragments),
                "key_topics": v.key_topics[:5],
                "total_mentions": v.total_mentions
            } for k, v in self.speaker_profiles.items()},
            "topic_clusters_summary": [{
                "cluster_name": tc.cluster_name,
                "fragments_count": len(tc.fragments),
                "key_keywords": tc.key_keywords[:5],
                "coherence": tc.internal_coherence
            } for tc in topic_clusters]
        }
    
    def _build_vector_index(self, fragments: List[Dict]):
        """ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶•"""
        if not self.use_advanced_nlp:
            return
        
        contents = []
        fragment_ids = []
        
        for fragment in fragments:
            if fragment['content'] and len(fragment['content'].strip()) > 0:
                contents.append(fragment['content'])
                fragment_ids.append(fragment['fragment_id'])
        
        if not contents:
            return
        
        try:
            # ì„ë² ë”© ìƒì„±
            embeddings = self.embedder.encode(contents, show_progress_bar=True)
            
            # ì°¨ì› í™•ì¸
            if len(embeddings.shape) != 2:
                st.error("ì„ë² ë”© ì°¨ì›ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                return
            
            # FAISS ì¸ë±ìŠ¤ êµ¬ì¶•
            dimension = embeddings.shape[1]
            self.vector_index = faiss.IndexFlatIP(dimension)  # Inner Product (cosine similarity)
            
            # L2 ì •ê·œí™” (cosine similarityë¥¼ ìœ„í•´) - ì•ˆì „í•œ ë³µì‚¬ë³¸ ì‚¬ìš©
            embeddings_copy = embeddings.copy().astype('float32')
            
            # ë²¡í„°ê°€ ìœ íš¨í•œì§€ í™•ì¸
            if not np.all(np.isfinite(embeddings_copy)):
                st.warning("ì¼ë¶€ ì„ë² ë”© ë²¡í„°ì— ë¬´í•œê°’ì´ë‚˜ NaNì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì •ë¦¬ ì¤‘...")
                embeddings_copy = np.nan_to_num(embeddings_copy, nan=0.0, posinf=1.0, neginf=-1.0)
            
            faiss.normalize_L2(embeddings_copy)
            self.vector_index.add(embeddings_copy)
            
            # ì„ë² ë”© ì €ì¥
            for i, fragment_id in enumerate(fragment_ids):
                self.fragment_embeddings[fragment_id] = embeddings[i].tolist()
            
            st.success(f"âœ… {len(contents)}ê°œ ì¡°ê°ì˜ ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ")
            
        except Exception as e:
            st.error(f"ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨: {e}")
            # ë””ë²„ê·¸ ì •ë³´ ì¶”ê°€
            st.error(f"ë””ë²„ê·¸ ì •ë³´: contents={len(contents)}, embeddings shape={embeddings.shape if 'embeddings' in locals() else 'N/A'}")
    
    def _find_semantic_connections(self, fragments: List[Dict]) -> List[SemanticConnection]:
        """ì˜ë¯¸ì  ìœ ì‚¬ì„± ê¸°ë°˜ ì—°ê²° íƒì§€"""
        connections = []
        
        if not self.use_advanced_nlp or not self.vector_index:
            return connections
        
        try:
            # ê° ì¡°ê°ì— ëŒ€í•´ ìœ ì‚¬í•œ ì¡°ê°ë“¤ ì°¾ê¸°
            for i, fragment in enumerate(fragments):
                if fragment['fragment_id'] not in self.fragment_embeddings:
                    continue
                
                try:
                    embedding = np.array(self.fragment_embeddings[fragment['fragment_id']]).reshape(1, -1).astype('float32')
                    
                    # ë²¡í„° ìœ íš¨ì„± ê²€ì‚¬
                    if not np.all(np.isfinite(embedding)):
                        st.warning(f"ì¡°ê° {fragment['fragment_id']}ì˜ ì„ë² ë”©ì— ìœ íš¨í•˜ì§€ ì•Šì€ ê°’ì´ ìˆìŠµë‹ˆë‹¤. ê±´ë„ˆëœ€.")
                        continue
                    
                    embedding_copy = embedding.copy()
                    faiss.normalize_L2(embedding_copy)
                    
                    # ìœ ì‚¬í•œ ì¡°ê°ë“¤ ê²€ìƒ‰ (ìê¸° ìì‹  ì œì™¸)
                    k = min(5, len(fragments))  # ìƒìœ„ 5ê°œ
                    scores, indices = self.vector_index.search(embedding_copy, k)
                    
                    for j, (score, idx) in enumerate(zip(scores[0], indices[0])):
                        if idx != i and score > 0.7:  # ë†’ì€ ìœ ì‚¬ë„ë§Œ
                            target_fragment = fragments[idx]
                            
                            connection = SemanticConnection(
                                connection_id=f"sem_{fragment['fragment_id']}_{target_fragment['fragment_id']}",
                                source_fragment=fragment['fragment_id'],
                                target_fragment=target_fragment['fragment_id'],
                                connection_type="semantic",
                                confidence=float(score),
                                evidence=f"ì˜ë¯¸ì  ìœ ì‚¬ë„: {score:.3f}",
                                relationship="similar"
                            )
                            connections.append(connection)
                            
                except Exception as inner_e:
                    st.warning(f"ì¡°ê° {fragment['fragment_id']} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {inner_e}")
                    continue
            
            self.connections.extend(connections)
            return connections
            
        except Exception as e:
            st.error(f"ì˜ë¯¸ì  ì—°ê²° íƒì§€ ì‹¤íŒ¨: {e}")
            return []
    
    def _find_speaker_connections(self, fragments: List[Dict]) -> List[SemanticConnection]:
        """ë°œí‘œì ê¸°ë°˜ ì—°ê²° íƒì§€"""
        connections = []
        speaker_fragments = defaultdict(list)
        
        # ë°œí‘œìë³„ ì¡°ê° ê·¸ë£¹í•‘
        for fragment in fragments:
            speaker = fragment.get('speaker')
            if speaker and speaker.strip():
                speaker_fragments[speaker].append(fragment)
        
        # ë°œí‘œì í”„ë¡œí•„ ìƒì„±
        for speaker, speaker_frags in speaker_fragments.items():
            if len(speaker_frags) < 2:
                continue
            
            # ë°œí‘œìì˜ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
            all_keywords = []
            for frag in speaker_frags:
                all_keywords.extend(frag.get('keywords', []))
            
            key_topics = [kw for kw, count in Counter(all_keywords).most_common(5)]
            
            # ë°œí‘œì í”„ë¡œí•„ ìƒì„±
            speaker_id = f"speaker_{speaker.replace(' ', '_')}"
            profile = SpeakerProfile(
                speaker_id=speaker_id,
                name=speaker,
                fragments=[f['fragment_id'] for f in speaker_frags],
                key_topics=key_topics,
                speaking_style={},  # ì¶”í›„ í™•ì¥
                total_mentions=len(speaker_frags)
            )
            self.speaker_profiles[speaker_id] = profile
            
            # ê°™ì€ ë°œí‘œìì˜ ì¡°ê°ë“¤ ê°„ ì—°ê²°
            for i in range(len(speaker_frags)):
                for j in range(i + 1, len(speaker_frags)):
                    frag1, frag2 = speaker_frags[i], speaker_frags[j]
                    
                    connection = SemanticConnection(
                        connection_id=f"spk_{frag1['fragment_id']}_{frag2['fragment_id']}",
                        source_fragment=frag1['fragment_id'],
                        target_fragment=frag2['fragment_id'],
                        connection_type="speaker",
                        confidence=0.9,
                        evidence=f"ë™ì¼ ë°œí‘œì: {speaker}",
                        relationship="continues"
                    )
                    connections.append(connection)
        
        self.connections.extend(connections)
        return connections
    
    def _cluster_by_topics(self, fragments: List[Dict]) -> List[TopicCluster]:
        """ì£¼ì œë³„ í´ëŸ¬ìŠ¤í„°ë§"""
        if not self.use_advanced_nlp or not self.fragment_embeddings:
            return []
        
        try:
            # ì„ë² ë”© ë§¤íŠ¸ë¦­ìŠ¤ êµ¬ì„±
            embeddings = []
            fragment_ids = []
            
            for fragment in fragments:
                if fragment['fragment_id'] in self.fragment_embeddings:
                    embeddings.append(self.fragment_embeddings[fragment['fragment_id']])
                    fragment_ids.append(fragment['fragment_id'])
            
            if len(embeddings) < 3:
                return []
            
            embeddings = np.array(embeddings)
            
            # ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ê²°ì • (3~8ê°œ)
            max_clusters = min(8, len(embeddings) // 2)
            n_clusters = max(3, max_clusters)
            
            # K-means í´ëŸ¬ìŠ¤í„°ë§
            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
            cluster_labels = kmeans.fit_predict(embeddings)
            
            # í´ëŸ¬ìŠ¤í„°ë³„ ì¡°ê° ê·¸ë£¹í•‘
            clusters = defaultdict(list)
            for fragment_id, label in zip(fragment_ids, cluster_labels):
                clusters[label].append(fragment_id)
            
            # TopicCluster ê°ì²´ ìƒì„±
            topic_clusters = []
            for cluster_id, cluster_fragments in clusters.items():
                if len(cluster_fragments) < 2:
                    continue
                
                # í´ëŸ¬ìŠ¤í„°ì˜ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
                cluster_keywords = []
                for frag_id in cluster_fragments:
                    fragment = next(f for f in fragments if f['fragment_id'] == frag_id)
                    cluster_keywords.extend(fragment.get('keywords', []))
                
                key_keywords = [kw for kw, _ in Counter(cluster_keywords).most_common(5)]
                
                # í´ëŸ¬ìŠ¤í„° ë‚´ë¶€ ì¼ê´€ì„± ê³„ì‚°
                cluster_embeddings = [self.fragment_embeddings[fid] for fid in cluster_fragments]
                coherence = self._calculate_cluster_coherence(cluster_embeddings)
                
                cluster_name = f"ì£¼ì œí´ëŸ¬ìŠ¤í„°_{cluster_id}_{key_keywords[0] if key_keywords else 'unknown'}"
                
                topic_cluster = TopicCluster(
                    cluster_id=f"topic_{cluster_id}",
                    cluster_name=cluster_name,
                    fragments=cluster_fragments,
                    key_keywords=key_keywords,
                    centrality_score=0.0,  # ì¶”í›„ ê³„ì‚°
                    internal_coherence=coherence
                )
                topic_clusters.append(topic_cluster)
            
            # ì¼ê´€ì„± ìˆœìœ¼ë¡œ ì •ë ¬
            topic_clusters.sort(key=lambda tc: tc.internal_coherence, reverse=True)
            self.topic_clusters = topic_clusters
            
            return topic_clusters
            
        except Exception as e:
            st.error(f"ì£¼ì œ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨: {e}")
            return []
    
    def _calculate_cluster_coherence(self, embeddings: List[List[float]]) -> float:
        """í´ëŸ¬ìŠ¤í„° ë‚´ë¶€ ì¼ê´€ì„± ê³„ì‚°"""
        if len(embeddings) < 2:
            return 0.0
        
        try:
            embeddings_array = np.array(embeddings)
            similarities = cosine_similarity(embeddings_array)
            
            # ìê¸° ìì‹ ê³¼ì˜ ìœ ì‚¬ë„ ì œì™¸í•˜ê³  í‰ê·  ê³„ì‚°
            mask = ~np.eye(similarities.shape[0], dtype=bool)
            mean_similarity = similarities[mask].mean()
            
            return float(mean_similarity)
            
        except Exception:
            return 0.0
    
    def _find_temporal_connections(self, fragments: List[Dict]) -> List[SemanticConnection]:
        """ì‹œê°„ì /ìˆœì„œì  ì—°ê²° íƒì§€"""
        connections = []
        
        # íŒŒì¼ ì†ŒìŠ¤ë³„ë¡œ ê·¸ë£¹í•‘ (ê°™ì€ íŒŒì¼ì—ì„œ ë‚˜ì˜¨ ì¡°ê°ë“¤ì€ ìˆœì„œê°€ ìˆìŒ)
        file_groups = defaultdict(list)
        for fragment in fragments:
            file_groups[fragment['file_source']].append(fragment)
        
        for file_source, file_fragments in file_groups.items():
            if len(file_fragments) < 2:
                continue
            
            # íŒŒì¼ ë‚´ ì¡°ê°ë“¤ì„ ìˆœì„œëŒ€ë¡œ ì—°ê²°
            for i in range(len(file_fragments) - 1):
                current_frag = file_fragments[i]
                next_frag = file_fragments[i + 1]
                
                connection = SemanticConnection(
                    connection_id=f"temp_{current_frag['fragment_id']}_{next_frag['fragment_id']}",
                    source_fragment=current_frag['fragment_id'],
                    target_fragment=next_frag['fragment_id'],
                    connection_type="temporal",
                    confidence=0.8,
                    evidence=f"ë™ì¼ íŒŒì¼ ìˆœì„œ: {file_source}",
                    relationship="continues"
                )
                connections.append(connection)
        
        self.connections.extend(connections)
        return connections
    
    def _build_connection_graph(self, fragments: List[Dict]):
        """ì—°ê²° ê·¸ë˜í”„ êµ¬ì¶•"""
        # ë…¸ë“œ ì¶”ê°€ (ê° ì¡°ê°)
        for fragment in fragments:
            self.connection_graph.add_node(
                fragment['fragment_id'],
                content=fragment['content'][:100],  # ë¯¸ë¦¬ë³´ê¸°
                speaker=fragment.get('speaker', 'Unknown'),
                file_type=fragment['file_type'],
                confidence=fragment['confidence']
            )
        
        # ì—£ì§€ ì¶”ê°€ (ì—°ê²°)
        for connection in self.connections:
            self.connection_graph.add_edge(
                connection.source_fragment,
                connection.target_fragment,
                connection_type=connection.connection_type,
                confidence=connection.confidence,
                relationship=connection.relationship
            )
    
    def _extract_connection_insights(self) -> List[str]:
        """ì—°ê²° ë¶„ì„ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ"""
        insights = []
        
        if not self.connections:
            insights.append("ğŸ“­ ë°œê²¬ëœ ì—°ê²°ì´ ì—†ìŠµë‹ˆë‹¤.")
            return insights
        
        # ì—°ê²° íƒ€ì…ë³„ í†µê³„
        connection_types = Counter(conn.connection_type for conn in self.connections)
        insights.append(f"ğŸ”— ì´ {len(self.connections)}ê°œì˜ ì—°ê²°ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤")
        
        for conn_type, count in connection_types.most_common():
            type_name = {
                'semantic': 'ì˜ë¯¸ì  ì—°ê²°',
                'speaker': 'ë°œí‘œì ì—°ê²°', 
                'temporal': 'ì‹œê°„ì  ì—°ê²°',
                'topical': 'ì£¼ì œë³„ ì—°ê²°'
            }.get(conn_type, conn_type)
            insights.append(f"  - {type_name}: {count}ê°œ")
        
        # ê°€ì¥ ì—°ê²°ì´ ë§ì€ ì¡°ê°
        if self.connection_graph.nodes():
            degrees = dict(self.connection_graph.degree())
            max_degree_node = max(degrees, key=degrees.get)
            max_degree = degrees[max_degree_node]
            
            if max_degree > 0:
                insights.append(f"ğŸ¯ ê°€ì¥ ë§ì´ ì—°ê²°ëœ ì¡°ê°: {max_degree_node} ({max_degree}ê°œ ì—°ê²°)")
        
        # ë°œí‘œì ë¶„ì„
        if self.speaker_profiles:
            most_active_speaker = max(self.speaker_profiles.values(), key=lambda sp: sp.total_mentions)
            insights.append(f"ğŸ‘¤ ê°€ì¥ í™œë°œí•œ ë°œí‘œì: {most_active_speaker.name} ({most_active_speaker.total_mentions}ê°œ ì¡°ê°)")
        
        # ì£¼ì œ í´ëŸ¬ìŠ¤í„° ë¶„ì„
        if self.topic_clusters:
            best_cluster = max(self.topic_clusters, key=lambda tc: tc.internal_coherence)
            insights.append(f"ğŸ’¡ ê°€ì¥ ì¼ê´€ì„± ìˆëŠ” ì£¼ì œ: {best_cluster.cluster_name} (ì¼ê´€ì„±: {best_cluster.internal_coherence:.3f})")
        
        return insights

# Streamlit UI
def main():
    st.title("ğŸ§  ì˜ë¯¸ì  ì—°ê²° ì—”ì§„")
    st.markdown("**íŒŒì¼ ê°„ ì˜ë¯¸ì  ì—°ê´€ì„±ì„ ìë™ìœ¼ë¡œ íƒì§€í•˜ê³  ì—°ê²°í•©ë‹ˆë‹¤**")
    
    # ì‚¬ì´ë“œë°” ì„¤ì •
    st.sidebar.header("âš™ï¸ ì„¤ì •")
    conference_name = st.sidebar.text_input("ì»¨í¼ëŸ°ìŠ¤ ì´ë¦„", "my_conference")
    
    # ì—”ì§„ ì´ˆê¸°í™”
    engine = SemanticConnectionEngine(conference_name)
    
    if not ADVANCED_NLP_AVAILABLE:
        st.error("âš ï¸ ê³ ê¸‰ NLP ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        st.code("pip install sentence-transformers faiss-cpu spacy scikit-learn")
        return
    
    st.info("ğŸš€ ê³ ê¸‰ NLP ì‹œìŠ¤í…œì´ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤!")
    
    # ë¶„ì„ ì‹¤í–‰
    if st.button("ğŸ” ì˜ë¯¸ì  ì—°ê²° ë¶„ì„ ì‹¤í–‰"):
        with st.spinner("ì˜ë¯¸ì  ì—°ê²°ì„ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)"):
            result = engine.analyze_semantic_connections()
            
            if "error" not in result:
                st.success("âœ… ì˜ë¯¸ì  ì—°ê²° ë¶„ì„ ì™„ë£Œ!")
                
                # ê²°ê³¼ í‘œì‹œ
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    st.metric("ì „ì²´ ì¡°ê°", result["total_fragments"])
                    st.metric("ì˜ë¯¸ì  ì—°ê²°", result["semantic_connections"])
                
                with col2:
                    st.metric("ë°œí‘œì ì—°ê²°", result["speaker_connections"])
                    st.metric("ì£¼ì œ í´ëŸ¬ìŠ¤í„°", result["topic_clusters"])
                
                with col3:
                    st.metric("ì‹œê°„ì  ì—°ê²°", result["temporal_connections"])
                
                # ì¸ì‚¬ì´íŠ¸
                st.markdown("## ğŸ’¡ ì—°ê²° ë¶„ì„ ì¸ì‚¬ì´íŠ¸")
                for insight in result["connection_insights"]:
                    st.markdown(f"- {insight}")
                
                # ë°œí‘œì í”„ë¡œí•„
                if result["speaker_profiles"]:
                    st.markdown("## ğŸ‘¥ ë°œí‘œì í”„ë¡œí•„")
                    for speaker_id, profile in result["speaker_profiles"].items():
                        with st.expander(f"ğŸ¤ {profile['name']}"):
                            st.write(f"**ë°œì–¸ ì¡°ê°:** {profile['fragments_count']}ê°œ")
                            st.write(f"**ì–¸ê¸‰ íšŸìˆ˜:** {profile['total_mentions']}íšŒ")
                            st.write(f"**ì£¼ìš” ì£¼ì œ:** {', '.join(profile['key_topics'])}")
                
                # ì£¼ì œ í´ëŸ¬ìŠ¤í„°
                if result["topic_clusters_summary"]:
                    st.markdown("## ğŸ¯ ì£¼ì œ í´ëŸ¬ìŠ¤í„°")
                    for cluster in result["topic_clusters_summary"]:
                        with st.expander(f"ğŸ“Š {cluster['cluster_name']}"):
                            st.write(f"**ê´€ë ¨ ì¡°ê°:** {cluster['fragments_count']}ê°œ")
                            st.write(f"**ì£¼ìš” í‚¤ì›Œë“œ:** {', '.join(cluster['key_keywords'])}")
                            st.write(f"**ë‚´ë¶€ ì¼ê´€ì„±:** {cluster['coherence']:.3f}")
                
                # ìƒì„¸ ì •ë³´
                with st.expander("ğŸ“Š ìƒì„¸ ë¶„ì„ ì •ë³´"):
                    st.json(result)
            else:
                st.error(result["error"])
    
    st.markdown("---")
    st.markdown("**ğŸ’¡ ì‚¬ìš©ë²•:** ë¨¼ì € í™€ë¦¬ìŠ¤í‹± ì»¨í¼ëŸ°ìŠ¤ ë¶„ì„ê¸°ì—ì„œ ë°ì´í„°ë¥¼ ë¶„ì„í•œ í›„ ì´ ì‹œìŠ¤í…œì„ ì‹¤í–‰í•˜ì„¸ìš”.")

if __name__ == "__main__":
    main()