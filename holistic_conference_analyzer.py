#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ¯ í™€ë¦¬ìŠ¤í‹± ì»¨í¼ëŸ°ìŠ¤ ë¶„ì„ ì—”ì§„
Holistic Conference Analysis Engine for SOLOMOND AI

í•µì‹¬ ëª©í‘œ: ê°œë³„ íŒŒì¼ ë¶„ì„ â†’ ì „ì²´ ì»¨í¼ëŸ°ìŠ¤ ìƒí™©ì˜ ì…ì²´ì  ì´í•´
"""

import json
import sqlite3
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
import streamlit as st
from dataclasses import dataclass, asdict
import hashlib
import re
from collections import defaultdict, Counter

try:
    from sentence_transformers import SentenceTransformer
    import faiss
    ADVANCED_NLP_AVAILABLE = True
except ImportError:
    ADVANCED_NLP_AVAILABLE = False
    st.warning("âš ï¸ ê³ ê¸‰ NLP ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ë³¸ ë¶„ì„ ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")

@dataclass
class ConferenceFragment:
    """ì»¨í¼ëŸ°ìŠ¤ ì¡°ê° ë°ì´í„° êµ¬ì¡°"""
    fragment_id: str
    file_source: str
    file_type: str  # image, audio, video, text
    timestamp: Optional[str]
    speaker: Optional[str]
    content: str
    confidence: float
    keywords: List[str]
    embedding: Optional[List[float]] = None

@dataclass
class ConferenceEntity:
    """ì»¨í¼ëŸ°ìŠ¤ ê°œì²´ (ì‚¬ëŒ, íšŒì‚¬, ì œí’ˆ ë“±)"""
    entity_id: str
    entity_type: str  # person, company, product, concept
    name: str
    mentions: List[str]  # fragment_ids where mentioned
    relations: Dict[str, List[str]]  # relations to other entities

@dataclass
class ConferenceTopic:
    """ì»¨í¼ëŸ°ìŠ¤ ì£¼ì œ/í…Œë§ˆ"""
    topic_id: str
    topic_name: str
    keywords: List[str]
    fragments: List[str]  # fragment_ids
    importance_score: float
    sentiment: str  # positive, negative, neutral

class HolisticConferenceAnalyzer:
    """í™€ë¦¬ìŠ¤í‹± ì»¨í¼ëŸ°ìŠ¤ ë¶„ì„ ì—”ì§„"""
    
    def __init__(self, conference_name: str = "default"):
        self.conference_name = conference_name
        self.db_path = f"conference_analysis_{conference_name}.db"
        self.fragments: List[ConferenceFragment] = []
        self.entities: List[ConferenceEntity] = []
        self.topics: List[ConferenceTopic] = []
        
        # ê³ ê¸‰ NLP ëª¨ë¸ ì´ˆê¸°í™”
        if ADVANCED_NLP_AVAILABLE:
            try:
                self.embedder = SentenceTransformer('all-MiniLM-L6-v2')
                self.use_embeddings = True
            except Exception as e:
                st.warning(f"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                self.use_embeddings = False
        else:
            self.use_embeddings = False
        
        self._init_database()
    
    def _init_database(self):
        """SQLite ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # ì»¨í¼ëŸ°ìŠ¤ ì¡°ê° í…Œì´ë¸”
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS fragments (
                fragment_id TEXT PRIMARY KEY,
                file_source TEXT,
                file_type TEXT,
                timestamp TEXT,
                speaker TEXT,
                content TEXT,
                confidence REAL,
                keywords TEXT,
                embedding BLOB,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # ê°œì²´ í…Œì´ë¸”
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS entities (
                entity_id TEXT PRIMARY KEY,
                entity_type TEXT,
                name TEXT,
                mentions TEXT,
                relations TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # ì£¼ì œ í…Œì´ë¸”
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS topics (
                topic_id TEXT PRIMARY KEY,
                topic_name TEXT,
                keywords TEXT,
                fragments TEXT,
                importance_score REAL,
                sentiment TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # ì»¨í¼ëŸ°ìŠ¤ ë©”íƒ€ë°ì´í„° í…Œì´ë¸”
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS conference_metadata (
                conference_name TEXT PRIMARY KEY,
                total_fragments INTEGER,
                total_entities INTEGER,
                total_topics INTEGER,
                analysis_completed BOOLEAN,
                last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        conn.commit()
        conn.close()
    
    def add_fragment_from_analysis(self, file_path: str, analysis_result: Dict[str, Any]):
        """ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ì—ì„œ ì¡°ê° ì¶”ê°€"""
        file_type = self._detect_file_type(file_path)
        
        # ë¶„ì„ ê²°ê³¼ì—ì„œ ë‚´ìš© ì¶”ì¶œ
        content = ""
        confidence = 0.0
        speaker = None
        keywords = []
        
        if 'extracted_text' in analysis_result:
            content = analysis_result['extracted_text']
            confidence = analysis_result.get('confidence', 0.8)
        elif 'transcribed_text' in analysis_result:
            content = analysis_result['transcribed_text']
            confidence = analysis_result.get('confidence', 0.8)
        elif 'summary' in analysis_result:
            content = analysis_result['summary']
            confidence = 0.7
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = self._extract_keywords(content)
        
        # ë°œí‘œì ì¶”ì¶œ (ê¸°ë³¸ì ì¸ íŒ¨í„´ ë§¤ì¹­)
        speaker = self._extract_speaker(content)
        
        # ì„ë² ë”© ìƒì„±
        embedding = None
        if self.use_embeddings and content:
            try:
                embedding = self.embedder.encode(content).tolist()
            except Exception as e:
                st.warning(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
        
        # ì¡°ê° ìƒì„±
        fragment_id = self._generate_fragment_id(file_path, content)
        fragment = ConferenceFragment(
            fragment_id=fragment_id,
            file_source=file_path,
            file_type=file_type,
            timestamp=datetime.now().isoformat(),
            speaker=speaker,
            content=content,
            confidence=confidence,
            keywords=keywords,
            embedding=embedding
        )
        
        self.fragments.append(fragment)
        self._save_fragment_to_db(fragment)
        
        return fragment_id
    
    def _detect_file_type(self, file_path: str) -> str:
        """íŒŒì¼ íƒ€ì… ê°ì§€"""
        ext = Path(file_path).suffix.lower()
        if ext in ['.jpg', '.jpeg', '.png', '.bmp', '.gif']:
            return 'image'
        elif ext in ['.mp3', '.wav', '.m4a', '.flac']:
            return 'audio'
        elif ext in ['.mp4', '.avi', '.mov', '.mkv']:
            return 'video'
        elif ext in ['.txt', '.md', '.doc', '.docx']:
            return 'text'
        else:
            return 'unknown'
    
    def _extract_keywords(self, content: str, max_keywords: int = 10) -> List[str]:
        """í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ TF-IDF ê¸°ë°˜)"""
        if not content:
            return []
        
        # ê¸°ë³¸ì ì¸ ì „ì²˜ë¦¬
        content = re.sub(r'[^\w\s]', ' ', content.lower())
        words = content.split()
        
        # ë¶ˆìš©ì–´ ì œê±° (ê¸°ë³¸ì ì¸ í•œêµ­ì–´/ì˜ì–´)
        stopwords = {
            'ê·¸', 'ì´', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ìˆ', 'í•˜', 'ë˜', 'ì˜', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì—ì„œ', 'ìœ¼ë¡œ',
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'
        }
        
        words = [w for w in words if len(w) > 2 and w not in stopwords]
        
        # ë¹ˆë„ ê³„ì‚°
        word_freq = Counter(words)
        
        # ìƒìœ„ í‚¤ì›Œë“œ ë°˜í™˜
        return [word for word, _ in word_freq.most_common(max_keywords)]
    
    def _extract_speaker(self, content: str) -> Optional[str]:
        """ë°œí‘œì ì¶”ì¶œ (ê¸°ë³¸ íŒ¨í„´ ë§¤ì¹­)"""
        if not content:
            return None
        
        # ì¼ë°˜ì ì¸ ë°œí‘œì íŒ¨í„´
        patterns = [
            r'ë°œí‘œì[:\s]*([ê°€-í£A-Za-z\s]+)',
            r'speaker[:\s]*([A-Za-z\s]+)',
            r'([ê°€-í£]+)\s*ë‹˜ì´?\s*ë§ì”€',
            r'([A-Za-z]+)\s*said',
            r'ì§ˆë¬¸ì[:\s]*([ê°€-í£A-Za-z\s]+)'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, content, re.IGNORECASE)
            if match:
                speaker = match.group(1).strip()
                if len(speaker) > 1 and len(speaker) < 30:
                    return speaker
        
        return None
    
    def _generate_fragment_id(self, file_path: str, content: str) -> str:
        """ì¡°ê° ID ìƒì„±"""
        unique_string = f"{file_path}_{content[:100]}_{datetime.now().isoformat()}"
        return hashlib.md5(unique_string.encode()).hexdigest()[:12]
    
    def _save_fragment_to_db(self, fragment: ConferenceFragment):
        """ì¡°ê°ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # ì„ë² ë”©ì„ JSONìœ¼ë¡œ ì§ë ¬í™”
        embedding_json = json.dumps(fragment.embedding) if fragment.embedding else None
        
        cursor.execute('''
            INSERT OR REPLACE INTO fragments 
            (fragment_id, file_source, file_type, timestamp, speaker, content, confidence, keywords, embedding)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            fragment.fragment_id,
            fragment.file_source,
            fragment.file_type,
            fragment.timestamp,
            fragment.speaker,
            fragment.content,
            fragment.confidence,
            json.dumps(fragment.keywords),
            embedding_json
        ))
        
        conn.commit()
        conn.close()
    
    def load_fragments_from_db(self) -> List[ConferenceFragment]:
        """ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¡°ê°ë“¤ ë¡œë“œ"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('SELECT * FROM fragments ORDER BY created_at')
        rows = cursor.fetchall()
        
        fragments = []
        for row in rows:
            embedding = json.loads(row[8]) if row[8] else None
            fragment = ConferenceFragment(
                fragment_id=row[0],
                file_source=row[1],
                file_type=row[2],
                timestamp=row[3],
                speaker=row[4],
                content=row[5],
                confidence=row[6],
                keywords=json.loads(row[7]) if row[7] else [],
                embedding=embedding
            )
            fragments.append(fragment)
        
        conn.close()
        self.fragments = fragments
        return fragments
    
    def analyze_conference_holistically(self) -> Dict[str, Any]:
        """í™€ë¦¬ìŠ¤í‹± ì»¨í¼ëŸ°ìŠ¤ ë¶„ì„ ì‹¤í–‰"""
        if not self.fragments:
            self.load_fragments_from_db()
        
        if not self.fragments:
            return {"error": "ë¶„ì„í•  ì¡°ê°ì´ ì—†ìŠµë‹ˆë‹¤."}
        
        # 1. ê°œì²´ ì¶”ì¶œ
        self._extract_entities()
        
        # 2. ì£¼ì œ ë¶„ì„
        self._analyze_topics()
        
        # 3. ì „ì²´ ìŠ¤í† ë¦¬ êµ¬ì„±
        story = self._generate_conference_story()
        
        # 4. í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ
        insights = self._extract_key_insights()
        
        # 5. ì•¡ì…˜ ì•„ì´í…œ ìƒì„±
        action_items = self._generate_action_items()
        
        return {
            "conference_name": self.conference_name,
            "total_fragments": len(self.fragments),
            "total_entities": len(self.entities),
            "total_topics": len(self.topics),
            "conference_story": story,
            "key_insights": insights,
            "action_items": action_items,
            "analysis_timestamp": datetime.now().isoformat()
        }
    
    def _extract_entities(self):
        """ê°œì²´ ì¶”ì¶œ (ì‚¬ëŒ, íšŒì‚¬, ì œí’ˆ ë“±)"""
        entity_mentions = defaultdict(list)
        
        for fragment in self.fragments:
            content = fragment.content
            
            # ê¸°ë³¸ì ì¸ ê°œì²´ ì¶”ì¶œ íŒ¨í„´
            patterns = {
                'person': [
                    r'([ê°€-í£]{2,4})\s*ë‹˜',
                    r'([A-Z][a-z]+\s+[A-Z][a-z]+)',  # John Smith
                    r'([ê°€-í£]{2,4})\s*ëŒ€í‘œ',
                    r'([ê°€-í£]{2,4})\s*êµìˆ˜'
                ],
                'company': [
                    r'([ê°€-í£A-Za-z]+)\s*íšŒì‚¬',
                    r'([A-Z][a-z]+)\s*Inc',
                    r'([A-Z][a-z]+)\s*Corp',
                    r'([ê°€-í£]+)\s*ê¸°ì—…'
                ],
                'product': [
                    r'([A-Z][a-z]+\s*[0-9]+)',  # iPhone 15
                    r'([ê°€-í£]+\s*[0-9]+)',
                ]
            }
            
            for entity_type, type_patterns in patterns.items():
                for pattern in type_patterns:
                    matches = re.finditer(pattern, content)
                    for match in matches:
                        entity_name = match.group(1).strip()
                        if len(entity_name) > 1:
                            entity_mentions[f"{entity_type}_{entity_name}"].append(fragment.fragment_id)
        
        # ê°œì²´ ê°ì²´ ìƒì„±
        self.entities = []
        for entity_key, mentions in entity_mentions.items():
            if len(mentions) >= 1:  # ìµœì†Œ 1ë²ˆ ì–¸ê¸‰ëœ ê°œì²´ë§Œ
                entity_type, entity_name = entity_key.split('_', 1)
                entity_id = hashlib.md5(entity_key.encode()).hexdigest()[:8]
                
                entity = ConferenceEntity(
                    entity_id=entity_id,
                    entity_type=entity_type,
                    name=entity_name,
                    mentions=mentions,
                    relations={}
                )
                self.entities.append(entity)
    
    def _analyze_topics(self):
        """ì£¼ì œ ë¶„ì„"""
        # ëª¨ë“  í‚¤ì›Œë“œ ìˆ˜ì§‘
        all_keywords = []
        for fragment in self.fragments:
            all_keywords.extend(fragment.keywords)
        
        # í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„
        keyword_freq = Counter(all_keywords)
        
        # ì£¼ìš” í‚¤ì›Œë“œ ê·¸ë£¹í•‘ (ê°„ë‹¨í•œ í´ëŸ¬ìŠ¤í„°ë§)
        topic_groups = defaultdict(list)
        processed_keywords = set()
        
        for keyword, freq in keyword_freq.most_common(20):
            if keyword in processed_keywords:
                continue
            
            # ê´€ë ¨ í‚¤ì›Œë“œ ì°¾ê¸°
            related_keywords = [keyword]
            for other_keyword, _ in keyword_freq.items():
                if other_keyword != keyword and other_keyword not in processed_keywords:
                    # ê°„ë‹¨í•œ ìœ ì‚¬ì„± ì²´í¬ (í¸ì§‘ ê±°ë¦¬ ê¸°ë°˜)
                    if self._is_similar_keyword(keyword, other_keyword):
                        related_keywords.append(other_keyword)
                        processed_keywords.add(other_keyword)
            
            if len(related_keywords) >= 1:
                topic_name = f"ì£¼ì œ_{len(topic_groups) + 1}_{keyword}"
                topic_groups[topic_name] = related_keywords
                processed_keywords.add(keyword)
        
        # ì£¼ì œ ê°ì²´ ìƒì„±
        self.topics = []
        for topic_name, keywords in topic_groups.items():
            # í•´ë‹¹ ì£¼ì œì™€ ê´€ë ¨ëœ ì¡°ê°ë“¤ ì°¾ê¸°
            related_fragments = []
            for fragment in self.fragments:
                if any(kw in fragment.keywords for kw in keywords):
                    related_fragments.append(fragment.fragment_id)
            
            if related_fragments:
                topic_id = hashlib.md5(topic_name.encode()).hexdigest()[:8]
                topic = ConferenceTopic(
                    topic_id=topic_id,
                    topic_name=topic_name.split('_', 2)[-1],  # ì‹¤ì œ í‚¤ì›Œë“œë§Œ
                    keywords=keywords,
                    fragments=related_fragments,
                    importance_score=len(related_fragments) * sum(keyword_freq[kw] for kw in keywords),
                    sentiment="neutral"  # ê¸°ë³¸ê°’
                )
                self.topics.append(topic)
        
        # ì¤‘ìš”ë„ìˆœ ì •ë ¬
        self.topics.sort(key=lambda t: t.importance_score, reverse=True)
    
    def _is_similar_keyword(self, kw1: str, kw2: str, threshold: float = 0.6) -> bool:
        """í‚¤ì›Œë“œ ìœ ì‚¬ì„± ì²´í¬ (ê°„ë‹¨í•œ Jaccard ìœ ì‚¬ë„)"""
        if len(kw1) < 2 or len(kw2) < 2:
            return False
        
        set1 = set(kw1)
        set2 = set(kw2)
        
        intersection = len(set1 & set2)
        union = len(set1 | set2)
        
        jaccard = intersection / union if union > 0 else 0
        return jaccard >= threshold
    
    def _generate_conference_story(self) -> str:
        """ì»¨í¼ëŸ°ìŠ¤ ì „ì²´ ìŠ¤í† ë¦¬ ìƒì„±"""
        if not self.fragments:
            return "ë¶„ì„í•  ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."
        
        story_parts = []
        
        # 1. ê°œìš”
        story_parts.append(f"ğŸ“Š **ì»¨í¼ëŸ°ìŠ¤ ê°œìš”**")
        story_parts.append(f"- ì´ {len(self.fragments)}ê°œì˜ ìë£Œê°€ ë¶„ì„ë˜ì—ˆìŠµë‹ˆë‹¤")
        story_parts.append(f"- {len(self.entities)}ê°œì˜ ì£¼ìš” ê°œì²´(ì¸ë¬¼/íšŒì‚¬/ì œí’ˆ)ê°€ ì‹ë³„ë˜ì—ˆìŠµë‹ˆë‹¤")
        story_parts.append(f"- {len(self.topics)}ê°œì˜ í•µì‹¬ ì£¼ì œê°€ ë…¼ì˜ë˜ì—ˆìŠµë‹ˆë‹¤")
        story_parts.append("")
        
        # 2. ì£¼ìš” ì£¼ì œë“¤
        if self.topics:
            story_parts.append("ğŸ¯ **ì£¼ìš” ë…¼ì˜ ì£¼ì œ**")
            for i, topic in enumerate(self.topics[:5], 1):
                story_parts.append(f"{i}. **{topic.topic_name}**")
                story_parts.append(f"   - ê´€ë ¨ í‚¤ì›Œë“œ: {', '.join(topic.keywords[:5])}")
                story_parts.append(f"   - ì–¸ê¸‰ ë¹ˆë„: {len(topic.fragments)}íšŒ")
            story_parts.append("")
        
        # 3. ì£¼ìš” ì°¸ì—¬ì
        if self.entities:
            people = [e for e in self.entities if e.entity_type == 'person']
            companies = [e for e in self.entities if e.entity_type == 'company']
            
            if people:
                story_parts.append("ğŸ‘¥ **ì£¼ìš” ì°¸ì—¬ì**")
                for person in people[:5]:
                    story_parts.append(f"- **{person.name}**: {len(person.mentions)}íšŒ ì–¸ê¸‰")
                story_parts.append("")
            
            if companies:
                story_parts.append("ğŸ¢ **ê´€ë ¨ ê¸°ì—…/ì¡°ì§**")
                for company in companies[:5]:
                    story_parts.append(f"- **{company.name}**: {len(company.mentions)}íšŒ ì–¸ê¸‰")
                story_parts.append("")
        
        # 4. ì „ì²´ íë¦„
        story_parts.append("ğŸ“‹ **ì»¨í¼ëŸ°ìŠ¤ íë¦„**")
        
        # íŒŒì¼ íƒ€ì…ë³„ ë¶„ë¥˜
        file_types = defaultdict(list)
        for fragment in self.fragments:
            file_types[fragment.file_type].append(fragment)
        
        for file_type, fragments in file_types.items():
            type_name = {
                'image': 'ğŸ“¸ ì´ë¯¸ì§€ ìë£Œ',
                'audio': 'ğŸµ ìŒì„± ê¸°ë¡',
                'video': 'ğŸ¬ ì˜ìƒ ìë£Œ',
                'text': 'ğŸ“ í…ìŠ¤íŠ¸ ìë£Œ'
            }.get(file_type, f'ğŸ“ {file_type} ìë£Œ')
            
            story_parts.append(f"**{type_name}** ({len(fragments)}ê°œ)")
            
            # ê° íƒ€ì…ë³„ ì£¼ìš” ë‚´ìš© ìš”ì•½
            for fragment in fragments[:3]:  # ìƒìœ„ 3ê°œë§Œ
                content_preview = fragment.content[:100] + "..." if len(fragment.content) > 100 else fragment.content
                story_parts.append(f"  - {content_preview}")
            
            if len(fragments) > 3:
                story_parts.append(f"  - ... ì™¸ {len(fragments) - 3}ê°œ ë”")
            story_parts.append("")
        
        return "\n".join(story_parts)
    
    def _extract_key_insights(self) -> List[str]:
        """í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ (3ì¤„ ìš”ì•½)"""
        insights = []
        
        if self.topics:
            # ê°€ì¥ ì¤‘ìš”í•œ ì£¼ì œ
            top_topic = self.topics[0]
            insights.append(f"ğŸ’¡ ê°€ì¥ í•µì‹¬ì ìœ¼ë¡œ ë‹¤ë¤„ì§„ ì£¼ì œëŠ” '{top_topic.topic_name}'ì…ë‹ˆë‹¤ ({len(top_topic.fragments)}íšŒ ì–¸ê¸‰)")
        
        if self.entities:
            # ê°€ì¥ ë§ì´ ì–¸ê¸‰ëœ ì¸ë¬¼
            people = [e for e in self.entities if e.entity_type == 'person']
            if people:
                top_person = max(people, key=lambda p: len(p.mentions))
                insights.append(f"ğŸ‘¤ '{top_person.name}'ì´(ê°€) ê°€ì¥ ë§ì´ ì–¸ê¸‰ë˜ì—ˆìŠµë‹ˆë‹¤ ({len(top_person.mentions)}íšŒ)")
        
        # ì „ì²´ì ì¸ ë¶„ì„ ê²°ê³¼
        total_confidence = np.mean([f.confidence for f in self.fragments]) if self.fragments else 0
        insights.append(f"ğŸ“Š ì „ì²´ ìë£Œì˜ ë¶„ì„ ì‹ ë¢°ë„ëŠ” {total_confidence:.1%}ì…ë‹ˆë‹¤")
        
        return insights[:3]  # ìµœëŒ€ 3ê°œ
    
    def _generate_action_items(self) -> List[str]:
        """ì•¡ì…˜ ì•„ì´í…œ ìƒì„± (5ê°€ì§€)"""
        action_items = []
        
        # 1. í›„ì† ë¯¸íŒ… ê´€ë ¨
        if len(self.fragments) > 5:
            action_items.append("ğŸ“… ì£¼ìš” ë…¼ì˜ì‚¬í•­ì— ëŒ€í•œ í›„ì† ë¯¸íŒ… ì¼ì • ì¡°ìœ¨")
        
        # 2. ìë£Œ ì •ë¦¬
        if len(self.topics) > 3:
            action_items.append(f"ğŸ“‹ {len(self.topics)}ê°œ ì£¼ì œë³„ë¡œ ì„¸ë¶€ ìë£Œ ì •ë¦¬ ë° ë¬¸ì„œí™”")
        
        # 3. ì´í•´ê´€ê³„ì ì—°ë½
        people = [e for e in self.entities if e.entity_type == 'person']
        if len(people) > 2:
            action_items.append(f"ğŸ“ ì£¼ìš” ì°¸ì—¬ì {len(people)}ëª…ê³¼ ê°œë³„ í›„ì† ë…¼ì˜")
        
        # 4. ê¸°ìˆ ì  ê²€í† 
        tech_keywords = [f for f in self.fragments if any(kw in f.content.lower() for kw in ['ê¸°ìˆ ', 'ê°œë°œ', 'technology', 'development'])]
        if tech_keywords:
            action_items.append("ğŸ”§ ê¸°ìˆ ì  ì´ìŠˆì— ëŒ€í•œ ì „ë¬¸ê°€ ê²€í†  ë° í”¼ë“œë°± ìˆ˜ì§‘")
        
        # 5. ë‹¤ìŒ ë‹¨ê³„ ê³„íš
        action_items.append("ğŸ¯ ë…¼ì˜ëœ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ êµ¬ì²´ì ì¸ ì‹¤í–‰ ê³„íš ìˆ˜ë¦½")
        
        return action_items[:5]  # ìµœëŒ€ 5ê°œ

# Streamlit UI ë¶€ë¶„
def main():
    st.title("ğŸ¯ í™€ë¦¬ìŠ¤í‹± ì»¨í¼ëŸ°ìŠ¤ ë¶„ì„ ì—”ì§„")
    st.markdown("**ê°œë³„ íŒŒì¼ ë¶„ì„ì„ ë„˜ì–´ì„œ ì „ì²´ ì»¨í¼ëŸ°ìŠ¤ ìƒí™©ì˜ ì…ì²´ì  ì´í•´**")
    
    # ì‚¬ì´ë“œë°” ì„¤ì •
    st.sidebar.header("âš™ï¸ ì„¤ì •")
    conference_name = st.sidebar.text_input("ì»¨í¼ëŸ°ìŠ¤ ì´ë¦„", "my_conference")
    
    # ë¶„ì„ê¸° ì´ˆê¸°í™”
    analyzer = HolisticConferenceAnalyzer(conference_name)
    
    # ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ ë¡œë“œ
    existing_fragments = analyzer.load_fragments_from_db()
    
    if existing_fragments:
        st.info(f"ğŸ“Š ê¸°ì¡´ ë¶„ì„ ê²°ê³¼: {len(existing_fragments)}ê°œ ì¡°ê°ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤")
        
        if st.button("ğŸ” í™€ë¦¬ìŠ¤í‹± ë¶„ì„ ì‹¤í–‰"):
            with st.spinner("ì „ì²´ ì»¨í¼ëŸ°ìŠ¤ ìƒí™©ì„ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                result = analyzer.analyze_conference_holistically()
                
                if "error" not in result:
                    st.success("âœ… í™€ë¦¬ìŠ¤í‹± ë¶„ì„ ì™„ë£Œ!")
                    
                    # ê²°ê³¼ í‘œì‹œ
                    st.markdown("## ğŸ“– ì»¨í¼ëŸ°ìŠ¤ ì „ì²´ ìŠ¤í† ë¦¬")
                    st.markdown(result["conference_story"])
                    
                    st.markdown("## ğŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸")
                    for insight in result["key_insights"]:
                        st.markdown(f"- {insight}")
                    
                    st.markdown("## âœ… ì•¡ì…˜ ì•„ì´í…œ")
                    for item in result["action_items"]:
                        st.markdown(f"- {item}")
                    
                    # ìƒì„¸ ì •ë³´
                    with st.expander("ğŸ“Š ìƒì„¸ ë¶„ì„ ì •ë³´"):
                        st.json(result)
                else:
                    st.error(result["error"])
    else:
        st.warning("ğŸ“ ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì»¨í¼ëŸ°ìŠ¤ ìë£Œë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.")
        st.markdown("**ì‚¬ìš©ë²•:**")
        st.markdown("1. ê¸°ì¡´ ì»¨í¼ëŸ°ìŠ¤ ë¶„ì„ ì‹œìŠ¤í…œ(8501)ì—ì„œ ìë£Œë¥¼ ë¶„ì„í•˜ì„¸ìš”")
        st.markdown("2. ë¶„ì„ì´ ì™„ë£Œë˜ë©´ ì´ ì‹œìŠ¤í…œì—ì„œ í™€ë¦¬ìŠ¤í‹± ë¶„ì„ì„ ì‹¤í–‰í•˜ì„¸ìš”")

if __name__ == "__main__":
    main()