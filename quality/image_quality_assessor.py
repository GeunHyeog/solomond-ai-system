"""
ğŸ“¸ Image Quality Assessor v2.1
ì´ë¯¸ì§€ í’ˆì§ˆ ì‹¤ì‹œê°„ í‰ê°€ ë° í˜„ì¥ ìµœì í™” ëª¨ë“ˆ

ì£¼ìš” ê¸°ëŠ¥:
- í•´ìƒë„/ì„ ëª…ë„ ì‹¤ì‹œê°„ ì¸¡ì •
- ì¡°ëª…/ëŒ€ë¹„ í’ˆì§ˆ ë¶„ì„
- ë¸”ëŸ¬/ë…¸ì´ì¦ˆ ê°ì§€
- ì£¼ì–¼ë¦¬ ì´¬ì˜ ìµœì í™” ê°€ì´ë“œ
- ì¬ì´¬ì˜ ê¶Œì¥ ì•Œê³ ë¦¬ì¦˜
"""

import cv2
import numpy as np
from PIL import Image, ImageStat, ImageFilter, ImageEnhance
from typing import Dict, List, Tuple, Optional, Union
import logging
import math
import warnings
warnings.filterwarnings("ignore")

class ImageQualityAssessor:
    """ì´ë¯¸ì§€ í’ˆì§ˆ ì‹¤ì‹œê°„ í‰ê°€ê¸°"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
        # ì´ë¯¸ì§€ í’ˆì§ˆ ê¸°ì¤€ê°’
        self.quality_thresholds = {
            # í•´ìƒë„ ê¸°ì¤€
            'resolution_min': 800,        # ìµœì†Œ ë„ˆë¹„/ë†’ì´
            'resolution_good': 1200,      # ê¶Œì¥ ë„ˆë¹„/ë†’ì´
            'resolution_excellent': 1920, # ìµœì  ë„ˆë¹„/ë†’ì´
            
            # ì„ ëª…ë„ ê¸°ì¤€ (ë¼í”Œë¼ì‹œì•ˆ ë¶„ì‚°)
            'sharpness_poor': 50,         # 50 ë¯¸ë§Œ = íë¦¼
            'sharpness_fair': 100,        # 50-100 = ë³´í†µ
            'sharpness_good': 200,        # 100-200 = ì–‘í˜¸
            'sharpness_excellent': 300,   # 200+ = ìš°ìˆ˜
            
            # ëŒ€ë¹„ ê¸°ì¤€
            'contrast_poor': 30,          # 30 ë¯¸ë§Œ = ë‚®ìŒ
            'contrast_fair': 50,          # 30-50 = ë³´í†µ
            'contrast_good': 80,          # 50-80 = ì–‘í˜¸
            'contrast_excellent': 120,    # 80+ = ìš°ìˆ˜
            
            # ë°ê¸° ê¸°ì¤€ (0-255)
            'brightness_too_dark': 50,    # 50 ë¯¸ë§Œ = ë„ˆë¬´ ì–´ë‘ì›€
            'brightness_dark': 80,        # 50-80 = ì–´ë‘ì›€
            'brightness_optimal_min': 100, # 100-180 = ìµœì 
            'brightness_optimal_max': 180,
            'brightness_bright': 200,     # 180-200 = ë°ìŒ
            'brightness_too_bright': 220, # 200+ = ë„ˆë¬´ ë°ìŒ
            
            # ë…¸ì´ì¦ˆ ê¸°ì¤€
            'noise_excellent': 5,         # 5 ë¯¸ë§Œ = ìš°ìˆ˜
            'noise_good': 10,             # 5-10 = ì–‘í˜¸
            'noise_fair': 20,             # 10-20 = ë³´í†µ
            'noise_poor': 30,             # 20+ = ë¶ˆëŸ‰
        }
        
        # ì£¼ì–¼ë¦¬ ì´¬ì˜ íŠ¹í™” ì„¤ì •
        self.jewelry_settings = {
            'preferred_backgrounds': ['white', 'black', 'neutral'],
            'optimal_lighting_zones': [(100, 180)],  # ìµœì  ë°ê¸° êµ¬ê°„
            'macro_focus_threshold': 0.3,            # ë§¤í¬ë¡œ ì´ˆì  ì„ê³„ê°’
            'reflection_detection_threshold': 240,   # ë°˜ì‚¬ ê°ì§€ ì„ê³„ê°’
        }

    def assess_image_quality(self, 
                           image_path: str = None, 
                           image_data: np.ndarray = None,
                           assessment_type: str = 'comprehensive') -> Dict:
        """
        ì´ë¯¸ì§€ í’ˆì§ˆ ì¢…í•© í‰ê°€
        
        Args:
            image_path: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ
            image_data: ì´ë¯¸ì§€ ë°ì´í„° (numpy array)
            assessment_type: í‰ê°€ ìœ í˜• ('quick', 'comprehensive', 'jewelry_focused')
            
        Returns:
            Dict: ì´ë¯¸ì§€ í’ˆì§ˆ í‰ê°€ ê²°ê³¼
        """
        try:
            # ì´ë¯¸ì§€ ë¡œë“œ
            if image_data is None:
                if image_path is None:
                    raise ValueError("image_path ë˜ëŠ” image_data ì¤‘ í•˜ë‚˜ëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤")
                image_data = cv2.imread(image_path)
                if image_data is None:
                    raise ValueError(f"ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {image_path}")
            
            # ê¸°ë³¸ ì •ë³´
            results = {
                'timestamp': self._get_timestamp(),
                'image_path': image_path or 'real_time_data',
                'assessment_type': assessment_type,
                'image_shape': image_data.shape
            }
            
            # ê¸°ë³¸ í’ˆì§ˆ ë¶„ì„
            basic_quality = self.analyze_basic_quality(image_data)
            results.update(basic_quality)
            
            # ê³ ê¸‰ í’ˆì§ˆ ë¶„ì„
            if assessment_type in ['comprehensive', 'jewelry_focused']:
                advanced_quality = self.analyze_advanced_quality(image_data)
                results.update(advanced_quality)
            
            # ì£¼ì–¼ë¦¬ íŠ¹í™” ë¶„ì„
            if assessment_type == 'jewelry_focused':
                jewelry_analysis = self.analyze_jewelry_photography(image_data)
                results['jewelry_analysis'] = jewelry_analysis
            
            # ì „ì²´ ì´ë¯¸ì§€ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
            overall_score = self.calculate_overall_image_score(results)
            results['overall_quality'] = overall_score
            
            # ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±
            recommendations = self.generate_image_recommendations(results)
            results['recommendations'] = recommendations
            
            return results
            
        except Exception as e:
            self.logger.error(f"ì´ë¯¸ì§€ í’ˆì§ˆ í‰ê°€ ì˜¤ë¥˜: {str(e)}")
            return {
                'error': str(e),
                'overall_quality': {'score': 0, 'level': 'error'}
            }

    def analyze_basic_quality(self, image_data: np.ndarray) -> Dict:
        """ê¸°ë³¸ ì´ë¯¸ì§€ í’ˆì§ˆ ë¶„ì„"""
        try:
            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
            if len(image_data.shape) == 3:
                gray = cv2.cvtColor(image_data, cv2.COLOR_BGR2GRAY)
            else:
                gray = image_data.copy()
            
            # í•´ìƒë„ ë¶„ì„
            height, width = gray.shape
            resolution_analysis = self.analyze_resolution(width, height)
            
            # ì„ ëª…ë„ ë¶„ì„ (ë¼í”Œë¼ì‹œì•ˆ ë¶„ì‚°)
            sharpness_score = cv2.Laplacian(gray, cv2.CV_64F).var()
            sharpness_level = self._classify_sharpness(sharpness_score)
            
            # ëŒ€ë¹„ ë¶„ì„
            contrast_score = float(np.std(gray))
            contrast_level = self._classify_contrast(contrast_score)
            
            # ë°ê¸° ë¶„ì„
            brightness_analysis = self.analyze_brightness(gray)
            
            # ë…¸ì´ì¦ˆ ë¶„ì„
            noise_analysis = self.analyze_noise(gray)
            
            return {
                'resolution': resolution_analysis,
                'sharpness': {
                    'score': round(sharpness_score, 1),
                    'level': sharpness_level
                },
                'contrast': {
                    'score': round(contrast_score, 1),
                    'level': contrast_level
                },
                'brightness': brightness_analysis,
                'noise': noise_analysis
            }
            
        except Exception as e:
            self.logger.error(f"ê¸°ë³¸ í’ˆì§ˆ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            return {
                'resolution': {'width': 0, 'height': 0},
                'sharpness': {'score': 0, 'level': 'poor'},
                'contrast': {'score': 0, 'level': 'poor'}
            }

    def analyze_advanced_quality(self, image_data: np.ndarray) -> Dict:
        """ê³ ê¸‰ ì´ë¯¸ì§€ í’ˆì§ˆ ë¶„ì„"""
        try:
            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
            if len(image_data.shape) == 3:
                gray = cv2.cvtColor(image_data, cv2.COLOR_BGR2GRAY)
            else:
                gray = image_data.copy()
            
            # íˆìŠ¤í† ê·¸ë¨ ë¶„ì„
            histogram_analysis = self.analyze_histogram(gray)
            
            # ì—£ì§€ í’ˆì§ˆ ë¶„ì„
            edge_analysis = self.analyze_edge_quality(gray)
            
            # í…ìŠ¤ì²˜ ë¶„ì„
            texture_analysis = self.analyze_texture(gray)
            
            # ì»¬ëŸ¬ ë¶„ì„ (ì»¬ëŸ¬ ì´ë¯¸ì§€ì¸ ê²½ìš°)
            color_analysis = {}
            if len(image_data.shape) == 3:
                color_analysis = self.analyze_color_quality(image_data)
            
            # êµ¬ë„ ë¶„ì„
            composition_analysis = self.analyze_composition(gray)
            
            # ì´ˆì  ë¶„ì„
            focus_analysis = self.analyze_focus_quality(gray)
            
            return {
                'histogram': histogram_analysis,
                'edge_quality': edge_analysis,
                'texture': texture_analysis,
                'color': color_analysis,
                'composition': composition_analysis,
                'focus': focus_analysis
            }
            
        except Exception as e:
            self.logger.error(f"ê³ ê¸‰ í’ˆì§ˆ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            return {
                'histogram': {},
                'edge_quality': {},
                'texture': {}
            }

    def analyze_jewelry_photography(self, image_data: np.ndarray) -> Dict:
        """ì£¼ì–¼ë¦¬ ì´¬ì˜ íŠ¹í™” ë¶„ì„"""
        try:
            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
            if len(image_data.shape) == 3:
                gray = cv2.cvtColor(image_data, cv2.COLOR_BGR2GRAY)
                color = image_data.copy()
            else:
                gray = image_data.copy()
                color = cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)
            
            # ë°°ê²½ ë¶„ì„
            background_analysis = self.analyze_background(color)
            
            # ë°˜ì‚¬/í•˜ì´ë¼ì´íŠ¸ ë¶„ì„
            reflection_analysis = self.analyze_reflections(gray)
            
            # ì„¸ë¶€ ë””í…Œì¼ ë¶„ì„ (ì£¼ì–¼ë¦¬ì˜ ì„¸ë°€í•œ ë¶€ë¶„)
            detail_analysis = self.analyze_detail_quality(gray)
            
            # ì¡°ëª… í’ˆì§ˆ ë¶„ì„
            lighting_analysis = self.analyze_lighting_quality(gray)
            
            # ë§¤í¬ë¡œ ì´¬ì˜ í’ˆì§ˆ
            macro_analysis = self.analyze_macro_quality(gray)
            
            # ì£¼ì–¼ë¦¬ ì í•©ì„± ì ìˆ˜
            jewelry_suitability = self.calculate_jewelry_suitability(
                background_analysis, reflection_analysis, detail_analysis,
                lighting_analysis, macro_analysis
            )
            
            return {
                'background': background_analysis,
                'reflections': reflection_analysis,
                'detail_quality': detail_analysis,
                'lighting': lighting_analysis,
                'macro_quality': macro_analysis,
                'jewelry_suitability': jewelry_suitability
            }
            
        except Exception as e:
            self.logger.error(f"ì£¼ì–¼ë¦¬ ì´¬ì˜ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            return {
                'background': {},
                'jewelry_suitability': {'score': 0, 'level': 'poor'}
            }

    def analyze_resolution(self, width: int, height: int) -> Dict:
        """í•´ìƒë„ ë¶„ì„"""
        total_pixels = width * height
        aspect_ratio = width / height if height > 0 else 0
        
        # í•´ìƒë„ ë“±ê¸‰ ë¶„ë¥˜
        min_dimension = min(width, height)
        if min_dimension >= self.quality_thresholds['resolution_excellent']:
            resolution_level = 'excellent'
        elif min_dimension >= self.quality_thresholds['resolution_good']:
            resolution_level = 'good'
        elif min_dimension >= self.quality_thresholds['resolution_min']:
            resolution_level = 'fair'
        else:
            resolution_level = 'poor'
        
        return {
            'width': width,
            'height': height,
            'total_pixels': total_pixels,
            'aspect_ratio': round(aspect_ratio, 2),
            'megapixels': round(total_pixels / 1000000, 1),
            'level': resolution_level
        }

    def analyze_brightness(self, gray_image: np.ndarray) -> Dict:
        """ë°ê¸° ë¶„ì„"""
        mean_brightness = float(np.mean(gray_image))
        brightness_std = float(np.std(gray_image))
        
        # ë°ê¸° íˆìŠ¤í† ê·¸ë¨
        hist = cv2.calcHist([gray_image], [0], None, [256], [0, 256])
        
        # ë°ê¸° ë¶„í¬ ë¶„ì„
        dark_pixels = np.sum(gray_image < 85) / gray_image.size
        bright_pixels = np.sum(gray_image > 170) / gray_image.size
        mid_pixels = 1 - dark_pixels - bright_pixels
        
        # ë°ê¸° ë“±ê¸‰ ë¶„ë¥˜
        if mean_brightness < self.quality_thresholds['brightness_too_dark']:
            brightness_level = 'too_dark'
        elif mean_brightness < self.quality_thresholds['brightness_dark']:
            brightness_level = 'dark'
        elif mean_brightness <= self.quality_thresholds['brightness_optimal_max']:
            brightness_level = 'optimal'
        elif mean_brightness < self.quality_thresholds['brightness_bright']:
            brightness_level = 'bright'
        else:
            brightness_level = 'too_bright'
        
        return {
            'mean': round(mean_brightness, 1),
            'std': round(brightness_std, 1),
            'level': brightness_level,
            'distribution': {
                'dark_ratio': round(dark_pixels, 3),
                'mid_ratio': round(mid_pixels, 3),
                'bright_ratio': round(bright_pixels, 3)
            }
        }

    def analyze_noise(self, gray_image: np.ndarray) -> Dict:
        """ë…¸ì´ì¦ˆ ë¶„ì„"""
        # ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ ì ìš© í›„ ì°¨ì´ ê³„ì‚°
        blurred = cv2.GaussianBlur(gray_image, (5, 5), 0)
        noise_image = cv2.absdiff(gray_image, blurred)
        noise_level = float(np.mean(noise_image))
        
        # ê³ ì£¼íŒŒ ë…¸ì´ì¦ˆ ë¶„ì„
        high_freq_noise = self._calculate_high_frequency_noise(gray_image)
        
        # ë…¸ì´ì¦ˆ ë“±ê¸‰ ë¶„ë¥˜
        if noise_level < self.quality_thresholds['noise_excellent']:
            noise_grade = 'excellent'
        elif noise_level < self.quality_thresholds['noise_good']:
            noise_grade = 'good'
        elif noise_level < self.quality_thresholds['noise_fair']:
            noise_grade = 'fair'
        else:
            noise_grade = 'poor'
        
        return {
            'level': round(noise_level, 2),
            'grade': noise_grade,
            'high_frequency_noise': round(high_freq_noise, 2)
        }

    def analyze_histogram(self, gray_image: np.ndarray) -> Dict:
        """íˆìŠ¤í† ê·¸ë¨ ë¶„ì„"""
        hist = cv2.calcHist([gray_image], [0], None, [256], [0, 256])
        hist = hist.flatten()
        
        # íˆìŠ¤í† ê·¸ë¨ í†µê³„
        peak_value = int(np.argmax(hist))
        histogram_spread = float(np.std(np.arange(256), weights=hist))
        
        # ë‹¤ì´ë‚˜ë¯¹ ë ˆì¸ì§€
        non_zero_indices = np.nonzero(hist)[0]
        if len(non_zero_indices) > 0:
            dynamic_range = int(non_zero_indices[-1] - non_zero_indices[0])
        else:
            dynamic_range = 0
        
        # íˆìŠ¤í† ê·¸ë¨ ê· ë“±ì„± (ì—”íŠ¸ë¡œí”¼)
        hist_normalized = hist / np.sum(hist)
        hist_normalized = hist_normalized[hist_normalized > 0]
        entropy = float(-np.sum(hist_normalized * np.log2(hist_normalized)))
        
        return {
            'peak_value': peak_value,
            'spread': round(histogram_spread, 1),
            'dynamic_range': dynamic_range,
            'entropy': round(entropy, 2)
        }

    def analyze_edge_quality(self, gray_image: np.ndarray) -> Dict:
        """ì—£ì§€ í’ˆì§ˆ ë¶„ì„"""
        # Canny ì—£ì§€ ê²€ì¶œ
        edges = cv2.Canny(gray_image, 50, 150)
        edge_density = float(np.sum(edges > 0) / edges.size)
        
        # Sobel ì—£ì§€ ê°•ë„
        sobel_x = cv2.Sobel(gray_image, cv2.CV_64F, 1, 0, ksize=3)
        sobel_y = cv2.Sobel(gray_image, cv2.CV_64F, 0, 1, ksize=3)
        sobel_magnitude = np.sqrt(sobel_x**2 + sobel_y**2)
        edge_strength = float(np.mean(sobel_magnitude))
        
        return {
            'density': round(edge_density, 4),
            'strength': round(edge_strength, 1),
            'quality_score': round((edge_density * 1000 + edge_strength) / 2, 1)
        }

    def analyze_texture(self, gray_image: np.ndarray) -> Dict:
        """í…ìŠ¤ì²˜ ë¶„ì„"""
        # Local Binary Patternì„ ê°„ë‹¨íˆ êµ¬í˜„
        texture_variance = self._calculate_texture_variance(gray_image)
        
        # í…ìŠ¤ì²˜ ì—ë„ˆì§€ (GLCM ê¸°ë°˜ ê°„ë‹¨ ë²„ì „)
        texture_energy = self._calculate_texture_energy(gray_image)
        
        return {
            'variance': round(texture_variance, 2),
            'energy': round(texture_energy, 4),
            'richness_score': round((texture_variance + texture_energy * 1000) / 2, 1)
        }

    def analyze_color_quality(self, color_image: np.ndarray) -> Dict:
        """ì»¬ëŸ¬ í’ˆì§ˆ ë¶„ì„"""
        # BGRì„ HSVë¡œ ë³€í™˜
        hsv = cv2.cvtColor(color_image, cv2.COLOR_BGR2HSV)
        
        # ì±„ë„ ë¶„ì„
        saturation = hsv[:, :, 1]
        mean_saturation = float(np.mean(saturation))
        
        # ìƒ‰ìƒ ë¶„í¬ ë¶„ì„
        hue = hsv[:, :, 0]
        hue_variance = float(np.var(hue))
        
        # ì»¬ëŸ¬ ê· í˜• ë¶„ì„
        b, g, r = cv2.split(color_image)
        color_balance = {
            'red_mean': float(np.mean(r)),
            'green_mean': float(np.mean(g)),
            'blue_mean': float(np.mean(b))
        }
        
        # í™”ì´íŠ¸ ë°¸ëŸ°ìŠ¤ ì ìˆ˜ (RGB í‰ê· ì˜ ê· ë“±ì„±)
        rgb_means = [color_balance['red_mean'], color_balance['green_mean'], color_balance['blue_mean']]
        white_balance_score = 1.0 - (np.std(rgb_means) / np.mean(rgb_means))
        
        return {
            'saturation': {
                'mean': round(mean_saturation, 1),
                'level': 'high' if mean_saturation > 100 else 'medium' if mean_saturation > 50 else 'low'
            },
            'hue_variance': round(hue_variance, 1),
            'color_balance': {k: round(v, 1) for k, v in color_balance.items()},
            'white_balance_score': round(white_balance_score, 3)
        }

    def analyze_composition(self, gray_image: np.ndarray) -> Dict:
        """êµ¬ë„ ë¶„ì„"""
        height, width = gray_image.shape
        
        # ë¬´ê²Œ ì¤‘ì‹¬ ê³„ì‚°
        moments = cv2.moments(gray_image)
        if moments['m00'] != 0:
            center_x = int(moments['m10'] / moments['m00'])
            center_y = int(moments['m01'] / moments['m00'])
        else:
            center_x, center_y = width // 2, height // 2
        
        # í™©ê¸ˆë¹„ìœ¨ ì ë“¤ê³¼ì˜ ê±°ë¦¬
        golden_points = [
            (width * 0.618, height * 0.618),
            (width * 0.382, height * 0.618),
            (width * 0.618, height * 0.382),
            (width * 0.382, height * 0.382)
        ]
        
        min_distance_to_golden = min([
            math.sqrt((center_x - gx)**2 + (center_y - gy)**2)
            for gx, gy in golden_points
        ])
        
        # êµ¬ë„ ì ìˆ˜ (ê°€ê¹Œìš¸ìˆ˜ë¡ ë†’ì€ ì ìˆ˜)
        composition_score = max(0, 1 - min_distance_to_golden / (width * 0.5))
        
        return {
            'center_x': center_x,
            'center_y': center_y,
            'golden_ratio_distance': round(min_distance_to_golden, 1),
            'composition_score': round(composition_score, 3)
        }

    def analyze_focus_quality(self, gray_image: np.ndarray) -> Dict:
        """ì´ˆì  í’ˆì§ˆ ë¶„ì„"""
        # ì´ë¯¸ì§€ë¥¼ ì—¬ëŸ¬ êµ¬ì—­ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ê° êµ¬ì—­ì˜ ì„ ëª…ë„ ì¸¡ì •
        height, width = gray_image.shape
        
        # 3x3 ê·¸ë¦¬ë“œë¡œ ë‚˜ëˆ„ê¸°
        focus_map = {}
        grid_height = height // 3
        grid_width = width // 3
        
        for i in range(3):
            for j in range(3):
                y1 = i * grid_height
                y2 = min((i + 1) * grid_height, height)
                x1 = j * grid_width
                x2 = min((j + 1) * grid_width, width)
                
                region = gray_image[y1:y2, x1:x2]
                region_sharpness = cv2.Laplacian(region, cv2.CV_64F).var()
                focus_map[f'region_{i}_{j}'] = round(region_sharpness, 1)
        
        # ì¤‘ì•™ ì˜ì—­ ì´ˆì  í’ˆì§ˆ
        center_region = gray_image[height//4:3*height//4, width//4:3*width//4]
        center_focus = cv2.Laplacian(center_region, cv2.CV_64F).var()
        
        # ì´ˆì  ì¼ê´€ì„± (ë¶„ì‚°ì´ ë‚®ì„ìˆ˜ë¡ ì¼ê´€ì„± ë†’ìŒ)
        focus_values = list(focus_map.values())
        focus_consistency = 1.0 / (1.0 + np.std(focus_values) / np.mean(focus_values))
        
        return {
            'center_focus': round(center_focus, 1),
            'focus_map': focus_map,
            'focus_consistency': round(focus_consistency, 3),
            'overall_focus_score': round(center_focus * focus_consistency / 100, 3)
        }

    def analyze_background(self, color_image: np.ndarray) -> Dict:
        """ë°°ê²½ ë¶„ì„ (ì£¼ì–¼ë¦¬ ì´¬ì˜ìš©)"""
        # í…Œë‘ë¦¬ ì˜ì—­ì„ ë°°ê²½ìœ¼ë¡œ ê°„ì£¼
        height, width = color_image.shape[:2]
        border_size = min(height, width) // 20
        
        # í…Œë‘ë¦¬ ì˜ì—­ ì¶”ì¶œ
        top_border = color_image[:border_size, :]
        bottom_border = color_image[-border_size:, :]
        left_border = color_image[:, :border_size]
        right_border = color_image[:, -border_size:]
        
        # ëª¨ë“  í…Œë‘ë¦¬ ì˜ì—­ ê²°í•©
        background_pixels = np.concatenate([
            top_border.reshape(-1, 3),
            bottom_border.reshape(-1, 3),
            left_border.reshape(-1, 3),
            right_border.reshape(-1, 3)
        ])
        
        # ë°°ê²½ìƒ‰ ë¶„ì„
        mean_bg_color = np.mean(background_pixels, axis=0)
        bg_color_std = np.std(background_pixels, axis=0)
        
        # ë°°ê²½ ê· ë“±ì„± (í‘œì¤€í¸ì°¨ê°€ ë‚®ì„ìˆ˜ë¡ ê· ë“±)
        bg_uniformity = 1.0 / (1.0 + np.mean(bg_color_std) / 255)
        
        # ë°°ê²½ìƒ‰ ë¶„ë¥˜
        bg_brightness = np.mean(mean_bg_color)
        if bg_brightness > 200:
            bg_type = 'white'
        elif bg_brightness < 50:
            bg_type = 'black'
        elif np.std(mean_bg_color) < 20:
            bg_type = 'neutral'
        else:
            bg_type = 'colored'
        
        return {
            'mean_color': [round(float(c), 1) for c in mean_bg_color],
            'uniformity': round(bg_uniformity, 3),
            'type': bg_type,
            'suitability': bg_type in self.jewelry_settings['preferred_backgrounds']
        }

    def analyze_reflections(self, gray_image: np.ndarray) -> Dict:
        """ë°˜ì‚¬/í•˜ì´ë¼ì´íŠ¸ ë¶„ì„"""
        # ë§¤ìš° ë°ì€ ì˜ì—­ ê°ì§€ (ë°˜ì‚¬ ì˜ì—­)
        reflection_threshold = self.jewelry_settings['reflection_detection_threshold']
        reflection_mask = gray_image > reflection_threshold
        reflection_ratio = float(np.sum(reflection_mask) / gray_image.size)
        
        # ë°˜ì‚¬ ì˜ì—­ì˜ ì—°ì†ì„± ë¶„ì„
        contours, _ = cv2.findContours(
            reflection_mask.astype(np.uint8), 
            cv2.RETR_EXTERNAL, 
            cv2.CHAIN_APPROX_SIMPLE
        )
        
        large_reflections = [c for c in contours if cv2.contourArea(c) > 100]
        
        return {
            'reflection_ratio': round(reflection_ratio, 4),
            'num_reflection_areas': len(large_reflections),
            'has_excessive_reflections': reflection_ratio > 0.05,
            'reflection_quality': 'good' if 0.001 < reflection_ratio < 0.02 else 'poor'
        }

    def analyze_detail_quality(self, gray_image: np.ndarray) -> Dict:
        """ì„¸ë¶€ ë””í…Œì¼ í’ˆì§ˆ ë¶„ì„"""
        # ê³ ì£¼íŒŒ ì„±ë¶„ ë¶„ì„ (ì„¸ë°€í•œ ë””í…Œì¼)
        kernel = np.array([[-1,-1,-1], [-1,8,-1], [-1,-1,-1]])
        high_pass = cv2.filter2D(gray_image, -1, kernel)
        detail_score = float(np.std(high_pass))
        
        # ë¯¸ì„¸ í…ìŠ¤ì²˜ ë¶„ì„
        sobel_x = cv2.Sobel(gray_image, cv2.CV_64F, 1, 0, ksize=3)
        sobel_y = cv2.Sobel(gray_image, cv2.CV_64F, 0, 1, ksize=3)
        gradient_magnitude = np.sqrt(sobel_x**2 + sobel_y**2)
        micro_detail = float(np.mean(gradient_magnitude))
        
        return {
            'detail_score': round(detail_score, 1),
            'micro_detail': round(micro_detail, 1),
            'detail_quality': 'excellent' if detail_score > 20 else 'good' if detail_score > 10 else 'poor'
        }

    def analyze_lighting_quality(self, gray_image: np.ndarray) -> Dict:
        """ì¡°ëª… í’ˆì§ˆ ë¶„ì„"""
        # ì¡°ëª… ê· ë“±ì„± ë¶„ì„
        # ì´ë¯¸ì§€ë¥¼ ì—¬ëŸ¬ êµ¬ì—­ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ë°ê¸° ë¶„í¬ í™•ì¸
        height, width = gray_image.shape
        
        # 4x4 ê·¸ë¦¬ë“œë¡œ ë‚˜ëˆ„ê¸°
        lighting_map = []
        grid_height = height // 4
        grid_width = width // 4
        
        for i in range(4):
            for j in range(4):
                y1 = i * grid_height
                y2 = min((i + 1) * grid_height, height)
                x1 = j * grid_width
                x2 = min((j + 1) * grid_width, width)
                
                region = gray_image[y1:y2, x1:x2]
                region_brightness = float(np.mean(region))
                lighting_map.append(region_brightness)
        
        # ì¡°ëª… ê· ë“±ì„± (í‘œì¤€í¸ì°¨ê°€ ë‚®ì„ìˆ˜ë¡ ê· ë“±)
        lighting_uniformity = 1.0 / (1.0 + np.std(lighting_map) / np.mean(lighting_map))
        
        # ì „ì²´ ì´ë¯¸ì§€ì˜ ê·¸ë¼ë””ì–¸íŠ¸ ë¶„ì„ (ê¸‰ê²©í•œ ëª…ì•” ë³€í™”)
        gradient_x = cv2.Sobel(gray_image, cv2.CV_64F, 1, 0, ksize=5)
        gradient_y = cv2.Sobel(gray_image, cv2.CV_64F, 0, 1, ksize=5)
        lighting_gradient = float(np.mean(np.sqrt(gradient_x**2 + gradient_y**2)))
        
        return {
            'uniformity': round(lighting_uniformity, 3),
            'gradient_score': round(lighting_gradient, 1),
            'quality': 'excellent' if lighting_uniformity > 0.8 else 'good' if lighting_uniformity > 0.6 else 'poor'
        }

    def analyze_macro_quality(self, gray_image: np.ndarray) -> Dict:
        """ë§¤í¬ë¡œ ì´¬ì˜ í’ˆì§ˆ ë¶„ì„"""
        # ì¤‘ì•™ ì˜ì—­ì˜ ì„ ëª…ë„ vs ê°€ì¥ìë¦¬ ì˜ì—­ì˜ ì„ ëª…ë„ ë¹„êµ
        height, width = gray_image.shape
        
        # ì¤‘ì•™ 50% ì˜ì—­
        center_h1, center_h2 = height // 4, 3 * height // 4
        center_w1, center_w2 = width // 4, 3 * width // 4
        center_region = gray_image[center_h1:center_h2, center_w1:center_w2]
        center_sharpness = cv2.Laplacian(center_region, cv2.CV_64F).var()
        
        # ê°€ì¥ìë¦¬ ì˜ì—­ë“¤
        edge_regions = [
            gray_image[:height//8, :],                    # ìƒë‹¨
            gray_image[-height//8:, :],                   # í•˜ë‹¨
            gray_image[:, :width//8],                     # ì¢Œì¸¡
            gray_image[:, -width//8:]                     # ìš°ì¸¡
        ]
        
        edge_sharpness = []
        for region in edge_regions:
            if region.size > 0:
                sharpness = cv2.Laplacian(region, cv2.CV_64F).var()
                edge_sharpness.append(sharpness)
        
        avg_edge_sharpness = np.mean(edge_sharpness) if edge_sharpness else 0
        
        # ë§¤í¬ë¡œ í’ˆì§ˆ ë¹„ìœ¨ (ì¤‘ì•™ì´ ê°€ì¥ìë¦¬ë³´ë‹¤ ì„ ëª…í•´ì•¼ í•¨)
        if avg_edge_sharpness > 0:
            macro_ratio = center_sharpness / avg_edge_sharpness
        else:
            macro_ratio = center_sharpness / 100
        
        return {
            'center_sharpness': round(center_sharpness, 1),
            'edge_sharpness': round(avg_edge_sharpness, 1),
            'macro_ratio': round(macro_ratio, 2),
            'quality': 'excellent' if macro_ratio > 2.0 else 'good' if macro_ratio > 1.5 else 'poor'
        }

    def calculate_jewelry_suitability(self, background: Dict, reflections: Dict, 
                                    detail: Dict, lighting: Dict, macro: Dict) -> Dict:
        """ì£¼ì–¼ë¦¬ ì´¬ì˜ ì í•©ì„± ì ìˆ˜ ê³„ì‚°"""
        # ê° ìš”ì†Œë³„ ì ìˆ˜ (0-1)
        background_score = 1.0 if background.get('suitability', False) else 0.3
        background_score *= background.get('uniformity', 0)
        
        reflection_score = 1.0 if reflections.get('reflection_quality') == 'good' else 0.3
        
        detail_score = {
            'excellent': 1.0,
            'good': 0.8,
            'poor': 0.3
        }.get(detail.get('detail_quality', 'poor'), 0.3)
        
        lighting_score = {
            'excellent': 1.0,
            'good': 0.8,
            'poor': 0.3
        }.get(lighting.get('quality', 'poor'), 0.3)
        
        macro_score = {
            'excellent': 1.0,
            'good': 0.8,
            'poor': 0.3
        }.get(macro.get('quality', 'poor'), 0.3)
        
        # ê°€ì¤‘ í‰ê· 
        weights = {
            'background': 0.25,
            'reflection': 0.20,
            'detail': 0.25,
            'lighting': 0.20,
            'macro': 0.10
        }
        
        overall_score = (
            background_score * weights['background'] +
            reflection_score * weights['reflection'] +
            detail_score * weights['detail'] +
            lighting_score * weights['lighting'] +
            macro_score * weights['macro']
        )
        
        # ë“±ê¸‰ ë¶„ë¥˜
        if overall_score >= 0.9:
            level = 'excellent'
            status = 'ìµœì '
        elif overall_score >= 0.75:
            level = 'good'
            status = 'ì–‘í˜¸'
        elif overall_score >= 0.6:
            level = 'fair'
            status = 'ë³´í†µ'
        else:
            level = 'poor'
            status = 'ë¶ˆëŸ‰'
        
        return {
            'score': round(overall_score, 3),
            'percentage': round(overall_score * 100, 1),
            'level': level,
            'status': status,
            'components': {
                'background': round(background_score, 3),
                'reflection': round(reflection_score, 3),
                'detail': round(detail_score, 3),
                'lighting': round(lighting_score, 3),
                'macro': round(macro_score, 3)
            }
        }

    def calculate_overall_image_score(self, results: Dict) -> Dict:
        """ì „ì²´ ì´ë¯¸ì§€ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        try:
            # ê¸°ë³¸ ìš”ì†Œ ì ìˆ˜
            resolution_score = self._normalize_resolution_score(results.get('resolution', {}))
            sharpness_score = self._normalize_sharpness_score(results.get('sharpness', {}).get('score', 0))
            contrast_score = self._normalize_contrast_score(results.get('contrast', {}).get('score', 0))
            brightness_score = self._normalize_brightness_score(results.get('brightness', {}))
            noise_score = self._normalize_noise_score(results.get('noise', {}))
            
            # ê¸°ë³¸ ì ìˆ˜ ê°€ì¤‘ í‰ê· 
            basic_weights = {
                'resolution': 0.2,
                'sharpness': 0.25,
                'contrast': 0.2,
                'brightness': 0.2,
                'noise': 0.15
            }
            
            basic_score = (
                resolution_score * basic_weights['resolution'] +
                sharpness_score * basic_weights['sharpness'] +
                contrast_score * basic_weights['contrast'] +
                brightness_score * basic_weights['brightness'] +
                noise_score * basic_weights['noise']
            )
            
            # ì£¼ì–¼ë¦¬ íŠ¹í™” ì ìˆ˜ (ìˆëŠ” ê²½ìš°)
            jewelry_analysis = results.get('jewelry_analysis', {})
            if jewelry_analysis:
                jewelry_score = jewelry_analysis.get('jewelry_suitability', {}).get('score', 0)
                # ê¸°ë³¸ ì ìˆ˜ì™€ ì£¼ì–¼ë¦¬ ì ìˆ˜ë¥¼ 7:3 ë¹„ìœ¨ë¡œ ê²°í•©
                overall_score = basic_score * 0.7 + jewelry_score * 0.3
            else:
                overall_score = basic_score
            
            # ë“±ê¸‰ ë¶„ë¥˜
            if overall_score >= 0.9:
                level, status, color = 'excellent', 'ìš°ìˆ˜', 'ğŸŸ¢'
            elif overall_score >= 0.8:
                level, status, color = 'good', 'ì–‘í˜¸', 'ğŸŸ¡'
            elif overall_score >= 0.7:
                level, status, color = 'fair', 'ë³´í†µ', 'ğŸŸ '
            else:
                level, status, color = 'poor', 'ë¶ˆëŸ‰', 'ğŸ”´'
            
            return {
                'score': round(overall_score, 3),
                'percentage': round(overall_score * 100, 1),
                'level': level,
                'status': status,
                'color': color,
                'components': {
                    'basic_score': round(basic_score, 3),
                    'resolution': round(resolution_score, 3),
                    'sharpness': round(sharpness_score, 3),
                    'contrast': round(contrast_score, 3),
                    'brightness': round(brightness_score, 3),
                    'noise': round(noise_score, 3)
                }
            }
            
        except Exception as e:
            self.logger.error(f"ì „ì²´ ì´ë¯¸ì§€ ì ìˆ˜ ê³„ì‚° ì˜¤ë¥˜: {str(e)}")
            return {
                'score': 0.0,
                'level': 'error',
                'status': 'ì˜¤ë¥˜'
            }

    def generate_image_recommendations(self, results: Dict) -> List[Dict]:
        """ì´ë¯¸ì§€ í’ˆì§ˆ ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        try:
            # í•´ìƒë„ ê¶Œì¥ì‚¬í•­
            resolution = results.get('resolution', {})
            if resolution.get('level') == 'poor':
                recommendations.append({
                    'type': 'critical',
                    'icon': 'ğŸ“',
                    'title': 'í•´ìƒë„ ë¶€ì¡±',
                    'message': 'ë” ë†’ì€ í•´ìƒë„ë¡œ ì´¬ì˜í•˜ê±°ë‚˜ ì¹´ë©”ë¼ë¥¼ í”¼ì‚¬ì²´ì— ë” ê°€ê¹Œì´ í•˜ì„¸ìš”',
                    'action': 'increase_resolution'
                })
            
            # ì„ ëª…ë„ ê¶Œì¥ì‚¬í•­
            sharpness = results.get('sharpness', {})
            if sharpness.get('level') in ['poor', 'fair']:
                recommendations.append({
                    'type': 'warning',
                    'icon': 'ğŸ”',
                    'title': 'ì´ë¯¸ì§€ íë¦¼',
                    'message': 'ì‚¼ê°ëŒ€ë¥¼ ì‚¬ìš©í•˜ê³  ì´ˆì ì„ ì •í™•íˆ ë§ì¶˜ í›„ ì´¬ì˜í•˜ì„¸ìš”',
                    'action': 'improve_focus_and_stability'
                })
            
            # ëŒ€ë¹„ ê¶Œì¥ì‚¬í•­
            contrast = results.get('contrast', {})
            if contrast.get('level') in ['poor', 'fair']:
                recommendations.append({
                    'type': 'warning',
                    'icon': 'ğŸŒ“',
                    'title': 'ëŒ€ë¹„ ë¶€ì¡±',
                    'message': 'ì¡°ëª…ì„ ê°œì„ í•˜ê±°ë‚˜ í›„ì²˜ë¦¬ì—ì„œ ëŒ€ë¹„ë¥¼ ë†’ì—¬ë³´ì„¸ìš”',
                    'action': 'improve_contrast'
                })
            
            # ë°ê¸° ê¶Œì¥ì‚¬í•­
            brightness = results.get('brightness', {})
            brightness_level = brightness.get('level')
            if brightness_level == 'too_dark':
                recommendations.append({
                    'type': 'warning',
                    'icon': 'ğŸ’¡',
                    'title': 'ì´ë¯¸ì§€ ë„ˆë¬´ ì–´ë‘ì›€',
                    'message': 'ì¡°ëª…ì„ ì¶”ê°€í•˜ê±°ë‚˜ ë…¸ì¶œì„ ì¦ê°€ì‹œí‚¤ì„¸ìš”',
                    'action': 'increase_lighting'
                })
            elif brightness_level == 'too_bright':
                recommendations.append({
                    'type': 'warning',
                    'icon': 'â˜€ï¸',
                    'title': 'ì´ë¯¸ì§€ ë„ˆë¬´ ë°ìŒ',
                    'message': 'ì¡°ëª…ì„ ì¤„ì´ê±°ë‚˜ ë…¸ì¶œì„ ê°ì†Œì‹œí‚¤ì„¸ìš”',
                    'action': 'reduce_lighting'
                })
            
            # ë…¸ì´ì¦ˆ ê¶Œì¥ì‚¬í•­
            noise = results.get('noise', {})
            if noise.get('grade') in ['poor', 'fair']:
                recommendations.append({
                    'type': 'info',
                    'icon': 'ğŸ”§',
                    'title': 'ë…¸ì´ì¦ˆ ê°ì§€',
                    'message': 'ISOë¥¼ ë‚®ì¶”ê±°ë‚˜ ë” ë°ì€ í™˜ê²½ì—ì„œ ì´¬ì˜í•˜ì„¸ìš”',
                    'action': 'reduce_noise'
                })
            
            # ì£¼ì–¼ë¦¬ íŠ¹í™” ê¶Œì¥ì‚¬í•­
            jewelry_analysis = results.get('jewelry_analysis', {})
            if jewelry_analysis:
                jewelry_suitability = jewelry_analysis.get('jewelry_suitability', {})
                if jewelry_suitability.get('level') in ['poor', 'fair']:
                    recommendations.append({
                        'type': 'info',
                        'icon': 'ğŸ’',
                        'title': 'ì£¼ì–¼ë¦¬ ì´¬ì˜ ìµœì í™” í•„ìš”',
                        'message': 'ë°°ê²½, ì¡°ëª…, ë°˜ì‚¬ë¥¼ ê°œì„ í•˜ì—¬ ì£¼ì–¼ë¦¬ê°€ ë” ì˜ ë³´ì´ë„ë¡ í•˜ì„¸ìš”',
                        'action': 'optimize_jewelry_photography'
                    })
                
                # ë°°ê²½ ê¶Œì¥ì‚¬í•­
                background = jewelry_analysis.get('background', {})
                if not background.get('suitability', True):
                    recommendations.append({
                        'type': 'info',
                        'icon': 'ğŸ­',
                        'title': 'ë°°ê²½ ê°œì„  ê¶Œì¥',
                        'message': 'í°ìƒ‰, ê²€ì€ìƒ‰ ë˜ëŠ” ì¤‘ì„±ìƒ‰ ë°°ê²½ì„ ì‚¬ìš©í•˜ì„¸ìš”',
                        'action': 'improve_background'
                    })
                
                # ë°˜ì‚¬ ê¶Œì¥ì‚¬í•­
                reflections = jewelry_analysis.get('reflections', {})
                if reflections.get('has_excessive_reflections', False):
                    recommendations.append({
                        'type': 'info',
                        'icon': 'âœ¨',
                        'title': 'ê³¼ë„í•œ ë°˜ì‚¬ ê°ì§€',
                        'message': 'ì¡°ëª… ê°ë„ë¥¼ ì¡°ì •í•˜ê±°ë‚˜ í™•ì‚° ì¡°ëª…ì„ ì‚¬ìš©í•˜ì„¸ìš”',
                        'action': 'reduce_reflections'
                    })
            
            # ì „ì²´ í’ˆì§ˆ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
            overall_quality = results.get('overall_quality', {})
            if overall_quality.get('level') == 'poor':
                recommendations.append({
                    'type': 'critical',
                    'icon': 'ğŸ”´',
                    'title': 'ì¬ì´¬ì˜ ê¶Œì¥',
                    'message': 'í˜„ì¬ ì´ë¯¸ì§€ í’ˆì§ˆì´ ì¢‹ì§€ ì•ŠìŠµë‹ˆë‹¤. ì„¤ì •ì„ ê°œì„ í•œ í›„ ë‹¤ì‹œ ì´¬ì˜í•˜ì„¸ìš”',
                    'action': 'retry_capture'
                })
            elif overall_quality.get('level') == 'excellent':
                recommendations.append({
                    'type': 'success',
                    'icon': 'ğŸŸ¢',
                    'title': 'ìµœì  í’ˆì§ˆ',
                    'message': 'í˜„ì¬ ì„¤ì •ì„ ìœ ì§€í•˜ì—¬ ê³„ì† ì´¬ì˜í•˜ì„¸ìš”',
                    'action': 'maintain_current_settings'
                })
            
            return recommendations
            
        except Exception as e:
            self.logger.error(f"ì´ë¯¸ì§€ ê¶Œì¥ì‚¬í•­ ìƒì„± ì˜¤ë¥˜: {str(e)}")
            return [{
                'type': 'error',
                'icon': 'âŒ',
                'title': 'ì´ë¯¸ì§€ ë¶„ì„ ì˜¤ë¥˜',
                'message': 'ì´ë¯¸ì§€ í’ˆì§ˆ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤',
                'action': 'retry_image_analysis'
            }]

    # === ë‚´ë¶€ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ ===
    
    def _classify_sharpness(self, sharpness_score: float) -> str:
        """ì„ ëª…ë„ ë“±ê¸‰ ë¶„ë¥˜"""
        if sharpness_score >= self.quality_thresholds['sharpness_excellent']:
            return 'excellent'
        elif sharpness_score >= self.quality_thresholds['sharpness_good']:
            return 'good'
        elif sharpness_score >= self.quality_thresholds['sharpness_fair']:
            return 'fair'
        else:
            return 'poor'
    
    def _classify_contrast(self, contrast_score: float) -> str:
        """ëŒ€ë¹„ ë“±ê¸‰ ë¶„ë¥˜"""
        if contrast_score >= self.quality_thresholds['contrast_excellent']:
            return 'excellent'
        elif contrast_score >= self.quality_thresholds['contrast_good']:
            return 'good'
        elif contrast_score >= self.quality_thresholds['contrast_fair']:
            return 'fair'
        else:
            return 'poor'
    
    def _calculate_high_frequency_noise(self, image: np.ndarray) -> float:
        """ê³ ì£¼íŒŒ ë…¸ì´ì¦ˆ ê³„ì‚°"""
        # ê³ ì£¼íŒŒ í†µê³¼ í•„í„°
        kernel = np.array([[-1,-1,-1], [-1,8,-1], [-1,-1,-1]])
        high_pass = cv2.filter2D(image, -1, kernel)
        return float(np.std(high_pass))
    
    def _calculate_texture_variance(self, image: np.ndarray) -> float:
        """í…ìŠ¤ì²˜ ë¶„ì‚° ê³„ì‚°"""
        # 3x3 ìœˆë„ìš°ì—ì„œ ê° í”½ì…€ì˜ ë¶„ì‚° ê³„ì‚°
        kernel = np.ones((3,3), np.float32) / 9
        mean_filtered = cv2.filter2D(image.astype(np.float32), -1, kernel)
        variance = np.mean((image.astype(np.float32) - mean_filtered) ** 2)
        return float(variance)
    
    def _calculate_texture_energy(self, image: np.ndarray) -> float:
        """í…ìŠ¤ì²˜ ì—ë„ˆì§€ ê³„ì‚°"""
        # ê°„ë‹¨í•œ GLCM ê¸°ë°˜ ì—ë„ˆì§€ ê³„ì‚°
        # í”½ì…€ ê°’ ì°¨ì´ì˜ ì œê³±í•©
        diff_h = np.diff(image, axis=1)
        diff_v = np.diff(image, axis=0)
        energy = np.mean(diff_h**2) + np.mean(diff_v**2)
        return float(energy) / 1000  # ì •ê·œí™”
    
    def _normalize_resolution_score(self, resolution: Dict) -> float:
        """í•´ìƒë„ ì ìˆ˜ ì •ê·œí™”"""
        level = resolution.get('level', 'poor')
        score_map = {
            'excellent': 1.0,
            'good': 0.8,
            'fair': 0.6,
            'poor': 0.3
        }
        return score_map.get(level, 0.3)
    
    def _normalize_sharpness_score(self, sharpness: float) -> float:
        """ì„ ëª…ë„ ì ìˆ˜ ì •ê·œí™”"""
        return min(1.0, sharpness / 300)
    
    def _normalize_contrast_score(self, contrast: float) -> float:
        """ëŒ€ë¹„ ì ìˆ˜ ì •ê·œí™”"""
        return min(1.0, contrast / 120)
    
    def _normalize_brightness_score(self, brightness: Dict) -> float:
        """ë°ê¸° ì ìˆ˜ ì •ê·œí™”"""
        level = brightness.get('level', 'too_dark')
        score_map = {
            'optimal': 1.0,
            'dark': 0.7,
            'bright': 0.7,
            'too_dark': 0.3,
            'too_bright': 0.3
        }
        return score_map.get(level, 0.3)
    
    def _normalize_noise_score(self, noise: Dict) -> float:
        """ë…¸ì´ì¦ˆ ì ìˆ˜ ì •ê·œí™” (ë…¸ì´ì¦ˆê°€ ì ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜)"""
        grade = noise.get('grade', 'poor')
        score_map = {
            'excellent': 1.0,
            'good': 0.8,
            'fair': 0.6,
            'poor': 0.3
        }
        return score_map.get(grade, 0.3)
    
    def _get_timestamp(self) -> str:
        """í˜„ì¬ íƒ€ì„ìŠ¤íƒ¬í”„ ë°˜í™˜"""
        from datetime import datetime
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")


# ì‚¬ìš© ì˜ˆì œ
if __name__ == "__main__":
    assessor = ImageQualityAssessor()
    
    print("ğŸ“¸ Image Quality Assessor v2.1 - í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 50)
    
    # ì‹¤ì œ ì‚¬ìš© ì‹œì—ëŠ” ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œë¥¼ ì œê³µ
    # result = assessor.assess_image_quality("test_jewelry.jpg", assessment_type="jewelry_focused")
    # print(f"ì „ì²´ ì´ë¯¸ì§€ í’ˆì§ˆ: {result['overall_quality']['percentage']}%")
    # print(f"ì£¼ì–¼ë¦¬ ì í•©ì„±: {result['jewelry_analysis']['jewelry_suitability']['percentage']}%")
    
    print("ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ âœ…")
