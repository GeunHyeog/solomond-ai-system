"""
ğŸ” Content Consistency Checker v2.1
ë©€í‹°ëª¨ë‹¬ ë‚´ìš© ì¼ê´€ì„± ê²€ì¦ ë° í˜„ì¥ ìµœì í™” ëª¨ë“ˆ

ì£¼ìš” ê¸°ëŠ¥:
- ìŒì„±-ì´ë¯¸ì§€-ë¬¸ì„œ ë‚´ìš© ë§¤ì¹­ ê²€ì¦
- ì‹œê°„ ë™ê¸°í™” í’ˆì§ˆ ì¸¡ì •
- ë‹¤êµ­ì–´ ë²ˆì—­ ì¼ê´€ì„± ê²€ì¦
- ì£¼ì–¼ë¦¬ ìš©ì–´ í†µì¼ì„± ë¶„ì„
- í¬ë¡œìŠ¤ ëª¨ë‹¬ ì‹ ë¢°ë„ ê³„ì‚°
"""

import numpy as np
import re
from typing import Dict, List, Tuple, Optional, Union, Any
from datetime import datetime, timedelta
import logging
from difflib import SequenceMatcher
import warnings
warnings.filterwarnings("ignore")

class ContentConsistencyChecker:
    """ë©€í‹°ëª¨ë‹¬ ë‚´ìš© ì¼ê´€ì„± ê²€ì¦ê¸°"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
        # ì¼ê´€ì„± ê²€ì¦ ê¸°ì¤€ê°’
        self.consistency_thresholds = {
            'semantic_similarity_excellent': 0.8,    # ì˜ë¯¸ ìœ ì‚¬ë„ 80% ì´ìƒ = ìš°ìˆ˜
            'semantic_similarity_good': 0.6,         # ì˜ë¯¸ ìœ ì‚¬ë„ 60-80% = ì–‘í˜¸
            'semantic_similarity_fair': 0.4,         # ì˜ë¯¸ ìœ ì‚¬ë„ 40-60% = ë³´í†µ
            'semantic_similarity_poor': 0.2,         # ì˜ë¯¸ ìœ ì‚¬ë„ 20% ë¯¸ë§Œ = ë¶ˆëŸ‰
            
            'temporal_sync_excellent': 2.0,          # ì‹œê°„ ë™ê¸°í™” ì˜¤ì°¨ 2ì´ˆ ì´ë‚´ = ìš°ìˆ˜
            'temporal_sync_good': 5.0,               # ì‹œê°„ ë™ê¸°í™” ì˜¤ì°¨ 5ì´ˆ ì´ë‚´ = ì–‘í˜¸
            'temporal_sync_fair': 10.0,              # ì‹œê°„ ë™ê¸°í™” ì˜¤ì°¨ 10ì´ˆ ì´ë‚´ = ë³´í†µ
            
            'terminology_consistency_excellent': 0.9, # ìš©ì–´ ì¼ê´€ì„± 90% ì´ìƒ = ìš°ìˆ˜
            'terminology_consistency_good': 0.8,      # ìš©ì–´ ì¼ê´€ì„± 80-90% = ì–‘í˜¸
            'terminology_consistency_fair': 0.7,      # ìš©ì–´ ì¼ê´€ì„± 70-80% = ë³´í†µ
        }
        
        # ì£¼ì–¼ë¦¬ ì „ë¬¸ìš©ì–´ ë§¤í•‘ (ë‹¤êµ­ì–´)
        self.jewelry_term_mappings = {
            'diamond': ['ë‹¤ì´ì•„ëª¬ë“œ', 'diamond', 'ãƒ€ã‚¤ãƒ¤ãƒ¢ãƒ³ãƒ‰', 'é’»çŸ³'],
            'gold': ['ê¸ˆ', 'gold', 'é‡‘', 'é»„é‡‘'],
            'silver': ['ì€', 'silver', 'éŠ€', 'é“¶'],
            'platinum': ['ë°±ê¸ˆ', 'platinum', 'ãƒ—ãƒ©ãƒãƒŠ', 'é“‚é‡‘'],
            'carat': ['ìºëŸ¿', 'ì¹´ë¼íŠ¸', 'carat', 'ã‚«ãƒ©ãƒƒãƒˆ', 'å…‹æ‹‰'],
            'cut': ['ì»·', 'cut', 'ã‚«ãƒƒãƒˆ', 'åˆ‡å·¥'],
            'clarity': ['íˆ¬ëª…ë„', 'clarity', 'é€æ˜åº¦', 'å‡€åº¦'],
            'color': ['ìƒ‰ìƒ', 'color', 'è‰²', 'é¢œè‰²'],
            'certificate': ['ê°ì •ì„œ', 'ì¸ì¦ì„œ', 'certificate', 'è¨¼æ˜æ›¸', 'è¯ä¹¦'],
            'gemstone': ['ë³´ì„', 'gemstone', 'å®çŸ³', 'å®çŸ³'],
            'jewelry': ['ì£¼ì–¼ë¦¬', 'ë³´ì„', 'jewelry', 'ã‚¸ãƒ¥ã‚¨ãƒªãƒ¼', 'ç å®'],
            'setting': ['ì„¸íŒ…', 'setting', 'ã‚»ãƒƒãƒ†ã‚£ãƒ³ã‚°', 'é•¶åµŒ'],
            'appraisal': ['í‰ê°€ì„œ', 'appraisal', 'è©•ä¾¡æ›¸', 'è¯„ä¼°'],
        }
        
        # ì‹œê°„ í‘œí˜„ íŒ¨í„´
        self.time_patterns = [
            r'(\d{1,2}):(\d{2}):(\d{2})',           # HH:MM:SS
            r'(\d{1,2}):(\d{2})',                   # HH:MM
            r'(\d+)\s*ë¶„\s*(\d+)\s*ì´ˆ',              # Xë¶„ Yì´ˆ
            r'(\d+)\s*ì‹œê°„\s*(\d+)\s*ë¶„',            # Xì‹œê°„ Yë¶„
            r'(\d+)\s*minutes?\s*(\d+)\s*seconds?',  # X minutes Y seconds
            r'(\d+)\s*hours?\s*(\d+)\s*minutes?',    # X hours Y minutes
        ]

    def check_content_consistency(self, 
                                audio_content: Dict = None,
                                image_content: Dict = None, 
                                document_content: Dict = None,
                                sync_data: Dict = None,
                                check_type: str = 'comprehensive') -> Dict:
        """
        ë©€í‹°ëª¨ë‹¬ ë‚´ìš© ì¼ê´€ì„± ì¢…í•© ê²€ì¦
        
        Args:
            audio_content: ìŒì„± ì¸ì‹ ê²°ê³¼ (í…ìŠ¤íŠ¸, íƒ€ì„ìŠ¤íƒ¬í”„ ë“±)
            image_content: ì´ë¯¸ì§€ OCR ê²°ê³¼ (í…ìŠ¤íŠ¸, ë©”íƒ€ë°ì´í„° ë“±)
            document_content: ë¬¸ì„œ ë‚´ìš© (í…ìŠ¤íŠ¸, êµ¬ì¡° ë“±)
            sync_data: ì‹œê°„ ë™ê¸°í™” ë°ì´í„°
            check_type: ê²€ì¦ ìœ í˜• ('quick', 'comprehensive', 'jewelry_focused')
            
        Returns:
            Dict: ì¼ê´€ì„± ê²€ì¦ ê²°ê³¼
        """
        try:
            # ê¸°ë³¸ ì •ë³´
            results = {
                'timestamp': self._get_timestamp(),
                'check_type': check_type,
                'input_sources': self._identify_input_sources(
                    audio_content, image_content, document_content
                )
            }
            
            # í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ
            texts = self._extract_texts(audio_content, image_content, document_content)
            results['extracted_texts'] = texts
            
            # ì˜ë¯¸ì  ì¼ê´€ì„± ê²€ì¦
            if len(texts) >= 2:
                semantic_consistency = self.analyze_semantic_consistency(texts)
                results['semantic_consistency'] = semantic_consistency
            
            # ì‹œê°„ ë™ê¸°í™” ê²€ì¦
            if sync_data or self._has_temporal_data(audio_content, image_content):
                temporal_consistency = self.analyze_temporal_consistency(
                    audio_content, image_content, sync_data
                )
                results['temporal_consistency'] = temporal_consistency
            
            # ì£¼ì–¼ë¦¬ ìš©ì–´ ì¼ê´€ì„± (ì£¼ì–¼ë¦¬ íŠ¹í™” ëª¨ë“œ)
            if check_type in ['comprehensive', 'jewelry_focused']:
                terminology_consistency = self.analyze_jewelry_terminology_consistency(texts)
                results['terminology_consistency'] = terminology_consistency
            
            # êµ¬ì¡°ì  ì¼ê´€ì„± ê²€ì¦
            structural_consistency = self.analyze_structural_consistency(
                audio_content, image_content, document_content
            )
            results['structural_consistency'] = structural_consistency
            
            # ë²ˆì—­ í’ˆì§ˆ ê²€ì¦ (ë‹¤êµ­ì–´ ë‚´ìš©ì´ ìˆëŠ” ê²½ìš°)
            translation_consistency = self.analyze_translation_consistency(texts)
            results['translation_consistency'] = translation_consistency
            
            # ì „ì²´ ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚°
            overall_consistency = self.calculate_overall_consistency_score(results)
            results['overall_consistency'] = overall_consistency
            
            # ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±
            recommendations = self.generate_consistency_recommendations(results)
            results['recommendations'] = recommendations
            
            return results
            
        except Exception as e:
            self.logger.error(f"ë‚´ìš© ì¼ê´€ì„± ê²€ì¦ ì˜¤ë¥˜: {str(e)}")
            return {
                'error': str(e),
                'overall_consistency': {'score': 0, 'level': 'error'}
            }

    def analyze_semantic_consistency(self, texts: Dict[str, str]) -> Dict:
        """ì˜ë¯¸ì  ì¼ê´€ì„± ë¶„ì„"""
        try:
            text_sources = list(texts.keys())
            text_contents = list(texts.values())
            
            if len(text_contents) < 2:
                return {
                    'similarity_scores': {},
                    'average_similarity': 0.0,
                    'consistency_level': 'insufficient_data'
                }
            
            # í…ìŠ¤íŠ¸ ê°„ ìœ ì‚¬ë„ ê³„ì‚°
            similarity_scores = {}
            similarities = []
            
            for i in range(len(text_sources)):
                for j in range(i + 1, len(text_sources)):
                    source1, source2 = text_sources[i], text_sources[j]
                    text1, text2 = text_contents[i], text_contents[j]
                    
                    # ê¸°ë³¸ ë¬¸ìì—´ ìœ ì‚¬ë„
                    basic_similarity = self._calculate_text_similarity(text1, text2)
                    
                    # í‚¤ì›Œë“œ ê¸°ë°˜ ìœ ì‚¬ë„
                    keyword_similarity = self._calculate_keyword_similarity(text1, text2)
                    
                    # ì£¼ì–¼ë¦¬ ìš©ì–´ ê¸°ë°˜ ìœ ì‚¬ë„
                    jewelry_similarity = self._calculate_jewelry_term_similarity(text1, text2)
                    
                    # ê°€ì¤‘ í‰ê· 
                    combined_similarity = (
                        basic_similarity * 0.4 +
                        keyword_similarity * 0.4 +
                        jewelry_similarity * 0.2
                    )
                    
                    pair_key = f"{source1}_vs_{source2}"
                    similarity_scores[pair_key] = {
                        'basic': round(basic_similarity, 3),
                        'keyword': round(keyword_similarity, 3),
                        'jewelry': round(jewelry_similarity, 3),
                        'combined': round(combined_similarity, 3)
                    }
                    similarities.append(combined_similarity)
            
            # í‰ê·  ìœ ì‚¬ë„
            average_similarity = np.mean(similarities) if similarities else 0.0
            
            # ì¼ê´€ì„± ë“±ê¸‰ ë¶„ë¥˜
            consistency_level = self._classify_semantic_consistency(average_similarity)
            
            # ê°€ì¥ ìœ ì‚¬í•œ/ë‹¤ë¥¸ ìŒ ì°¾ê¸°
            if similarities:
                max_sim_idx = np.argmax(similarities)
                min_sim_idx = np.argmin(similarities)
                pair_keys = list(similarity_scores.keys())
                
                most_similar_pair = pair_keys[max_sim_idx]
                most_different_pair = pair_keys[min_sim_idx]
            else:
                most_similar_pair = None
                most_different_pair = None
            
            return {
                'similarity_scores': similarity_scores,
                'average_similarity': round(average_similarity, 3),
                'consistency_level': consistency_level,
                'most_similar_pair': most_similar_pair,
                'most_different_pair': most_different_pair,
                'similarity_distribution': {
                    'max': round(max(similarities), 3) if similarities else 0,
                    'min': round(min(similarities), 3) if similarities else 0,
                    'std': round(np.std(similarities), 3) if similarities else 0
                }
            }
            
        except Exception as e:
            self.logger.error(f"ì˜ë¯¸ì  ì¼ê´€ì„± ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            return {
                'similarity_scores': {},
                'average_similarity': 0.0,
                'consistency_level': 'error'
            }

    def analyze_temporal_consistency(self, 
                                   audio_content: Dict = None,
                                   image_content: Dict = None,
                                   sync_data: Dict = None) -> Dict:
        """ì‹œê°„ ë™ê¸°í™” ì¼ê´€ì„± ë¶„ì„"""
        try:
            # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ
            timestamps = self._extract_timestamps(audio_content, image_content, sync_data)
            
            if len(timestamps) < 2:
                return {
                    'sync_quality': 'insufficient_data',
                    'time_offsets': {},
                    'average_offset': 0.0
                }
            
            # ì‹œê°„ ì˜¤í”„ì…‹ ê³„ì‚°
            time_offsets = {}
            offsets = []
            
            timestamp_items = list(timestamps.items())
            for i in range(len(timestamp_items)):
                for j in range(i + 1, len(timestamp_items)):
                    source1, time1 = timestamp_items[i]
                    source2, time2 = timestamp_items[j]
                    
                    offset = abs(time1 - time2)
                    pair_key = f"{source1}_vs_{source2}"
                    time_offsets[pair_key] = round(offset, 2)
                    offsets.append(offset)
            
            # í‰ê·  ì˜¤í”„ì…‹
            average_offset = np.mean(offsets) if offsets else 0.0
            
            # ë™ê¸°í™” í’ˆì§ˆ ë“±ê¸‰
            sync_quality = self._classify_temporal_sync(average_offset)
            
            # ì‹œê°„ ì¼ê´€ì„± ì ìˆ˜ (ì˜¤í”„ì…‹ì´ ì‘ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜)
            max_acceptable_offset = 30.0  # 30ì´ˆ
            consistency_score = max(0.0, 1.0 - average_offset / max_acceptable_offset)
            
            return {
                'timestamps': timestamps,
                'time_offsets': time_offsets,
                'average_offset': round(average_offset, 2),
                'sync_quality': sync_quality,
                'consistency_score': round(consistency_score, 3),
                'temporal_analysis': {
                    'max_offset': round(max(offsets), 2) if offsets else 0,
                    'min_offset': round(min(offsets), 2) if offsets else 0,
                    'offset_std': round(np.std(offsets), 2) if offsets else 0
                }
            }
            
        except Exception as e:
            self.logger.error(f"ì‹œê°„ ì¼ê´€ì„± ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            return {
                'sync_quality': 'error',
                'time_offsets': {},
                'average_offset': 0.0
            }

    def analyze_jewelry_terminology_consistency(self, texts: Dict[str, str]) -> Dict:
        """ì£¼ì–¼ë¦¬ ìš©ì–´ ì¼ê´€ì„± ë¶„ì„"""
        try:
            # ê° í…ìŠ¤íŠ¸ì—ì„œ ì£¼ì–¼ë¦¬ ìš©ì–´ ì¶”ì¶œ
            term_usage = {}
            
            for source, text in texts.items():
                extracted_terms = self._extract_jewelry_terms(text)
                term_usage[source] = extracted_terms
            
            # ìš©ì–´ ë§¤í•‘ ì¼ê´€ì„± ê²€ì¦
            consistency_results = {}
            
            for standard_term, variants in self.jewelry_term_mappings.items():
                term_consistency = self._check_term_consistency_across_sources(
                    standard_term, variants, term_usage
                )
                if term_consistency['found_in_sources']:
                    consistency_results[standard_term] = term_consistency
            
            # ì „ì²´ ìš©ì–´ ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚°
            if consistency_results:
                consistency_scores = [
                    result['consistency_score'] 
                    for result in consistency_results.values()
                ]
                overall_score = np.mean(consistency_scores)
            else:
                overall_score = 1.0  # ìš©ì–´ê°€ ì—†ìœ¼ë©´ ì¼ê´€ì„± ë¬¸ì œë„ ì—†ìŒ
            
            # ì¼ê´€ì„± ë“±ê¸‰ ë¶„ë¥˜
            consistency_level = self._classify_terminology_consistency(overall_score)
            
            # ë¶ˆì¼ì¹˜ ìš©ì–´ ì°¾ê¸°
            inconsistent_terms = [
                term for term, result in consistency_results.items()
                if result['consistency_score'] < 0.7
            ]
            
            return {
                'term_usage_by_source': term_usage,
                'consistency_results': consistency_results,
                'overall_score': round(overall_score, 3),
                'consistency_level': consistency_level,
                'inconsistent_terms': inconsistent_terms,
                'term_statistics': {
                    'total_unique_terms': len(set().union(*[
                        terms.keys() for terms in term_usage.values()
                    ])),
                    'terms_checked': len(consistency_results),
                    'consistent_terms': len([
                        t for t in consistency_results.values() 
                        if t['consistency_score'] >= 0.7
                    ])
                }
            }
            
        except Exception as e:
            self.logger.error(f"ì£¼ì–¼ë¦¬ ìš©ì–´ ì¼ê´€ì„± ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            return {
                'term_usage_by_source': {},
                'overall_score': 0.0,
                'consistency_level': 'error'
            }

    def analyze_structural_consistency(self, 
                                     audio_content: Dict = None,
                                     image_content: Dict = None,
                                     document_content: Dict = None) -> Dict:
        """êµ¬ì¡°ì  ì¼ê´€ì„± ë¶„ì„"""
        try:
            # ê° ì†ŒìŠ¤ì˜ êµ¬ì¡° ì •ë³´ ì¶”ì¶œ
            structure_info = {}
            
            if audio_content:
                structure_info['audio'] = self._analyze_audio_structure(audio_content)
            
            if image_content:
                structure_info['image'] = self._analyze_image_structure(image_content)
            
            if document_content:
                structure_info['document'] = self._analyze_document_structure(document_content)
            
            # êµ¬ì¡° ì¼ê´€ì„± ê²€ì¦
            consistency_checks = {}
            
            # ì„¹ì…˜/ì£¼ì œ ì¼ê´€ì„±
            if len(structure_info) >= 2:
                section_consistency = self._check_section_consistency(structure_info)
                consistency_checks['sections'] = section_consistency
            
            # ì •ë³´ ê³„ì¸µ ì¼ê´€ì„±
            hierarchy_consistency = self._check_hierarchy_consistency(structure_info)
            consistency_checks['hierarchy'] = hierarchy_consistency
            
            # ì „ì²´ êµ¬ì¡° ì¼ê´€ì„± ì ìˆ˜
            if consistency_checks:
                structure_scores = [
                    check['score'] for check in consistency_checks.values()
                    if isinstance(check, dict) and 'score' in check
                ]
                overall_score = np.mean(structure_scores) if structure_scores else 0.5
            else:
                overall_score = 0.5
            
            return {
                'structure_info': structure_info,
                'consistency_checks': consistency_checks,
                'overall_score': round(overall_score, 3),
                'structure_level': self._classify_structural_consistency(overall_score)
            }
            
        except Exception as e:
            self.logger.error(f"êµ¬ì¡°ì  ì¼ê´€ì„± ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            return {
                'structure_info': {},
                'overall_score': 0.0,
                'structure_level': 'error'
            }

    def analyze_translation_consistency(self, texts: Dict[str, str]) -> Dict:
        """ë²ˆì—­ í’ˆì§ˆ/ì¼ê´€ì„± ë¶„ì„"""
        try:
            # ì–¸ì–´ë³„ í…ìŠ¤íŠ¸ ë¶„ë¥˜
            language_texts = {}
            
            for source, text in texts.items():
                language = self._detect_primary_language(text)
                if language not in language_texts:
                    language_texts[language] = {}
                language_texts[language][source] = text
            
            if len(language_texts) < 2:
                return {
                    'languages_detected': list(language_texts.keys()),
                    'translation_quality': 'monolingual_content',
                    'consistency_score': 1.0
                }
            
            # ì–¸ì–´ ê°„ ë‚´ìš© ì¼ê´€ì„± ê²€ì¦
            translation_checks = {}
            
            languages = list(language_texts.keys())
            for i in range(len(languages)):
                for j in range(i + 1, len(languages)):
                    lang1, lang2 = languages[i], languages[j]
                    
                    # ë²ˆì—­ í’ˆì§ˆ ì¶”ì •
                    quality_score = self._estimate_translation_quality(
                        language_texts[lang1], language_texts[lang2]
                    )
                    
                    translation_checks[f"{lang1}_to_{lang2}"] = quality_score
            
            # ì „ì²´ ë²ˆì—­ ì¼ê´€ì„± ì ìˆ˜
            if translation_checks:
                translation_scores = [
                    check['consistency_score'] 
                    for check in translation_checks.values()
                ]
                overall_score = np.mean(translation_scores)
            else:
                overall_score = 1.0
            
            return {
                'languages_detected': list(language_texts.keys()),
                'language_distribution': {
                    lang: len(texts) for lang, texts in language_texts.items()
                },
                'translation_checks': translation_checks,
                'consistency_score': round(overall_score, 3),
                'translation_quality': self._classify_translation_quality(overall_score)
            }
            
        except Exception as e:
            self.logger.error(f"ë²ˆì—­ ì¼ê´€ì„± ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            return {
                'languages_detected': [],
                'translation_quality': 'error',
                'consistency_score': 0.0
            }

    def calculate_overall_consistency_score(self, results: Dict) -> Dict:
        """ì „ì²´ ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚°"""
        try:
            # ê°€ì¤‘ì¹˜ ì„¤ì •
            weights = {
                'semantic': 0.4,      # ì˜ë¯¸ì  ì¼ê´€ì„± 40%
                'temporal': 0.2,      # ì‹œê°„ ì¼ê´€ì„± 20%
                'terminology': 0.2,   # ìš©ì–´ ì¼ê´€ì„± 20%
                'structural': 0.1,    # êµ¬ì¡°ì  ì¼ê´€ì„± 10%
                'translation': 0.1    # ë²ˆì—­ ì¼ê´€ì„± 10%
            }
            
            # ê°œë³„ ì ìˆ˜ ì¶”ì¶œ ë° ì •ê·œí™”
            semantic_score = results.get('semantic_consistency', {}).get('average_similarity', 0)
            temporal_score = results.get('temporal_consistency', {}).get('consistency_score', 0)
            terminology_score = results.get('terminology_consistency', {}).get('overall_score', 0)
            structural_score = results.get('structural_consistency', {}).get('overall_score', 0)
            translation_score = results.get('translation_consistency', {}).get('consistency_score', 0)
            
            # ê°€ì¤‘ í‰ê·  ê³„ì‚°
            overall_score = (
                semantic_score * weights['semantic'] +
                temporal_score * weights['temporal'] +
                terminology_score * weights['terminology'] +
                structural_score * weights['structural'] +
                translation_score * weights['translation']
            )
            
            # ë“±ê¸‰ ë¶„ë¥˜
            if overall_score >= 0.8:
                level, status, color = 'excellent', 'ìš°ìˆ˜', 'ğŸŸ¢'
            elif overall_score >= 0.6:
                level, status, color = 'good', 'ì–‘í˜¸', 'ğŸŸ¡'
            elif overall_score >= 0.4:
                level, status, color = 'fair', 'ë³´í†µ', 'ğŸŸ '
            else:
                level, status, color = 'poor', 'ë¶ˆëŸ‰', 'ğŸ”´'
            
            return {
                'score': round(overall_score, 3),
                'percentage': round(overall_score * 100, 1),
                'level': level,
                'status': status,
                'color': color,
                'components': {
                    'semantic_score': round(semantic_score, 3),
                    'temporal_score': round(temporal_score, 3),
                    'terminology_score': round(terminology_score, 3),
                    'structural_score': round(structural_score, 3),
                    'translation_score': round(translation_score, 3)
                }
            }
            
        except Exception as e:
            self.logger.error(f"ì „ì²´ ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚° ì˜¤ë¥˜: {str(e)}")
            return {
                'score': 0.0,
                'level': 'error',
                'status': 'ì˜¤ë¥˜'
            }

    def generate_consistency_recommendations(self, results: Dict) -> List[Dict]:
        """ì¼ê´€ì„± ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        try:
            # ì˜ë¯¸ì  ì¼ê´€ì„± ê¶Œì¥ì‚¬í•­
            semantic = results.get('semantic_consistency', {})
            if semantic.get('consistency_level') in ['poor', 'fair']:
                recommendations.append({
                    'type': 'warning',
                    'icon': 'ğŸ“',
                    'title': 'ë‚´ìš© ì¼ê´€ì„± ë¶€ì¡±',
                    'message': 'ìŒì„±, ì´ë¯¸ì§€, ë¬¸ì„œ ê°„ ë‚´ìš©ì´ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë™ì¼í•œ ì£¼ì œì— ëŒ€í•´ ì¼ê´€ëœ ì„¤ëª…ì„ í•˜ì„¸ìš”',
                    'action': 'improve_content_consistency'
                })
            
            # ì‹œê°„ ë™ê¸°í™” ê¶Œì¥ì‚¬í•­
            temporal = results.get('temporal_consistency', {})
            if temporal.get('sync_quality') in ['poor', 'fair']:
                recommendations.append({
                    'type': 'warning',
                    'icon': 'â°',
                    'title': 'ì‹œê°„ ë™ê¸°í™” ë¬¸ì œ',
                    'message': 'ìŒì„±ê³¼ ì´ë¯¸ì§€ì˜ ì‹œê°„ì´ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë™ì‹œì— ë…¹ìŒ/ì´¬ì˜í•˜ê±°ë‚˜ ì‹œê°„ì„ ë§ì¶°ì£¼ì„¸ìš”',
                    'action': 'improve_temporal_sync'
                })
            
            # ìš©ì–´ ì¼ê´€ì„± ê¶Œì¥ì‚¬í•­
            terminology = results.get('terminology_consistency', {})
            if terminology.get('inconsistent_terms'):
                recommendations.append({
                    'type': 'info',
                    'icon': 'ğŸ’',
                    'title': 'ì£¼ì–¼ë¦¬ ìš©ì–´ ë¶ˆì¼ì¹˜',
                    'message': f"ìš©ì–´ í†µì¼ í•„ìš”: {', '.join(terminology.get('inconsistent_terms', [])[:3])}",
                    'action': 'standardize_jewelry_terminology'
                })
            
            # êµ¬ì¡°ì  ì¼ê´€ì„± ê¶Œì¥ì‚¬í•­
            structural = results.get('structural_consistency', {})
            if structural.get('structure_level') in ['poor', 'fair']:
                recommendations.append({
                    'type': 'info',
                    'icon': 'ğŸ—ï¸',
                    'title': 'êµ¬ì¡° ê°œì„  í•„ìš”',
                    'message': 'ê° ë§¤ì²´ì˜ ì •ë³´ êµ¬ì¡°ë¥¼ ì¼ì¹˜ì‹œì¼œ ì£¼ì„¸ìš”',
                    'action': 'improve_content_structure'
                })
            
            # ë²ˆì—­ í’ˆì§ˆ ê¶Œì¥ì‚¬í•­
            translation = results.get('translation_consistency', {})
            if translation.get('translation_quality') in ['poor', 'fair']:
                recommendations.append({
                    'type': 'info',
                    'icon': 'ğŸŒ',
                    'title': 'ë²ˆì—­ í’ˆì§ˆ ê°œì„ ',
                    'message': 'ë‹¤êµ­ì–´ ë‚´ìš© ê°„ ë²ˆì—­ í’ˆì§ˆì„ ê°œì„ í•´ì£¼ì„¸ìš”',
                    'action': 'improve_translation_quality'
                })
            
            # ì „ì²´ ì¼ê´€ì„± ê¶Œì¥ì‚¬í•­
            overall = results.get('overall_consistency', {})
            if overall.get('level') == 'poor':
                recommendations.append({
                    'type': 'critical',
                    'icon': 'ğŸ”´',
                    'title': 'ì „ì²´ ì¬ê²€í†  í•„ìš”',
                    'message': 'ë©€í‹°ëª¨ë‹¬ ë‚´ìš©ì˜ ì¼ê´€ì„±ì´ ë§¤ìš° ë‚®ìŠµë‹ˆë‹¤. ì „ì²´ì ìœ¼ë¡œ ì¬ê²€í† í•´ì£¼ì„¸ìš”',
                    'action': 'comprehensive_review_needed'
                })
            elif overall.get('level') == 'excellent':
                recommendations.append({
                    'type': 'success',
                    'icon': 'ğŸŸ¢',
                    'title': 'ì¼ê´€ì„± ìš°ìˆ˜',
                    'message': 'ëª¨ë“  ë§¤ì²´ ê°„ ë‚´ìš©ì´ ì˜ ì¼ì¹˜í•©ë‹ˆë‹¤',
                    'action': 'maintain_consistency'
                })
            
            return recommendations
            
        except Exception as e:
            self.logger.error(f"ì¼ê´€ì„± ê¶Œì¥ì‚¬í•­ ìƒì„± ì˜¤ë¥˜: {str(e)}")
            return [{
                'type': 'error',
                'icon': 'âŒ',
                'title': 'ì¼ê´€ì„± ë¶„ì„ ì˜¤ë¥˜',
                'message': 'ë‚´ìš© ì¼ê´€ì„± ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤',
                'action': 'retry_consistency_check'
            }]

    # === ë‚´ë¶€ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ ===
    
    def _identify_input_sources(self, audio_content, image_content, document_content) -> List[str]:
        """ì…ë ¥ ì†ŒìŠ¤ ì‹ë³„"""
        sources = []
        if audio_content:
            sources.append('audio')
        if image_content:
            sources.append('image') 
        if document_content:
            sources.append('document')
        return sources
    
    def _extract_texts(self, audio_content, image_content, document_content) -> Dict[str, str]:
        """ê° ì†ŒìŠ¤ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        texts = {}
        
        if audio_content and isinstance(audio_content, dict):
            if 'text' in audio_content:
                texts['audio'] = audio_content['text']
            elif 'transcription' in audio_content:
                texts['audio'] = audio_content['transcription']
        
        if image_content and isinstance(image_content, dict):
            if 'text' in image_content:
                texts['image'] = image_content['text']
            elif 'ocr_text' in image_content:
                texts['image'] = image_content['ocr_text']
        
        if document_content and isinstance(document_content, dict):
            if 'text' in document_content:
                texts['document'] = document_content['text']
            elif 'content' in document_content:
                texts['document'] = document_content['content']
        
        return texts
    
    def _has_temporal_data(self, audio_content, image_content) -> bool:
        """ì‹œê°„ ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
        temporal_keys = ['timestamp', 'time', 'created_at', 'recorded_at', 'captured_at']
        
        if audio_content and isinstance(audio_content, dict):
            if any(key in audio_content for key in temporal_keys):
                return True
        
        if image_content and isinstance(image_content, dict):
            if any(key in image_content for key in temporal_keys):
                return True
        
        return False
    
    def _extract_timestamps(self, audio_content, image_content, sync_data) -> Dict[str, float]:
        """íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ ë° í†µì¼"""
        timestamps = {}
        
        # ê¸°ì¤€ ì‹œê°„ (Unix timestamp)
        try:
            if sync_data and 'reference_time' in sync_data:
                ref_time = sync_data['reference_time']
            else:
                ref_time = datetime.now().timestamp()
            
            # ìŒì„± íƒ€ì„ìŠ¤íƒ¬í”„
            if audio_content and isinstance(audio_content, dict):
                audio_time = self._parse_timestamp(audio_content, ref_time)
                if audio_time is not None:
                    timestamps['audio'] = audio_time
            
            # ì´ë¯¸ì§€ íƒ€ì„ìŠ¤íƒ¬í”„
            if image_content and isinstance(image_content, dict):
                image_time = self._parse_timestamp(image_content, ref_time)
                if image_time is not None:
                    timestamps['image'] = image_time
            
        except Exception as e:
            self.logger.error(f"íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
        
        return timestamps
    
    def _parse_timestamp(self, content: Dict, ref_time: float) -> Optional[float]:
        """ê°œë³„ íƒ€ì„ìŠ¤íƒ¬í”„ íŒŒì‹±"""
        temporal_keys = ['timestamp', 'time', 'created_at', 'recorded_at', 'captured_at']
        
        for key in temporal_keys:
            if key in content:
                value = content[key]
                
                if isinstance(value, (int, float)):
                    return float(value)
                elif isinstance(value, str):
                    try:
                        # ISO í˜•ì‹ íŒŒì‹± ì‹œë„
                        dt = datetime.fromisoformat(value.replace('Z', '+00:00'))
                        return dt.timestamp()
                    except:
                        try:
                            # Unix timestamp íŒŒì‹± ì‹œë„
                            return float(value)
                        except:
                            continue
        
        return None
    
    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """ê¸°ë³¸ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        if not text1 or not text2:
            return 0.0
        
        # SequenceMatcherë¥¼ ì‚¬ìš©í•œ ìœ ì‚¬ë„
        similarity = SequenceMatcher(None, text1.lower(), text2.lower()).ratio()
        return similarity
    
    def _calculate_keyword_similarity(self, text1: str, text2: str) -> float:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚°"""
        if not text1 or not text2:
            return 0.0
        
        # ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„í• 
        words1 = set(re.findall(r'\w+', text1.lower()))
        words2 = set(re.findall(r'\w+', text2.lower()))
        
        # ìì¹´ë“œ ìœ ì‚¬ë„
        intersection = len(words1.intersection(words2))
        union = len(words1.union(words2))
        
        return intersection / union if union > 0 else 0.0
    
    def _calculate_jewelry_term_similarity(self, text1: str, text2: str) -> float:
        """ì£¼ì–¼ë¦¬ ìš©ì–´ ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚°"""
        if not text1 or not text2:
            return 0.0
        
        # ì£¼ì–¼ë¦¬ ìš©ì–´ ì¶”ì¶œ
        terms1 = self._extract_jewelry_terms(text1)
        terms2 = self._extract_jewelry_terms(text2)
        
        if not terms1 and not terms2:
            return 1.0  # ë‘˜ ë‹¤ ì£¼ì–¼ë¦¬ ìš©ì–´ê°€ ì—†ìœ¼ë©´ ì¼ê´€ì„± ìˆìŒ
        
        # ì •ê·œí™”ëœ ìš©ì–´ë¡œ ë³€í™˜
        normalized_terms1 = set()
        normalized_terms2 = set()
        
        for standard_term, variants in self.jewelry_term_mappings.items():
            for term in terms1:
                if term.lower() in [v.lower() for v in variants]:
                    normalized_terms1.add(standard_term)
            for term in terms2:
                if term.lower() in [v.lower() for v in variants]:
                    normalized_terms2.add(standard_term)
        
        # ìì¹´ë“œ ìœ ì‚¬ë„
        if not normalized_terms1 and not normalized_terms2:
            return 1.0
        
        intersection = len(normalized_terms1.intersection(normalized_terms2))
        union = len(normalized_terms1.union(normalized_terms2))
        
        return intersection / union if union > 0 else 0.0
    
    def _extract_jewelry_terms(self, text: str) -> Dict[str, int]:
        """í…ìŠ¤íŠ¸ì—ì„œ ì£¼ì–¼ë¦¬ ìš©ì–´ ì¶”ì¶œ"""
        terms = {}
        text_lower = text.lower()
        
        # ëª¨ë“  ì£¼ì–¼ë¦¬ ìš©ì–´ ë³€í˜• ê²€ìƒ‰
        for standard_term, variants in self.jewelry_term_mappings.items():
            count = 0
            for variant in variants:
                count += len(re.findall(r'\b' + re.escape(variant.lower()) + r'\b', text_lower))
            
            if count > 0:
                terms[standard_term] = count
        
        # ì¶”ê°€ ì£¼ì–¼ë¦¬ ê´€ë ¨ ë‹¨ì–´ ê²€ìƒ‰
        additional_terms = [
            'ring', 'ë°˜ì§€', 'necklace', 'ëª©ê±¸ì´', 'bracelet', 'íŒ”ì°Œ',
            'earring', 'ê·€ê±¸ì´', 'brooch', 'ë¸Œë¡œì¹˜', 'pendant', 'íœë˜íŠ¸'
        ]
        
        for term in additional_terms:
            count = len(re.findall(r'\b' + re.escape(term.lower()) + r'\b', text_lower))
            if count > 0:
                terms[term] = count
        
        return terms
    
    def _check_term_consistency_across_sources(self, standard_term: str, 
                                             variants: List[str], 
                                             term_usage: Dict) -> Dict:
        """ì†ŒìŠ¤ ê°„ ìš©ì–´ ì¼ê´€ì„± ê²€ì¦"""
        found_variants = {}
        sources_with_term = []
        
        for source, terms in term_usage.items():
            if standard_term in terms:
                found_variants[source] = standard_term
                sources_with_term.append(source)
        
        # ìš©ì–´ê°€ ë°œê²¬ëœ ì†ŒìŠ¤ê°€ ì—†ìœ¼ë©´ ê²€ì¦ ë¶ˆê°€
        if not sources_with_term:
            return {
                'found_in_sources': [],
                'consistency_score': 1.0,  # ì—†ìœ¼ë©´ ì¼ê´€ì„± ë¬¸ì œë„ ì—†ìŒ
                'variants_used': {}
            }
        
        # ëª¨ë“  ì†ŒìŠ¤ì—ì„œ ê°™ì€ í‘œì¤€ ìš©ì–´ë¥¼ ì‚¬ìš©í•˜ë©´ ì™„ì „ ì¼ê´€ì„±
        consistency_score = 1.0
        
        return {
            'found_in_sources': sources_with_term,
            'consistency_score': consistency_score,
            'variants_used': found_variants
        }
    
    def _detect_primary_language(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ì˜ ì£¼ìš” ì–¸ì–´ ê°ì§€"""
        if not text:
            return 'unknown'
        
        # í•œê¸€ ë¬¸ì ë¹„ìœ¨
        korean_chars = len(re.findall(r'[ê°€-í£]', text))
        # ì˜ë¬¸ ë¬¸ì ë¹„ìœ¨
        english_chars = len(re.findall(r'[a-zA-Z]', text))
        # ì¤‘ë¬¸ ë¬¸ì ë¹„ìœ¨
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        # ì¼ë¬¸ ë¬¸ì ë¹„ìœ¨
        japanese_chars = len(re.findall(r'[\u3040-\u309f\u30a0-\u30ff]', text))
        
        total_chars = korean_chars + english_chars + chinese_chars + japanese_chars
        
        if total_chars == 0:
            return 'unknown'
        
        # ê°€ì¥ ë§ì€ ë¬¸ìì˜ ì–¸ì–´ ë°˜í™˜
        char_counts = {
            'korean': korean_chars,
            'english': english_chars,
            'chinese': chinese_chars,
            'japanese': japanese_chars
        }
        
        return max(char_counts, key=char_counts.get)
    
    def _estimate_translation_quality(self, texts1: Dict, texts2: Dict) -> Dict:
        """ë²ˆì—­ í’ˆì§ˆ ì¶”ì •"""
        # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜ ë²ˆì—­ í’ˆì§ˆ ì¶”ì •
        # ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ë²ˆì—­ í’ˆì§ˆ ì¸¡ì • ì•Œê³ ë¦¬ì¦˜ í•„ìš”
        
        all_texts1 = ' '.join(texts1.values())
        all_texts2 = ' '.join(texts2.values())
        
        # ê¸°ë³¸ ìœ ì‚¬ë„ (êµ¬ì¡°ì  ìœ ì‚¬ì„±)
        basic_similarity = self._calculate_text_similarity(all_texts1, all_texts2)
        
        # ì£¼ì–¼ë¦¬ ìš©ì–´ ë§¤í•‘ ì¼ê´€ì„±
        jewelry_consistency = self._calculate_jewelry_term_similarity(all_texts1, all_texts2)
        
        # ë²ˆì—­ í’ˆì§ˆ ì ìˆ˜ (êµ¬ì¡° + ìš©ì–´ ì¼ê´€ì„±)
        quality_score = (basic_similarity * 0.6 + jewelry_consistency * 0.4)
        
        return {
            'consistency_score': quality_score,
            'basic_similarity': basic_similarity,
            'jewelry_consistency': jewelry_consistency
        }
    
    def _analyze_audio_structure(self, audio_content: Dict) -> Dict:
        """ìŒì„± ë‚´ìš© êµ¬ì¡° ë¶„ì„"""
        structure = {
            'type': 'audio',
            'has_timestamps': 'timestamp' in audio_content,
            'has_segments': 'segments' in audio_content,
            'estimated_duration': audio_content.get('duration', 0)
        }
        
        # í…ìŠ¤íŠ¸ì—ì„œ êµ¬ì¡° ì¶”ì •
        if 'text' in audio_content:
            text = audio_content['text']
            structure['word_count'] = len(text.split())
            structure['sentence_count'] = len(re.split(r'[.!?]', text))
            structure['paragraph_count'] = len(text.split('\n\n'))
        
        return structure
    
    def _analyze_image_structure(self, image_content: Dict) -> Dict:
        """ì´ë¯¸ì§€ ë‚´ìš© êµ¬ì¡° ë¶„ì„"""
        structure = {
            'type': 'image',
            'has_ocr_text': 'text' in image_content or 'ocr_text' in image_content,
            'has_regions': 'regions' in image_content,
        }
        
        # OCR í…ìŠ¤íŠ¸ì—ì„œ êµ¬ì¡° ì¶”ì •
        text_key = 'text' if 'text' in image_content else 'ocr_text'
        if text_key in image_content:
            text = image_content[text_key]
            structure['text_length'] = len(text)
            structure['line_count'] = len(text.split('\n'))
        
        return structure
    
    def _analyze_document_structure(self, document_content: Dict) -> Dict:
        """ë¬¸ì„œ ë‚´ìš© êµ¬ì¡° ë¶„ì„"""
        structure = {
            'type': 'document',
            'has_sections': 'sections' in document_content,
            'has_headers': 'headers' in document_content,
        }
        
        # í…ìŠ¤íŠ¸ì—ì„œ êµ¬ì¡° ì¶”ì •
        text_key = 'text' if 'text' in document_content else 'content'
        if text_key in document_content:
            text = document_content[text_key]
            structure['total_length'] = len(text)
            structure['paragraph_count'] = len(text.split('\n\n'))
            
            # í—¤ë” ì¶”ì • (# ë˜ëŠ” ëŒ€ë¬¸ì ë¼ì¸)
            headers = re.findall(r'^[A-Z\s]{5,}$', text, re.MULTILINE)
            structure['estimated_headers'] = len(headers)
        
        return structure
    
    def _check_section_consistency(self, structure_info: Dict) -> Dict:
        """ì„¹ì…˜ ì¼ê´€ì„± ê²€ì¦"""
        # ê° ì†ŒìŠ¤ì˜ ì„¹ì…˜/êµ¬ì¡° ì •ë³´ ë¹„êµ
        section_scores = []
        
        sources = list(structure_info.keys())
        for i in range(len(sources)):
            for j in range(i + 1, len(sources)):
                source1, source2 = sources[i], sources[j]
                struct1, struct2 = structure_info[source1], structure_info[source2]
                
                # êµ¬ì¡° ìœ ì‚¬ì„± ì ìˆ˜ ê³„ì‚° (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
                similarity = 0.5  # ê¸°ë³¸ê°’
                
                # í…ìŠ¤íŠ¸ ê¸¸ì´ ë¹„êµ
                if 'word_count' in struct1 and 'text_length' in struct2:
                    ratio = min(struct1['word_count'], struct2['text_length']) / max(struct1['word_count'], struct2['text_length'])
                    similarity = ratio
                
                section_scores.append(similarity)
        
        avg_score = np.mean(section_scores) if section_scores else 0.5
        
        return {
            'score': round(avg_score, 3),
            'comparisons': len(section_scores)
        }
    
    def _check_hierarchy_consistency(self, structure_info: Dict) -> Dict:
        """ì •ë³´ ê³„ì¸µ ì¼ê´€ì„± ê²€ì¦"""
        # ê°„ë‹¨í•œ ê³„ì¸µ ì¼ê´€ì„± ê²€ì¦
        hierarchy_score = 0.5  # ê¸°ë³¸ê°’
        
        # ëª¨ë“  ì†ŒìŠ¤ê°€ ë¹„ìŠ·í•œ ë³µì¡ë„ë¥¼ ê°€ì§€ëŠ”ì§€ í™•ì¸
        complexities = []
        for source, struct in structure_info.items():
            complexity = 0
            
            if struct.get('paragraph_count', 0) > 3:
                complexity += 1
            if struct.get('estimated_headers', 0) > 1:
                complexity += 1
            if struct.get('sentence_count', 0) > 10:
                complexity += 1
            
            complexities.append(complexity)
        
        if complexities:
            # ë³µì¡ë„ ë¶„ì‚°ì´ ë‚®ì„ìˆ˜ë¡ ì¼ê´€ì„± ë†’ìŒ
            std_dev = np.std(complexities)
            hierarchy_score = max(0.0, 1.0 - std_dev / 3)
        
        return {
            'score': round(hierarchy_score, 3),
            'complexities': complexities
        }
    
    def _classify_semantic_consistency(self, similarity: float) -> str:
        """ì˜ë¯¸ì  ì¼ê´€ì„± ë“±ê¸‰ ë¶„ë¥˜"""
        if similarity >= self.consistency_thresholds['semantic_similarity_excellent']:
            return 'excellent'
        elif similarity >= self.consistency_thresholds['semantic_similarity_good']:
            return 'good'
        elif similarity >= self.consistency_thresholds['semantic_similarity_fair']:
            return 'fair'
        else:
            return 'poor'
    
    def _classify_temporal_sync(self, offset: float) -> str:
        """ì‹œê°„ ë™ê¸°í™” ë“±ê¸‰ ë¶„ë¥˜"""
        if offset <= self.consistency_thresholds['temporal_sync_excellent']:
            return 'excellent'
        elif offset <= self.consistency_thresholds['temporal_sync_good']:
            return 'good'
        elif offset <= self.consistency_thresholds['temporal_sync_fair']:
            return 'fair'
        else:
            return 'poor'
    
    def _classify_terminology_consistency(self, score: float) -> str:
        """ìš©ì–´ ì¼ê´€ì„± ë“±ê¸‰ ë¶„ë¥˜"""
        if score >= self.consistency_thresholds['terminology_consistency_excellent']:
            return 'excellent'
        elif score >= self.consistency_thresholds['terminology_consistency_good']:
            return 'good'
        elif score >= self.consistency_thresholds['terminology_consistency_fair']:
            return 'fair'
        else:
            return 'poor'
    
    def _classify_structural_consistency(self, score: float) -> str:
        """êµ¬ì¡°ì  ì¼ê´€ì„± ë“±ê¸‰ ë¶„ë¥˜"""
        if score >= 0.8:
            return 'excellent'
        elif score >= 0.6:
            return 'good'
        elif score >= 0.4:
            return 'fair'
        else:
            return 'poor'
    
    def _classify_translation_quality(self, score: float) -> str:
        """ë²ˆì—­ í’ˆì§ˆ ë“±ê¸‰ ë¶„ë¥˜"""
        if score >= 0.8:
            return 'excellent'
        elif score >= 0.6:
            return 'good'
        elif score >= 0.4:
            return 'fair'
        else:
            return 'poor'
    
    def _get_timestamp(self) -> str:
        """í˜„ì¬ íƒ€ì„ìŠ¤íƒ¬í”„ ë°˜í™˜"""
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")


# ì‚¬ìš© ì˜ˆì œ
if __name__ == "__main__":
    checker = ContentConsistencyChecker()
    
    print("ğŸ” Content Consistency Checker v2.1 - í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 50)
    
    # ì‹¤ì œ ì‚¬ìš© ì˜ˆì œ
    # audio_data = {'text': 'ë‹¤ì´ì•„ëª¬ë“œ ë°˜ì§€ì˜ í’ˆì§ˆì€ 4Cë¡œ í‰ê°€ë©ë‹ˆë‹¤', 'timestamp': 1625097600}
    # image_data = {'text': 'Diamond ring quality: 4C evaluation', 'timestamp': 1625097605}
    # 
    # result = checker.check_content_consistency(
    #     audio_content=audio_data,
    #     image_content=image_data,
    #     check_type='jewelry_focused'
    # )
    # 
    # print(f"ì „ì²´ ì¼ê´€ì„±: {result['overall_consistency']['percentage']}%")
    # print(f"ì˜ë¯¸ì  ìœ ì‚¬ë„: {result['semantic_consistency']['average_similarity']:.3f}")
    
    print("ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ âœ…")
