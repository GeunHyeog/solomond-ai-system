{
  "name": "ğŸ¤– Ollama AI Intelligence Common Workflow",
  "nodes": [
    {
      "parameters": {
        "httpMethod": "POST",
        "path": "ollama-ai-intelligence",
        "responseMode": "responseNode",
        "options": {}
      },
      "id": "ollama_trigger",
      "name": "ğŸ¤– Ollama AI ìš”ì²­",
      "type": "n8n-nodes-base.webhook",
      "typeVersion": 1.1,
      "position": [240, 400]
    },
    {
      "parameters": {
        "jsCode": "// ğŸ¤– Ollama AI ëª¨ë¸ ì§€ëŠ¥ì  ì„ íƒ ë° ìµœì í™” ì‹œìŠ¤í…œ\nconst request = $input.first().json;\n\n// 1. ìš”ì²­ ë¶„ì„ ë° ëª¨ë“ˆ íƒ€ì… ì‹ë³„\nconst requestAnalysis = {\n  timestamp: new Date().toISOString(),\n  session_id: request.session_id || Date.now().toString(),\n  \n  // ëª¨ë“ˆ íƒ€ì… ê²°ì •\n  module_type: request.module_type || request.selected_module || 'general',\n  \n  // ì‘ì—… ë³µì¡ë„ ë¶„ì„\n  task_complexity: {\n    content_length: request.content ? request.content.length : 0,\n    file_count: request.file_count || request.files?.length || 0,\n    multimodal: !!(request.has_audio || request.has_image || request.has_video),\n    analysis_depth: request.analysis_strategy || 'standard'\n  },\n  \n  // ì„±ëŠ¥ ìš”êµ¬ì‚¬í•­ ë¶„ì„\n  performance_requirements: {\n    speed_priority: request.priority === 'high' || request.urgent,\n    accuracy_priority: request.module_type === 'gemstone_analysis',\n    resource_constraints: request.resource_allocation?.memory_requirement === 'high'\n  }\n};\n\n// 2. SOLOMOND AI ëª¨ë¸ ì„ íƒ ë§¤íŠ¸ë¦­ìŠ¤ (ì‹¤ì œ êµ¬í˜„ëœ ë¡œì§)\nconst modelSelectionMatrix = {\n  'conference_analysis': {\n    primary: 'qwen2.5:7b',\n    reasoning: 'ë¹ ë¥¸ íšŒì˜ ë¶„ì„ì— ìµœì í™”, í•œêµ­ì–´ ì²˜ë¦¬ ìš°ìˆ˜, ì£¼ì–¼ë¦¬ ì»¨í…ìŠ¤íŠ¸ ì´í•´',\n    fallbacks: ['llama3.2:3b', 'gemma2:2b'],\n    performance_profile: {\n      speed: 'high',\n      accuracy: 'high',\n      korean_support: 'excellent',\n      domain_knowledge: 'jewelry_conference'\n    }\n  },\n  'web_crawler': {\n    primary: 'qwen2.5:7b',\n    reasoning: 'ë‰´ìŠ¤ ìš”ì•½ ë° ë²ˆì—­ íŠ¹í™”, ë‹¤êµ­ì–´ ì²˜ë¦¬ ëŠ¥ë ¥',\n    fallbacks: ['qwen2.5:7b', 'gemma3:4b'],\n    performance_profile: {\n      speed: 'high',\n      accuracy: 'high',\n      multilingual: 'excellent',\n      domain_knowledge: 'news_analysis'\n    }\n  },\n  'gemstone_analysis': {\n    primary: 'qwen2.5:7b',\n    reasoning: 'ë³´ì„ ì „ë¬¸ ë¶„ì„ ë° ì‚°ì§€ ì‹ë³„, ì •ë°€í•œ ë¶„ì„ ëŠ¥ë ¥',\n    fallbacks: ['gemma3:27b', 'qwen2.5:7b'],\n    performance_profile: {\n      speed: 'medium',\n      accuracy: 'very_high',\n      precision: 'critical',\n      domain_knowledge: 'gemology'\n    }\n  },\n  'cad_conversion': {\n    primary: 'llama3.2:3b',\n    reasoning: 'ë¹ ë¥¸ ì´ë¯¸ì§€ ì²˜ë¦¬, 3D ë³€í™˜ ìµœì í™”, ê²½ëŸ‰í™”',\n    fallbacks: ['gemma3:4b', 'qwen2.5:7b'],\n    performance_profile: {\n      speed: 'very_high',\n      accuracy: 'medium',\n      image_processing: 'optimized',\n      domain_knowledge: '3d_modeling'\n    }\n  },\n  'general': {\n    primary: 'qwen2.5:7b',\n    reasoning: 'ë²”ìš© ëª©ì , ê· í˜•ì¡íŒ ì„±ëŠ¥',\n    fallbacks: ['llama3.2:3b', 'gemma3:4b'],\n    performance_profile: {\n      speed: 'high',\n      accuracy: 'high',\n      versatility: 'excellent',\n      domain_knowledge: 'general'\n    }\n  }\n};\n\n// 3. ëª¨ë¸ ê°€ìš©ì„± í™•ì¸ ë¡œì§ (ì‹¤ì œ ì‹œìŠ¤í…œ ë°˜ì˜)\nconst modelAvailability = {\n  available_models: [\n    'gpt-oss:20b',    // 13GB - ìµœì‹  8ì‹œê°„ ì „ ì—…ë°ì´íŠ¸\n    'qwen3:8b',       // 5.2GB - ìƒˆë¡œìš´ Qwen3 ëª¨ë¸  \n    'gemma3:27b',     // 17GB - ì´ˆëŒ€í˜• ëª¨ë¸\n    'qwen2.5:7b',     // 4.7GB - ì•ˆì •ì  ì„±ëŠ¥\n    'gemma3:4b',      // 3.3GB - ê²½ëŸ‰ ê³ ì„±ëŠ¥\n    'llama3.2:3b'     // ê²½ëŸ‰ ë¹ ë¥¸ ì²˜ë¦¬\n  ],\n  \n  // ì‹¤ì‹œê°„ ëª¨ë¸ ìƒíƒœ ì²´í¬\n  model_health_check: {\n    endpoint: 'http://localhost:11434/api/tags',\n    timeout: 5,\n    required_for_selection: true\n  }\n};\n\n// 4. ì§€ëŠ¥ì  ëª¨ë¸ ì„ íƒ ì•Œê³ ë¦¬ì¦˜\nconst selectedConfig = modelSelectionMatrix[requestAnalysis.module_type] || modelSelectionMatrix['general'];\nconst modelDecision = {\n  primary_model: selectedConfig.primary,\n  selection_reasoning: [\n    `ëª¨ë“ˆ íƒ€ì…: ${requestAnalysis.module_type}`,\n    `ì„ íƒ ì´ìœ : ${selectedConfig.reasoning}`,\n    `ì„±ëŠ¥ í”„ë¡œí•„: ${Object.entries(selectedConfig.performance_profile).map(([k,v]) => `${k}=${v}`).join(', ')}`\n  ],\n  fallback_strategy: {\n    models: selectedConfig.fallbacks,\n    criteria: [\n      '1ìˆœìœ„: ì¶”ì²œ ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥ ì‹œ',\n      '2ìˆœìœ„: qwen2.5:7b â†’ llama3.2:3b â†’ gemma2:2b ìˆœì„œ',\n      '3ìˆœìœ„: ì‚¬ìš© ê°€ëŠ¥í•œ ì²« ë²ˆì§¸ ëª¨ë¸'\n    ]\n  }\n};\n\n// 5. í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ì „ëµ\nconst promptStrategy = {\n  template_selection: {\n    jewelry_prompts: {\n      'conference_analysis': {\n        context: 'ì£¼ì–¼ë¦¬ ì—…ê³„ ì»¨í¼ëŸ°ìŠ¤ ë¶„ì„ ì „ë¬¸ê°€',\n        focus: ['í•µì‹¬ ë…¼ì˜ì‚¬í•­', 'ì—…ê³„ ì¸ì‚¬ì´íŠ¸', 'ì‹¤í–‰ ì•¡ì…˜', 'ì¶”ê°€ ì¡°ì‚¬ ì˜ì—­'],\n        format: 'ê°„ê²°í•˜ê³  êµ¬ì²´ì ì¸ ë¶„ì„ ê²°ê³¼'\n      },\n      'news_summary': {\n        context: 'ì£¼ì–¼ë¦¬ ì—…ê³„ ì „ë¬¸ ì• ë„ë¦¬ìŠ¤íŠ¸',\n        focus: ['í•µì‹¬ ë‚´ìš©', 'ì—…ê³„ ì˜í–¥', 'í‚¤ì›Œë“œ', 'ì¤‘ìš”ë„'],\n        format: 'êµ¬ì¡°í™”ëœ ìš”ì•½'\n      },\n      'gemstone_identification': {\n        context: 'ë³´ì„í•™ ì „ë¬¸ê°€',\n        focus: ['ìƒ‰ìƒ íŠ¹ì„±', 'ë‚´í¬ë¬¼ íŒ¨í„´', 'ê´‘í•™ì  íŠ¹ì„±', 'ì§€ì§ˆí•™ì  ë°°ê²½'],\n        format: 'ì •ë°€í•œ ì‚°ì§€ ë¶„ì„'\n      }\n    }\n  },\n  \n  // ëª¨ë¸ë³„ ìµœì í™” íŒŒë¼ë¯¸í„°\n  optimization_parameters: {\n    temperature: requestAnalysis.module_type === 'cad_conversion' ? 0.5 : 0.7,\n    max_tokens: requestAnalysis.module_type === 'gemstone_analysis' ? 1500 : 2000,\n    top_k: 40,\n    top_p: 0.9,\n    reasoning: [\n      `Temperature ${requestAnalysis.module_type === 'cad_conversion' ? '0.5 (ì •í™•ì„± ìš°ì„ )' : '0.7 (ì°½ì˜ì„±-ì •í™•ì„± ê· í˜•)'}`,\n      `Max tokens ${requestAnalysis.module_type === 'gemstone_analysis' ? '1500 (ê°„ê²°í•¨)' : '2000 (ì¶©ë¶„í•œ ì„¤ëª…)'}`\n    ]\n  }\n};\n\n// 6. í’ˆì§ˆ ê´€ë¦¬ ë° ëª¨ë‹ˆí„°ë§\nconst qualityControl = {\n  response_validation: {\n    min_confidence: 0.7,\n    hallucination_check: true,\n    domain_relevance_check: requestAnalysis.module_type !== 'general',\n    korean_language_check: true\n  },\n  \n  performance_monitoring: {\n    response_time_limit: requestAnalysis.performance_requirements.speed_priority ? 30 : 60,\n    memory_usage_limit: requestAnalysis.performance_requirements.resource_constraints ? 'high' : 'medium',\n    concurrent_request_limit: 3\n  },\n  \n  error_handling: {\n    retry_strategy: 'exponential_backoff',\n    max_retries: 2,\n    fallback_model_on_error: true,\n    graceful_degradation: true\n  }\n};\n\n// 7. ìµœì¢… Ollama ì§€ëŠ¥ ê²°ì •\nconst ollamaIntelligence = {\n  timestamp: requestAnalysis.timestamp,\n  session_id: requestAnalysis.session_id,\n  \n  model_decision: modelDecision,\n  prompt_strategy: promptStrategy,\n  quality_control: qualityControl,\n  \n  execution_config: {\n    endpoint: 'http://localhost:11434/api/generate',\n    model: modelDecision.primary_model,\n    stream: false,\n    options: promptStrategy.optimization_parameters\n  },\n  \n  success_criteria: {\n    model_availability: 'primary ë˜ëŠ” fallback ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥',\n    response_quality: 'ì‹ ë¢°ë„ > 70%',\n    performance: `ì‘ë‹µ ì‹œê°„ < ${qualityControl.performance_monitoring.response_time_limit}ì´ˆ`,\n    relevance: 'ë„ë©”ì¸ ê´€ë ¨ì„± í™•ì¸'\n  },\n  \n  monitoring_metrics: {\n    model_selection_accuracy: 'track',\n    prompt_effectiveness: 'measure',\n    user_satisfaction: 'collect_feedback',\n    system_performance: 'continuous_monitoring'\n  }\n};\n\nreturn ollamaIntelligence;"
      },
      "id": "ollama_intelligence_analyzer",
      "name": "ğŸ§  Ollama ì§€ëŠ¥ ë¶„ì„ê¸°",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [460, 400]
    },
    {
      "parameters": {
        "method": "GET",
        "url": "{{ $json.execution_config.endpoint.replace('/api/generate', '/api/tags') }}",
        "options": {
          "timeout": 5000
        }
      },
      "id": "check_model_availability",
      "name": "ğŸ” ëª¨ë¸ ê°€ìš©ì„± ì²´í¬",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [680, 350]
    },
    {
      "parameters": {
        "jsCode": "// ğŸ¯ ì‹¤ì‹œê°„ ëª¨ë¸ ì„ íƒ ìµœì¢… ê²°ì •\nconst intelligence = $input.first().json;\nconst availabilityResponse = $input.all()[1] ? $input.all()[1].json : null;\n\n// 1. ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ íŒŒì‹±\nlet availableModels = [];\nif (availabilityResponse && availabilityResponse.models) {\n  availableModels = availabilityResponse.models.map(m => m.name);\n} else {\n  // í´ë°±: ê¸°ë³¸ ëª¨ë¸ ëª©ë¡ ì‚¬ìš©\n  availableModels = ['qwen2.5:7b', 'llama3.2:3b', 'gemma3:4b'];\n}\n\n// 2. ìµœì  ëª¨ë¸ ì„ íƒ ë¡œì§ ì‹¤í–‰\nlet finalModel = intelligence.model_decision.primary_model;\nlet selectionReason = 'ì¶”ì²œ ëª¨ë¸ ì„ íƒ';\n\n// ì¶”ì²œ ëª¨ë¸ì´ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸\nif (!availableModels.includes(intelligence.model_decision.primary_model)) {\n  // í´ë°± ì „ëµ ì‹¤í–‰\n  const fallbacks = intelligence.model_decision.fallback_strategy.models;\n  let modelFound = false;\n  \n  for (const fallbackModel of fallbacks) {\n    if (availableModels.includes(fallbackModel)) {\n      finalModel = fallbackModel;\n      selectionReason = `í´ë°± ëª¨ë¸ ì„ íƒ (${intelligence.model_decision.primary_model} ë¶ˆê°€ìš©)`;\n      modelFound = true;\n      break;\n    }\n  }\n  \n  // í´ë°± ëª¨ë¸ë„ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸\n  if (!modelFound && availableModels.length > 0) {\n    finalModel = availableModels[0];\n    selectionReason = 'ì²« ë²ˆì§¸ ì‚¬ìš© ê°€ëŠ¥ ëª¨ë¸ ì„ íƒ (ëª¨ë“  ì¶”ì²œ ëª¨ë¸ ë¶ˆê°€ìš©)';\n  }\n}\n\n// 3. ì„ íƒëœ ëª¨ë¸ì— ë§ëŠ” ìµœì í™” ì¡°ì •\nconst modelOptimization = {\n  'gpt-oss:20b': { temperature: 0.6, max_tokens: 3000, reasoning: 'ëŒ€í˜• ëª¨ë¸ - ì°½ì˜ì„± ì•½ê°„ ì–µì œ, í† í° ì¦ê°€' },\n  'qwen3:8b': { temperature: 0.7, max_tokens: 2500, reasoning: 'ì‹ í˜• ëª¨ë¸ - ê¸°ë³¸ ì„¤ì • ìµœì í™”' },\n  'gemma3:27b': { temperature: 0.5, max_tokens: 4000, reasoning: 'ì´ˆëŒ€í˜• ëª¨ë¸ - ì •í™•ì„± ìš°ì„ , ìµœëŒ€ í† í°' },\n  'qwen2.5:7b': { temperature: 0.7, max_tokens: 2000, reasoning: 'ê²€ì¦ëœ ëª¨ë¸ - ê· í˜•ì¡íŒ ì„¤ì •' },\n  'gemma3:4b': { temperature: 0.8, max_tokens: 1500, reasoning: 'ê²½ëŸ‰ ëª¨ë¸ - ì°½ì˜ì„± ì¦ê°€, í† í° ì ˆì•½' },\n  'llama3.2:3b': { temperature: 0.8, max_tokens: 1500, reasoning: 'ê³ ì† ëª¨ë¸ - ì°½ì˜ì„± ì¦ê°€, íš¨ìœ¨ì„± ìš°ì„ ' }\n};\n\nconst selectedOptimization = modelOptimization[finalModel] || {\n  temperature: 0.7,\n  max_tokens: 2000,\n  reasoning: 'ê¸°ë³¸ ì„¤ì • ì ìš©'\n};\n\n// 4. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìµœì¢… êµ¬ì„±\nconst promptTemplate = intelligence.prompt_strategy.template_selection.jewelry_prompts[\n  intelligence.session_id.includes('conference') ? 'conference_analysis' :\n  intelligence.session_id.includes('news') ? 'news_summary' :\n  intelligence.session_id.includes('gemstone') ? 'gemstone_identification' :\n  'conference_analysis' // ê¸°ë³¸ê°’\n];\n\n// 5. ìµœì¢… ì‹¤í–‰ êµ¬ì„±\nconst finalExecutionConfig = {\n  timestamp: new Date().toISOString(),\n  session_id: intelligence.session_id,\n  \n  model_selection: {\n    selected_model: finalModel,\n    selection_reason: selectionReason,\n    available_models_count: availableModels.length,\n    primary_model_available: availableModels.includes(intelligence.model_decision.primary_model),\n    optimization_applied: selectedOptimization\n  },\n  \n  execution_parameters: {\n    endpoint: intelligence.execution_config.endpoint,\n    model: finalModel,\n    stream: false,\n    options: {\n      num_predict: selectedOptimization.max_tokens,\n      temperature: selectedOptimization.temperature,\n      top_k: intelligence.prompt_strategy.optimization_parameters.top_k,\n      top_p: intelligence.prompt_strategy.optimization_parameters.top_p\n    },\n    timeout: intelligence.quality_control.performance_monitoring.response_time_limit * 1000\n  },\n  \n  prompt_configuration: {\n    template: promptTemplate,\n    context_setup: promptTemplate.context,\n    analysis_focus: promptTemplate.focus,\n    output_format: promptTemplate.format\n  },\n  \n  quality_assurance: {\n    validation_rules: intelligence.quality_control.response_validation,\n    monitoring_enabled: true,\n    error_handling: intelligence.quality_control.error_handling,\n    success_criteria: intelligence.success_criteria\n  },\n  \n  decision_log: {\n    original_request: intelligence.model_decision.selection_reasoning,\n    availability_check: `${availableModels.length}ê°œ ëª¨ë¸ í™•ì¸ë¨`,\n    final_decision: `${finalModel} ì„ íƒ - ${selectionReason}`,\n    optimization_reasoning: selectedOptimization.reasoning,\n    expected_performance: intelligence.monitoring_metrics\n  }\n};\n\nreturn finalExecutionConfig;"
      },
      "id": "final_model_selector",
      "name": "ğŸ¯ ìµœì¢… ëª¨ë¸ ì„ íƒê¸°",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [680, 450]
    },
    {
      "parameters": {
        "method": "POST",
        "url": "{{ $json.execution_parameters.endpoint }}",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{ { \"model\": $json.execution_parameters.model, \"prompt\": \"í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸: ì´ ëª¨ë¸ì´ ì •ìƒ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸\", \"stream\": $json.execution_parameters.stream, \"options\": $json.execution_parameters.options } }}",
        "options": {
          "timeout": "={{ $json.execution_parameters.timeout }}"
        }
      },
      "id": "test_model_response",
      "name": "ğŸ§ª ëª¨ë¸ ì‘ë‹µ í…ŒìŠ¤íŠ¸",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [900, 400]
    },
    {
      "parameters": {
        "jsCode": "// ğŸ† Ollama AI ì§€ëŠ¥ ì‹œìŠ¤í…œ ìµœì¢… ê²€ì¦ ë° ë³´ê³ \nconst executionConfig = $input.first().json;\nconst testResponse = $input.all()[1] ? $input.all()[1].json : null;\n\n// 1. ëª¨ë¸ ì‘ë‹µ í’ˆì§ˆ í‰ê°€\nconst responseQuality = {\n  model_responsive: !!(testResponse && testResponse.response),\n  response_length: testResponse ? (testResponse.response ? testResponse.response.length : 0) : 0,\n  response_time: testResponse ? (testResponse.total_duration ? testResponse.total_duration / 1000000 : 0) : 0, // ë‚˜ë…¸ì´ˆë¥¼ ë°€ë¦¬ì´ˆë¡œ\n  load_duration: testResponse ? (testResponse.load_duration ? testResponse.load_duration / 1000000 : 0) : 0,\n  prompt_eval_duration: testResponse ? (testResponse.prompt_eval_duration ? testResponse.prompt_eval_duration / 1000000 : 0) : 0,\n  eval_duration: testResponse ? (testResponse.eval_duration ? testResponse.eval_duration / 1000000 : 0) : 0\n};\n\n// 2. ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°\nconst performanceMetrics = {\n  response_speed: responseQuality.response_time < 30000 ? 'fast' : \n                 responseQuality.response_time < 60000 ? 'medium' : 'slow',\n  \n  model_efficiency: {\n    tokens_per_second: testResponse && testResponse.eval_count && responseQuality.eval_duration > 0 ? \n                      (testResponse.eval_count / (responseQuality.eval_duration / 1000)).toFixed(2) : 'N/A',\n    load_time_acceptable: responseQuality.load_duration < 10000, // 10ì´ˆ ì´ë‚´\n    memory_usage: executionConfig.model_selection.selected_model.includes('27b') ? 'high' :\n                 executionConfig.model_selection.selected_model.includes('8b') ? 'medium' : 'low'\n  },\n  \n  quality_indicators: {\n    response_completeness: responseQuality.response_length > 50,\n    model_coherence: testResponse ? !testResponse.response?.includes('Error') : false,\n    context_awareness: testResponse ? testResponse.response?.includes('í…ŒìŠ¤íŠ¸') : false\n  }\n};\n\n// 3. ì„ íƒ ê²°ì • ê²€ì¦\nconst selectionValidation = {\n  primary_model_success: executionConfig.model_selection.primary_model_available && responseQuality.model_responsive,\n  fallback_necessity: !executionConfig.model_selection.primary_model_available,\n  optimization_effectiveness: {\n    temperature_appropriate: executionConfig.execution_parameters.options.temperature >= 0.5 && \n                           executionConfig.execution_parameters.options.temperature <= 0.8,\n    token_limit_reasonable: executionConfig.execution_parameters.options.num_predict >= 1500 &&\n                           executionConfig.execution_parameters.options.num_predict <= 4000,\n    model_specific_tuning: !!executionConfig.model_selection.optimization_applied\n  }\n};\n\n// 4. ì „ì²´ ì‹œìŠ¤í…œ ê±´ê°•ë„ í‰ê°€\nlet systemHealthScore = 0;\nlet maxScore = 0;\n\n// ëª¨ë¸ ì‘ë‹µì„± (30ì )\nmaxScore += 30;\nif (responseQuality.model_responsive) systemHealthScore += 30;\nelse if (responseQuality.response_length > 0) systemHealthScore += 15;\n\n// ì„±ëŠ¥ ì†ë„ (25ì )\nmaxScore += 25;\nif (performanceMetrics.response_speed === 'fast') systemHealthScore += 25;\nelse if (performanceMetrics.response_speed === 'medium') systemHealthScore += 15;\nelse if (performanceMetrics.response_speed === 'slow') systemHealthScore += 5;\n\n// ëª¨ë¸ íš¨ìœ¨ì„± (25ì )\nmaxScore += 25;\nif (performanceMetrics.model_efficiency.load_time_acceptable) systemHealthScore += 10;\nif (performanceMetrics.model_efficiency.tokens_per_second !== 'N/A' && \n    parseFloat(performanceMetrics.model_efficiency.tokens_per_second) > 10) systemHealthScore += 15;\n\n// í’ˆì§ˆ ì§€í‘œ (20ì )\nmaxScore += 20;\nif (performanceMetrics.quality_indicators.response_completeness) systemHealthScore += 7;\nif (performanceMetrics.quality_indicators.model_coherence) systemHealthScore += 7;\nif (performanceMetrics.quality_indicators.context_awareness) systemHealthScore += 6;\n\nconst healthPercentage = (systemHealthScore / maxScore) * 100;\n\n// 5. ìµœì¢… Ollama ì§€ëŠ¥ ë³´ê³ ì„œ\nconst ollamaIntelligenceReport = {\n  timestamp: new Date().toISOString(),\n  session_id: executionConfig.session_id,\n  \n  model_decision_summary: {\n    selected_model: executionConfig.model_selection.selected_model,\n    selection_reasoning: executionConfig.decision_log.final_decision,\n    optimization_applied: executionConfig.model_selection.optimization_applied,\n    primary_model_available: executionConfig.model_selection.primary_model_available\n  },\n  \n  performance_evaluation: {\n    response_quality: responseQuality,\n    performance_metrics: performanceMetrics,\n    system_health_score: healthPercentage.toFixed(1)\n  },\n  \n  validation_results: selectionValidation,\n  \n  operational_status: {\n    ready_for_production: healthPercentage > 70 && responseQuality.model_responsive,\n    recommended_actions: healthPercentage < 70 ? [\n      'ëª¨ë¸ ê°€ìš©ì„± ì¬í™•ì¸',\n      'ë„¤íŠ¸ì›Œí¬ ì—°ê²° ìƒíƒœ ì ê²€', \n      'Ollama ì„œë¹„ìŠ¤ ì¬ì‹œì‘ ê³ ë ¤'\n    ] : ['ì‹œìŠ¤í…œ ì •ìƒ - í”„ë¡œë•ì…˜ ì‚¬ìš© ê°€ëŠ¥'],\n    \n    next_optimization_opportunities: [\n      'ëª¨ë¸ë³„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ A/B í…ŒìŠ¤íŠ¸',\n      'ì‘ë‹µ í’ˆì§ˆ í”¼ë“œë°± ë£¨í”„ êµ¬ì¶•',\n      'ë™ì  íŒŒë¼ë¯¸í„° ì¡°ì • ì‹œìŠ¤í…œ'\n    ]\n  },\n  \n  intelligence_insights: {\n    model_selection_accuracy: executionConfig.model_selection.primary_model_available ? 'high' : 'fallback_success',\n    prompt_optimization_effectiveness: selectionValidation.optimization_effectiveness,\n    system_reliability: healthPercentage > 80 ? 'excellent' : \n                      healthPercentage > 60 ? 'good' : 'needs_improvement'\n  }\n};\n\nreturn ollamaIntelligenceReport;"
      },
      "id": "ollama_intelligence_validator",
      "name": "ğŸ† Ollama ì§€ëŠ¥ ê²€ì¦ê¸°",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [1120, 400]
    },
    {
      "parameters": {
        "respondWith": "json",
        "responseBody": "={{ { \"status\": \"ollama_intelligence_ready\", \"message\": \"Ollama AI ì§€ëŠ¥ ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ\", \"session_id\": $json.session_id, \"selected_model\": $json.model_decision_summary.selected_model, \"system_health\": $json.performance_evaluation.system_health_score, \"ready_for_production\": $json.operational_status.ready_for_production, \"optimization_applied\": $json.model_decision_summary.optimization_applied, \"intelligence_level\": $json.intelligence_insights.system_reliability, \"detailed_report\": $json } }}"
      },
      "id": "ollama_response",
      "name": "âœ… Ollama ì§€ëŠ¥ ì™„ë£Œ ì‘ë‹µ",
      "type": "n8n-nodes-base.respondToWebhook",
      "typeVersion": 1.1,
      "position": [1340, 400]
    }
  ],
  "connections": {
    "ğŸ¤– Ollama AI ìš”ì²­": {
      "main": [
        [
          {
            "node": "ğŸ§  Ollama ì§€ëŠ¥ ë¶„ì„ê¸°",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "ğŸ§  Ollama ì§€ëŠ¥ ë¶„ì„ê¸°": {
      "main": [
        [
          {
            "node": "ğŸ” ëª¨ë¸ ê°€ìš©ì„± ì²´í¬",
            "type": "main",
            "index": 0
          },
          {
            "node": "ğŸ¯ ìµœì¢… ëª¨ë¸ ì„ íƒê¸°",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "ğŸ” ëª¨ë¸ ê°€ìš©ì„± ì²´í¬": {
      "main": [
        [
          {
            "node": "ğŸ¯ ìµœì¢… ëª¨ë¸ ì„ íƒê¸°",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "ğŸ¯ ìµœì¢… ëª¨ë¸ ì„ íƒê¸°": {
      "main": [
        [
          {
            "node": "ğŸ§ª ëª¨ë¸ ì‘ë‹µ í…ŒìŠ¤íŠ¸",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "ğŸ§ª ëª¨ë¸ ì‘ë‹µ í…ŒìŠ¤íŠ¸": {
      "main": [
        [
          {
            "node": "ğŸ† Ollama ì§€ëŠ¥ ê²€ì¦ê¸°",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "ğŸ† Ollama ì§€ëŠ¥ ê²€ì¦ê¸°": {
      "main": [
        [
          {
            "node": "âœ… Ollama ì§€ëŠ¥ ì™„ë£Œ ì‘ë‹µ",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "active": true,
  "settings": {
    "executionOrder": "v1"
  },
  "staticData": {},
  "meta": {
    "templateCredsSetupCompleted": true
  }
}