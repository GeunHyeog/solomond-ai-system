# -*- coding: utf-8 -*-
"""
í™”ì êµ¬ë¶„ ë¶„ì„ê¸° - ì‚¬íšŒì 1ëª… + ë°œí‘œì 3ëª… êµ¬ë¶„
ì˜¤ë””ì˜¤ì—ì„œ ê° í™”ìë¥¼ êµ¬ë¶„í•˜ì—¬ ëˆ„ê°€ ì–¸ì œ ë¬´ì—‡ì„ ë§í–ˆëŠ”ì§€ ë¶„ì„
"""

import os
import sys
import time
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import json

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€  
sys.path.append(str(Path(__file__).parent))

class SpeakerDiarizationAnalyzer:
    """í™”ì êµ¬ë¶„ ë¶„ì„ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.expected_speakers = {
            'moderator': 'ì‚¬íšŒì',
            'speaker1': 'ë°œí‘œì 1',
            'speaker2': 'ë°œí‘œì 2', 
            'speaker3': 'ë°œí‘œì 3'
        }
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ í™”ì ë¶„ë¦¬ ë„êµ¬ í™•ì¸
        self.tools_available = self._check_diarization_tools()
        
    def _check_diarization_tools(self):
        """í™”ì ë¶„ë¦¬ ë„êµ¬ ì‚¬ìš© ê°€ëŠ¥ì„± í™•ì¸"""
        
        print("í™”ì ë¶„ë¦¬ ë„êµ¬ í™•ì¸ ì¤‘...")
        
        tools = {
            'pyannote': False,
            'whisper_enhanced': False,
            'resemblyzer': False,
            'basic_vad': False
        }
        
        # pyannote.audio í™•ì¸ (ê°€ì¥ ì •í™•í•¨)
        try:
            import torch
            # pyannoteëŠ” ë³„ë„ ì„¤ì¹˜ í•„ìš”í•˜ë¯€ë¡œ ì¼ë‹¨ False
            print("  pyannote.audio: ë¯¸ì„¤ì¹˜ (ë³„ë„ ì„¤ì¹˜ í•„ìš”)")
        except:
            pass
        
        # Whisper ê¸°ë°˜ í™”ì êµ¬ë¶„ í™•ì¸
        try:
            import whisper
            tools['whisper_enhanced'] = True
            print("  Whisper (íƒ€ì„ìŠ¤íƒ¬í”„): ì‚¬ìš© ê°€ëŠ¥")
        except:
            print("  Whisper: ë¶ˆê°€ëŠ¥")
        
        # ê¸°ë³¸ ìŒì„± í™œë™ ê°ì§€
        try:
            import librosa
            import numpy as np
            tools['basic_vad'] = True
            print("  ê¸°ë³¸ VAD (ìŒì„± êµ¬ê°„ ê°ì§€): ì‚¬ìš© ê°€ëŠ¥")
        except:
            print("  ê¸°ë³¸ VAD: ë¶ˆê°€ëŠ¥")
        
        return tools
    
    def analyze_speakers_in_audio(self, audio_path: str) -> Dict[str, Any]:
        """ì˜¤ë””ì˜¤ íŒŒì¼ì—ì„œ í™”ì êµ¬ë¶„ ë¶„ì„"""
        
        print(f"\nğŸ¤ í™”ì êµ¬ë¶„ ë¶„ì„: {os.path.basename(audio_path)}")
        
        if not os.path.exists(audio_path):
            return {'error': 'íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤'}
        
        result = {
            'filename': os.path.basename(audio_path),
            'file_size': os.path.getsize(audio_path),
            'total_duration': 0,
            'speakers_detected': 0,
            'speaker_segments': [],
            'speaker_profiles': {},
            'transcript_by_speaker': {},
            'analysis_method': 'unknown'
        }
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ë°©ë²•ìœ¼ë¡œ ë¶„ì„
        if self.tools_available['whisper_enhanced']:
            result = self._analyze_with_whisper_timestamps(audio_path, result)
        elif self.tools_available['basic_vad']:
            result = self._analyze_with_basic_vad(audio_path, result)
        else:
            result['error'] = 'í™”ì ë¶„ë¦¬ ë„êµ¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤'
        
        # í™”ì ì—­í•  ì¶”ì •
        if result['speakers_detected'] > 0:
            result = self._assign_speaker_roles(result)
        
        return result
    
    def _analyze_with_whisper_timestamps(self, audio_path: str, result: Dict) -> Dict:
        """Whisper íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜ í™”ì êµ¬ë¶„"""
        
        print("  Whisper íƒ€ì„ìŠ¤íƒ¬í”„ ë¶„ì„ ì‚¬ìš©")
        
        try:
            import whisper
            
            # ìƒì„¸ íƒ€ì„ìŠ¤íƒ¬í”„ì™€ í•¨ê»˜ transcription
            model = whisper.load_model("base")
            
            # word_timestamps=Trueë¡œ ë‹¨ì–´ë³„ íƒ€ì„ìŠ¤íƒ¬í”„ í™œì„±í™”
            transcript_result = model.transcribe(
                audio_path, 
                language="ko",
                word_timestamps=True,
                verbose=False
            )
            
            result['total_duration'] = transcript_result.get('duration', 0)
            result['analysis_method'] = 'whisper_timestamps'
            
            # segments ë¶„ì„
            segments = transcript_result.get('segments', [])
            
            if segments:
                # ìŒì„± ë³€í™”ì  ê¸°ë°˜ í™”ì ì¶”ì •
                speaker_changes = self._detect_speaker_changes_from_segments(segments)
                
                result['speaker_segments'] = speaker_changes
                result['speakers_detected'] = len(set(seg['speaker_id'] for seg in speaker_changes))
                
                # í™”ìë³„ í…ìŠ¤íŠ¸ ì •ë¦¬
                for segment in speaker_changes:
                    speaker_id = segment['speaker_id']
                    if speaker_id not in result['transcript_by_speaker']:
                        result['transcript_by_speaker'][speaker_id] = []
                    
                    result['transcript_by_speaker'][speaker_id].append({
                        'start': segment['start'],
                        'end': segment['end'],
                        'text': segment['text']
                    })
                
                print(f"    í™”ì {result['speakers_detected']}ëª… ê°ì§€")
                print(f"    ì´ êµ¬ê°„: {len(speaker_changes)}ê°œ")
            
            return result
            
        except Exception as e:
            print(f"    Whisper ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            result['error'] = str(e)
            return result
    
    def _detect_speaker_changes_from_segments(self, segments: List[Dict]) -> List[Dict]:
        """Whisper segmentsì—ì„œ í™”ì ë³€í™”ì  ê°ì§€"""
        
        speaker_segments = []
        current_speaker = 0
        
        for i, segment in enumerate(segments):
            # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±: ê¸´ ì¹¨ë¬µì´ë‚˜ ìŒì„± íŒ¨í„´ ë³€í™”ë¡œ í™”ì ì¶”ì •
            
            # ì´ì „ ì„¸ê·¸ë¨¼íŠ¸ì™€ì˜ ì‹œê°„ ê°„ê²© í™•ì¸
            if i > 0:
                prev_end = segments[i-1]['end']
                current_start = segment['start']
                silence_duration = current_start - prev_end
                
                # 3ì´ˆ ì´ìƒ ì¹¨ë¬µì´ë©´ í™”ì ë³€ê²½ ê°€ëŠ¥ì„±
                if silence_duration > 3.0:
                    current_speaker = (current_speaker + 1) % 4  # 4ëª… ìˆœí™˜
            
            # ì„¸ê·¸ë¨¼íŠ¸ ê¸¸ì´ë‚˜ í…ìŠ¤íŠ¸ íŒ¨í„´ìœ¼ë¡œë„ ì¶”ê°€ íŒë‹¨ ê°€ëŠ¥
            text_length = len(segment['text'])
            
            # ì§§ì€ ë°œì–¸ì€ ì‚¬íšŒìì¼ ê°€ëŠ¥ì„± (ì§ˆë¬¸, ì†Œê°œ ë“±)
            if text_length < 50 and any(word in segment['text'] for word in ['ë„¤', 'ê°ì‚¬', 'ë‹¤ìŒ', 'ì§ˆë¬¸']):
                speaker_id = 'moderator'
            else:
                speaker_id = f'speaker{(current_speaker % 3) + 1}' if current_speaker > 0 else 'moderator'
            
            speaker_segments.append({
                'start': segment['start'],
                'end': segment['end'],
                'text': segment['text'],
                'speaker_id': speaker_id,
                'confidence': 0.7  # ê¸°ë³¸ ì‹ ë¢°ë„
            })
        
        return speaker_segments
    
    def _analyze_with_basic_vad(self, audio_path: str, result: Dict) -> Dict:
        """ê¸°ë³¸ ìŒì„± í™œë™ ê°ì§€ ê¸°ë°˜ ë¶„ì„"""
        
        print("  ê¸°ë³¸ VAD ë¶„ì„ ì‚¬ìš©")
        
        try:
            import librosa
            import numpy as np
            
            # ì˜¤ë””ì˜¤ ë¡œë“œ
            y, sr = librosa.load(audio_path, sr=16000)
            result['total_duration'] = len(y) / sr
            result['analysis_method'] = 'basic_vad'
            
            # ê°„ë‹¨í•œ ìŒì„± êµ¬ê°„ ê°ì§€
            # RMS ì—ë„ˆì§€ ê¸°ë°˜ ìŒì„± í™œë™ ê°ì§€
            frame_length = int(0.025 * sr)  # 25ms í”„ë ˆì„
            hop_length = int(0.01 * sr)     # 10ms í™‰
            
            rms = librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop_length)[0]
            
            # ì„ê³„ê°’ ê¸°ë°˜ ìŒì„± êµ¬ê°„ ì°¾ê¸°
            threshold = np.percentile(rms, 60)  # ìƒìœ„ 40% ì—ë„ˆì§€
            voice_frames = rms > threshold
            
            # ì—°ì†ëœ ìŒì„± êµ¬ê°„ ì°¾ê¸°
            voice_segments = []
            in_voice = False
            start_frame = 0
            
            for i, is_voice in enumerate(voice_frames):
                if is_voice and not in_voice:
                    start_frame = i
                    in_voice = True
                elif not is_voice and in_voice:
                    # ìŒì„± êµ¬ê°„ ì¢…ë£Œ
                    start_time = start_frame * hop_length / sr
                    end_time = i * hop_length / sr
                    
                    if end_time - start_time > 1.0:  # 1ì´ˆ ì´ìƒì¸ êµ¬ê°„ë§Œ
                        voice_segments.append({
                            'start': start_time,
                            'end': end_time,
                            'speaker_id': f'speaker{len(voice_segments) % 4}',
                            'text': f'[{len(voice_segments)+1}ë²ˆì§¸ ë°œì–¸ êµ¬ê°„]',
                            'confidence': 0.5
                        })
                    in_voice = False
            
            result['speaker_segments'] = voice_segments
            result['speakers_detected'] = min(len(voice_segments), 4)
            
            print(f"    ìŒì„± êµ¬ê°„ {len(voice_segments)}ê°œ ê°ì§€")
            
            return result
            
        except Exception as e:
            print(f"    VAD ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            result['error'] = str(e)
            return result
    
    def _assign_speaker_roles(self, result: Dict) -> Dict:
        """ê°ì§€ëœ í™”ìë“¤ì—ê²Œ ì—­í•  í• ë‹¹"""
        
        print("  í™”ì ì—­í•  í• ë‹¹ ì¤‘...")
        
        speaker_profiles = {}
        transcript_by_speaker = result.get('transcript_by_speaker', {})
        
        # ê° í™”ìì˜ íŠ¹ì„± ë¶„ì„
        for speaker_id, segments in transcript_by_speaker.items():
            total_text = ' '.join([seg['text'] for seg in segments])
            total_duration = sum([seg['end'] - seg['start'] for seg in segments])
            segment_count = len(segments)
            
            # í™”ì íŠ¹ì„± ë¶„ì„
            avg_segment_length = len(total_text) / segment_count if segment_count > 0 else 0
            
            # ì‚¬íšŒì íŠ¹ì„±: ì§§ì€ ë°œì–¸, ë§ì€ ì„¸ê·¸ë¨¼íŠ¸, íŠ¹ì • í‚¤ì›Œë“œ
            moderator_keywords = ['ê°ì‚¬', 'ë„¤', 'ë‹¤ìŒ', 'ì§ˆë¬¸', 'ë°œí‘œ', 'ì†Œê°œ', 'ì‹œê°„']
            moderator_score = sum(1 for keyword in moderator_keywords if keyword in total_text)
            
            # ë°œí‘œì íŠ¹ì„±: ê¸´ ë°œì–¸, ì ì€ ì„¸ê·¸ë¨¼íŠ¸
            speaker_score = avg_segment_length / 10  # í‰ê·  ê¸¸ì´ ê¸°ë°˜ ì ìˆ˜
            
            speaker_profiles[speaker_id] = {
                'total_duration': total_duration,
                'segment_count': segment_count,
                'total_text_length': len(total_text),
                'avg_segment_length': avg_segment_length,
                'moderator_score': moderator_score,
                'speaker_score': speaker_score,
                'sample_text': total_text[:100] + '...' if len(total_text) > 100 else total_text
            }
        
        # ì—­í•  í• ë‹¹
        sorted_speakers = sorted(
            speaker_profiles.keys(), 
            key=lambda x: speaker_profiles[x]['moderator_score'], 
            reverse=True
        )
        
        role_assignments = {}
        if sorted_speakers:
            # ê°€ì¥ ë†’ì€ moderator_scoreë¥¼ ê°€ì§„ í™”ìë¥¼ ì‚¬íšŒìë¡œ
            role_assignments[sorted_speakers[0]] = 'ì‚¬íšŒì'
            
            # ë‚˜ë¨¸ì§€ë¥¼ ë°œí‘œìë¡œ
            for i, speaker in enumerate(sorted_speakers[1:4], 1):
                role_assignments[speaker] = f'ë°œí‘œì {i}'
        
        # í”„ë¡œí•„ì— ì—­í•  ì •ë³´ ì¶”ê°€
        for speaker_id, profile in speaker_profiles.items():
            profile['assigned_role'] = role_assignments.get(speaker_id, 'ë¯¸ë¶„ë¥˜')
        
        result['speaker_profiles'] = speaker_profiles
        result['role_assignments'] = role_assignments
        
        print(f"    ì—­í•  í• ë‹¹ ì™„ë£Œ: {len(role_assignments)}ëª…")
        
        return result
    
    def analyze_all_audio_files(self) -> Dict[str, Any]:
        """user_files/audioì˜ ëª¨ë“  ì˜¤ë””ì˜¤ íŒŒì¼ í™”ì êµ¬ë¶„ ë¶„ì„"""
        
        audio_folder = "user_files/audio"
        
        if not os.path.exists(audio_folder):
            print("user_files/audio í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {}
        
        audio_files = []
        for file in os.listdir(audio_folder):
            if file.lower().endswith(('.wav', '.m4a', '.mp3')):
                audio_files.append(os.path.join(audio_folder, file))
        
        if not audio_files:
            print("ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return {}
        
        print(f"ì˜¤ë””ì˜¤ íŒŒì¼ {len(audio_files)}ê°œ ë°œê²¬")
        
        all_results = {}
        
        for audio_path in audio_files:
            result = self.analyze_speakers_in_audio(audio_path)
            all_results[os.path.basename(audio_path)] = result
        
        return all_results
    
    def save_speaker_analysis(self, results: Dict[str, Any]):
        """í™”ì ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        filename = f"speaker_analysis_{timestamp}.json"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            print(f"\në¶„ì„ ê²°ê³¼ ì €ì¥: {filename}")
        except Exception as e:
            print(f"ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        
        # ìš”ì•½ ë¦¬í¬íŠ¸ë„ ìƒì„±
        self._create_speaker_report(results, timestamp)
    
    def _create_speaker_report(self, results: Dict[str, Any], timestamp: str):
        """í™”ì ë¶„ì„ ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±"""
        
        report_filename = f"speaker_report_{timestamp}.md"
        
        report_lines = []
        report_lines.append("# í™”ì êµ¬ë¶„ ë¶„ì„ ë¦¬í¬íŠ¸")
        report_lines.append(f"ìƒì„±ì¼ì‹œ: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append("")
        
        for filename, result in results.items():
            if 'error' in result:
                continue
                
            report_lines.append(f"## ğŸ“ {filename}")
            report_lines.append("")
            
            # ê¸°ë³¸ ì •ë³´
            duration = result.get('total_duration', 0)
            speakers_count = result.get('speakers_detected', 0)
            
            report_lines.append(f"- **ì´ ê¸¸ì´**: {duration:.1f}ì´ˆ ({duration/60:.1f}ë¶„)")
            report_lines.append(f"- **ê°ì§€ëœ í™”ì**: {speakers_count}ëª…")
            report_lines.append(f"- **ë¶„ì„ ë°©ë²•**: {result.get('analysis_method', 'unknown')}")
            report_lines.append("")
            
            # í™”ìë³„ ì •ë³´
            if 'speaker_profiles' in result:
                report_lines.append("### ğŸ‘¥ í™”ìë³„ ì •ë³´")
                report_lines.append("")
                
                for speaker_id, profile in result['speaker_profiles'].items():
                    role = profile.get('assigned_role', 'ë¯¸ë¶„ë¥˜')
                    duration = profile.get('total_duration', 0)
                    segments = profile.get('segment_count', 0)
                    
                    report_lines.append(f"#### {role} ({speaker_id})")
                    report_lines.append(f"- ë°œì–¸ ì‹œê°„: {duration:.1f}ì´ˆ")
                    report_lines.append(f"- ë°œì–¸ íšŸìˆ˜: {segments}íšŒ")
                    report_lines.append(f"- ìƒ˜í”Œ í…ìŠ¤íŠ¸: {profile.get('sample_text', '')}")
                    report_lines.append("")
        
        report_lines.append("---")
        report_lines.append("*ì†”ë¡œëª¬ë“œ AI í™”ì êµ¬ë¶„ ë¶„ì„ê¸°ë¡œ ìƒì„±ë¨*")
        
        try:
            with open(report_filename, 'w', encoding='utf-8') as f:
                f.write('\n'.join(report_lines))
            print(f"ìš”ì•½ ë¦¬í¬íŠ¸ ì €ì¥: {report_filename}")
        except Exception as e:
            print(f"ë¦¬í¬íŠ¸ ì €ì¥ ì‹¤íŒ¨: {str(e)}")

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    
    print("=== í™”ì êµ¬ë¶„ ë¶„ì„ê¸° ===")
    print("ì‚¬íšŒì 1ëª… + ë°œí‘œì 3ëª…ì„ êµ¬ë¶„í•˜ì—¬ ë¶„ì„í•©ë‹ˆë‹¤.")
    
    try:
        analyzer = SpeakerDiarizationAnalyzer()
        
        # ëª¨ë“  ì˜¤ë””ì˜¤ íŒŒì¼ ë¶„ì„
        results = analyzer.analyze_all_audio_files()
        
        if results:
            print(f"\n" + "="*60)
            print("í™”ì êµ¬ë¶„ ë¶„ì„ ê²°ê³¼")
            print("="*60)
            
            for filename, result in results.items():
                print(f"\níŒŒì¼: {filename}")
                
                if 'error' in result:
                    print(f"  ì˜¤ë¥˜: {result['error']}")
                    continue
                
                duration = result.get('total_duration', 0)
                speakers = result.get('speakers_detected', 0)
                
                print(f"  ê¸¸ì´: {duration:.1f}ì´ˆ ({duration/60:.1f}ë¶„)")
                print(f"  í™”ì: {speakers}ëª… ê°ì§€")
                
                # ì—­í•  í• ë‹¹ ê²°ê³¼
                if 'role_assignments' in result:
                    print("  ì—­í•  í• ë‹¹:")
                    for speaker_id, role in result['role_assignments'].items():
                        profile = result['speaker_profiles'].get(speaker_id, {})
                        segments = profile.get('segment_count', 0)
                        print(f"    {role}: {segments}íšŒ ë°œì–¸")
            
            # ê²°ê³¼ ì €ì¥
            analyzer.save_speaker_analysis(results)
            
        else:
            print("\në¶„ì„í•  ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            print("user_files/audio/ í´ë”ì— ì˜¤ë””ì˜¤ íŒŒì¼ì„ ë„£ì–´ì£¼ì„¸ìš”.")
    
    except KeyboardInterrupt:
        print("\në¶„ì„ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nì˜¤ë¥˜ ë°œìƒ: {str(e)}")

if __name__ == "__main__":
    main()