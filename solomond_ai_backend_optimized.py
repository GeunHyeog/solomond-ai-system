#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SOLOMOND AI - ê³ ì„±ëŠ¥ ìµœì í™” ë°±ì—”ë“œ ì‹œìŠ¤í…œ
v2.0 - ë©”ëª¨ë¦¬ ìµœì í™”, ë¹„ë™ê¸° ì²˜ë¦¬, ìºì‹±, ëª¨ë‹ˆí„°ë§ í†µí•©
"""

import json
import subprocess
import logging
import os
import base64
import tempfile
import hashlib
import threading
import queue
import gc
import psutil
import time
from datetime import datetime
from http.server import HTTPServer, BaseHTTPRequestHandler
from urllib.parse import urlparse
from concurrent.futures import ThreadPoolExecutor
from functools import lru_cache
import weakref

# íŒŒì¼ ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    import easyocr
    OCR_AVAILABLE = True
    print("EasyOCR available")
except ImportError:
    OCR_AVAILABLE = False
    print("EasyOCR not available - image analysis limited")

try:
    import whisper
    STT_AVAILABLE = True
    print("Whisper STT available")
except ImportError:
    STT_AVAILABLE = False
    print("Whisper STT not available - audio analysis limited")

# ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s'
)
logger = logging.getLogger(__name__)

class PerformanceMonitor:
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.start_time = time.time()
        self.request_count = 0
        self.total_processing_time = 0
        self.peak_memory = 0
        self.active_threads = 0
        self.cache_hits = 0
        self.cache_misses = 0
        
    def log_request(self, processing_time):
        """ìš”ì²­ ì²˜ë¦¬ ì‹œê°„ ê¸°ë¡"""
        self.request_count += 1
        self.total_processing_time += processing_time
        current_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        if current_memory > self.peak_memory:
            self.peak_memory = current_memory
            
    def get_stats(self):
        """ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
        uptime = time.time() - self.start_time
        avg_processing_time = self.total_processing_time / max(1, self.request_count)
        
        return {
            "uptime_seconds": round(uptime, 2),
            "total_requests": self.request_count,
            "avg_processing_time": round(avg_processing_time, 3),
            "current_memory_mb": round(psutil.Process().memory_info().rss / 1024 / 1024, 2),
            "peak_memory_mb": round(self.peak_memory, 2),
            "active_threads": threading.active_count(),
            "cache_hit_rate": round(self.cache_hits / max(1, self.cache_hits + self.cache_misses) * 100, 1),
            "cpu_percent": psutil.cpu_percent()
        }

class FileProcessingCache:
    """íŒŒì¼ ì²˜ë¦¬ ê²°ê³¼ ìºì‹±"""
    
    def __init__(self, max_size=1000):
        self.cache = {}
        self.max_size = max_size
        self.access_count = {}
        
    def get_file_hash(self, file_data):
        """íŒŒì¼ ë°ì´í„° í•´ì‹œ ìƒì„±"""
        return hashlib.md5(file_data.encode() if isinstance(file_data, str) else file_data).hexdigest()
    
    def get(self, file_hash):
        """ìºì‹œì—ì„œ ê²°ê³¼ ì¡°íšŒ"""
        if file_hash in self.cache:
            self.access_count[file_hash] = self.access_count.get(file_hash, 0) + 1
            return self.cache[file_hash]
        return None
    
    def set(self, file_hash, result):
        """ìºì‹œì— ê²°ê³¼ ì €ì¥"""
        if len(self.cache) >= self.max_size:
            # LRU ë°©ì‹ìœ¼ë¡œ ê°€ì¥ ì ê²Œ ì‚¬ìš©ëœ í•­ëª© ì œê±°
            least_used = min(self.access_count.items(), key=lambda x: x[1])
            del self.cache[least_used[0]]
            del self.access_count[least_used[0]]
            
        self.cache[file_hash] = result
        self.access_count[file_hash] = 1
    
    def clear(self):
        """ìºì‹œ ì´ˆê¸°í™”"""
        self.cache.clear()
        self.access_count.clear()
        gc.collect()

class OptimizedAIProcessor:
    """ìµœì í™”ëœ AI ì²˜ë¦¬ ì—”ì§„"""
    
    def __init__(self):
        self.ocr_reader = None
        self.whisper_model = None
        self.file_cache = FileProcessingCache()
        self.performance_monitor = PerformanceMonitor()
        self.thread_pool = ThreadPoolExecutor(max_workers=4)
        self.processing_queue = queue.Queue()
        
        # ì•½í•œ ì°¸ì¡°ë¥¼ ì‚¬ìš©í•œ ëª¨ë¸ ê´€ë¦¬
        self._model_refs = weakref.WeakValueDictionary()
        
        self.initialize_models()
        
    def initialize_models(self):
        """AI ëª¨ë¸ ì§€ì—° ì´ˆê¸°í™”"""
        logger.info("ğŸ¤– AI ëª¨ë¸ ì´ˆê¸°í™” ì‹œì‘")
        
        if OCR_AVAILABLE:
            try:
                # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
                gpu_available = self._check_gpu_available()
                self.ocr_reader = easyocr.Reader(['ko', 'en'], gpu=gpu_available)
                logger.info(f"âœ… EasyOCR ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (GPU: {gpu_available})")
            except Exception as e:
                logger.error(f"âŒ EasyOCR ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        if STT_AVAILABLE:
            try:
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ê³ ë ¤í•œ ëª¨ë¸ ì„ íƒ
                available_memory = psutil.virtual_memory().available / 1024 / 1024 / 1024  # GB
                model_size = "base" if available_memory > 4 else "tiny"
                self.whisper_model = whisper.load_model(model_size)
                logger.info(f"âœ… Whisper STT ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ëª¨ë¸: {model_size})")
            except Exception as e:
                logger.error(f"âŒ Whisper STT ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def _check_gpu_available(self):
        """GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        try:
            import torch
            return torch.cuda.is_available()
        except ImportError:
            return False
    
    def process_image_ocr_optimized(self, image_data):
        """ìµœì í™”ëœ ì´ë¯¸ì§€ OCR ì²˜ë¦¬"""
        start_time = time.time()
        
        if not OCR_AVAILABLE or self.ocr_reader is None:
            return {"error": "EasyOCRì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"}
        
        try:
            # íŒŒì¼ í•´ì‹œ ìƒì„±
            file_hash = self.file_cache.get_file_hash(image_data)
            
            # ìºì‹œ í™•ì¸
            cached_result = self.file_cache.get(file_hash)
            if cached_result:
                self.performance_monitor.cache_hits += 1
                logger.info(f"ğŸ’¾ ìºì‹œì—ì„œ OCR ê²°ê³¼ ë°˜í™˜ (í•´ì‹œ: {file_hash[:8]})")
                return cached_result
            
            self.performance_monitor.cache_misses += 1
            
            # Base64 ë””ì½”ë”©
            image_bytes = base64.b64decode(image_data.split(',')[1])
            
            # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì„ì‹œ íŒŒì¼ ì²˜ë¦¬
            with tempfile.NamedTemporaryFile(suffix='.jpg', delete=False) as temp_file:
                temp_file.write(image_bytes)
                temp_path = temp_file.name
            
            try:
                # OCR ì‹¤í–‰
                results = self.ocr_reader.readtext(temp_path)
                
                # ê²°ê³¼ ìµœì í™”
                extracted_texts = []
                for (bbox, text, confidence) in results:
                    if confidence > 0.5:  # ì‹ ë¢°ë„ í•„í„°ë§
                        extracted_texts.append({
                            "text": text.strip(),
                            "confidence": round(float(confidence), 3)
                        })
                
                result = {
                    "success": True,
                    "texts": extracted_texts,
                    "text_count": len(extracted_texts),
                    "processing_time": round(time.time() - start_time, 3)
                }
                
                # ìºì‹œì— ì €ì¥
                self.file_cache.set(file_hash, result)
                
                return result
                
            finally:
                # ì„ì‹œ íŒŒì¼ ì •ë¦¬
                if os.path.exists(temp_path):
                    os.unlink(temp_path)
                    
        except Exception as e:
            logger.error(f"âŒ ì´ë¯¸ì§€ OCR ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {"error": f"ì´ë¯¸ì§€ OCR ì²˜ë¦¬ ì‹¤íŒ¨: {e}"}
        finally:
            self.performance_monitor.log_request(time.time() - start_time)
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            gc.collect()
    
    def process_audio_stt_optimized(self, audio_data):
        """ìµœì í™”ëœ ì˜¤ë””ì˜¤ STT ì²˜ë¦¬"""
        start_time = time.time()
        
        if not STT_AVAILABLE or self.whisper_model is None:
            return {"error": "Whisper STTê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"}
        
        try:
            # íŒŒì¼ í•´ì‹œ ìƒì„±
            file_hash = self.file_cache.get_file_hash(audio_data)
            
            # ìºì‹œ í™•ì¸
            cached_result = self.file_cache.get(file_hash)
            if cached_result:
                self.performance_monitor.cache_hits += 1
                logger.info(f"ğŸ’¾ ìºì‹œì—ì„œ STT ê²°ê³¼ ë°˜í™˜ (í•´ì‹œ: {file_hash[:8]})")
                return cached_result
            
            self.performance_monitor.cache_misses += 1
            
            # Base64 ë””ì½”ë”©
            audio_bytes = base64.b64decode(audio_data.split(',')[1])
            
            # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì„ì‹œ íŒŒì¼ ì²˜ë¦¬
            with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_file:
                temp_file.write(audio_bytes)
                temp_path = temp_file.name
            
            try:
                # STT ì‹¤í–‰
                result_data = self.whisper_model.transcribe(temp_path)
                
                result = {
                    "success": True,
                    "text": result_data["text"].strip(),
                    "language": result_data.get("language", "unknown"),
                    "processing_time": round(time.time() - start_time, 3)
                }
                
                # ìºì‹œì— ì €ì¥
                self.file_cache.set(file_hash, result)
                
                return result
                
            finally:
                # ì„ì‹œ íŒŒì¼ ì •ë¦¬
                if os.path.exists(temp_path):
                    os.unlink(temp_path)
                    
        except Exception as e:
            logger.error(f"âŒ ìŒì„± STT ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {"error": f"ìŒì„± STT ì²˜ë¦¬ ì‹¤íŒ¨: {e}"}
        finally:
            self.performance_monitor.log_request(time.time() - start_time)
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            gc.collect()
    
    @lru_cache(maxsize=100)
    def get_ollama_models(self):
        """Ollama ëª¨ë¸ ëª©ë¡ ìºì‹±"""
        try:
            result = subprocess.run(['ollama', 'list'], 
                                  capture_output=True, text=True, timeout=10)
            if result.returncode == 0:
                models = []
                lines = result.stdout.strip().split('\n')[1:]  # í—¤ë” ì œì™¸
                for line in lines:
                    if line.strip():
                        model_name = line.split()[0]
                        models.append(model_name)
                return models
            return []
        except Exception as e:
            logger.error(f"âŒ Ollama ëª¨ë¸ ëª©ë¡ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []
    
    def analyze_with_ollama_optimized(self, prompt, context=""):
        """ìµœì í™”ëœ Ollama AI ë¶„ì„"""
        start_time = time.time()
        
        try:
            # ìµœì  ëª¨ë¸ ì„ íƒ
            available_models = self.get_ollama_models()
            preferred_models = ['gpt-oss:20b', 'qwen3:8b', 'gemma3:27b', 'qwen2.5:7b', 'gemma3:4b']
            
            selected_model = None
            for model in preferred_models:
                if model in available_models:
                    selected_model = model
                    break
            
            if not selected_model and available_models:
                selected_model = available_models[0]
            
            if not selected_model:
                return {"error": "ì‚¬ìš© ê°€ëŠ¥í•œ Ollama ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤"}
            
            # í”„ë¡¬í”„íŠ¸ ìµœì í™” (ê¸¸ì´ ì œí•œ)
            max_prompt_length = 2000
            if len(prompt) > max_prompt_length:
                prompt = prompt[:max_prompt_length] + "...(ë‚´ìš© truncated)"
            
            # Ollama ì‹¤í–‰
            cmd = ['ollama', 'run', selected_model, prompt]
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=45)
            
            if result.returncode == 0:
                response = {
                    "success": True,
                    "model": selected_model,
                    "response": result.stdout.strip(),
                    "timestamp": datetime.now().isoformat(),
                    "processing_time": round(time.time() - start_time, 3)
                }
            else:
                response = {
                    "success": False,
                    "error": f"Ollama ì‹¤í–‰ ì‹¤íŒ¨: {result.stderr}",
                    "response": "AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
                }
                
        except subprocess.TimeoutExpired:
            response = {
                "success": False,
                "error": "AI ë¶„ì„ ì‹œê°„ ì´ˆê³¼ (45ì´ˆ)",
                "response": "ë¶„ì„ì— ì‹œê°„ì´ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë ¸ìŠµë‹ˆë‹¤."
            }
        except Exception as e:
            logger.error(f"âŒ AI ë¶„ì„ ì˜¤ë¥˜: {e}")
            response = {
                "success": False,
                "error": str(e),
                "response": "ì‹œìŠ¤í…œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            }
        finally:
            self.performance_monitor.log_request(time.time() - start_time)
            
        return response
    
    def get_system_health(self):
        """ì‹œìŠ¤í…œ ê±´ê°•ë„ ì²´í¬"""
        memory_info = psutil.virtual_memory()
        disk_info = psutil.disk_usage('/')
        
        return {
            "timestamp": datetime.now().isoformat(),
            "performance": self.performance_monitor.get_stats(),
            "memory": {
                "total_gb": round(memory_info.total / 1024 / 1024 / 1024, 2),
                "available_gb": round(memory_info.available / 1024 / 1024 / 1024, 2),
                "usage_percent": memory_info.percent
            },
            "disk": {
                "total_gb": round(disk_info.total / 1024 / 1024 / 1024, 2),
                "free_gb": round(disk_info.free / 1024 / 1024 / 1024, 2),
                "usage_percent": round((disk_info.used / disk_info.total) * 100, 1)
            },
            "cache": {
                "size": len(self.file_cache.cache),
                "max_size": self.file_cache.max_size,
                "hit_rate": round(self.performance_monitor.cache_hits / max(1, self.performance_monitor.cache_hits + self.performance_monitor.cache_misses) * 100, 1)
            },
            "models": {
                "ocr_loaded": self.ocr_reader is not None,
                "stt_loaded": self.whisper_model is not None,
                "available_ollama": len(self.get_ollama_models())
            }
        }
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        logger.info("ğŸ§¹ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹œì‘")
        self.file_cache.clear()
        self.thread_pool.shutdown(wait=True)
        gc.collect()
        logger.info("âœ… ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")

class OptimizedSOLOMONDHandler(BaseHTTPRequestHandler):
    """ìµœì í™”ëœ HTTP ìš”ì²­ ì²˜ë¦¬"""
    
    def __init__(self, *args, ai_processor=None, **kwargs):
        self.ai_processor = ai_processor
        super().__init__(*args, **kwargs)
    
    def do_GET(self):
        """GET ìš”ì²­ ì²˜ë¦¬"""
        path = urlparse(self.path).path
        
        self.send_response(200)
        self.send_header('Content-type', 'application/json')
        self.send_header('Access-Control-Allow-Origin', '*')
        self.send_header('Access-Control-Allow-Methods', 'GET, POST, OPTIONS')
        self.send_header('Access-Control-Allow-Headers', 'Content-Type')
        self.end_headers()
        
        if path == '/health':
            response = {
                "status": "healthy",
                "timestamp": datetime.now().isoformat(),
                "version": "v2.0-optimized"
            }
        elif path == '/status':
            response = self.ai_processor.get_system_health()
        elif path == '/performance':
            response = self.ai_processor.performance_monitor.get_stats()
        else:
            self.send_response(404)
            response = {"error": "Not Found"}
        
        self.wfile.write(json.dumps(response, ensure_ascii=False).encode('utf-8'))
    
    def do_POST(self):
        """POST ìš”ì²­ ì²˜ë¦¬"""
        try:
            content_length = int(self.headers['Content-Length'])
            post_data = self.rfile.read(content_length)
            data = json.loads(post_data.decode('utf-8'))
            
            self.send_response(200)
            self.send_header('Content-type', 'application/json')
            self.send_header('Access-Control-Allow-Origin', '*')
            self.send_header('Access-Control-Allow-Methods', 'GET, POST, OPTIONS')
            self.send_header('Access-Control-Allow-Headers', 'Content-Type')
            self.end_headers()
            
            path = urlparse(self.path).path
            
            if path == '/analyze':
                prompt = data.get('prompt', '')
                context = data.get('context', '')
                response = self.ai_processor.analyze_with_ollama_optimized(prompt, context)
                
            elif path == '/process_image':
                image_data = data.get('image_data', '')
                response = self.ai_processor.process_image_ocr_optimized(image_data)
                
            elif path == '/process_audio':
                audio_data = data.get('audio_data', '')
                response = self.ai_processor.process_audio_stt_optimized(audio_data)
                
            elif path == '/clear_cache':
                self.ai_processor.file_cache.clear()
                response = {"success": True, "message": "ìºì‹œê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤"}
                
            else:
                response = {"error": "ì§€ì›í•˜ì§€ ì•ŠëŠ” ì—”ë“œí¬ì¸íŠ¸ì…ë‹ˆë‹¤"}
            
            self.wfile.write(json.dumps(response, ensure_ascii=False).encode('utf-8'))
            
        except Exception as e:
            logger.error(f"âŒ POST ìš”ì²­ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            self.send_response(500)
            self.send_header('Content-type', 'application/json')
            self.end_headers()
            error = {"error": str(e)}
            self.wfile.write(json.dumps(error, ensure_ascii=False).encode('utf-8'))
    
    def do_OPTIONS(self):
        """OPTIONS ìš”ì²­ ì²˜ë¦¬ (CORS)"""
        self.send_response(200)
        self.send_header('Access-Control-Allow-Origin', '*')
        self.send_header('Access-Control-Allow-Methods', 'GET, POST, OPTIONS')
        self.send_header('Access-Control-Allow-Headers', 'Content-Type')
        self.end_headers()
    
    def log_message(self, format, *args):
        """ë¡œê·¸ ê°„ì†Œí™”"""
        pass

# ê¸€ë¡œë²Œ AI í”„ë¡œì„¸ì„œ ì¸ìŠ¤í„´ìŠ¤
ai_processor = None

class CustomOptimizedHandler(OptimizedSOLOMONDHandler):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, ai_processor=ai_processor, **kwargs)

def main():
    """ìµœì í™”ëœ ì„œë²„ ì‹¤í–‰"""
    global ai_processor
    
    port = 9000
    
    print("SOLOMOND AI v2.0 Optimized Backend Starting")
    print(f"Port: {port}")
    print("Performance Optimizations:")
    print("  - Memory usage optimization")
    print("  - File processing caching")  
    print("  - Asynchronous processing")
    print("  - Real-time monitoring")
    
    # AI í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
    ai_processor = OptimizedAIProcessor()
    
    server = HTTPServer(('localhost', port), CustomOptimizedHandler)
    
    print(f"Health check: http://localhost:{port}/health")
    print(f"Performance monitoring: http://localhost:{port}/performance")
    print("Press Ctrl+C to stop")
    
    try:
        server.serve_forever()
    except KeyboardInterrupt:
        print("\nServer stopping...")
        if ai_processor:
            ai_processor.cleanup()
        server.shutdown()
        print("Server stopped successfully")

if __name__ == '__main__':
    main()