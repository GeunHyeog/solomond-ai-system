#!/usr/bin/env python3
"""
GEMMA3 ìë™ ì„¤ì¹˜ ë° ì„¤ì • ìŠ¤í¬ë¦½íŠ¸
"""

import subprocess
import time
import asyncio
import sys
import os
from pathlib import Path

class AutoGEMMA3Setup:
    """GEMMA3 ìë™ ì„¤ì¹˜ ë° ì„¤ì •"""
    
    def __init__(self):
        self.ollama_installed = False
        self.ollama_running = False
        
    def check_ollama_installation(self):
        """Ollama ì„¤ì¹˜ ìƒíƒœ í™•ì¸"""
        
        print("=== Ollama ì„¤ì¹˜ ìƒíƒœ í™•ì¸ ===")
        
        # Windowsì—ì„œ Ollama ì„¤ì¹˜ í™•ì¸
        try:
            # 1. ëª…ë ¹ì–´ë¡œ í™•ì¸
            result = subprocess.run(
                ["ollama", "--version"], 
                capture_output=True, 
                text=True,
                timeout=10
            )
            
            if result.returncode == 0:
                print(f"âœ“ Ollama ì„¤ì¹˜ë¨: {result.stdout.strip()}")
                self.ollama_installed = True
                return True
                
        except (subprocess.TimeoutExpired, FileNotFoundError):
            pass
        
        # 2. ì¼ë°˜ì ì¸ ì„¤ì¹˜ ê²½ë¡œ í™•ì¸
        common_paths = [
            r"C:\Users\{}\AppData\Local\Programs\Ollama\ollama.exe".format(os.getenv("USERNAME")),
            r"C:\Program Files\Ollama\ollama.exe",
            r"C:\Program Files (x86)\Ollama\ollama.exe"
        ]
        
        for path in common_paths:
            if os.path.exists(path):
                print(f"âœ“ Ollama ë°œê²¬: {path}")
                self.ollama_installed = True
                return True
        
        print("âœ— Ollamaê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
        return False
    
    def install_ollama(self):
        """Ollama ìë™ ì„¤ì¹˜"""
        
        print("\n=== Ollama ìë™ ì„¤ì¹˜ ===")
        
        if self.ollama_installed:
            print("âœ“ Ollama ì´ë¯¸ ì„¤ì¹˜ë¨")
            return True
        
        try:
            # Windowsì—ì„œ wingetìœ¼ë¡œ ì„¤ì¹˜ ì‹œë„
            print("wingetìœ¼ë¡œ Ollama ì„¤ì¹˜ ì‹œë„...")
            
            result = subprocess.run(
                ["winget", "install", "--id=Ollama.Ollama", "--silent"],
                capture_output=True,
                text=True,
                timeout=300  # 5ë¶„ ëŒ€ê¸°
            )
            
            if result.returncode == 0:
                print("âœ“ Ollama ì„¤ì¹˜ ì„±ê³µ")
                self.ollama_installed = True
                return True
            else:
                print(f"winget ì„¤ì¹˜ ì‹¤íŒ¨: {result.stderr}")
                
        except Exception as e:
            print(f"ìë™ ì„¤ì¹˜ ì‹¤íŒ¨: {str(e)}")
        
        # ìˆ˜ë™ ì„¤ì¹˜ ì•ˆë‚´
        print("\nìˆ˜ë™ ì„¤ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤:")
        print("1. https://ollama.ai/download ë°©ë¬¸")
        print("2. Windowsìš© Ollama ë‹¤ìš´ë¡œë“œ")
        print("3. ì„¤ì¹˜ í›„ ë‹¤ì‹œ ì‹¤í–‰")
        
        return False
    
    def start_ollama_service(self):
        """Ollama ì„œë¹„ìŠ¤ ì‹œì‘"""
        
        print("\n=== Ollama ì„œë¹„ìŠ¤ ì‹œì‘ ===")
        
        # 1. ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
        try:
            result = subprocess.run(
                ["tasklist", "/FI", "IMAGENAME eq ollama.exe"],
                capture_output=True,
                text=True,
                timeout=10
            )
            
            if "ollama.exe" in result.stdout:
                print("âœ“ Ollama ì„œë¹„ìŠ¤ ì´ë¯¸ ì‹¤í–‰ ì¤‘")
                self.ollama_running = True
                return True
                
        except Exception as e:
            print(f"í”„ë¡œì„¸ìŠ¤ í™•ì¸ ì‹¤íŒ¨: {str(e)}")
        
        # 2. ì„œë¹„ìŠ¤ ì‹œì‘ ì‹œë„
        try:
            print("Ollama ì„œë¹„ìŠ¤ ì‹œì‘ ì¤‘...")
            
            # ë°±ê·¸ë¼ìš´ë“œë¡œ ollama serve ì‹¤í–‰
            subprocess.Popen(
                ["ollama", "serve"],
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL,
                creationflags=subprocess.CREATE_NO_WINDOW
            )
            
            # ì„œë¹„ìŠ¤ ì‹œì‘ ëŒ€ê¸°
            print("ì„œë¹„ìŠ¤ ì‹œì‘ ëŒ€ê¸° ì¤‘... (10ì´ˆ)")
            time.sleep(10)
            
            # ì„œë¹„ìŠ¤ í™•ì¸
            if self.check_ollama_server():
                print("âœ“ Ollama ì„œë¹„ìŠ¤ ì‹œì‘ ì„±ê³µ")
                self.ollama_running = True
                return True
            else:
                print("âœ— ì„œë¹„ìŠ¤ ì‹œì‘ ì‹¤íŒ¨")
                return False
                
        except Exception as e:
            print(f"ì„œë¹„ìŠ¤ ì‹œì‘ ì‹¤íŒ¨: {str(e)}")
            return False
    
    def check_ollama_server(self):
        """Ollama ì„œë²„ ì‘ë‹µ í™•ì¸"""
        
        try:
            import requests
            response = requests.get("http://localhost:11434/api/tags", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    async def install_gemma3_models(self):
        """GEMMA3 ëª¨ë¸ ìë™ ì„¤ì¹˜"""
        
        print("\n=== GEMMA3 ëª¨ë¸ ìë™ ì„¤ì¹˜ ===")
        
        if not self.ollama_running:
            print("âœ— Ollama ì„œë¹„ìŠ¤ê°€ ì‹¤í–‰ë˜ì§€ ì•ŠìŒ")
            return False
        
        # ì„¤ì¹˜í•  ëª¨ë¸ ëª©ë¡ (ê²½ëŸ‰ -> ê³ ì„±ëŠ¥ ìˆœ)
        models_to_install = [
            ("gemma2:2b", "ê²½ëŸ‰ ë²„ì „ (2B)"),
            ("gemma2:9b", "ê¶Œì¥ ë²„ì „ (9B)")
        ]
        
        installed_models = []
        
        for model_id, description in models_to_install:
            print(f"\n{description} ì„¤ì¹˜ ì¤‘: {model_id}")
            
            try:
                # ollama pull ëª…ë ¹ì–´ ì‹¤í–‰
                process = subprocess.Popen(
                    ["ollama", "pull", model_id],
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    text=True,
                    bufsize=1,
                    universal_newlines=True
                )
                
                # ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© í‘œì‹œ
                while True:
                    output = process.stdout.readline()
                    if output == '' and process.poll() is not None:
                        break
                    if output:
                        # ê°„ë‹¨í•œ ì§„í–‰ ìƒí™©ë§Œ í‘œì‹œ
                        if "pulling" in output.lower() or "%" in output:
                            print(f"  ì§„í–‰: {output.strip()}")
                
                # í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ ëŒ€ê¸°
                return_code = process.poll()
                
                if return_code == 0:
                    print(f"âœ“ {model_id} ì„¤ì¹˜ ì™„ë£Œ")
                    installed_models.append(model_id)
                else:
                    stderr_output = process.stderr.read()
                    print(f"âœ— {model_id} ì„¤ì¹˜ ì‹¤íŒ¨: {stderr_output}")
                
            except Exception as e:
                print(f"âœ— {model_id} ì„¤ì¹˜ ì˜¤ë¥˜: {str(e)}")
        
        if installed_models:
            print(f"\nâœ“ ì„¤ì¹˜ëœ ëª¨ë¸: {len(installed_models)}ê°œ")
            for model in installed_models:
                print(f"  - {model}")
            return True
        else:
            print("\nâœ— ëª¨ë¸ ì„¤ì¹˜ ì‹¤íŒ¨")
            return False
    
    async def test_gemma3_performance(self):
        """GEMMA3 ì„±ëŠ¥ ìë™ í…ŒìŠ¤íŠ¸"""
        
        print("\n=== GEMMA3 ì„±ëŠ¥ ìë™ í…ŒìŠ¤íŠ¸ ===")
        
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
        try:
            result = subprocess.run(
                [sys.executable, "simple_gemma3_test.py"],
                capture_output=True,
                text=True,
                timeout=60
            )
            
            if result.returncode == 0:
                print("âœ“ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
                print(result.stdout)
                return True
            else:
                print(f"âœ— ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {result.stderr}")
                return False
                
        except Exception as e:
            print(f"âœ— í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")
            return False
    
    async def integrate_with_SOLOMONDd_ai(self):
        """ì†”ë¡œëª¬ë“œ AI ì‹œìŠ¤í…œì— GEMMA3 í†µí•©"""
        
        print("\n=== ì†”ë¡œëª¬ë“œ AI í†µí•© ===")
        
        # ollama_integration_engine.py ì—…ë°ì´íŠ¸
        try:
            engine_file = Path("core/ollama_integration_engine.py")
            
            if engine_file.exists():
                # ê¸°ì¡´ íŒŒì¼ ì½ê¸°
                with open(engine_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # GEMMA3 ëª¨ë¸ ì¶”ê°€ (ê¸°ì¡´ ëª¨ë¸ê³¼ í•¨ê»˜)
                if "gemma2:9b" not in content:
                    # ëª¨ë¸ ì„¤ì • ë¶€ë¶„ ì°¾ì•„ì„œ GEMMA3 ì¶”ê°€
                    models_section = '''        self.models = {
            "korean_chat": "llama3.1:8b-korean",  # í•œêµ­ì–´ ëŒ€í™” ì´í•´
            "emotion_analysis": "mistral:7b",      # ê°ì • ë¶„ì„
            "structured_output": "codellama:7b",   # êµ¬ì¡°í™”ëœ ì¶œë ¥
            "gemma3_korean": "gemma2:9b",          # GEMMA3 í•œêµ­ì–´ ë¶„ì„ (NEW)
            "gemma3_fast": "gemma2:2b"             # GEMMA3 ë¹ ë¥¸ ì²˜ë¦¬ (NEW)
        }'''
                    
                    # ê¸°ì¡´ ëª¨ë¸ ì„¤ì • ì°¾ì•„ì„œ êµì²´
                    import re
                    pattern = r'self\.models = \{[^}]+\}'
                    
                    if re.search(pattern, content):
                        updated_content = re.sub(pattern, models_section, content)
                        
                        # ë°±ì—… íŒŒì¼ ìƒì„±
                        backup_file = engine_file.with_suffix('.py.backup')
                        with open(backup_file, 'w', encoding='utf-8') as f:
                            f.write(content)
                        
                        # ì—…ë°ì´íŠ¸ëœ ë‚´ìš© ì €ì¥
                        with open(engine_file, 'w', encoding='utf-8') as f:
                            f.write(updated_content)
                        
                        print(f"âœ“ {engine_file} ì—…ë°ì´íŠ¸ ì™„ë£Œ")
                        print(f"âœ“ ë°±ì—… íŒŒì¼: {backup_file}")
                        
                        return True
                
                else:
                    print("âœ“ GEMMA3 ì´ë¯¸ í†µí•©ë¨")
                    return True
                    
            else:
                print("âœ— ollama_integration_engine.py íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return False
                
        except Exception as e:
            print(f"âœ— í†µí•© ì‹¤íŒ¨: {str(e)}")
            return False
    
    async def run_full_setup(self):
        """ì „ì²´ ì„¤ì • ìë™ ì‹¤í–‰"""
        
        print("ğŸš€ GEMMA3 ìë™ ì„¤ì • ì‹œì‘")
        print("="*50)
        
        try:
            # 1. Ollama ì„¤ì¹˜ í™•ì¸
            if not self.check_ollama_installation():
                if not self.install_ollama():
                    print("âŒ Ollama ì„¤ì¹˜ ì‹¤íŒ¨ - ìˆ˜ë™ ì„¤ì¹˜ í•„ìš”")
                    return False
            
            # 2. Ollama ì„œë¹„ìŠ¤ ì‹œì‘
            if not self.start_ollama_service():
                print("âŒ Ollama ì„œë¹„ìŠ¤ ì‹œì‘ ì‹¤íŒ¨")
                return False
            
            # 3. GEMMA3 ëª¨ë¸ ì„¤ì¹˜
            if not await self.install_gemma3_models():
                print("âŒ GEMMA3 ëª¨ë¸ ì„¤ì¹˜ ì‹¤íŒ¨")
                return False
            
            # 4. ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
            if not await self.test_gemma3_performance():
                print("âš ï¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ - ìˆ˜ë™ í™•ì¸ í•„ìš”")
            
            # 5. ì†”ë¡œëª¬ë“œ AI í†µí•©
            if not await self.integrate_with_SOLOMONDd_ai():
                print("âš ï¸ ì‹œìŠ¤í…œ í†µí•© ì‹¤íŒ¨ - ìˆ˜ë™ ì„¤ì • í•„ìš”")
            
            print("\nğŸ‰ GEMMA3 ìë™ ì„¤ì • ì™„ë£Œ!")
            print("âœ“ Ollama ì„œë¹„ìŠ¤ ì‹¤í–‰ ì¤‘")
            print("âœ“ GEMMA3 ëª¨ë¸ ì„¤ì¹˜ ì™„ë£Œ")
            print("âœ“ ì†”ë¡œëª¬ë“œ AI í†µí•© ì¤€ë¹„ ì™„ë£Œ")
            
            print(f"\në‹¤ìŒ ë‹¨ê³„:")
            print(f"1. python demo_integrated_system.py - í†µí•© í…ŒìŠ¤íŠ¸")
            print(f"2. ë©”ì¸ ì‹œìŠ¤í…œì—ì„œ GEMMA3 ì„±ëŠ¥ í™•ì¸")
            print(f"3. ê¸°ì¡´ ëª¨ë¸ê³¼ ì„±ëŠ¥ ë¹„êµ")
            
            return True
            
        except Exception as e:
            print(f"âŒ ìë™ ì„¤ì • ì‹¤íŒ¨: {str(e)}")
            return False

async def main():
    """ë©”ì¸ ì‹¤í–‰"""
    
    setup = AutoGEMMA3Setup()
    success = await setup.run_full_setup()
    
    if success:
        print("\nâœ¨ ëª¨ë“  ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        print("\nâš ï¸ ì¼ë¶€ ì„¤ì •ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ìˆ˜ë™ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")

if __name__ == "__main__":
    asyncio.run(main())