#!/usr/bin/env python3
"""
ë©€í‹°ëª¨ë‹¬ í™”ì ë¶„ë¦¬ ì‹œìŠ¤í…œ
- ìŒì„± íŠ¹ì§• (29ì°¨ì›) + ì‹œê°ì  íŠ¹ì§• (ì–¼êµ´ ì¸ì‹) ìœµí•©
- The Rise of the Eco-Friendly Luxury Consumer 3ëª… ë°œí‘œì ë¶„ì„ íŠ¹í™”
"""

import os
import sys
import cv2
import json
import time
import librosa
import numpy as np
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime, timedelta
import subprocess
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score

# Whisper STT
try:
    import whisper
    WHISPER_AVAILABLE = True
except ImportError:
    WHISPER_AVAILABLE = False

class MultimodalSpeakerDiarization:
    """ë©€í‹°ëª¨ë‹¬ í™”ì ë¶„ë¦¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.logger = self._setup_logging()
        
        # ìŒì„± ë¶„ì„ ì„¤ì •
        self.sample_rate = 16000
        self.hop_length = 512
        self.n_mfcc = 13
        
        # ì˜ìƒ ë¶„ì„ ì„¤ì •
        self.face_cascade = None
        self.frame_sample_rate = 1  # 1ì´ˆë§ˆë‹¤ 1í”„ë ˆì„ ìƒ˜í”Œë§
        
        # í™”ì ë¶„ë¦¬ ì„¤ì •
        self.min_speakers = 2
        self.max_speakers = 6
        self.min_segment_duration = 2.0  # ìµœì†Œ ì„¸ê·¸ë¨¼íŠ¸ ê¸¸ì´ (ì´ˆ)
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        self._init_models()
        
    def _setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        import logging
        logging.basicConfig(level=logging.INFO)
        return logging.getLogger(__name__)
    
    def _init_models(self):
        """ëª¨ë¸ ì´ˆê¸°í™”"""
        print("Initializing Multimodal Speaker Diarization System...")
        
        # Load OpenCV face recognition model
        try:
            cascade_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
            self.face_cascade = cv2.CascadeClassifier(cascade_path)
            print("Face recognition model loaded successfully")
        except Exception as e:
            print(f"Failed to load face recognition model: {e}")
        
        # Load Whisper model
        if WHISPER_AVAILABLE:
            try:
                self.whisper_model = whisper.load_model("base")
                print("Whisper STT model loaded successfully")
            except Exception as e:
                print(f"Failed to load Whisper model: {e}")
                self.whisper_model = None
        else:
            print("Whisper library not available")
            self.whisper_model = None
    
    def analyze_video_multimodal(self, video_path: str, context: Dict[str, Any] = None) -> Dict[str, Any]:
        """Multimodal video analysis"""
        print(f"Starting multimodal video analysis: {os.path.basename(video_path)}")
        
        if not os.path.exists(video_path):
            raise FileNotFoundError(f"Video file not found: {video_path}")
        
        start_time = time.time()
        
        # 1. Extract video information
        video_info = self._extract_video_info(video_path)
        print(f"Video info: {video_info['duration']:.1f}s, {video_info['fps']:.1f}fps")
        
        # 2. Extract and analyze audio
        audio_path = self._extract_audio(video_path)
        audio_analysis = self._analyze_audio_multimodal(audio_path)
        
        # 3. Visual speaker analysis
        visual_analysis = self._analyze_visual_speakers(video_path)
        
        # 4. Multimodal fusion
        fused_analysis = self._fuse_multimodal_features(audio_analysis, visual_analysis)
        
        # 5. Context-based refinement
        if context:
            fused_analysis = self._apply_context(fused_analysis, context)
        
        processing_time = time.time() - start_time
        
        result = {
            "video_info": video_info,
            "audio_analysis": audio_analysis,
            "visual_analysis": visual_analysis,
            "multimodal_result": fused_analysis,
            "processing_time": processing_time,
            "timestamp": datetime.now().isoformat()
        }
        
        print(f"Multimodal analysis completed ({processing_time:.1f}s)")
        return result
    
    def _extract_video_info(self, video_path: str) -> Dict[str, Any]:
        """ì˜ìƒ ì •ë³´ ì¶”ì¶œ"""
        cap = cv2.VideoCapture(video_path)
        
        info = {
            "fps": cap.get(cv2.CAP_PROP_FPS),
            "frame_count": int(cap.get(cv2.CAP_PROP_FRAME_COUNT)),
            "width": int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),
            "height": int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)),
            "duration": 0,
            "file_size": os.path.getsize(video_path) / (1024 * 1024)  # MB
        }
        
        if info["fps"] > 0:
            info["duration"] = info["frame_count"] / info["fps"]
        
        cap.release()
        return info
    
    def _extract_audio(self, video_path: str) -> str:
        """ì˜ìƒì—ì„œ ìŒì„± ì¶”ì¶œ"""
        audio_path = video_path.replace('.MOV', '_audio.wav').replace('.mp4', '_audio.wav')
        
        try:
            # FFmpegë¥¼ ì‚¬ìš©í•œ ìŒì„± ì¶”ì¶œ
            cmd = [
                'ffmpeg', '-i', video_path,
                '-ar', str(self.sample_rate),
                '-ac', '1',  # ëª¨ë…¸
                '-y',  # ë®ì–´ì“°ê¸°
                audio_path
            ]
            
            subprocess.run(cmd, capture_output=True, check=True)
            print(f"Audio extraction completed: {os.path.basename(audio_path)}")
            return audio_path
            
        except subprocess.CalledProcessError as e:
            print(f"Audio extraction failed: {e}")
            return None
    
    def _analyze_audio_multimodal(self, audio_path: str) -> Dict[str, Any]:
        """ë©€í‹°ëª¨ë‹¬ ìŒì„± ë¶„ì„"""
        if not audio_path or not os.path.exists(audio_path):
            return {"error": "ìŒì„± íŒŒì¼ ì—†ìŒ"}
        
        print("Extracting 29-dimensional voice features...")
        
        # 1. Load audio data
        y, sr = librosa.load(audio_path, sr=self.sample_rate)
        duration = len(y) / sr
        
        # 2. STT processing
        transcription = self._transcribe_audio(audio_path)
        
        # 3. Extract 29-dimensional voice features
        voice_features = self._extract_voice_features_29d(y, sr)
        
        # 4. Speaker diarization
        speaker_segments = self._identify_speakers_multimodal(voice_features, transcription)
        
        return {
            "duration": duration,
            "transcription": transcription,
            "voice_features": voice_features,
            "speaker_segments": speaker_segments,
            "feature_dimensions": len(voice_features[0]) if voice_features else 0
        }
    
    def _extract_voice_features_29d(self, y: np.ndarray, sr: int) -> List[List[float]]:
        """29ì°¨ì› ìŒì„± íŠ¹ì§• ë²¡í„° ì¶”ì¶œ"""
        
        # ì„¸ê·¸ë¨¼íŠ¸ ë‹¨ìœ„ë¡œ ë¶„í•  (2ì´ˆì”©)
        segment_length = int(2 * sr)  # 2ì´ˆ
        hop_length = int(1 * sr)      # 1ì´ˆ hop
        
        features = []
        
        for start in range(0, len(y) - segment_length, hop_length):
            end = start + segment_length
            segment = y[start:end]
            
            if len(segment) < segment_length:
                # íŒ¨ë”©
                segment = np.pad(segment, (0, segment_length - len(segment)))
            
            # 29ì°¨ì› íŠ¹ì§• ì¶”ì¶œ
            feature_vector = []
            
            # 1-13: MFCC (13ì°¨ì›)
            mfccs = librosa.feature.mfcc(y=segment, sr=sr, n_mfcc=self.n_mfcc)
            feature_vector.extend(np.mean(mfccs, axis=1))
            
            # 14-15: ìŠ¤í™íŠ¸ëŸ´ ì„¼íŠ¸ë¡œì´ë“œ (í‰ê· , í‘œì¤€í¸ì°¨)
            centroid = librosa.feature.spectral_centroid(y=segment, sr=sr)
            feature_vector.extend([np.mean(centroid), np.std(centroid)])
            
            # 16-17: ìŠ¤í™íŠ¸ëŸ´ ë¡¤ì˜¤í”„ (í‰ê· , í‘œì¤€í¸ì°¨)
            rolloff = librosa.feature.spectral_rolloff(y=segment, sr=sr)
            feature_vector.extend([np.mean(rolloff), np.std(rolloff)])
            
            # 18-19: ì œë¡œ í¬ë¡œì‹± ë ˆì´íŠ¸ (í‰ê· , í‘œì¤€í¸ì°¨)
            zcr = librosa.feature.zero_crossing_rate(segment)
            feature_vector.extend([np.mean(zcr), np.std(zcr)])
            
            # 20-21: RMS ì—ë„ˆì§€ (í‰ê· , í‘œì¤€í¸ì°¨)
            rms = librosa.feature.rms(y=segment)
            feature_vector.extend([np.mean(rms), np.std(rms)])
            
            # 22-23: ìŠ¤í™íŠ¸ëŸ´ ëŒ€ì—­í­ (í‰ê· , í‘œì¤€í¸ì°¨)
            bandwidth = librosa.feature.spectral_bandwidth(y=segment, sr=sr)
            feature_vector.extend([np.mean(bandwidth), np.std(bandwidth)])
            
            # 24-25: í”¼ì¹˜ (í‰ê· , í‘œì¤€í¸ì°¨)
            pitches, magnitudes = librosa.piptrack(y=segment, sr=sr)
            pitch_values = []
            for t in range(pitches.shape[1]):
                index = magnitudes[:, t].argmax()
                pitch = pitches[index, t]
                if pitch > 0:
                    pitch_values.append(pitch)
            
            if pitch_values:
                feature_vector.extend([np.mean(pitch_values), np.std(pitch_values)])
            else:
                feature_vector.extend([0, 0])
            
            # 26-29: ì¶”ê°€ ìŠ¤í™íŠ¸ëŸ´ íŠ¹ì§•ë“¤
            contrast = librosa.feature.spectral_contrast(y=segment, sr=sr)
            feature_vector.extend([
                np.mean(contrast),
                np.std(contrast),
                np.mean(segment**2),  # íŒŒì›Œ
                len([x for x in segment if abs(x) > 0.01]) / len(segment)  # í™œì„±ë„
            ])
            
            features.append(feature_vector)
        
        return features
    
    def _transcribe_audio(self, audio_path: str) -> Dict[str, Any]:
        """ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        if not self.whisper_model:
            return {"segments": [], "text": ""}
        
        try:
            result = self.whisper_model.transcribe(audio_path)
            return result
        except Exception as e:
            print(f"âš ï¸ STT ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {"segments": [], "text": ""}
    
    def _analyze_visual_speakers(self, video_path: str) -> Dict[str, Any]:
        """ì‹œê°ì  í™”ì ë¶„ì„"""
        print("Visual speaker analysis in progress...")
        
        if not self.face_cascade:
            return {"error": "Face recognition model not available"}
        
        cap = cv2.VideoCapture(video_path)
        fps = cap.get(cv2.CAP_PROP_FPS)
        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        face_detections = []
        frame_samples = []
        
        # 1ì´ˆë§ˆë‹¤ í”„ë ˆì„ ìƒ˜í”Œë§
        sample_interval = int(fps / self.frame_sample_rate)
        
        for frame_idx in range(0, frame_count, sample_interval):
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
            ret, frame = cap.read()
            
            if not ret:
                break
            
            timestamp = frame_idx / fps
            
            # ì–¼êµ´ ê°ì§€
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            faces = self.face_cascade.detectMultiScale(
                gray, scaleFactor=1.1, minNeighbors=5, minSize=(50, 50)
            )
            
            face_info = {
                "timestamp": timestamp,
                "frame_idx": frame_idx,
                "face_count": len(faces),
                "faces": []
            }
            
            for (x, y, w, h) in faces:
                face_data = {
                    "bbox": [int(x), int(y), int(w), int(h)],
                    "center": [int(x + w/2), int(y + h/2)],
                    "area": int(w * h),
                    "confidence": 1.0  # ê¸°ë³¸ê°’
                }
                face_info["faces"].append(face_data)
            
            face_detections.append(face_info)
            
            # ëŒ€í‘œ í”„ë ˆì„ ì €ì¥ (ë§¤ 30ì´ˆë§ˆë‹¤)
            if int(timestamp) % 30 == 0:
                frame_samples.append({
                    "timestamp": timestamp,
                    "face_count": len(faces),
                    "frame_path": f"frame_{int(timestamp)}.jpg"
                })
        
        cap.release()
        
        # í™”ì ìˆ˜ ì¶”ì • (ì–¼êµ´ ê°ì§€ ê¸°ë°˜)
        face_counts = [detection["face_count"] for detection in face_detections]
        estimated_speakers = max(face_counts) if face_counts else 1
        
        # í™”ì ì „í™˜ ê°ì§€
        speaker_transitions = self._detect_speaker_transitions(face_detections)
        
        return {
            "estimated_speakers": estimated_speakers,
            "face_detections": face_detections,
            "speaker_transitions": speaker_transitions,
            "frame_samples": frame_samples,
            "total_frames_analyzed": len(face_detections)
        }
    
    def _detect_speaker_transitions(self, face_detections: List[Dict]) -> List[Dict]:
        """í™”ì ì „í™˜ ê°ì§€"""
        transitions = []
        prev_count = 0
        
        for detection in face_detections:
            current_count = detection["face_count"]
            
            # ì–¼êµ´ ìˆ˜ ë³€í™” ê°ì§€
            if current_count != prev_count and current_count > 0:
                transitions.append({
                    "timestamp": detection["timestamp"],
                    "from_speakers": prev_count,
                    "to_speakers": current_count,
                    "transition_type": "visual_change"
                })
            
            prev_count = current_count
        
        return transitions
    
    def _identify_speakers_multimodal(self, voice_features: List, transcription: Dict) -> Dict[str, Any]:
        """ë©€í‹°ëª¨ë‹¬ í™”ì ë¶„ë¦¬"""
        if not voice_features or len(voice_features) < 2:
            return {"speakers": 1, "segments": [], "quality_score": 0}
        
        try:
            # íŠ¹ì§• ë²¡í„° ì •ê·œí™”
            scaler = StandardScaler()
            features_scaled = scaler.fit_transform(voice_features)
            
            # PCA ì°¨ì› ì¶•ì†Œ
            n_components = min(10, len(voice_features) - 1, len(voice_features[0]))
            if n_components < 2:
                return {"speakers": 1, "segments": [], "quality_score": 0}
            
            pca = PCA(n_components=n_components)
            features_pca = pca.fit_transform(features_scaled)
            
            # ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ì°¾ê¸°
            best_score = -1
            best_n_speakers = 1
            
            for n_speakers in range(self.min_speakers, min(self.max_speakers + 1, len(voice_features))):
                kmeans = KMeans(n_clusters=n_speakers, random_state=42, n_init=10)
                labels = kmeans.fit_predict(features_pca)
                
                if len(set(labels)) > 1:
                    score = silhouette_score(features_pca, labels)
                    if score > best_score:
                        best_score = score
                        best_n_speakers = n_speakers
            
            # ìµœì  í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
            kmeans = KMeans(n_clusters=best_n_speakers, random_state=42, n_init=10)
            speaker_labels = kmeans.fit_predict(features_pca)
            
            # ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±
            segments = []
            for i, label in enumerate(speaker_labels):
                segments.append({
                    "start": i * 1.0,  # 1ì´ˆ ê°„ê²©
                    "end": (i + 1) * 1.0,
                    "speaker": f"Speaker_{label + 1}",
                    "confidence": 0.8
                })
            
            return {
                "speakers": best_n_speakers,
                "segments": segments,
                "quality_score": best_score,
                "method": "voice_based_29d"
            }
            
        except Exception as e:
            print(f"âš ï¸ ìŒì„± ê¸°ë°˜ í™”ì ë¶„ë¦¬ ì‹¤íŒ¨: {e}")
            return {"speakers": 1, "segments": [], "quality_score": 0}
    
    def _fuse_multimodal_features(self, audio_analysis: Dict, visual_analysis: Dict) -> Dict[str, Any]:
        """ë©€í‹°ëª¨ë‹¬ íŠ¹ì§• ìœµí•©"""
        print("Fusing multimodal features...")
        
        # ìŒì„± ê¸°ë°˜ í™”ì ìˆ˜
        audio_speakers = audio_analysis.get("speaker_segments", {}).get("speakers", 1)
        
        # ì‹œê°ì  ê¸°ë°˜ í™”ì ìˆ˜
        visual_speakers = visual_analysis.get("estimated_speakers", 1)
        
        # ìœµí•© ê²°ì • ë¡œì§
        if visual_speakers >= 3 and audio_speakers < 3:
            # ì‹œê°ì ìœ¼ë¡œ 3ëª… ì´ìƒ ê°ì§€ë˜ë©´ ì‹œê°ì  ê²°ê³¼ ìš°ì„ 
            final_speakers = visual_speakers
            confidence = "visual_priority"
        elif audio_speakers >= 2 and visual_speakers == 1:
            # ìŒì„±ìœ¼ë¡œ 2ëª… ì´ìƒ ê°ì§€ë˜ê³  ì‹œê°ì ìœ¼ë¡œ 1ëª…ì´ë©´ ìŒì„± ê²°ê³¼ ìš°ì„ 
            final_speakers = audio_speakers
            confidence = "audio_priority"
        else:
            # í‰ê· ê°’ ì‚¬ìš©
            final_speakers = max(audio_speakers, visual_speakers)
            confidence = "consensus"
        
        # ì‹œê°ì  ì „í™˜ì ê³¼ ìŒì„± ì„¸ê·¸ë¨¼íŠ¸ ë§¤ì¹­
        transitions = visual_analysis.get("speaker_transitions", [])
        audio_segments = audio_analysis.get("speaker_segments", {}).get("segments", [])
        
        # ì •ì œëœ ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±
        refined_segments = self._create_refined_segments(audio_segments, transitions, final_speakers)
        
        return {
            "final_speaker_count": final_speakers,
            "confidence_method": confidence,
            "audio_speakers": audio_speakers,
            "visual_speakers": visual_speakers,
            "refined_segments": refined_segments,
            "visual_transitions": transitions,
            "fusion_quality": self._calculate_fusion_quality(audio_analysis, visual_analysis)
        }
    
    def _create_refined_segments(self, audio_segments: List, visual_transitions: List, speaker_count: int) -> List[Dict]:
        """ì •ì œëœ ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±"""
        refined = []
        
        # ì‹œê°ì  ì „í™˜ì ì„ ê¸°ì¤€ìœ¼ë¡œ ì„¸ê·¸ë¨¼íŠ¸ ì¡°ì •
        for segment in audio_segments:
            # í•´ë‹¹ ì‹œê°„ëŒ€ì˜ ì‹œê°ì  ì „í™˜ í™•ì¸
            segment_start = segment.get("start", 0)
            segment_end = segment.get("end", 0)
            
            # ì‹œê°ì  ì „í™˜ì ê³¼ ë§¤ì¹­
            matching_transition = None
            for transition in visual_transitions:
                if segment_start <= transition["timestamp"] <= segment_end:
                    matching_transition = transition
                    break
            
            refined_segment = {
                "start": segment_start,
                "end": segment_end,
                "speaker": segment.get("speaker", "Speaker_1"),
                "confidence": segment.get("confidence", 0.5),
                "modality": "multimodal",
                "visual_support": matching_transition is not None
            }
            
            if matching_transition:
                refined_segment["visual_transition"] = matching_transition
                refined_segment["confidence"] = min(0.9, refined_segment["confidence"] + 0.2)
            
            refined.append(refined_segment)
        
        return refined
    
    def _calculate_fusion_quality(self, audio_analysis: Dict, visual_analysis: Dict) -> float:
        """ìœµí•© í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        score = 0.5  # ê¸°ë³¸ ì ìˆ˜
        
        # ìŒì„± í’ˆì§ˆ ì ìˆ˜
        audio_quality = audio_analysis.get("speaker_segments", {}).get("quality_score", 0)
        if audio_quality > 0:
            score += audio_quality * 0.3
        
        # ì‹œê°ì  ê°ì§€ í’ˆì§ˆ
        visual_detections = len(visual_analysis.get("face_detections", []))
        if visual_detections > 0:
            score += min(0.3, visual_detections / 100)
        
        # ì „í™˜ì  ì¼ì¹˜ë„
        transitions = len(visual_analysis.get("speaker_transitions", []))
        if transitions > 0:
            score += min(0.2, transitions / 10)
        
        return min(1.0, score)
    
    def _apply_context(self, analysis: Dict, context: Dict) -> Dict[str, Any]:
        """ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ì ìš©"""
        if not context:
            return analysis
        
        # ì»¨í¼ëŸ°ìŠ¤ëª…ì—ì„œ í™”ì ìˆ˜ íŒíŠ¸
        conference_name = context.get("conference_name", "")
        if "3ëª…" in conference_name or "three" in conference_name.lower():
            if analysis["final_speaker_count"] != 3:
                analysis["context_adjusted"] = True
                analysis["original_speaker_count"] = analysis["final_speaker_count"]
                analysis["final_speaker_count"] = 3
                analysis["confidence_method"] += "_context_adjusted"
        
        # ë°œí‘œì ì •ë³´ ì ìš©
        speaker_info = context.get("speaker_info", {})
        if speaker_info:
            analysis["expected_speakers"] = speaker_info
        
        return analysis
    
    def save_analysis_result(self, result: Dict[str, Any], output_path: str = None) -> str:
        """ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        if not output_path:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = f"multimodal_analysis_{timestamp}.json"
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"ğŸ“Š ë¶„ì„ ê²°ê³¼ ì €ì¥: {output_path}")
        return output_path

def main():
    """Main execution function"""
    print("=== Multimodal Speaker Diarization System ===")
    
    # System initialization
    analyzer = MultimodalSpeakerDiarization()
    
    # Test video path
    video_path = "C:/Users/PC_58410/solomond-ai-system/user_files/JGA2025_D1/IMG_0032.MOV"
    
    # Context information
    context = {
        "conference_name": "The Rise of the Eco-Friendly Luxury Consumer",
        "expected_speakers": 3,
        "speaker_info": {
            "total_speakers": 3,
            "presentation_type": "sequential"
        }
    }
    
    if os.path.exists(video_path):
        try:
            # Execute multimodal analysis
            result = analyzer.analyze_video_multimodal(video_path, context)
            
            # Print results
            print("\n" + "="*50)
            print("MULTIMODAL SPEAKER DIARIZATION RESULTS")
            print("="*50)
            
            multimodal = result["multimodal_result"]
            print(f"Final speaker count: {multimodal['final_speaker_count']} speakers")
            print(f"Confidence method: {multimodal['confidence_method']}")
            print(f"Audio-based: {multimodal['audio_speakers']} speakers")
            print(f"Visual-based: {multimodal['visual_speakers']} speakers")
            print(f"Fusion quality: {multimodal['fusion_quality']:.3f}")
            
            # Save results
            output_file = analyzer.save_analysis_result(result)
            print(f"\nAnalysis completed! Result file: {output_file}")
            
        except Exception as e:
            print(f"Analysis failed: {e}")
            import traceback
            traceback.print_exc()
    else:
        print(f"Video file not found: {video_path}")

if __name__ == "__main__":
    main()