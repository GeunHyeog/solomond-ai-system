#!/usr/bin/env python3
"""
ğŸ›¡ï¸ Defensive Model Loader - ì—°ì‡„ ë¬¸ì œ ë°©ì§€ ì‹œìŠ¤í…œ
PyTorch 2.7.1 meta tensor ë¬¸ì œ ì™„ì „ í•´ê²° ë° ì•ˆì „í•œ ëª¨ë¸ ë¡œë”©
"""

import torch
import warnings
import traceback
from typing import Optional, Any, Union

def safe_model_load(load_function, *args, **kwargs):
    """
    ì•ˆì „í•œ ëª¨ë¸ ë¡œë”© ë˜í¼ - ëª¨ë“  PyTorch ëª¨ë¸ì— ì‚¬ìš© ê°€ëŠ¥
    """
    try:
        # 1ì°¨ ì‹œë„: ì›ë³¸ íŒŒë¼ë¯¸í„°ë¡œ ë¡œë”©
        return load_function(*args, **kwargs)
    except Exception as e:
        if "meta tensor" in str(e):
            print(f"âš ï¸ Meta tensor ë¬¸ì œ ê°ì§€, ì•ˆì „ ëª¨ë“œë¡œ ì „í™˜: {e}")
            
            # 2ì°¨ ì‹œë„: CPU ê°•ì œ ë¡œë”©
            try:
                if 'device' in kwargs:
                    kwargs['device'] = 'cpu'
                elif len(args) > 1:
                    # whisper.load_model("base", device="cpu") í˜•íƒœ
                    args = list(args)
                    if len(args) == 1:
                        args.append('cpu')
                    else:
                        args[1] = 'cpu'
                
                model = load_function(*args, **kwargs)
                
                # 3ì°¨ ì‹œë„: GPUë¡œ ì•ˆì „í•˜ê²Œ ì´ë™
                if torch.cuda.is_available():
                    try:
                        model = model.to('cuda')
                        print("âœ… CPU ë¡œë”© â†’ GPU ì´ë™ ì„±ê³µ")
                    except:
                        print("âš ï¸ GPU ì´ë™ ì‹¤íŒ¨, CPU ëª¨ë“œ ìœ ì§€")
                
                return model
                
            except Exception as e2:
                print(f"âŒ ì•ˆì „ ëª¨ë“œë„ ì‹¤íŒ¨: {e2}")
                raise e2
        else:
            raise e

def safe_whisper_load(model_size: str = "base", device: Optional[str] = None):
    """Whisper ì „ìš© ì•ˆì „ ë¡œë”"""
    import whisper
    
    def _load():
        if device:
            return whisper.load_model(model_size, device=device)
        else:
            return whisper.load_model(model_size)
    
    return safe_model_load(_load)

def safe_sentence_transformer_load(model_name: str, device: Optional[str] = None):
    """SentenceTransformer ì „ìš© ì•ˆì „ ë¡œë”"""
    try:
        from sentence_transformers import SentenceTransformer
        
        def _load():
            if device:
                return SentenceTransformer(model_name, device=device)
            else:
                return SentenceTransformer(model_name)
        
        return safe_model_load(_load)
    except ImportError:
        print("âŒ SentenceTransformer ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
        return None

# ì „ì—­ ì•ˆì „ ëª¨ë“œ ì„¤ì •
def enable_defensive_mode():
    """
    ì „ì—­ì ìœ¼ë¡œ ì•ˆì „ ëª¨ë“œ í™œì„±í™”
    """
    # PyTorch ê²½ê³  í•„í„°ë§
    warnings.filterwarnings("ignore", category=UserWarning, message=".*meta tensor.*")
    
    # CUDA ë©”ëª¨ë¦¬ ìµœì í™”
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        # ë©”ëª¨ë¦¬ í• ë‹¹ ìµœì í™”
        torch.backends.cudnn.benchmark = True
        torch.backends.cuda.matmul.allow_tf32 = True

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    enable_defensive_mode()
    
    print("Defensive Model Loader í…ŒìŠ¤íŠ¸")
    
    # Whisper í…ŒìŠ¤íŠ¸
    try:
        model = safe_whisper_load("base")
        print("OK Whisper ì•ˆì „ ë¡œë”© ì„±ê³µ")
    except Exception as e:
        print(f"ERROR Whisper í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    # SentenceTransformer í…ŒìŠ¤íŠ¸  
    try:
        embedder = safe_sentence_transformer_load('paraphrase-multilingual-MiniLM-L12-v2')
        if embedder:
            print("OK SentenceTransformer ì•ˆì „ ë¡œë”© ì„±ê³µ")
    except Exception as e:
        print(f"ERROR SentenceTransformer í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")