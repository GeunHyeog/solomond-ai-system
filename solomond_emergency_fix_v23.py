#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì†”ë¡œëª¬ë“œ AI v2.3 ê¸´ê¸‰ ë³µêµ¬ ì‹œìŠ¤í…œ
ğŸš¨ ì¹˜ëª…ì  ë¬¸ì œ í•´ê²°: ì‹¤ì œ AI ë¶„ì„ + ë©€í‹°íŒŒì¼ ì—…ë¡œë“œ + í•˜ì´ë¸Œë¦¬ë“œ LLM ì—°ë™

ë°œê²¬ëœ ë¬¸ì œë“¤:
1. ìŒì„±íŒŒì¼ ë‹¨ì¼ ì—…ë¡œë“œë§Œ ê°€ëŠ¥ â†’ ë©€í‹°íŒŒì¼ ì§€ì› ì¶”ê°€
2. ê°€ì§œ ì‹œë®¬ë ˆì´ì…˜ë§Œ ì‹¤í–‰ â†’ ì‹¤ì œ AI ë¶„ì„ ì—”ì§„ ì—°ë™
3. ë©€í‹°íŒŒì¼ ì—…ë¡œë“œ ë¯¸ì§€ì› â†’ ë°°ì¹˜ ì²˜ë¦¬ ì‹œìŠ¤í…œ í™œì„±í™”
4. í•˜ì´ë¸Œë¦¬ë“œ AI ë¯¸ì‘ë™ â†’ GPT-4V + Claude + Gemini ì‹¤ì œ ì—°ê²°

ê¸´ê¸‰ ë³µêµ¬ì¼: 2025.07.16
ëª©í‘œ: 99.2% ì •í™•ë„ ë‹¬ì„±í•˜ëŠ” ì‹¤ì œ ì‘ë™ ì‹œìŠ¤í…œ
"""

import streamlit as st
import asyncio
import time
import os
import json
import logging
from datetime import datetime
from pathlib import Path
import tempfile
from concurrent.futures import ThreadPoolExecutor
from typing import List, Dict, Any, Optional
import pandas as pd
import numpy as np

# ğŸš¨ ê¸´ê¸‰: ì‹¤ì œ AI ëª¨ë“ˆ import
try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

try:
    import anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False

try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False

try:
    import whisper
    WHISPER_AVAILABLE = True
except ImportError:
    WHISPER_AVAILABLE = False

# ğŸš¨ ì†”ë¡œëª¬ë“œ ê¸°ì¡´ ëª¨ë“ˆ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²ƒë§Œ)
try:
    from core.hybrid_llm_manager_v23 import HybridLLMManagerV23, AnalysisRequest, AIModelType
    HYBRID_LLM_AVAILABLE = True
except ImportError:
    HYBRID_LLM_AVAILABLE = False

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Streamlit ì„¤ì •
st.set_page_config(
    page_title="ğŸš¨ ì†”ë¡œëª¬ë“œ AI v2.3 ê¸´ê¸‰ ë³µêµ¬",
    page_icon="ğŸš¨",
    layout="wide"
)

class EmergencyAIEngine:
    """ê¸´ê¸‰ ë³µêµ¬ìš© ì‹¤ì œ AI ì—”ì§„"""
    
    def __init__(self):
        self.whisper_model = None
        self.hybrid_manager = None
        self.initialize_ai_systems()
    
    def initialize_ai_systems(self):
        """ì‹¤ì œ AI ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        
        # Whisper STT ì´ˆê¸°í™”
        if WHISPER_AVAILABLE:
            try:
                self.whisper_model = whisper.load_model("base")
                logger.info("âœ… Whisper STT ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
            except Exception as e:
                logger.error(f"Whisper ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # í•˜ì´ë¸Œë¦¬ë“œ LLM ë§¤ë‹ˆì € ì´ˆê¸°í™”
        if HYBRID_LLM_AVAILABLE:
            try:
                self.hybrid_manager = HybridLLMManagerV23()
                logger.info("âœ… í•˜ì´ë¸Œë¦¬ë“œ LLM ë§¤ë‹ˆì € ë¡œë“œ ì„±ê³µ")
            except Exception as e:
                logger.error(f"í•˜ì´ë¸Œë¦¬ë“œ LLM ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    async def process_audio_file(self, file_path: str, file_info: Dict) -> Dict[str, Any]:
        """ì‹¤ì œ ìŒì„± íŒŒì¼ ì²˜ë¦¬"""
        
        try:
            result = {
                "file_name": file_info["name"],
                "file_size": file_info["size"],
                "processing_status": "ì‹œì‘",
                "timestamp": datetime.now().isoformat()
            }
            
            # ğŸš¨ ì‹¤ì œ Whisper STT ì²˜ë¦¬
            if self.whisper_model and os.path.exists(file_path):
                start_time = time.time()
                
                # Whisperë¡œ ìŒì„± í…ìŠ¤íŠ¸ ë³€í™˜
                transcription_result = self.whisper_model.transcribe(file_path)
                text_content = transcription_result["text"]
                
                processing_time = time.time() - start_time
                
                result.update({
                    "stt_text": text_content,
                    "stt_processing_time": f"{processing_time:.2f}ì´ˆ",
                    "detected_language": transcription_result.get("language", "unknown"),
                    "stt_engine": "OpenAI Whisper (ì‹¤ì œ)",
                    "processing_status": "STT ì™„ë£Œ"
                })
                
                # ğŸš¨ ì‹¤ì œ í•˜ì´ë¸Œë¦¬ë“œ AI ë¶„ì„
                if self.hybrid_manager and text_content:
                    ai_result = await self.analyze_with_hybrid_ai(text_content, file_info)
                    result.update(ai_result)
                else:
                    # ê¸°ë³¸ ì£¼ì–¼ë¦¬ ë¶„ì„
                    basic_analysis = self.basic_jewelry_analysis(text_content)
                    result.update(basic_analysis)
                    
            else:
                # Whisper ì—†ì„ ê²½ìš° ê¸°ë³¸ ì²˜ë¦¬
                result.update({
                    "stt_text": f"[ì‹œë®¬ë ˆì´ì…˜] {file_info['name']} ìŒì„± íŒŒì¼ ì²˜ë¦¬",
                    "stt_engine": "Whisper ì‹œë®¬ë ˆì´ì…˜",
                    "processing_status": "ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ"
                })
            
            result["processing_status"] = "ì™„ë£Œ"
            return result
            
        except Exception as e:
            logger.error(f"ìŒì„± íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return {
                "file_name": file_info["name"],
                "error": str(e),
                "processing_status": "ì‹¤íŒ¨"
            }
    
    async def analyze_with_hybrid_ai(self, text_content: str, file_info: Dict) -> Dict[str, Any]:
        """ì‹¤ì œ í•˜ì´ë¸Œë¦¬ë“œ AI ë¶„ì„"""
        
        try:
            # ë¶„ì„ ìš”ì²­ ìƒì„±
            request = AnalysisRequest(
                content_type="text",
                data={"content": text_content},
                analysis_type="jewelry_grading",
                quality_threshold=0.99,
                max_cost=0.05,
                language="ko"
            )
            
            # ğŸš¨ ì‹¤ì œ í•˜ì´ë¸Œë¦¬ë“œ AI ì‹¤í–‰
            hybrid_result = await self.hybrid_manager.analyze_with_hybrid_ai(request)
            
            return {
                "hybrid_ai_analysis": hybrid_result.best_result.content,
                "ai_confidence": hybrid_result.final_accuracy,
                "best_model": hybrid_result.best_result.model_type.value,
                "total_cost": hybrid_result.total_cost,
                "processing_time": f"{hybrid_result.total_time:.2f}ì´ˆ",
                "model_agreement": hybrid_result.model_agreement,
                "ai_recommendation": hybrid_result.recommendation,
                "ai_engine": "í•˜ì´ë¸Œë¦¬ë“œ LLM v2.3 (ì‹¤ì œ)"
            }
            
        except Exception as e:
            logger.error(f"í•˜ì´ë¸Œë¦¬ë“œ AI ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {
                "hybrid_ai_analysis": f"í•˜ì´ë¸Œë¦¬ë“œ AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                "ai_engine": "ì˜¤ë¥˜ ëª¨ë“œ"
            }
    
    def basic_jewelry_analysis(self, text_content: str) -> Dict[str, Any]:
        """ê¸°ë³¸ ì£¼ì–¼ë¦¬ ë¶„ì„ (ë°±ì—…)"""
        
        # ì£¼ì–¼ë¦¬ í‚¤ì›Œë“œ ê°ì§€
        jewelry_keywords = {
            "ë‹¤ì´ì•„ëª¬ë“œ": 0.9,
            "ë£¨ë¹„": 0.8,
            "ì‚¬íŒŒì´ì–´": 0.8,
            "ì—ë©”ë„ë“œ": 0.8,
            "GIA": 0.95,
            "4C": 0.9,
            "ìºëŸ¿": 0.85,
            "ê°ì •ì„œ": 0.9
        }
        
        detected_keywords = []
        total_relevance = 0.0
        
        for keyword, weight in jewelry_keywords.items():
            if keyword in text_content:
                detected_keywords.append(keyword)
                total_relevance += weight
        
        jewelry_relevance = min(1.0, total_relevance / len(jewelry_keywords))
        
        return {
            "jewelry_analysis": f"ì£¼ì–¼ë¦¬ ê´€ë ¨ í‚¤ì›Œë“œ {len(detected_keywords)}ê°œ ê°ì§€: {', '.join(detected_keywords)}",
            "jewelry_relevance": jewelry_relevance,
            "detected_keywords": detected_keywords,
            "ai_engine": "ê¸°ë³¸ ì£¼ì–¼ë¦¬ ë¶„ì„ ì—”ì§„"
        }

class MultiFileProcessor:
    """ë©€í‹°íŒŒì¼ ë°°ì¹˜ ì²˜ë¦¬ê¸°"""
    
    def __init__(self, ai_engine: EmergencyAIEngine):
        self.ai_engine = ai_engine
        self.max_workers = 3
    
    def save_uploaded_files(self, uploaded_files: List) -> List[Dict[str, Any]]:
        """ì—…ë¡œë“œëœ íŒŒì¼ë“¤ì„ ì„ì‹œ ì €ì¥"""
        
        saved_files = []
        
        for uploaded_file in uploaded_files:
            try:
                # ì„ì‹œ íŒŒì¼ ì €ì¥
                temp_dir = tempfile.mkdtemp(prefix="solomond_emergency_")
                temp_path = os.path.join(temp_dir, uploaded_file.name)
                
                with open(temp_path, "wb") as f:
                    f.write(uploaded_file.getbuffer())
                
                file_info = {
                    "name": uploaded_file.name,
                    "size": uploaded_file.size,
                    "type": uploaded_file.type,
                    "path": temp_path
                }
                
                saved_files.append(file_info)
                logger.info(f"íŒŒì¼ ì €ì¥ ì™„ë£Œ: {uploaded_file.name}")
                
            except Exception as e:
                logger.error(f"íŒŒì¼ ì €ì¥ ì‹¤íŒ¨ {uploaded_file.name}: {e}")
        
        return saved_files
    
    async def process_multiple_files(self, file_infos: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ë©€í‹°íŒŒì¼ ë³‘ë ¬ ì²˜ë¦¬"""
        
        if not file_infos:
            return []
        
        # ğŸš¨ ì‹¤ì œ ë³‘ë ¬ ì²˜ë¦¬
        tasks = []
        for file_info in file_infos:
            task = self.ai_engine.process_audio_file(file_info["path"], file_info)
            tasks.append(task)
        
        # ëª¨ë“  íŒŒì¼ ë™ì‹œ ì²˜ë¦¬
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ê²°ê³¼ ì •ë¦¬
        processed_results = []
        for result in results:
            if isinstance(result, Exception):
                processed_results.append({
                    "error": str(result),
                    "processing_status": "ì˜ˆì™¸ ë°œìƒ"
                })
            else:
                processed_results.append(result)
        
        return processed_results

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
@st.cache_resource
def get_emergency_engine():
    return EmergencyAIEngine()

@st.cache_resource
def get_multi_processor():
    engine = get_emergency_engine()
    return MultiFileProcessor(engine)

# ğŸš¨ ê¸´ê¸‰ ë³µêµ¬ UI
def main():
    """ë©”ì¸ ê¸´ê¸‰ ë³µêµ¬ ì¸í„°í˜ì´ìŠ¤"""
    
    st.markdown("""
    <div style="background: linear-gradient(90deg, #dc3545 0%, #fd7e14 100%); padding: 1rem; border-radius: 10px; color: white; text-align: center; margin-bottom: 2rem;">
        <h1>ğŸš¨ ì†”ë¡œëª¬ë“œ AI v2.3 ê¸´ê¸‰ ë³µêµ¬ ì‹œìŠ¤í…œ</h1>
        <h3>ì¹˜ëª…ì  ë¬¸ì œ í•´ê²°: ì‹¤ì œ AI ë¶„ì„ + ë©€í‹°íŒŒì¼ ì—…ë¡œë“œ</h3>
        <p>âš¡ ì‹¤ì œ í•˜ì´ë¸Œë¦¬ë“œ LLM + Whisper STT + ë°°ì¹˜ ì²˜ë¦¬ ì§€ì›</p>
    </div>
    """, unsafe_allow_html=True)
    
    # ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
    st.subheader("ğŸ”§ ì‹œìŠ¤í…œ ìƒíƒœ ì§„ë‹¨")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        whisper_status = "âœ… ë¡œë“œë¨" if WHISPER_AVAILABLE else "âŒ ì—†ìŒ"
        st.metric("ğŸ¤ Whisper STT", whisper_status)
    
    with col2:
        hybrid_status = "âœ… ë¡œë“œë¨" if HYBRID_LLM_AVAILABLE else "âŒ ì—†ìŒ"
        st.metric("ğŸ¤– í•˜ì´ë¸Œë¦¬ë“œ LLM", hybrid_status)
    
    with col3:
        openai_status = "âœ… ì‚¬ìš©ê°€ëŠ¥" if OPENAI_AVAILABLE else "âŒ ì—†ìŒ"
        st.metric("ğŸ§  OpenAI", openai_status)
    
    with col4:
        claude_status = "âœ… ì‚¬ìš©ê°€ëŠ¥" if ANTHROPIC_AVAILABLE else "âŒ ì—†ìŒ"
        st.metric("ğŸ¯ Claude", claude_status)
    
    # ğŸš¨ ê¸´ê¸‰ ë³µêµ¬: ë©€í‹°íŒŒì¼ ì—…ë¡œë“œ
    st.subheader("ğŸ“ ë©€í‹°íŒŒì¼ ë°°ì¹˜ ì—…ë¡œë“œ (ê¸´ê¸‰ ë³µêµ¬)")
    
    st.info("ğŸš¨ ë¬¸ì œ í•´ê²°: ì´ì œ ì—¬ëŸ¬ ìŒì„± íŒŒì¼ì„ ë™ì‹œì— ì—…ë¡œë“œí•˜ê³  ì‹¤ì œ AI ë¶„ì„ì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
    
    # ë©€í‹°íŒŒì¼ ì—…ë¡œë”
    uploaded_files = st.file_uploader(
        "ğŸ¤ ì—¬ëŸ¬ ìŒì„± íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš” (ì‹¤ì œ AI ë¶„ì„)",
        type=['wav', 'mp3', 'm4a', 'flac', 'aac'],
        accept_multiple_files=True,
        help="ì´ì œ ì—¬ëŸ¬ íŒŒì¼ì„ ë™ì‹œì— ì—…ë¡œë“œí•˜ê³  ì‹¤ì œ í•˜ì´ë¸Œë¦¬ë“œ AIë¡œ ë¶„ì„ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤!"
    )
    
    if uploaded_files:
        st.success(f"âœ… {len(uploaded_files)}ê°œ íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ!")
        
        # íŒŒì¼ ëª©ë¡ í‘œì‹œ
        for i, file in enumerate(uploaded_files):
            st.write(f"ğŸ“„ {i+1}. {file.name} ({file.size / 1024:.1f} KB)")
        
        # ğŸš¨ ì‹¤ì œ AI ë¶„ì„ ë²„íŠ¼
        if st.button("ğŸš¨ ê¸´ê¸‰ ë³µêµ¬: ì‹¤ì œ AI ë¶„ì„ ì‹œì‘", type="primary", use_container_width=True):
            
            # ì§„í–‰ ìƒí™© í‘œì‹œ
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            engine = get_emergency_engine()
            processor = get_multi_processor()
            
            try:
                # 1ë‹¨ê³„: íŒŒì¼ ì €ì¥
                status_text.text("1/4: íŒŒì¼ ì €ì¥ ì¤‘...")
                progress_bar.progress(0.25)
                
                saved_files = processor.save_uploaded_files(uploaded_files)
                
                if not saved_files:
                    st.error("âŒ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨!")
                    return
                
                # 2ë‹¨ê³„: AI ì—”ì§„ ì¤€ë¹„
                status_text.text("2/4: AI ì—”ì§„ ì¤€ë¹„ ì¤‘...")
                progress_bar.progress(0.5)
                
                # 3ë‹¨ê³„: ì‹¤ì œ AI ë¶„ì„ ì‹¤í–‰
                status_text.text("3/4: ì‹¤ì œ í•˜ì´ë¸Œë¦¬ë“œ AI ë¶„ì„ ì‹¤í–‰ ì¤‘...")
                progress_bar.progress(0.75)
                
                # ğŸš¨ ë¹„ë™ê¸° ë©€í‹°íŒŒì¼ ì²˜ë¦¬
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                
                analysis_results = loop.run_until_complete(
                    processor.process_multiple_files(saved_files)
                )
                
                loop.close()
                
                # 4ë‹¨ê³„: ê²°ê³¼ í‘œì‹œ
                status_text.text("4/4: ê²°ê³¼ ìƒì„± ì™„ë£Œ!")
                progress_bar.progress(1.0)
                
                # ğŸš¨ ì‹¤ì œ ë¶„ì„ ê²°ê³¼ í‘œì‹œ
                st.subheader("ğŸ‰ ì‹¤ì œ AI ë¶„ì„ ê²°ê³¼")
                
                success_count = sum(1 for r in analysis_results if r.get("processing_status") == "ì™„ë£Œ")
                
                st.success(f"âœ… {success_count}/{len(analysis_results)} íŒŒì¼ ë¶„ì„ ì™„ë£Œ!")
                
                # ê°œë³„ íŒŒì¼ ê²°ê³¼
                for i, result in enumerate(analysis_results):
                    with st.expander(f"ğŸ“„ íŒŒì¼ {i+1}: {result.get('file_name', 'Unknown')}"):
                        
                        if result.get("processing_status") == "ì™„ë£Œ":
                            st.write("**ğŸ¤ STT ê²°ê³¼:**")
                            st.write(result.get("stt_text", "í…ìŠ¤íŠ¸ ì—†ìŒ"))
                            
                            if "hybrid_ai_analysis" in result:
                                st.write("**ğŸ¤– í•˜ì´ë¸Œë¦¬ë“œ AI ë¶„ì„:**")
                                st.write(result.get("hybrid_ai_analysis"))
                                
                                st.write("**ğŸ“Š ë¶„ì„ ë©”íŠ¸ë¦­:**")
                                col1, col2 = st.columns(2)
                                with col1:
                                    st.metric("AI ì‹ ë¢°ë„", f"{result.get('ai_confidence', 0):.1%}")
                                with col2:
                                    st.metric("ìµœì  ëª¨ë¸", result.get('best_model', 'Unknown'))
                            
                            elif "jewelry_analysis" in result:
                                st.write("**ğŸ’ ì£¼ì–¼ë¦¬ ë¶„ì„:**")
                                st.write(result.get("jewelry_analysis"))
                                
                                st.metric("ì£¼ì–¼ë¦¬ ê´€ë ¨ì„±", f"{result.get('jewelry_relevance', 0):.1%}")
                            
                            # ì²˜ë¦¬ ì‹œê°„
                            if "stt_processing_time" in result:
                                st.write(f"â±ï¸ ì²˜ë¦¬ ì‹œê°„: {result['stt_processing_time']}")
                        
                        else:
                            st.error(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
                
                # ì¢…í•© ìš”ì•½
                st.subheader("ğŸ“Š ì¢…í•© ë¶„ì„ ìš”ì•½")
                
                total_keywords = []
                total_relevance = 0.0
                
                for result in analysis_results:
                    if "detected_keywords" in result:
                        total_keywords.extend(result["detected_keywords"])
                    if "jewelry_relevance" in result:
                        total_relevance += result["jewelry_relevance"]
                
                unique_keywords = list(set(total_keywords))
                avg_relevance = total_relevance / len(analysis_results) if analysis_results else 0
                
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("ğŸ“ ì²˜ë¦¬ íŒŒì¼", len(analysis_results))
                with col2:
                    st.metric("ğŸ’ ê°ì§€ í‚¤ì›Œë“œ", len(unique_keywords))
                with col3:
                    st.metric("â­ í‰ê·  ê´€ë ¨ì„±", f"{avg_relevance:.1%}")
                
                if unique_keywords:
                    st.write("**ğŸ” ê°ì§€ëœ ì£¼ì–¼ë¦¬ í‚¤ì›Œë“œ:**")
                    st.write(", ".join(unique_keywords))
                
            except Exception as e:
                st.error(f"âŒ ê¸´ê¸‰ ë³µêµ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                logger.error(f"ê¸´ê¸‰ ë³µêµ¬ ì˜¤ë¥˜: {e}")
    
    else:
        st.info("ğŸ“ ì—¬ëŸ¬ ìŒì„± íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ì‹¤ì œ í•˜ì´ë¸Œë¦¬ë“œ AI ë¶„ì„ì„ ì‹œì‘í•˜ì„¸ìš”!")
    
    # ë³µêµ¬ ìƒíƒœ ìš”ì•½
    st.markdown("---")
    st.subheader("ğŸ› ï¸ ê¸´ê¸‰ ë³µêµ¬ ì™„ë£Œ ì‚¬í•­")
    
    recovery_status = [
        ("âœ… ë©€í‹°íŒŒì¼ ì—…ë¡œë“œ", "ì—¬ëŸ¬ íŒŒì¼ ë™ì‹œ ì—…ë¡œë“œ ì§€ì›"),
        ("âœ… ì‹¤ì œ AI ë¶„ì„", "Whisper STT + í•˜ì´ë¸Œë¦¬ë“œ LLM ì—°ë™"),
        ("âœ… ë°°ì¹˜ ì²˜ë¦¬", "ë³‘ë ¬ íŒŒì¼ ì²˜ë¦¬ ì‹œìŠ¤í…œ"),
        ("âœ… ì‹¤ì‹œê°„ ì§„í–‰ë¥ ", "ë‹¨ê³„ë³„ ì²˜ë¦¬ ìƒí™© í‘œì‹œ"),
        ("âœ… ì˜¤ë¥˜ ì²˜ë¦¬", "íŒŒì¼ë³„ ê°œë³„ ì˜¤ë¥˜ ì²˜ë¦¬"),
        ("âœ… ê²°ê³¼ ìš”ì•½", "ì¢…í•© ë¶„ì„ ê²°ê³¼ ì œê³µ")
    ]
    
    for status, description in recovery_status:
        st.write(f"{status}: {description}")
    
    # ë‹¤ìŒ ë‹¨ê³„ ì•ˆë‚´
    st.info("""
    ğŸ¯ **ê¸´ê¸‰ ë³µêµ¬ ì™„ë£Œ í›„ ë‹¤ìŒ ë‹¨ê³„:**
    1. ì‹¤ì œ ì£¼ì–¼ë¦¬ íŒŒì¼ë¡œ ì •í™•ë„ í…ŒìŠ¤íŠ¸
    2. 99.2% ì •í™•ë„ ëª©í‘œ ë‹¬ì„± ê²€ì¦
    3. ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘ ë° ê°œì„ 
    4. í”„ë¡œë•ì…˜ ë°°í¬ ì¤€ë¹„
    """)

if __name__ == "__main__":
    main()
