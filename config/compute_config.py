"""
ğŸ”§ GPU/CPU ì»´í“¨íŒ… ì„¤ì • í†µí•© ëª¨ë“ˆ
ì†”ë¡œëª¬ë“œ AI ì‹œìŠ¤í…œ - ì¤‘ë³µ ì½”ë“œ ì œê±° (2/3ë‹¨ê³„)

ëª©ì : 17ê°œ íŒŒì¼ì—ì„œ ë°˜ë³µë˜ëŠ” GPU/CPU ì„¤ì •ì„ ì¤‘ì•™í™”
íš¨ê³¼: í™˜ê²½ë³„ ì„¤ì • ê´€ë¦¬ ê°œì„  ë° í•˜ë“œì½”ë”© ì œê±°
"""

import os
import platform
import subprocess
from typing import Optional, Dict, List
from enum import Enum
import logging

class ComputeMode(Enum):
    """ì»´í“¨íŒ… ëª¨ë“œ ì—´ê±°í˜•"""
    AUTO = "auto"        # ìë™ ê°ì§€
    CPU_ONLY = "cpu"     # CPUë§Œ ì‚¬ìš©
    GPU_CUDA = "cuda"    # NVIDIA CUDA GPU
    GPU_MPS = "mps"      # Apple Metal Performance Shaders
    GPU_OPENCL = "opencl" # OpenCL GPU

class ComputeConfig:
    """ì†”ë¡œëª¬ë“œ AI ì‹œìŠ¤í…œ ì»´í“¨íŒ… ì„¤ì • ê´€ë¦¬ì"""
    
    _instance = None
    _initialized = False
    
    def __new__(cls):
        """ì‹±ê¸€í†¤ íŒ¨í„´ êµ¬í˜„"""
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        if self._initialized:
            return
            
        self.logger = logging.getLogger(__name__)
        self.current_mode = ComputeMode.AUTO
        self.available_modes = []
        self.gpu_info = {}
        
        # ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘
        self._detect_system_capabilities()
        self._set_optimal_mode()
        
        self._initialized = True
    
    def _detect_system_capabilities(self):
        """ì‹œìŠ¤í…œ GPU/CPU ê¸°ëŠ¥ ê°ì§€"""
        
        self.available_modes = [ComputeMode.CPU_ONLY]  # CPUëŠ” í•­ìƒ ì‚¬ìš© ê°€ëŠ¥
        
        # NVIDIA CUDA ì§€ì› í™•ì¸
        if self._check_cuda_support():
            self.available_modes.append(ComputeMode.GPU_CUDA)
            self.gpu_info['cuda'] = self._get_cuda_info()
        
        # Apple Metal ì§€ì› í™•ì¸ (macOS)
        if platform.system() == "Darwin" and self._check_mps_support():
            self.available_modes.append(ComputeMode.GPU_MPS)
            self.gpu_info['mps'] = True
        
        # OpenCL ì§€ì› í™•ì¸
        if self._check_opencl_support():
            self.available_modes.append(ComputeMode.GPU_OPENCL)
            self.gpu_info['opencl'] = True
        
        self.logger.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ ì»´í“¨íŒ… ëª¨ë“œ: {[mode.value for mode in self.available_modes]}")
    
    def _check_cuda_support(self) -> bool:
        """CUDA ì§€ì› ì—¬ë¶€ í™•ì¸"""
        try:
            # nvidia-smi ëª…ë ¹ìœ¼ë¡œ GPU í™•ì¸
            result = subprocess.run(['nvidia-smi'], 
                                  capture_output=True, 
                                  text=True, 
                                  timeout=5)
            return result.returncode == 0
        except (subprocess.TimeoutExpired, FileNotFoundError):
            return False
    
    def _get_cuda_info(self) -> Dict:
        """CUDA GPU ì •ë³´ ìˆ˜ì§‘"""
        try:
            result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total,driver_version', 
                                   '--format=csv,noheader,nounits'], 
                                  capture_output=True, text=True, timeout=5)
            
            if result.returncode == 0:
                lines = result.stdout.strip().split('\n')
                gpus = []
                for line in lines:
                    if line.strip():
                        parts = line.split(', ')
                        if len(parts) >= 3:
                            gpus.append({
                                'name': parts[0].strip(),
                                'memory_mb': int(parts[1].strip()),
                                'driver_version': parts[2].strip()
                            })
                return {'gpus': gpus, 'count': len(gpus)}
        except:
            pass
        
        return {'gpus': [], 'count': 0}
    
    def _check_mps_support(self) -> bool:
        """Apple Metal Performance Shaders ì§€ì› í™•ì¸"""
        try:
            import torch
            return torch.backends.mps.is_available()
        except:
            return False
    
    def _check_opencl_support(self) -> bool:
        """OpenCL ì§€ì› í™•ì¸"""
        try:
            import pyopencl as cl
            platforms = cl.get_platforms()
            return len(platforms) > 0
        except:
            return False
    
    def _set_optimal_mode(self):
        """ìµœì  ì»´í“¨íŒ… ëª¨ë“œ ì„¤ì •"""
        
        # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
        env_mode = os.getenv('SOLOMOND_COMPUTE_MODE', '').lower()
        if env_mode:
            for mode in ComputeMode:
                if mode.value == env_mode and mode in self.available_modes:
                    self.current_mode = mode
                    self.logger.info(f"í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •ëœ ì»´í“¨íŒ… ëª¨ë“œ: {mode.value}")
                    return
        
        # ìë™ ëª¨ë“œ ì„ íƒ (ìš°ì„ ìˆœìœ„: CUDA > MPS > CPU)
        if ComputeMode.GPU_CUDA in self.available_modes:
            # CUDA GPUê°€ ìˆì§€ë§Œ ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•œ ê²½ìš° CPU ëª¨ë“œ ì„ íƒ
            cuda_info = self.gpu_info.get('cuda', {})
            if cuda_info.get('count', 0) > 0:
                min_memory = min(gpu['memory_mb'] for gpu in cuda_info['gpus'])
                if min_memory < 4000:  # 4GB ë¯¸ë§Œì´ë©´ CPU ëª¨ë“œ
                    self.current_mode = ComputeMode.CPU_ONLY
                    self.logger.warning(f"GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ({min_memory}MB < 4GB), CPU ëª¨ë“œë¡œ ì „í™˜")
                else:
                    self.current_mode = ComputeMode.GPU_CUDA
            else:
                self.current_mode = ComputeMode.CPU_ONLY
        elif ComputeMode.GPU_MPS in self.available_modes:
            self.current_mode = ComputeMode.GPU_MPS
        else:
            self.current_mode = ComputeMode.CPU_ONLY
        
        self.logger.info(f"ìë™ ì„ íƒëœ ì»´í“¨íŒ… ëª¨ë“œ: {self.current_mode.value}")
    
    def set_mode(self, mode: ComputeMode) -> bool:
        """ì»´í“¨íŒ… ëª¨ë“œ ì„¤ì •
        
        Args:
            mode: ì„¤ì •í•  ì»´í“¨íŒ… ëª¨ë“œ
            
        Returns:
            bool: ì„¤ì • ì„±ê³µ ì—¬ë¶€
        """
        
        if mode not in self.available_modes:
            self.logger.error(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ì»´í“¨íŒ… ëª¨ë“œ: {mode.value}")
            return False
        
        self.current_mode = mode
        self._apply_environment_settings()
        self.logger.info(f"ì»´í“¨íŒ… ëª¨ë“œ ë³€ê²½: {mode.value}")
        return True
    
    def _apply_environment_settings(self):
        """í˜„ì¬ ëª¨ë“œì— ë”°ë¥¸ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •"""
        
        if self.current_mode == ComputeMode.CPU_ONLY:
            # CPU ì „ìš© ëª¨ë“œ
            os.environ['CUDA_VISIBLE_DEVICES'] = ''
            os.environ['OMP_NUM_THREADS'] = str(os.cpu_count())
            
        elif self.current_mode == ComputeMode.GPU_CUDA:
            # CUDA GPU ëª¨ë“œ
            if 'CUDA_VISIBLE_DEVICES' in os.environ and os.environ['CUDA_VISIBLE_DEVICES'] == '':
                del os.environ['CUDA_VISIBLE_DEVICES']  # ê¸°ë³¸ê°’ìœ¼ë¡œ ë³µì›
        
        # ë©”ëª¨ë¦¬ ê´€ë ¨ ì„¤ì •
        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'
        os.environ['TOKENIZERS_PARALLELISM'] = 'false'  # Hugging Face ê²½ê³  ë°©ì§€
    
    def get_device_string(self) -> str:
        """PyTorch/TensorFlowì—ì„œ ì‚¬ìš©í•  device ë¬¸ìì—´ ë°˜í™˜
        
        Returns:
            str: device ë¬¸ìì—´ ('cpu', 'cuda', 'mps' ë“±)
        """
        
        if self.current_mode == ComputeMode.CPU_ONLY:
            return 'cpu'
        elif self.current_mode == ComputeMode.GPU_CUDA:
            return 'cuda'
        elif self.current_mode == ComputeMode.GPU_MPS:
            return 'mps'
        else:
            return 'cpu'
    
    def get_whisper_device(self) -> str:
        """Whisper ëª¨ë¸ìš© device ì„¤ì • ë°˜í™˜"""
        if self.current_mode == ComputeMode.CPU_ONLY:
            return 'cpu'
        elif self.current_mode == ComputeMode.GPU_CUDA:
            return 'cuda'
        else:
            return 'cpu'  # WhisperëŠ” CUDAì™€ CPUë§Œ ì§€ì›
    
    def get_transformers_device(self) -> str:
        """Transformers ëª¨ë¸ìš© device ì„¤ì • ë°˜í™˜"""
        return self.get_device_string()
    
    def is_gpu_available(self) -> bool:
        """GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€"""
        return self.current_mode != ComputeMode.CPU_ONLY
    
    def get_memory_info(self) -> Dict:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì •ë³´ ë°˜í™˜"""
        info = {'mode': self.current_mode.value}
        
        if self.current_mode == ComputeMode.GPU_CUDA and self.gpu_info.get('cuda'):
            try:
                import torch
                if torch.cuda.is_available():
                    info['gpu_memory_allocated'] = torch.cuda.memory_allocated()
                    info['gpu_memory_reserved'] = torch.cuda.memory_reserved()
                    info['gpu_memory_total'] = torch.cuda.get_device_properties(0).total_memory
            except:
                pass
        
        return info
    
    def optimize_for_inference(self):
        """ì¶”ë¡  ìµœì í™” ì„¤ì •"""
        if self.current_mode == ComputeMode.GPU_CUDA:
            try:
                import torch
                torch.backends.cudnn.benchmark = True
                torch.backends.cudnn.deterministic = False
            except:
                pass
    
    def get_recommended_batch_size(self, model_type: str = "default") -> int:
        """ëª¨ë¸ íƒ€ì…ì— ë”°ë¥¸ ê¶Œì¥ ë°°ì¹˜ í¬ê¸° ë°˜í™˜"""
        
        batch_sizes = {
            'whisper': {
                ComputeMode.CPU_ONLY: 1,
                ComputeMode.GPU_CUDA: 4,
                ComputeMode.GPU_MPS: 2
            },
            'transformers': {
                ComputeMode.CPU_ONLY: 1,
                ComputeMode.GPU_CUDA: 8,
                ComputeMode.GPU_MPS: 4
            },
            'default': {
                ComputeMode.CPU_ONLY: 1,
                ComputeMode.GPU_CUDA: 2,
                ComputeMode.GPU_MPS: 2
            }
        }
        
        return batch_sizes.get(model_type, batch_sizes['default']).get(
            self.current_mode, 1
        )

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
_compute_config = None

def get_compute_config() -> ComputeConfig:
    """ì „ì—­ ComputeConfig ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _compute_config
    if _compute_config is None:
        _compute_config = ComputeConfig()
    return _compute_config

def setup_compute_environment(mode: str = None) -> ComputeConfig:
    """ì»´í“¨íŒ… í™˜ê²½ ì„¤ì • - ë©”ì¸ ì§„ì…ì 
    
    ê¸°ì¡´ ì½”ë“œì—ì„œ ì´ë ‡ê²Œ ì‚¬ìš©:
    ```python
    from config.compute_config import setup_compute_environment
    config = setup_compute_environment()  # ë˜ëŠ” setup_compute_environment('cpu')
    ```
    
    Args:
        mode: ê°•ì œ ì„¤ì •í•  ëª¨ë“œ ('cpu', 'cuda', 'auto' ë“±)
        
    Returns:
        ComputeConfig: ì„¤ì •ëœ ì»´í“¨íŒ… ì„¤ì • ê°ì²´
    """
    
    config = get_compute_config()
    
    if mode:
        try:
            compute_mode = ComputeMode(mode.lower())
            config.set_mode(compute_mode)
        except ValueError:
            config.logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” ì»´í“¨íŒ… ëª¨ë“œ: {mode}, ìë™ ëª¨ë“œ ì‚¬ìš©")
    
    return config

def force_cpu_mode():
    """CPU ì „ìš© ëª¨ë“œ ê°•ì œ ì„¤ì • (ê¸°ì¡´ ì½”ë“œ í˜¸í™˜ìš©)
    
    ê¸°ì¡´ ì½”ë“œì—ì„œ ì´ë ‡ê²Œ ì‚¬ìš©:
    ```python
    from config.compute_config import force_cpu_mode
    force_cpu_mode()
    # ì´í›„ os.environ['CUDA_VISIBLE_DEVICES'] = '' ì™€ ë™ì¼ íš¨ê³¼
    ```
    """
    config = get_compute_config()
    config.set_mode(ComputeMode.CPU_ONLY)

def get_device_string() -> str:
    """í˜„ì¬ ì„¤ì •ëœ device ë¬¸ìì—´ ë°˜í™˜ (ê¸°ì¡´ ì½”ë“œ í˜¸í™˜ìš©)"""
    return get_compute_config().get_device_string()

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì • ìë™ ì ìš©
_config = get_compute_config()  # ëª¨ë“ˆ import ì‹œ ìë™ ì´ˆê¸°í™”