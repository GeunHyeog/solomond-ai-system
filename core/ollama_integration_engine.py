#!/usr/bin/env python3
"""
Ollama í†µí•© ì—”ì§„
ë¡œì»¬ LLMì„ í™œìš©í•œ í•œêµ­ì–´ íŠ¹í™” ë¶„ì„ ë° ê°œì¸ì •ë³´ ë³´í˜¸
"""

import asyncio
import aiohttp
import json
from typing import Dict, List, Any, Optional
from datetime import datetime
from utils.logger import get_logger

class OllamaIntegrationEngine:
    """Ollama ë¡œì»¬ LLM í†µí•© ì—”ì§„"""
    
    def __init__(self, base_url: str = "http://localhost:11434"):
        self.base_url = base_url
        self.logger = self._setup_logging()
        
        # ì‚¬ìš©í•  ëª¨ë¸ë“¤ - 2025ë…„ ìµœì‹  3ì„¸ëŒ€ ëª¨ë¸ë¡œ ì—…ë°ì´íŠ¸
        self.models = {
            "korean_chat": "qwen3:8b",          # ğŸ¥‡ Qwen3 - í•œêµ­ì–´ ìµœê°• 3ì„¸ëŒ€
            "emotion_analysis": "gemma3:4b",     # ğŸ¥ˆ GEMMA3 4B - ë¹ ë¥¸ ê°ì • ë¶„ì„
            "structured_output": "gemma3:27b",   # ğŸ¥‰ GEMMA3 27B - ìµœê³  ì„±ëŠ¥ êµ¬ì¡°í™”
            "high_quality": "gemma3:27b",        # ğŸ† ìµœê³  í’ˆì§ˆ ë¶„ì„ìš©
            "fast_response": "gemma3:4b",        # âš¡ ë¹ ë¥¸ ì‘ë‹µìš©
            "backup_model": "gemma2:9b"          # ğŸ”„ ë°±ì—…ìš©
        }
        
        # í•œêµ­ì–´ íŠ¹í™” í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        self.korean_prompts = {
            "emotion_analysis": """
GEMMA3 ê³ ê¸‰ ê°ì • ë¶„ì„: ë‹¤ìŒ í•œêµ­ì–´ ì£¼ì–¼ë¦¬ ìƒë‹´ ëŒ€í™”ë¥¼ ì •ë°€ ë¶„ì„í•´ì£¼ì„¸ìš”:

ëŒ€í™” ë‚´ìš©:
{conversation}

GEMMA3 ë¶„ì„ ìš”ì²­:
1. ê°ì • ìƒíƒœ (ê¸ì •/ë¶€ì •/ì¤‘ë¦½/ê´€ì‹¬/ë§ì„¤ì„/í¥ë¯¸/ìš°ë ¤) - ì •í™•ë„ 95%+
2. êµ¬ë§¤ ì˜ë„ ìˆ˜ì¤€ (1-10ì ) + ê·¼ê±°
3. í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ (ìš°ì„ ìˆœìœ„ë³„)
4. ê³ ê° ìœ í˜• ë¶„ë¥˜ (ì‹ ì¤‘í˜•/ì ê·¹í˜•/ê°€ê²©ë¯¼ê°í˜•/í’ˆì§ˆì¤‘ì‹œí˜•)
5. ìµœì  ëŒ€ì‘ ì „ëµ

JSON í˜•íƒœë¡œ êµ¬ì¡°í™”í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”.
""",
            "conversation_summary": """
Qwen3 í•œêµ­ì–´ ì „ë¬¸ ë¶„ì„: ë‹¤ìŒ ì£¼ì–¼ë¦¬ ìƒë‹´ ëŒ€í™”ë¥¼ í•œêµ­ ë¬¸í™”ì— ë§ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”:

{conversation}

Qwen3 ìš”ì•½ ìš”êµ¬ì‚¬í•­:
- ê³ ê° í•µì‹¬ ìš”êµ¬ì‚¬í•­ (ëª…í™•í•œ ë‹ˆì¦ˆ)
- ë…¼ì˜ëœ ì œí’ˆ/ì„œë¹„ìŠ¤ (êµ¬ì²´ì  ì–¸ê¸‰ì‚¬í•­)
- ê°€ê²©ëŒ€ ë° ì˜ˆì‚° ìƒí™© (ë¯¼ê°ë„ í¬í•¨)
- ì˜ì‚¬ê²°ì • ë‹¨ê³„ (ê³ ë¯¼/í™•ì‹ /ë³´ë¥˜ ë“±)
- ë¬¸í™”ì  ë§¥ë½ ê³ ë ¤ (í•œêµ­ ì£¼ì–¼ë¦¬ ì„ í˜¸ë„)
- ì œì•ˆí•  ë‹¤ìŒ ë‹¨ê³„

í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê³  ì „ë¬¸ì ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.
""",
            "recommendation_generation": """
ë‹¤ìŒ ìƒí™©ì—ì„œ ê³ ê°ì—ê²Œ ìµœì ì˜ ì œì•ˆì„ ìƒì„±í•´ì£¼ì„¸ìš”:

ê³ ê° ì •ë³´: {customer_info}
ìƒë‹´ ë‚´ìš©: {conversation_summary}
ì‹œì¥ ì •ë³´: {market_data}

ë‹¤ìŒ í˜•íƒœë¡œ ì œì•ˆí•´ì£¼ì„¸ìš”:
1. ë§ì¶¤ ì œí’ˆ ì¶”ì²œ (3ê°œ)
2. ê°€ê²© í˜œíƒ ì œì•ˆ
3. ì¶”ê°€ ì„œë¹„ìŠ¤ ì˜µì…˜
4. ê²°ì •ì„ ìœ„í•œ ì¶”ê°€ ì •ë³´

ì¹œê·¼í•˜ê³  ì „ë¬¸ì ì¸ í†¤ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""
        }
        
        self.logger.info("ğŸ¦™ Ollama í†µí•© ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        return get_logger(f'{__name__}.OllamaIntegrationEngine')
    
    async def check_ollama_availability(self) -> Dict[str, Any]:
        """Ollama ì„œë²„ ë° ëª¨ë¸ ê°€ìš©ì„± í™•ì¸"""
        
        status = {
            "server_available": False,
            "available_models": [],
            "missing_models": [],
            "recommendations": []
        }
        
        try:
            # Ollama ì„œë²„ ì—°ê²° í™•ì¸
            async with aiohttp.ClientSession() as session:
                async with session.get(f"{self.base_url}/api/tags") as response:
                    if response.status == 200:
                        status["server_available"] = True
                        data = await response.json()
                        installed_models = [model["name"] for model in data.get("models", [])]
                        status["available_models"] = installed_models
                        
                        # í•„ìš”í•œ ëª¨ë¸ í™•ì¸
                        for purpose, model_name in self.models.items():
                            if model_name not in installed_models:
                                status["missing_models"].append({
                                    "purpose": purpose,
                                    "model": model_name,
                                    "install_command": f"ollama pull {model_name}"
                                })
                        
                        # ì„¤ì¹˜ ê¶Œì¥ì‚¬í•­
                        if status["missing_models"]:
                            status["recommendations"] = [
                                "ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ í•„ìš”í•œ ëª¨ë¸ë“¤ì„ ì„¤ì¹˜í•˜ì„¸ìš”:",
                                *[m["install_command"] for m in status["missing_models"]]
                            ]
                        else:
                            status["recommendations"] = ["ëª¨ë“  í•„ìš”í•œ ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤!"]
                    
        except Exception as e:
            self.logger.error(f"âŒ Ollama ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {str(e)}")
            status["recommendations"] = [
                "Ollamaë¥¼ ì„¤ì¹˜í•˜ê³  ì‹¤í–‰í•˜ì„¸ìš”:",
                "1. https://ollama.ai/ ì—ì„œ ë‹¤ìš´ë¡œë“œ",
                "2. ì„¤ì¹˜ í›„ 'ollama serve' ì‹¤í–‰",
                "3. í•„ìš”í•œ ëª¨ë¸ë“¤ ì„¤ì¹˜"
            ]
        
        return status
    
    async def analyze_korean_conversation(self, conversation: str, context: Dict[str, Any] = None) -> Dict[str, Any]:
        """í•œêµ­ì–´ ëŒ€í™” ë¶„ì„ - Ollama í™œìš©"""
        
        try:
            # ê°ì • ë¶„ì„
            emotion_result = await self._call_ollama_model(
                "emotion_analysis",
                self.korean_prompts["emotion_analysis"].format(conversation=conversation)
            )
            
            # ëŒ€í™” ìš”ì•½
            summary_result = await self._call_ollama_model(
                "korean_chat", 
                self.korean_prompts["conversation_summary"].format(conversation=conversation)
            )
            
            # êµ¬ì¡°í™”ëœ ê²°ê³¼ ìƒì„±
            structured_result = await self._generate_structured_analysis(
                emotion_result, summary_result, context
            )
            
            return {
                "timestamp": datetime.now().isoformat(),
                "emotion_analysis": emotion_result,
                "conversation_summary": summary_result,
                "structured_insights": structured_result,
                "processing_method": "ollama_korean_optimized"
            }
            
        except Exception as e:
            self.logger.error(f"âŒ í•œêµ­ì–´ ëŒ€í™” ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            return self._create_fallback_analysis(conversation)
    
    async def generate_personalized_recommendations(self, 
                                                  customer_info: Dict,
                                                  conversation_summary: str,
                                                  market_data: Dict = None) -> Dict[str, Any]:
        """ê°œì¸í™”ëœ ì¶”ì²œ ìƒì„±"""
        
        try:
            prompt = self.korean_prompts["recommendation_generation"].format(
                customer_info=json.dumps(customer_info, ensure_ascii=False),
                conversation_summary=conversation_summary,
                market_data=json.dumps(market_data or {}, ensure_ascii=False)
            )
            
            recommendation = await self._call_ollama_model("korean_chat", prompt)
            
            return {
                "timestamp": datetime.now().isoformat(),
                "personalized_recommendations": recommendation,
                "confidence": "high",
                "method": "ollama_korean_llm"
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ê°œì¸í™” ì¶”ì²œ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return {"error": str(e), "fallback": True}
    
    async def _call_ollama_model(self, model_purpose: str, prompt: str) -> str:
        """Ollama ëª¨ë¸ í˜¸ì¶œ"""
        
        model_name = self.models.get(model_purpose, "llama3.1:8b")
        
        try:
            async with aiohttp.ClientSession() as session:
                payload = {
                    "model": model_name,
                    "prompt": prompt,
                    "stream": False
                }
                
                async with session.post(
                    f"{self.base_url}/api/generate",
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=60)
                ) as response:
                    
                    if response.status == 200:
                        result = await response.json()
                        return result.get("response", "ì‘ë‹µì„ ë°›ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    else:
                        error_text = await response.text()
                        raise Exception(f"Ollama API ì˜¤ë¥˜: {response.status} - {error_text}")
                        
        except Exception as e:
            self.logger.error(f"âŒ Ollama ëª¨ë¸ í˜¸ì¶œ ì‹¤íŒ¨ ({model_name}): {str(e)}")
            raise
    
    async def _generate_structured_analysis(self, emotion_result: str, summary_result: str, context: Dict) -> Dict[str, Any]:
        """êµ¬ì¡°í™”ëœ ë¶„ì„ ê²°ê³¼ ìƒì„±"""
        
        try:
            # ê°ì • ë¶„ì„ ê²°ê³¼ íŒŒì‹± ì‹œë„
            emotion_data = self._parse_emotion_analysis(emotion_result)
            
            # ìš”ì•½ ê²°ê³¼ êµ¬ì¡°í™”
            summary_data = self._parse_conversation_summary(summary_result)
            
            # ì»¨í…ìŠ¤íŠ¸ì™€ ê²°í•©
            structured_insights = {
                "customer_emotions": emotion_data,
                "conversation_insights": summary_data,
                "context_integration": self._integrate_context(context),
                "ai_confidence": self._calculate_confidence(emotion_data, summary_data)
            }
            
            return structured_insights
            
        except Exception as e:
            self.logger.error(f"âŒ êµ¬ì¡°í™”ëœ ë¶„ì„ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return {"error": str(e), "raw_emotion": emotion_result, "raw_summary": summary_result}
    
    def _parse_emotion_analysis(self, emotion_text: str) -> Dict[str, Any]:
        """ê°ì • ë¶„ì„ ê²°ê³¼ íŒŒì‹±"""
        
        # JSON í˜•íƒœë¡œ ì‘ë‹µì´ ì˜¨ ê²½ìš° íŒŒì‹± ì‹œë„
        try:
            return json.loads(emotion_text)
        except:
            # í…ìŠ¤íŠ¸ í˜•íƒœì¸ ê²½ìš° í‚¤ì›Œë“œ ê¸°ë°˜ íŒŒì‹±
            emotions = {
                "positive_indicators": [],
                "negative_indicators": [],
                "purchase_intent": 5  # ê¸°ë³¸ê°’
            }
            
            positive_words = ["ì¢‹ë‹¤", "ì˜ˆì˜ë‹¤", "ë§ˆìŒì—", "ì›í•œë‹¤", "ê´€ì‹¬"]
            negative_words = ["ë¹„ì‹¸ë‹¤", "ê³ ë¯¼", "ë§ì„¤", "ì–´ë µë‹¤"]
            
            for word in positive_words:
                if word in emotion_text:
                    emotions["positive_indicators"].append(word)
            
            for word in negative_words:
                if word in emotion_text:
                    emotions["negative_indicators"].append(word)
            
            # êµ¬ë§¤ ì˜ë„ ì ìˆ˜ ê³„ì‚°
            positive_score = len(emotions["positive_indicators"])
            negative_score = len(emotions["negative_indicators"])
            emotions["purchase_intent"] = min(10, max(1, 5 + positive_score - negative_score))
            
            return emotions
    
    def _parse_conversation_summary(self, summary_text: str) -> Dict[str, Any]:
        """ëŒ€í™” ìš”ì•½ ê²°ê³¼ íŒŒì‹±"""
        
        return {
            "summary": summary_text,
            "key_points": self._extract_key_points(summary_text),
            "action_items": self._extract_action_items(summary_text)
        }
    
    def _extract_key_points(self, text: str) -> List[str]:
        """í•µì‹¬ í¬ì¸íŠ¸ ì¶”ì¶œ"""
        
        key_indicators = ["ìš”êµ¬ì‚¬í•­", "ì œí’ˆ", "ê°€ê²©", "ê²°ì •", "ê´€ì‹¬"]
        key_points = []
        
        sentences = text.split('.')
        for sentence in sentences:
            if any(indicator in sentence for indicator in key_indicators):
                key_points.append(sentence.strip())
        
        return key_points[:5]  # ìƒìœ„ 5ê°œ
    
    def _extract_action_items(self, text: str) -> List[str]:
        """ì•¡ì…˜ ì•„ì´í…œ ì¶”ì¶œ"""
        
        action_indicators = ["ë‹¤ìŒ", "í•„ìš”", "ì¤€ë¹„", "ì œì•ˆ", "ì¶”ì²œ"]
        action_items = []
        
        sentences = text.split('.')
        for sentence in sentences:
            if any(indicator in sentence for indicator in action_indicators):
                action_items.append(sentence.strip())
        
        return action_items[:3]  # ìƒìœ„ 3ê°œ
    
    def _integrate_context(self, context: Dict) -> Dict[str, Any]:
        """ì»¨í…ìŠ¤íŠ¸ í†µí•©"""
        
        if not context:
            return {"status": "no_context"}
        
        return {
            "participants": context.get('participants', ''),
            "situation": context.get('situation', ''),
            "keywords": context.get('keywords', ''),
            "integration_score": 0.8  # ê¸°ë³¸ í†µí•© ì ìˆ˜
        }
    
    def _calculate_confidence(self, emotion_data: Dict, summary_data: Dict) -> float:
        """AI ì‹ ë¢°ë„ ê³„ì‚°"""
        
        confidence_factors = [
            len(emotion_data.get('positive_indicators', [])) > 0,
            len(summary_data.get('key_points', [])) > 2,
            emotion_data.get('purchase_intent', 0) > 0
        ]
        
        return sum(confidence_factors) / len(confidence_factors)
    
    def _create_fallback_analysis(self, conversation: str) -> Dict[str, Any]:
        """Ollama ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë¶„ì„"""
        
        return {
            "timestamp": datetime.now().isoformat(),
            "status": "ollama_unavailable",
            "basic_analysis": {
                "conversation_length": len(conversation),
                "has_korean": any(ord(char) > 127 for char in conversation),
                "fallback_summary": "Ollama ì—°ê²° ì‹¤íŒ¨ë¡œ ê¸°ë³¸ ë¶„ì„ ì œê³µ"
            },
            "recommendations": [
                "Ollama ì„œë²„ ìƒíƒœ í™•ì¸ í•„ìš”",
                "í•„ìš”í•œ ëª¨ë¸ ì„¤ì¹˜ í™•ì¸ í•„ìš”"
            ]
        }

# ì‚¬ìš© ì˜ˆì‹œ ë° í…ŒìŠ¤íŠ¸
async def test_ollama_integration():
    """Ollama í†µí•© í…ŒìŠ¤íŠ¸"""
    
    engine = OllamaIntegrationEngine()
    
    # 1. ê°€ìš©ì„± í™•ì¸
    print("=== Ollama ê°€ìš©ì„± í™•ì¸ ===")
    status = await engine.check_ollama_availability()
    print(json.dumps(status, ensure_ascii=False, indent=2))
    
    # 2. í•œêµ­ì–´ ëŒ€í™” ë¶„ì„ (ê°€ìš©í•  ê²½ìš°)
    if status["server_available"]:
        test_conversation = """
        ê³ ê°: ê²°í˜¼ë°˜ì§€ ì¢€ ë³´ê³  ì‹¶ì–´ìš”. ì˜ˆì‚°ì€ 200ë§Œì› ì •ë„ ìƒê°í•˜ê³  ìˆì–´ìš”.
        ìƒë‹´ì‚¬: ë„¤, ì¢‹ì€ ì„ íƒì´ì‹œë„¤ìš”. ì–´ë–¤ ìŠ¤íƒ€ì¼ì„ ì„ í˜¸í•˜ì‹œë‚˜ìš”?
        ê³ ê°: ë„ˆë¬´ í™”ë ¤í•˜ì§€ ì•Šê³  ì‹¬í”Œí•œ ê²Œ ì¢‹ì„ ê²ƒ ê°™ì•„ìš”. ë‹¤ì´ì•„ëª¬ë“œëŠ” ê¼­ í•„ìš”í•œê°€ìš”?
        ìƒë‹´ì‚¬: ë‹¤ì´ì•„ëª¬ë“œ ì—†ì´ë„ ì•„ë¦„ë‹¤ìš´ ë””ìì¸ë“¤ì´ ë§ì´ ìˆìŠµë‹ˆë‹¤. ë³´ì—¬ë“œë¦´ê¹Œìš”?
        """
        
        print("\n=== í•œêµ­ì–´ ëŒ€í™” ë¶„ì„ ===")
        result = await engine.analyze_korean_conversation(test_conversation)
        print(json.dumps(result, ensure_ascii=False, indent=2))

if __name__ == "__main__":
    asyncio.run(test_ollama_integration())