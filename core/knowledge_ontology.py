#!/usr/bin/env python3
"""
ğŸ§  SOLOMOND AI ì˜¨í†¨ë¡œì§€ ì§€ì‹ ë² ì´ìŠ¤ ì‹œìŠ¤í…œ
Knowledge Ontology System for Domain-Specific Intelligence

ğŸ¯ ì£¼ìš” ê¸°ëŠ¥:
1. ê³„ì¸µì  ì§€ì‹ êµ¬ì¡° - ë„ë©”ì¸ë³„ ì „ë¬¸ ì§€ì‹ ë¶„ë¥˜
2. ì‹œë§¨í‹± ê´€ê³„ ë§¤í•‘ - ê°œë… ê°„ ì˜ë¯¸ì  ì—°ê´€ì„±
3. ì¸ì‚¬ì´íŠ¸ ì¶”ë¡  ì—”ì§„ - ì§€ì‹ ê¸°ë°˜ ìë™ ì¶”ë¡ 
4. ë™ì  í•™ìŠµ ì‹œìŠ¤í…œ - ìƒˆë¡œìš´ ì§€ì‹ ìë™ í†µí•©
5. ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ê²€ìƒ‰ - ìƒí™© ë§ì¶¤ ì§€ì‹ ì œê³µ
"""

import json
import logging
from typing import Dict, List, Any, Optional, Set, Tuple
from dataclasses import dataclass, asdict
from pathlib import Path
from datetime import datetime
import networkx as nx
import numpy as np
from collections import defaultdict, Counter
import re

# ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class KnowledgeNode:
    """ì§€ì‹ ë…¸ë“œ êµ¬ì¡°"""
    id: str
    concept: str
    domain: str
    category: str
    description: str
    confidence: float
    evidence: List[str]
    metadata: Dict[str, Any]
    created_at: str
    updated_at: str

@dataclass
class SemanticRelation:
    """ì‹œë§¨í‹± ê´€ê³„ êµ¬ì¡°"""
    source_id: str
    target_id: str
    relation_type: str
    strength: float
    context: str
    evidence: List[str]
    created_at: str

class KnowledgeOntology:
    """ì˜¨í†¨ë¡œì§€ ì§€ì‹ ë² ì´ìŠ¤ í•µì‹¬ ì—”ì§„"""
    
    def __init__(self, domain: str = "conference_analysis"):
        self.domain = domain
        self.knowledge_graph = nx.DiGraph()
        self.nodes: Dict[str, KnowledgeNode] = {}
        self.relations: List[SemanticRelation] = []
        self.domain_schemas = self._initialize_domain_schemas()
        
        # ì¶”ë¡  ê·œì¹™
        self.inference_rules = self._initialize_inference_rules()
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        self.stats = {
            'total_nodes': 0,
            'total_relations': 0,
            'domains_covered': 0,
            'inference_accuracy': 0.0,
            'last_updated': None
        }
        
        logger.info(f"ğŸ§  ì˜¨í†¨ë¡œì§€ ì‹œìŠ¤í…œ ì´ˆê¸°í™” - ë„ë©”ì¸: {domain}")
        
    def _initialize_domain_schemas(self) -> Dict[str, Dict]:
        """ë„ë©”ì¸ë³„ ìŠ¤í‚¤ë§ˆ ì´ˆê¸°í™”"""
        schemas = {
            "conference_analysis": {
                "categories": [
                    "presentation_content",
                    "speaker_analysis", 
                    "audience_reaction",
                    "technical_concepts",
                    "business_insights",
                    "industry_trends"
                ],
                "relation_types": [
                    "causes", "influences", "relates_to", "precedes", 
                    "contradicts", "supports", "exemplifies", "generalizes"
                ],
                "quality_factors": [
                    "audio_clarity", "visual_quality", "content_relevance",
                    "speaker_expertise", "audience_engagement"
                ]
            },
            "jewelry_analysis": {
                "categories": [
                    "gemstone_properties",
                    "design_elements",
                    "market_trends", 
                    "manufacturing_process",
                    "cultural_significance",
                    "pricing_factors"
                ],
                "relation_types": [
                    "material_of", "style_influences", "market_affects",
                    "culturally_related", "price_correlates"
                ]
            },
            "business_intelligence": {
                "categories": [
                    "market_analysis",
                    "customer_behavior",
                    "competitive_landscape",
                    "financial_metrics",
                    "strategic_initiatives"
                ],
                "relation_types": [
                    "drives", "impacts", "competes_with", "depends_on",
                    "enables", "disrupts"
                ]
            }
        }
        
        return schemas
    
    def _initialize_inference_rules(self) -> List[Dict]:
        """ì¶”ë¡  ê·œì¹™ ì´ˆê¸°í™”"""
        rules = [
            {
                "name": "transitive_relation",
                "pattern": "A -> B, B -> C => A -> C",
                "relation_types": ["causes", "influences", "precedes"],
                "confidence_decay": 0.8
            },
            {
                "name": "contradiction_detection",
                "pattern": "A contradicts B, B supports C => A likely contradicts C",
                "confidence_threshold": 0.7
            },
            {
                "name": "evidence_aggregation",
                "pattern": "Multiple evidence points => Higher confidence",
                "weight_function": "logarithmic"
            },
            {
                "name": "temporal_decay",
                "pattern": "Older evidence => Lower weight",
                "decay_rate": 0.95  # per day
            },
            {
                "name": "domain_expertise",
                "pattern": "Expert source => Higher confidence",
                "expertise_multiplier": 1.5
            }
        ]
        
        return rules
    
    def add_knowledge_from_analysis(self, analysis_results: Dict[str, Any]) -> None:
        """ë¶„ì„ ê²°ê³¼ì—ì„œ ì§€ì‹ ì¶”ì¶œ ë° ì¶”ê°€"""
        logger.info("ğŸ” ë¶„ì„ ê²°ê³¼ì—ì„œ ì§€ì‹ ì¶”ì¶œ ì¤‘...")
        
        # ë©€í‹°ëª¨ë‹¬ ê²°ê³¼ì—ì„œ ì§€ì‹ ì¶”ì¶œ
        if "multimodal_results" in analysis_results:
            self._extract_from_multimodal_results(analysis_results["multimodal_results"])
        
        # í¬ë¡œìŠ¤ ëª¨ë‹¬ ì¸ì‚¬ì´íŠ¸ì—ì„œ ì§€ì‹ ì¶”ì¶œ
        if "cross_modal_insights" in analysis_results:
            self._extract_from_cross_modal_insights(analysis_results["cross_modal_insights"])
        
        # ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ì—ì„œ ì§€ì‹ ì¶”ì¶œ (í•„ìš”ì‹œ)
        if "traditional_results" in analysis_results:
            self._extract_from_traditional_analysis(analysis_results["traditional_results"])
        
        # ì§€ì‹ ê·¸ë˜í”„ ì—…ë°ì´íŠ¸
        self._update_knowledge_graph()
        
        self.stats['last_updated'] = datetime.now().isoformat()
        logger.info(f"âœ… ì§€ì‹ ì¶”ì¶œ ì™„ë£Œ - {len(self.nodes)}ê°œ ë…¸ë“œ, {len(self.relations)}ê°œ ê´€ê³„")
    
    def _extract_from_multimodal_results(self, multimodal_results: List[Dict]) -> None:
        """ë©€í‹°ëª¨ë‹¬ ê²°ê³¼ì—ì„œ ì§€ì‹ ì¶”ì¶œ"""
        for result in multimodal_results:
            # íŒŒì¼ íƒ€ì…ë³„ ì§€ì‹ ì¶”ì¶œ
            if result.get("file_type") == "audio":
                self._extract_audio_knowledge(result)
            elif result.get("file_type") == "image": 
                self._extract_image_knowledge(result)
            elif result.get("file_type") == "text":
                self._extract_text_knowledge(result)
    
    def _extract_audio_knowledge(self, audio_result: Dict) -> None:
        """ì˜¤ë””ì˜¤ ë¶„ì„ ê²°ê³¼ì—ì„œ ì§€ì‹ ì¶”ì¶œ"""
        content = audio_result.get("content", "")
        confidence = audio_result.get("confidence", 0.0)
        
        if not content or confidence < 0.5:
            return
        
        # í•µì‹¬ ê°œë… ì¶”ì¶œ
        concepts = self._extract_concepts_from_text(content)
        
        for concept in concepts:
            node_id = f"audio_concept_{concept}_{datetime.now().timestamp()}"
            
            knowledge_node = KnowledgeNode(
                id=node_id,
                concept=concept,
                domain=self.domain,
                category="presentation_content",
                description=f"ì˜¤ë””ì˜¤ì—ì„œ ì¶”ì¶œëœ ê°œë…: {concept}",
                confidence=confidence * 0.9,  # ì˜¤ë””ì˜¤ëŠ” ì•½ê°„ ë‚®ì€ ì‹ ë¢°ë„
                evidence=[content[:200]],
                metadata={
                    "source_type": "audio",
                    "file_path": audio_result.get("file_path"),
                    "extraction_method": "speech_to_text"
                },
                created_at=datetime.now().isoformat(),
                updated_at=datetime.now().isoformat()
            )
            
            self.nodes[node_id] = knowledge_node
    
    def _extract_image_knowledge(self, image_result: Dict) -> None:
        """ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼ì—ì„œ ì§€ì‹ ì¶”ì¶œ"""
        content = image_result.get("content", "")
        confidence = image_result.get("confidence", 0.0)
        
        if not content or confidence < 0.4:
            return
        
        # ì‹œê°ì  ìš”ì†Œ ì¶”ì¶œ
        visual_elements = self._extract_visual_concepts(content)
        
        for element in visual_elements:
            node_id = f"visual_element_{element}_{datetime.now().timestamp()}"
            
            knowledge_node = KnowledgeNode(
                id=node_id,
                concept=element,
                domain=self.domain,
                category="technical_concepts",
                description=f"ì´ë¯¸ì§€ì—ì„œ ì¶”ì¶œëœ ì‹œê°ì  ìš”ì†Œ: {element}",
                confidence=confidence * 1.1,  # ì‹œê°ì  ì •ë³´ëŠ” ë†’ì€ ì‹ ë¢°ë„
                evidence=[content],
                metadata={
                    "source_type": "image",
                    "file_path": image_result.get("file_path"),
                    "extraction_method": "optical_character_recognition"
                },
                created_at=datetime.now().isoformat(),
                updated_at=datetime.now().isoformat()
            )
            
            self.nodes[node_id] = knowledge_node
    
    def _extract_text_knowledge(self, text_result: Dict) -> None:
        """í…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼ì—ì„œ ì§€ì‹ ì¶”ì¶œ"""
        content = text_result.get("content", "")
        
        if not content:
            return
        
        # êµ¬ì¡°í™”ëœ ì§€ì‹ ì¶”ì¶œ
        structured_knowledge = self._extract_structured_knowledge(content)
        
        for knowledge in structured_knowledge:
            node_id = f"text_knowledge_{knowledge['concept']}_{datetime.now().timestamp()}"
            
            knowledge_node = KnowledgeNode(
                id=node_id,
                concept=knowledge['concept'],
                domain=self.domain,
                category=knowledge.get('category', 'business_insights'),
                description=knowledge['description'],
                confidence=1.0,  # í…ìŠ¤íŠ¸ëŠ” ìµœê³  ì‹ ë¢°ë„
                evidence=[content[:300]],
                metadata={
                    "source_type": "text",
                    "file_path": text_result.get("file_path"),
                    "extraction_method": "natural_language_processing"
                },
                created_at=datetime.now().isoformat(),
                updated_at=datetime.now().isoformat()
            )
            
            self.nodes[node_id] = knowledge_node
    
    def _extract_from_cross_modal_insights(self, insights: Dict) -> None:
        """í¬ë¡œìŠ¤ ëª¨ë‹¬ ì¸ì‚¬ì´íŠ¸ì—ì„œ ê´€ê³„ì„± ì§€ì‹ ì¶”ì¶œ"""
        # ê³ ìƒê´€ ìŒì—ì„œ ê´€ê³„ ì¶”ì¶œ
        for pair in insights.get("high_correlation_pairs", []):
            relation = SemanticRelation(
                source_id=f"file_{pair['primary_file']}",
                target_id=f"files_{len(pair['related_files'])}",
                relation_type="strongly_correlates",
                strength=pair['correlation_score'],
                context="cross_modal_analysis",
                evidence=[f"ìƒê´€ê³„ìˆ˜: {pair['correlation_score']:.3f}"],
                created_at=datetime.now().isoformat()
            )
            
            self.relations.append(relation)
        
        # ì£¼ìš” í…Œë§ˆì—ì„œ ë„ë©”ì¸ ì§€ì‹ ì¶”ì¶œ
        for theme in insights.get("dominant_themes", []):
            node_id = f"theme_{theme}_{datetime.now().timestamp()}"
            
            knowledge_node = KnowledgeNode(
                id=node_id,
                concept=theme,
                domain=self.domain,
                category="industry_trends",
                description=f"í¬ë¡œìŠ¤ ëª¨ë‹¬ ë¶„ì„ì—ì„œ ì‹ë³„ëœ ì£¼ìš” í…Œë§ˆ: {theme}",
                confidence=0.8,
                evidence=["ë©€í‹°ëª¨ë‹¬ ë¶„ì„ ê²°ê³¼"],
                metadata={
                    "source_type": "cross_modal_analysis",
                    "extraction_method": "theme_analysis"
                },
                created_at=datetime.now().isoformat(),
                updated_at=datetime.now().isoformat()
            )
            
            self.nodes[node_id] = knowledge_node
    
    def _extract_concepts_from_text(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ ê°œë… ì¶”ì¶œ"""
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ NER/NLP ì‚¬ìš©)
        import re
        
        # í•œêµ­ì–´ ëª…ì‚¬êµ¬ ì¶”ì¶œ
        korean_concepts = re.findall(r'[ê°€-í£]{3,8}', text)
        
        # ì˜ì–´ ê¸°ìˆ  ìš©ì–´ ì¶”ì¶œ
        english_concepts = re.findall(r'\b[A-Z][a-zA-Z]{2,15}\b', text)
        
        # ìˆ«ì + ë‹¨ìœ„ íŒ¨í„´
        numerical_concepts = re.findall(r'\d+\s*[ê°€-í£]{1,3}|\d+%|\d+\$', text)
        
        all_concepts = korean_concepts + english_concepts + numerical_concepts
        
        # ë¹ˆë„ ê¸°ë°˜ í•„í„°ë§
        from collections import Counter
        concept_freq = Counter(all_concepts)
        
        # ìµœì†Œ 2íšŒ ì´ìƒ ì–¸ê¸‰ëœ ê°œë…ë§Œ ì„ íƒ
        filtered_concepts = [concept for concept, freq in concept_freq.items() if freq >= 2]
        
        return filtered_concepts[:20]  # ìƒìœ„ 20ê°œë§Œ
    
    def _extract_visual_concepts(self, ocr_text: str) -> List[str]:
        """OCR í…ìŠ¤íŠ¸ì—ì„œ ì‹œê°ì  ê°œë… ì¶”ì¶œ"""
        visual_indicators = [
            "ì°¨íŠ¸", "ê·¸ë˜í”„", "í‘œ", "ë„í‘œ", "ìŠ¬ë¼ì´ë“œ", "ì´ë¯¸ì§€", 
            "ì‚¬ì§„", "ê·¸ë¦¼", "ë„ì‹", "ë‹¤ì´ì–´ê·¸ë¨", "Chart", "Graph", 
            "Table", "Figure", "Image", "Slide"
        ]
        
        found_elements = []
        for indicator in visual_indicators:
            if indicator in ocr_text:
                found_elements.append(indicator)
        
        # OCR í…ìŠ¤íŠ¸ì˜ í•µì‹¬ í‚¤ì›Œë“œë„ ì¶”ì¶œ
        concepts = self._extract_concepts_from_text(ocr_text)
        found_elements.extend(concepts[:10])  # ìƒìœ„ 10ê°œ ì¶”ê°€
        
        return list(set(found_elements))
    
    def _extract_structured_knowledge(self, text: str) -> List[Dict]:
        """êµ¬ì¡°í™”ëœ ì§€ì‹ ì¶”ì¶œ"""
        knowledge_items = []
        
        # ì •ì˜ íŒ¨í„´ ì¸ì‹ ("XëŠ” Yì´ë‹¤", "Xë€ Yë¥¼ ë§í•œë‹¤")
        definition_patterns = [
            r'([ê°€-í£A-Za-z0-9\s]{2,20})ëŠ”\s+([ê°€-í£A-Za-z0-9\s]{5,50})ì´ë‹¤',
            r'([ê°€-í£A-Za-z0-9\s]{2,20})ë€\s+([ê°€-í£A-Za-z0-9\s]{5,50})ë¥¼?\s*ë§í•œë‹¤',
            r'([ê°€-í£A-Za-z0-9\s]{2,20})\s*:\s*([ê°€-í£A-Za-z0-9\s]{5,50})'
        ]
        
        for pattern in definition_patterns:
            matches = re.findall(pattern, text)
            for concept, description in matches:
                knowledge_items.append({
                    'concept': concept.strip(),
                    'description': description.strip(),
                    'category': 'technical_concepts'
                })
        
        # ì¸ê³¼ê´€ê³„ íŒ¨í„´ ("X ë•Œë¬¸ì— Y", "Xë¡œ ì¸í•´ Y")
        causal_patterns = [
            r'([ê°€-í£A-Za-z0-9\s]{2,30})\s*ë•Œë¬¸ì—\s*([ê°€-í£A-Za-z0-9\s]{2,30})',
            r'([ê°€-í£A-Za-z0-9\s]{2,30})\s*ë¡œ\s*ì¸í•´\s*([ê°€-í£A-Za-z0-9\s]{2,30})'
        ]
        
        for pattern in causal_patterns:
            matches = re.findall(pattern, text)
            for cause, effect in matches:
                knowledge_items.append({
                    'concept': f"{cause.strip()} â†’ {effect.strip()}",
                    'description': f"ì¸ê³¼ê´€ê³„: {cause.strip()}ê°€ {effect.strip()}ë¥¼ ì•¼ê¸°í•¨",
                    'category': 'business_insights'
                })
        
        return knowledge_items[:15]  # ìµœëŒ€ 15ê°œ
    
    def _update_knowledge_graph(self) -> None:
        """ì§€ì‹ ê·¸ë˜í”„ ì—…ë°ì´íŠ¸"""
        # ë…¸ë“œ ì¶”ê°€
        for node_id, node in self.nodes.items():
            self.knowledge_graph.add_node(
                node_id,
                concept=node.concept,
                domain=node.domain,
                category=node.category,
                confidence=node.confidence
            )
        
        # ê´€ê³„ ì¶”ê°€
        for relation in self.relations:
            if (relation.source_id in self.knowledge_graph and 
                relation.target_id in self.knowledge_graph):
                self.knowledge_graph.add_edge(
                    relation.source_id,
                    relation.target_id,
                    relation_type=relation.relation_type,
                    strength=relation.strength
                )
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        self.stats.update({
            'total_nodes': len(self.nodes),
            'total_relations': len(self.relations),
            'domains_covered': len(set(node.domain for node in self.nodes.values()))
        })
    
    def infer_insights(self, query_context: str = "") -> List[Dict[str, Any]]:
        """ì§€ì‹ ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ ì¶”ë¡ """
        logger.info("ğŸ”® ì§€ì‹ ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ ì¶”ë¡  ì‹œì‘...")
        
        insights = []
        
        # 1. ì¤‘ì‹¬ì„± ë¶„ì„ - ê°€ì¥ ì¤‘ìš”í•œ ê°œë…ë“¤
        centrality_insights = self._analyze_concept_centrality()
        insights.extend(centrality_insights)
        
        # 2. í´ëŸ¬ìŠ¤í„° ë¶„ì„ - ê´€ë ¨ ê°œë… ê·¸ë£¹
        cluster_insights = self._analyze_concept_clusters()
        insights.extend(cluster_insights)
        
        # 3. íŠ¸ë Œë“œ ë¶„ì„ - ì‹œê°„ì  íŒ¨í„´
        trend_insights = self._analyze_temporal_trends()
        insights.extend(trend_insights)
        
        # 4. ì¶”ë¡  ê·œì¹™ ì ìš©
        rule_based_insights = self._apply_inference_rules()
        insights.extend(rule_based_insights)
        
        # ì‹ ë¢°ë„ ê¸°ì¤€ ì •ë ¬
        insights.sort(key=lambda x: x.get('confidence', 0), reverse=True)
        
        logger.info(f"âœ… {len(insights)}ê°œ ì¸ì‚¬ì´íŠ¸ ì¶”ë¡  ì™„ë£Œ")
        return insights[:20]  # ìƒìœ„ 20ê°œë§Œ ë°˜í™˜
    
    def _analyze_concept_centrality(self) -> List[Dict[str, Any]]:
        """ê°œë… ì¤‘ì‹¬ì„± ë¶„ì„"""
        if len(self.knowledge_graph.nodes) == 0:
            return []
        
        # ë‹¤ì–‘í•œ ì¤‘ì‹¬ì„± ë©”íŠ¸ë¦­ ê³„ì‚°
        degree_centrality = nx.degree_centrality(self.knowledge_graph)
        betweenness_centrality = nx.betweenness_centrality(self.knowledge_graph)
        
        insights = []
        
        # ìƒìœ„ 5ê°œ ì¤‘ì‹¬ ê°œë…
        top_central = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:5]
        
        for node_id, centrality_score in top_central:
            if node_id in self.nodes:
                node = self.nodes[node_id]
                insights.append({
                    'type': 'central_concept',
                    'concept': node.concept,
                    'description': f"í•µì‹¬ ê°œë…: {node.concept} (ì¤‘ì‹¬ì„±: {centrality_score:.3f})",
                    'confidence': min(0.9, centrality_score * 2),
                    'evidence': [f"ë„¤íŠ¸ì›Œí¬ ì¤‘ì‹¬ì„± ì ìˆ˜: {centrality_score:.3f}"],
                    'category': 'structural_analysis'
                })
        
        return insights
    
    def _analyze_concept_clusters(self) -> List[Dict[str, Any]]:
        """ê°œë… í´ëŸ¬ìŠ¤í„° ë¶„ì„"""
        insights = []
        
        if len(self.knowledge_graph.nodes) < 3:
            return insights
        
        # ì»¤ë®¤ë‹ˆí‹° íƒì§€
        try:
            communities = nx.community.greedy_modularity_communities(
                self.knowledge_graph.to_undirected()
            )
            
            for i, community in enumerate(communities):
                if len(community) >= 2:  # ìµœì†Œ 2ê°œ ë…¸ë“œ
                    concepts = []
                    for node_id in community:
                        if node_id in self.nodes:
                            concepts.append(self.nodes[node_id].concept)
                    
                    if concepts:
                        insights.append({
                            'type': 'concept_cluster',
                            'description': f"ê´€ë ¨ ê°œë… ê·¸ë£¹ {i+1}: {', '.join(concepts[:5])}",
                            'concepts': concepts,
                            'confidence': 0.7,
                            'evidence': [f"ë„¤íŠ¸ì›Œí¬ í´ëŸ¬ìŠ¤í„°ë§ìœ¼ë¡œ {len(concepts)}ê°œ ê°œë… ê·¸ë£¹í™”"],
                            'category': 'relationship_analysis'
                        })
                        
        except Exception as e:
            logger.warning(f"í´ëŸ¬ìŠ¤í„° ë¶„ì„ ì‹¤íŒ¨: {e}")
        
        return insights
    
    def _analyze_temporal_trends(self) -> List[Dict[str, Any]]:
        """ì‹œê°„ì  íŠ¸ë Œë“œ ë¶„ì„"""
        insights = []
        
        # ì‹œê°„ë³„ ì§€ì‹ ìƒì„± íŒ¨í„´
        time_distribution = defaultdict(int)
        
        for node in self.nodes.values():
            try:
                created_date = datetime.fromisoformat(node.created_at).date()
                time_distribution[created_date] += 1
            except:
                continue
        
        if len(time_distribution) > 1:
            dates = sorted(time_distribution.keys())
            recent_growth = time_distribution[dates[-1]] if len(dates) > 1 else 0
            
            insights.append({
                'type': 'temporal_trend',
                'description': f"ì§€ì‹ ìƒì„± ì¶”ì„¸: ìµœê·¼ {recent_growth}ê°œ ìƒˆ ê°œë… ë°œê²¬",
                'confidence': 0.6,
                'evidence': [f"ì´ {len(dates)}ì¼ê°„ {sum(time_distribution.values())}ê°œ ì§€ì‹ ìƒì„±"],
                'category': 'trend_analysis'
            })
        
        return insights
    
    def _apply_inference_rules(self) -> List[Dict[str, Any]]:
        """ì¶”ë¡  ê·œì¹™ ì ìš©"""
        insights = []
        
        # ì¶”ì´ì  ê´€ê³„ ì¶”ë¡ 
        transitive_relations = self._find_transitive_relations()
        for source, target, path_length in transitive_relations[:3]:  # ìƒìœ„ 3ê°œ
            if source in self.nodes and target in self.nodes:
                source_concept = self.nodes[source].concept
                target_concept = self.nodes[target].concept
                
                insights.append({
                    'type': 'inferred_relation',
                    'description': f"ì¶”ë¡ ëœ ê´€ê³„: {source_concept} â†’ {target_concept}",
                    'confidence': max(0.4, 0.9 ** path_length),  # ê²½ë¡œê°€ ê¸¸ìˆ˜ë¡ ë‚®ì€ ì‹ ë¢°ë„
                    'evidence': [f"{path_length}ë‹¨ê³„ ì¶”ë¡  ê²½ë¡œ"],
                    'category': 'logical_inference'
                })
        
        return insights
    
    def _find_transitive_relations(self) -> List[Tuple[str, str, int]]:
        """ì¶”ì´ì  ê´€ê³„ íƒì§€"""
        transitive_relations = []
        
        # ëª¨ë“  ë…¸ë“œ ìŒì— ëŒ€í•´ ê²½ë¡œ í™•ì¸
        nodes = list(self.knowledge_graph.nodes())[:20]  # ì„±ëŠ¥ì„ ìœ„í•´ ì œí•œ
        
        for source in nodes:
            for target in nodes:
                if source != target:
                    try:
                        if nx.has_path(self.knowledge_graph, source, target):
                            shortest_path = nx.shortest_path(self.knowledge_graph, source, target)
                            if len(shortest_path) > 2:  # ì§ì ‘ ì—°ê²°ì´ ì•„ë‹Œ ê²½ìš°
                                transitive_relations.append((source, target, len(shortest_path) - 1))
                    except:
                        continue
        
        return transitive_relations
    
    def save_knowledge_base(self, file_path: str) -> None:
        """ì§€ì‹ ë² ì´ìŠ¤ ì €ì¥"""
        knowledge_data = {
            'domain': self.domain,
            'nodes': {node_id: asdict(node) for node_id, node in self.nodes.items()},
            'relations': [asdict(relation) for relation in self.relations],
            'stats': self.stats,
            'saved_at': datetime.now().isoformat()
        }
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(knowledge_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ’¾ ì§€ì‹ ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ: {file_path}")
    
    def load_knowledge_base(self, file_path: str) -> None:
        """ì§€ì‹ ë² ì´ìŠ¤ ë¡œë“œ"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                knowledge_data = json.load(f)
            
            self.domain = knowledge_data['domain']
            
            # ë…¸ë“œ ë³µì›
            for node_id, node_data in knowledge_data['nodes'].items():
                self.nodes[node_id] = KnowledgeNode(**node_data)
            
            # ê´€ê³„ ë³µì›
            for relation_data in knowledge_data['relations']:
                self.relations.append(SemanticRelation(**relation_data))
            
            # í†µê³„ ë³µì›
            self.stats = knowledge_data['stats']
            
            # ì§€ì‹ ê·¸ë˜í”„ ì¬êµ¬ì¶•
            self._update_knowledge_graph()
            
            logger.info(f"ğŸ“š ì§€ì‹ ë² ì´ìŠ¤ ë¡œë“œ ì™„ë£Œ: {file_path}")
            
        except Exception as e:
            logger.error(f"âŒ ì§€ì‹ ë² ì´ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def get_knowledge_summary(self) -> Dict[str, Any]:
        """ì§€ì‹ ë² ì´ìŠ¤ ìš”ì•½ ì •ë³´"""
        category_distribution = defaultdict(int)
        confidence_distribution = []
        
        for node in self.nodes.values():
            category_distribution[node.category] += 1
            confidence_distribution.append(node.confidence)
        
        return {
            'total_knowledge_nodes': len(self.nodes),
            'total_relations': len(self.relations),
            'domain': self.domain,
            'category_distribution': dict(category_distribution),
            'average_confidence': np.mean(confidence_distribution) if confidence_distribution else 0,
            'knowledge_graph_density': nx.density(self.knowledge_graph) if len(self.knowledge_graph) > 1 else 0,
            'last_updated': self.stats.get('last_updated', 'Never')
        }

# ì‚¬ìš© ì˜ˆì œ
def main():
    """ì‚¬ìš© ì˜ˆì œ"""
    # ì˜¨í†¨ë¡œì§€ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    ontology = KnowledgeOntology("conference_analysis")
    
    # ìƒ˜í”Œ ë¶„ì„ ê²°ê³¼ (ì‹¤ì œ ì‚¬ìš©ì‹œì—ëŠ” ë¶„ì„ ì‹œìŠ¤í…œì—ì„œ ì œê³µ)
    sample_analysis = {
        "multimodal_results": [
            {
                "file_type": "audio",
                "content": "ì£¼ì–¼ë¦¬ ì‹œì¥ì˜ ìƒˆë¡œìš´ íŠ¸ë Œë“œëŠ” ì§€ì†ê°€ëŠ¥í•œ ì¬ë£Œ ì‚¬ìš©ì…ë‹ˆë‹¤",
                "confidence": 0.85,
                "file_path": "sample.wav"
            },
            {
                "file_type": "image", 
                "content": "Chart ì£¼ì–¼ë¦¬ ë§¤ì¶œ ì¦ê°€ íŠ¸ë Œë“œ 2023-2024",
                "confidence": 0.92,
                "file_path": "chart.png"
            }
        ],
        "cross_modal_insights": {
            "high_correlation_pairs": [
                {
                    "primary_file": "audio_1.wav",
                    "correlation_score": 0.87,
                    "related_files": [{"file": "chart_1.png", "similarity": 0.87}]
                }
            ],
            "dominant_themes": ["ì§€ì†ê°€ëŠ¥ì„±", "íŠ¸ë Œë“œ", "ë§¤ì¶œ", "ì„±ì¥"]
        }
    }
    
    # ì§€ì‹ ì¶”ì¶œ
    ontology.add_knowledge_from_analysis(sample_analysis)
    
    # ì¸ì‚¬ì´íŠ¸ ì¶”ë¡ 
    insights = ontology.infer_insights()
    
    print("ğŸ§  ì˜¨í†¨ë¡œì§€ ì§€ì‹ ë² ì´ìŠ¤ ë¶„ì„ ê²°ê³¼:")
    print(f"ğŸ“Š ìš”ì•½: {ontology.get_knowledge_summary()}")
    
    print("\nğŸ”® ì¶”ë¡ ëœ ì¸ì‚¬ì´íŠ¸:")
    for i, insight in enumerate(insights[:5], 1):
        print(f"{i}. {insight['description']} (ì‹ ë¢°ë„: {insight['confidence']:.2f})")

if __name__ == "__main__":
    main()