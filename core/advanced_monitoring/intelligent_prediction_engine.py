#!/usr/bin/env python3
"""
ì§€ëŠ¥í˜• ì˜ˆì¸¡ ì•Œê³ ë¦¬ì¦˜ ì—”ì§„ v2.6
AI ëª¨ë¸ë³„ ì„¸ë¶„í™” ë° ì²˜ë¦¬ ì‹œê°„ ìµœì í™”
"""

import time
import json
import numpy as np
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from pathlib import Path
import logging
from collections import deque
import threading
import psutil

@dataclass
class ProcessingTimeMetrics:
    """ì²˜ë¦¬ ì‹œê°„ ë©”íŠ¸ë¦­ ë°ì´í„° í´ë˜ìŠ¤"""
    model_name: str
    file_type: str
    file_size_mb: float
    processing_time_ms: float
    memory_usage_mb: float
    cpu_usage_percent: float
    timestamp: str
    success: bool
    error_message: Optional[str] = None

@dataclass
class PredictionResult:
    """ì˜ˆì¸¡ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    model_name: str
    file_type: str
    file_size_mb: float
    predicted_time_ms: float
    confidence_score: float
    predicted_memory_mb: float
    predicted_cpu_percent: float
    optimization_suggestions: List[str]
    timestamp: str

@dataclass
class ModelPerformanceProfile:
    """ëª¨ë¸ ì„±ëŠ¥ í”„ë¡œí•„ ë°ì´í„° í´ë˜ìŠ¤"""
    model_name: str
    base_processing_rate: float  # ms per MB
    memory_efficiency: float     # MB per processing MB
    cpu_intensity: float         # CPU% during processing
    error_rate: float           # 0.0-1.0
    last_updated: str
    sample_count: int
    confidence_level: float

class IntelligentPredictionEngine:
    """ì§€ëŠ¥í˜• ì˜ˆì¸¡ ì•Œê³ ë¦¬ì¦˜ ì—”ì§„"""
    
    def __init__(self, history_retention_hours: int = 168):  # 1ì£¼ì¼
        self.history_retention_hours = history_retention_hours
        self.logger = self._setup_logging()
        
        # ë©”íŠ¸ë¦­ ì €ì¥ì†Œ
        self.processing_history = deque(maxlen=10000)
        
        # AI ëª¨ë¸ë³„ ì„±ëŠ¥ í”„ë¡œí•„ (Ollama 7ê°œ ëª¨ë¸ + ê¸°íƒ€)
        self.model_profiles = self._initialize_model_profiles()
        
        # íŒŒì¼ íƒ€ì…ë³„ ê¸°ë³¸ ì²˜ë¦¬ìœ¨
        self.file_type_rates = {
            'audio/wav': {'base_rate': 8.0, 'complexity_factor': 1.2},
            'audio/mp3': {'base_rate': 6.0, 'complexity_factor': 1.0},
            'audio/m4a': {'base_rate': 10.0, 'complexity_factor': 1.5},
            'image/jpeg': {'base_rate': 2.0, 'complexity_factor': 0.8},
            'image/png': {'base_rate': 3.0, 'complexity_factor': 1.0},
            'video/mp4': {'base_rate': 15.0, 'complexity_factor': 2.0},
            'text/plain': {'base_rate': 0.5, 'complexity_factor': 0.3}
        }
        
        # ì˜ˆì¸¡ ì •í™•ë„ ì¶”ì 
        self.prediction_accuracy = {
            'total_predictions': 0,
            'accurate_predictions': 0,
            'accuracy_threshold': 0.2,  # 20% ì˜¤ì°¨ í—ˆìš©
            'current_accuracy': 0.0
        }
        
        # ìµœì í™” ì¶”ì²œ ì—”ì§„
        self.optimization_rules = self._initialize_optimization_rules()
        
        # ìŠ¤ë ˆë“œ ì•ˆì „ì„±
        self.lock = threading.Lock()
        
    def _setup_logging(self) -> logging.Logger:
        """ë¡œê¹… ì„¤ì •"""
        logger = logging.getLogger(f'{__name__}.IntelligentPredictionEngine')
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s | %(name)s | %(levelname)s | %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            logger.setLevel(logging.INFO)
        return logger
    
    def _initialize_model_profiles(self) -> Dict[str, ModelPerformanceProfile]:
        """AI ëª¨ë¸ë³„ ì„±ëŠ¥ í”„ë¡œí•„ ì´ˆê¸°í™”"""
        profiles = {}
        
        # Ollama 7ê°œ ëª¨ë¸ í”„ë¡œí•„ (ì‚¬ìš©ì ì‹¤ì œ ëª¨ë¸ ê¸°ë°˜)
        ollama_models = {
            'gemma3:27b': {
                'base_processing_rate': 25.0,  # ê³ í’ˆì§ˆì´ì§€ë§Œ ëŠë¦¼
                'memory_efficiency': 3.5,
                'cpu_intensity': 85.0,
                'error_rate': 0.02
            },
            'qwen3:8b': {
                'base_processing_rate': 12.0,  # ê· í˜•ì¡íŒ ì„±ëŠ¥
                'memory_efficiency': 2.0,
                'cpu_intensity': 60.0,
                'error_rate': 0.03
            },
            'qwen2.5:7b': {
                'base_processing_rate': 10.0,  # ë¹ ë¥¸ ì²˜ë¦¬
                'memory_efficiency': 1.8,
                'cpu_intensity': 55.0,
                'error_rate': 0.04
            },
            'gemma3:4b': {
                'base_processing_rate': 8.0,   # ê°€ì¥ ë¹ ë¦„
                'memory_efficiency': 1.2,
                'cpu_intensity': 40.0,
                'error_rate': 0.05
            },
            'solar:latest': {
                'base_processing_rate': 15.0,
                'memory_efficiency': 2.5,
                'cpu_intensity': 70.0,
                'error_rate': 0.03
            },
            'mistral:latest': {
                'base_processing_rate': 14.0,
                'memory_efficiency': 2.2,
                'cpu_intensity': 65.0,
                'error_rate': 0.04
            },
            'llama3.2:latest': {
                'base_processing_rate': 18.0,
                'memory_efficiency': 2.8,
                'cpu_intensity': 75.0,
                'error_rate': 0.03
            }
        }
        
        # ì „í†µì  ë¶„ì„ ë„êµ¬ë“¤
        traditional_models = {
            'whisper_cpu': {
                'base_processing_rate': 8.0,
                'memory_efficiency': 1.0,
                'cpu_intensity': 45.0,
                'error_rate': 0.01
            },
            'easyocr': {
                'base_processing_rate': 2.0,
                'memory_efficiency': 0.8,
                'cpu_intensity': 30.0,
                'error_rate': 0.02
            },
            'transformers_cpu': {
                'base_processing_rate': 20.0,
                'memory_efficiency': 2.0,
                'cpu_intensity': 70.0,
                'error_rate': 0.02
            }
        }
        
        # í”„ë¡œí•„ ìƒì„±
        all_models = {**ollama_models, **traditional_models}
        
        for model_name, config in all_models.items():
            profiles[model_name] = ModelPerformanceProfile(
                model_name=model_name,
                base_processing_rate=config['base_processing_rate'],
                memory_efficiency=config['memory_efficiency'],
                cpu_intensity=config['cpu_intensity'],
                error_rate=config['error_rate'],
                last_updated=datetime.now().isoformat(),
                sample_count=0,
                confidence_level=0.5  # ì´ˆê¸° ì‹ ë¢°ë„
            )
        
        return profiles
    
    def _initialize_optimization_rules(self) -> List[Dict[str, Any]]:
        """ìµœì í™” ê·œì¹™ ì´ˆê¸°í™”"""
        return [
            {
                'condition': 'high_memory_usage',
                'threshold': 80.0,
                'suggestions': [
                    'ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ìŠµë‹ˆë‹¤. ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš©ì„ ê³ ë ¤í•˜ì„¸ìš”',
                    'GEMMA3:4B ë˜ëŠ” QWEN2.5:7B ëª¨ë¸ë¡œ ì „í™˜ì„ ê¶Œì¥í•©ë‹ˆë‹¤',
                    'íŒŒì¼ì„ ë” ì‘ì€ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ì—¬ ì²˜ë¦¬í•˜ì„¸ìš”'
                ]
            },
            {
                'condition': 'slow_processing',
                'threshold': 30000.0,  # 30ì´ˆ
                'suggestions': [
                    'ì²˜ë¦¬ ì‹œê°„ì´ ê¹ë‹ˆë‹¤. ë” ë¹ ë¥¸ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì„¸ìš”',
                    'GEMMA3:4B ëª¨ë¸ì´ ê°€ì¥ ë¹ ë¥¸ ì²˜ë¦¬ ì†ë„ë¥¼ ì œê³µí•©ë‹ˆë‹¤',
                    'CPU ëª¨ë“œ ëŒ€ì‹  GPU ì‚¬ìš©ì„ ê²€í† í•˜ì„¸ìš”'
                ]
            },
            {
                'condition': 'high_cpu_usage',
                'threshold': 90.0,
                'suggestions': [
                    'CPU ì‚¬ìš©ë¥ ì´ ë†’ìŠµë‹ˆë‹¤. ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ì¤„ì´ì„¸ìš”',
                    'ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ë¥¼ ì¢…ë£Œí•˜ê³  ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”',
                    'ë” ê°€ë²¼ìš´ ëª¨ë¸(QWEN2.5:7B)ì„ ì‚¬ìš©í•˜ì„¸ìš”'
                ]
            },
            {
                'condition': 'large_file_size',
                'threshold': 100.0,  # 100MB
                'suggestions': [
                    'ëŒ€ìš©ëŸ‰ íŒŒì¼ì…ë‹ˆë‹¤. ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤',
                    'íŒŒì¼ì„ 10MB ë‹¨ìœ„ë¡œ ë¶„í• í•˜ì—¬ ì²˜ë¦¬í•˜ì„¸ìš”',
                    'GEMMA3:27B ëŒ€ì‹  QWEN3:8B ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤'
                ]
            },
            {
                'condition': 'frequent_errors',
                'threshold': 0.1,  # 10% ì—ëŸ¬ìœ¨
                'suggestions': [
                    'ì—ëŸ¬ê°€ ìì£¼ ë°œìƒí•©ë‹ˆë‹¤. íŒŒì¼ í˜•ì‹ì„ í™•ì¸í•˜ì„¸ìš”',
                    'ë” ì•ˆì •ì ì¸ ëª¨ë¸(Whisper, EasyOCR)ì„ ì‚¬ìš©í•˜ì„¸ìš”',
                    'ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ê°€ ì¶©ë¶„í•œì§€ í™•ì¸í•˜ì„¸ìš”'
                ]
            }
        ]
    
    def record_processing_metrics(self, metrics: ProcessingTimeMetrics) -> None:
        """ì²˜ë¦¬ ë©”íŠ¸ë¦­ ê¸°ë¡"""
        with self.lock:
            self.processing_history.append(metrics)
            
            # ëª¨ë¸ í”„ë¡œí•„ ì—…ë°ì´íŠ¸
            if metrics.model_name in self.model_profiles:
                self._update_model_profile(metrics)
            
            self.logger.info(f"ğŸ“Š ë©”íŠ¸ë¦­ ê¸°ë¡: {metrics.model_name} - {metrics.processing_time_ms:.1f}ms")
    
    def _update_model_profile(self, metrics: ProcessingTimeMetrics) -> None:
        """ëª¨ë¸ í”„ë¡œí•„ ì—…ë°ì´íŠ¸ (ì´ë™ í‰ê· )"""
        profile = self.model_profiles[metrics.model_name]
        
        if metrics.success and metrics.file_size_mb > 0:
            # ì²˜ë¦¬ìœ¨ ì—…ë°ì´íŠ¸ (ì´ë™ í‰ê· )
            current_rate = metrics.processing_time_ms / metrics.file_size_mb
            alpha = 0.1  # í•™ìŠµë¥ 
            
            profile.base_processing_rate = (
                (1 - alpha) * profile.base_processing_rate + 
                alpha * current_rate
            )
            
            # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ì—…ë°ì´íŠ¸
            memory_rate = metrics.memory_usage_mb / metrics.file_size_mb
            profile.memory_efficiency = (
                (1 - alpha) * profile.memory_efficiency + 
                alpha * memory_rate
            )
            
            # CPU ê°•ë„ ì—…ë°ì´íŠ¸
            profile.cpu_intensity = (
                (1 - alpha) * profile.cpu_intensity + 
                alpha * metrics.cpu_usage_percent
            )
        
        # ì—ëŸ¬ìœ¨ ì—…ë°ì´íŠ¸
        if not metrics.success:
            profile.error_rate = min(1.0, profile.error_rate + 0.01)
        else:
            profile.error_rate = max(0.0, profile.error_rate - 0.001)
        
        # ìƒ˜í”Œ ìˆ˜ ë° ì‹ ë¢°ë„ ì—…ë°ì´íŠ¸
        profile.sample_count += 1
        profile.confidence_level = min(1.0, profile.sample_count / 100.0)
        profile.last_updated = datetime.now().isoformat()
    
    def predict_processing_time(self, 
                              model_name: str, 
                              file_type: str, 
                              file_size_mb: float,
                              current_system_load: Optional[Dict] = None) -> PredictionResult:
        """ì²˜ë¦¬ ì‹œê°„ ì˜ˆì¸¡"""
        
        # ì‹œìŠ¤í…œ ë¶€í•˜ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        if current_system_load is None:
            current_system_load = self._get_current_system_load()
        
        # ê¸°ë³¸ ì˜ˆì¸¡ ê³„ì‚°
        base_prediction = self._calculate_base_prediction(model_name, file_type, file_size_mb)
        
        # ì‹œìŠ¤í…œ ë¶€í•˜ ë³´ì •
        load_adjusted_prediction = self._adjust_for_system_load(base_prediction, current_system_load)
        
        # íŒŒì¼ í¬ê¸° ë³´ì •
        size_adjusted_prediction = self._adjust_for_file_size(load_adjusted_prediction, file_size_mb)
        
        # ì‹ ë¢°ë„ ê³„ì‚°
        confidence_score = self._calculate_prediction_confidence(model_name, file_type)
        
        # ìµœì í™” ì œì•ˆ ìƒì„±
        optimization_suggestions = self._generate_optimization_suggestions(
            model_name, file_type, file_size_mb, size_adjusted_prediction, current_system_load
        )
        
        result = PredictionResult(
            model_name=model_name,
            file_type=file_type,
            file_size_mb=file_size_mb,
            predicted_time_ms=size_adjusted_prediction['processing_time_ms'],
            confidence_score=confidence_score,
            predicted_memory_mb=size_adjusted_prediction['memory_usage_mb'],
            predicted_cpu_percent=size_adjusted_prediction['cpu_usage_percent'],
            optimization_suggestions=optimization_suggestions,
            timestamp=datetime.now().isoformat()
        )
        
        self.logger.info(f"ğŸ”® ì˜ˆì¸¡ ì™„ë£Œ: {model_name} - {file_size_mb:.1f}MB â†’ {result.predicted_time_ms:.1f}ms")
        return result
    
    def _calculate_base_prediction(self, model_name: str, file_type: str, file_size_mb: float) -> Dict[str, float]:
        """ê¸°ë³¸ ì˜ˆì¸¡ ê³„ì‚°"""
        # ëª¨ë¸ í”„ë¡œí•„ ê°€ì ¸ì˜¤ê¸°
        if model_name in self.model_profiles:
            profile = self.model_profiles[model_name]
        else:
            # ê¸°ë³¸ í”„ë¡œí•„ ì‚¬ìš©
            profile = ModelPerformanceProfile(
                model_name="default",
                base_processing_rate=15.0,
                memory_efficiency=2.0,
                cpu_intensity=60.0,
                error_rate=0.05,
                last_updated=datetime.now().isoformat(),
                sample_count=0,
                confidence_level=0.3
            )
        
        # íŒŒì¼ íƒ€ì…ë³„ ë³´ì • ì¸ìˆ˜
        file_config = self.file_type_rates.get(file_type, {
            'base_rate': 10.0, 
            'complexity_factor': 1.0
        })
        
        # ê¸°ë³¸ ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
        base_time = profile.base_processing_rate * file_size_mb * file_config['complexity_factor']
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì˜ˆì¸¡
        memory_usage = profile.memory_efficiency * file_size_mb
        
        # CPU ì‚¬ìš©ë¥  ì˜ˆì¸¡
        cpu_usage = profile.cpu_intensity
        
        return {
            'processing_time_ms': base_time,
            'memory_usage_mb': memory_usage,
            'cpu_usage_percent': cpu_usage
        }
    
    def _adjust_for_system_load(self, base_prediction: Dict[str, float], system_load: Dict) -> Dict[str, float]:
        """ì‹œìŠ¤í…œ ë¶€í•˜ì— ë”°ë¥¸ ì˜ˆì¸¡ ë³´ì •"""
        adjusted = base_prediction.copy()
        
        # CPU ë¶€í•˜ ë³´ì •
        cpu_load_factor = 1.0 + (system_load.get('cpu_percent', 0) / 100.0)
        adjusted['processing_time_ms'] *= cpu_load_factor
        
        # ë©”ëª¨ë¦¬ ë¶€í•˜ ë³´ì •
        memory_load_factor = 1.0 + (system_load.get('memory_percent', 0) / 200.0)
        adjusted['memory_usage_mb'] *= memory_load_factor
        
        # ë””ìŠ¤í¬ I/O ë³´ì •
        disk_load_factor = 1.0 + (system_load.get('disk_usage', 0) / 300.0)
        adjusted['processing_time_ms'] *= disk_load_factor
        
        return adjusted
    
    def _adjust_for_file_size(self, prediction: Dict[str, float], file_size_mb: float) -> Dict[str, float]:
        """íŒŒì¼ í¬ê¸°ì— ë”°ë¥¸ ë¹„ì„ í˜• ë³´ì •"""
        adjusted = prediction.copy()
        
        # ëŒ€ìš©ëŸ‰ íŒŒì¼ì— ëŒ€í•œ ë¹„ì„ í˜• ë³´ì •
        if file_size_mb > 50:
            # 50MB ì´ìƒì—ì„œëŠ” ì²˜ë¦¬ ì‹œê°„ì´ ë¹„ì„ í˜•ì ìœ¼ë¡œ ì¦ê°€
            size_penalty = 1.0 + ((file_size_mb - 50) / 100.0) * 0.3
            adjusted['processing_time_ms'] *= size_penalty
            
        elif file_size_mb < 1:
            # ì†Œìš©ëŸ‰ íŒŒì¼ì— ëŒ€í•œ ìµœì†Œ ì²˜ë¦¬ ì‹œê°„ ë³´ì¥
            adjusted['processing_time_ms'] = max(adjusted['processing_time_ms'], 500.0)
        
        return adjusted
    
    def _calculate_prediction_confidence(self, model_name: str, file_type: str) -> float:
        """ì˜ˆì¸¡ ì‹ ë¢°ë„ ê³„ì‚°"""
        base_confidence = 0.5
        
        # ëª¨ë¸ í”„ë¡œí•„ ê¸°ë°˜ ì‹ ë¢°ë„
        if model_name in self.model_profiles:
            profile = self.model_profiles[model_name]
            model_confidence = profile.confidence_level
        else:
            model_confidence = 0.3
        
        # íŒŒì¼ íƒ€ì… ê¸°ë°˜ ì‹ ë¢°ë„
        if file_type in self.file_type_rates:
            type_confidence = 0.8
        else:
            type_confidence = 0.4
        
        # ì „ì²´ ì˜ˆì¸¡ ì •í™•ë„ ê¸°ë°˜ ì‹ ë¢°ë„
        accuracy_confidence = self.prediction_accuracy['current_accuracy']
        
        # ê°€ì¤‘ í‰ê·  ê³„ì‚°
        final_confidence = (
            0.4 * model_confidence + 
            0.3 * type_confidence + 
            0.3 * accuracy_confidence
        )
        
        return min(1.0, max(0.1, final_confidence))
    
    def _get_current_system_load(self) -> Dict[str, float]:
        """í˜„ì¬ ì‹œìŠ¤í…œ ë¶€í•˜ ì •ë³´"""
        try:
            return {
                'cpu_percent': psutil.cpu_percent(interval=0.1),
                'memory_percent': psutil.virtual_memory().percent,
                'disk_usage': psutil.disk_usage('/').percent if hasattr(psutil, 'disk_usage') else 0.0
            }
        except Exception as e:
            self.logger.warning(f"ì‹œìŠ¤í…œ ë¶€í•˜ ì¸¡ì • ì‹¤íŒ¨: {e}")
            return {'cpu_percent': 50.0, 'memory_percent': 50.0, 'disk_usage': 30.0}
    
    def _generate_optimization_suggestions(self, 
                                         model_name: str, 
                                         file_type: str, 
                                         file_size_mb: float,
                                         prediction: Dict[str, float],
                                         system_load: Dict) -> List[str]:
        """ìµœì í™” ì œì•ˆ ìƒì„±"""
        suggestions = []
        
        # ê·œì¹™ ê¸°ë°˜ ì œì•ˆ
        for rule in self.optimization_rules:
            condition = rule['condition']
            threshold = rule['threshold']
            
            if condition == 'high_memory_usage' and prediction['memory_usage_mb'] > threshold:
                suggestions.extend(rule['suggestions'])
            elif condition == 'slow_processing' and prediction['processing_time_ms'] > threshold:
                suggestions.extend(rule['suggestions'])
            elif condition == 'high_cpu_usage' and system_load.get('cpu_percent', 0) > threshold:
                suggestions.extend(rule['suggestions'])
            elif condition == 'large_file_size' and file_size_mb > threshold:
                suggestions.extend(rule['suggestions'])
        
        # ëª¨ë¸ë³„ íŠ¹í™” ì œì•ˆ
        if model_name == 'gemma3:27b' and file_size_mb > 50:
            suggestions.append("GEMMA3:27BëŠ” ëŒ€ìš©ëŸ‰ íŒŒì¼ì—ì„œ ë©”ëª¨ë¦¬ë¥¼ ë§ì´ ì‚¬ìš©í•©ë‹ˆë‹¤. QWEN3:8B ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤")
        
        if model_name == 'gemma3:4b' and prediction['processing_time_ms'] < 2000:
            suggestions.append("ì‘ì€ íŒŒì¼ì—ëŠ” GEMMA3:4Bê°€ ë§¤ìš° íš¨ìœ¨ì ì…ë‹ˆë‹¤")
        
        # ì¤‘ë³µ ì œê±° ë° ìƒìœ„ 5ê°œë§Œ ë°˜í™˜
        unique_suggestions = list(set(suggestions))
        return unique_suggestions[:5]
    
    def validate_prediction(self, prediction: PredictionResult, actual_metrics: ProcessingTimeMetrics) -> float:
        """ì˜ˆì¸¡ ì •í™•ë„ ê²€ì¦"""
        # ì²˜ë¦¬ ì‹œê°„ ì˜¤ì°¨ ê³„ì‚°
        time_error = abs(prediction.predicted_time_ms - actual_metrics.processing_time_ms) / actual_metrics.processing_time_ms
        
        # ì •í™•ë„ ì—…ë°ì´íŠ¸
        with self.lock:
            self.prediction_accuracy['total_predictions'] += 1
            
            if time_error <= self.prediction_accuracy['accuracy_threshold']:
                self.prediction_accuracy['accurate_predictions'] += 1
            
            # í˜„ì¬ ì •í™•ë„ ê³„ì‚°
            if self.prediction_accuracy['total_predictions'] > 0:
                self.prediction_accuracy['current_accuracy'] = (
                    self.prediction_accuracy['accurate_predictions'] / 
                    self.prediction_accuracy['total_predictions']
                )
        
        self.logger.info(f"ğŸ¯ ì˜ˆì¸¡ ê²€ì¦: ì˜¤ì°¨ {time_error:.2%}, ì „ì²´ ì •í™•ë„ {self.prediction_accuracy['current_accuracy']:.2%}")
        
        return time_error
    
    def get_model_recommendations(self, file_type: str, file_size_mb: float, priority: str = 'balanced') -> List[Dict[str, Any]]:
        """ìƒí™©ë³„ ìµœì  ëª¨ë¸ ì¶”ì²œ"""
        recommendations = []
        
        # ëª¨ë“  ëª¨ë¸ì— ëŒ€í•´ ì˜ˆì¸¡ ìˆ˜í–‰
        for model_name in self.model_profiles.keys():
            prediction = self.predict_processing_time(model_name, file_type, file_size_mb)
            
            recommendations.append({
                'model_name': model_name,
                'predicted_time_ms': prediction.predicted_time_ms,
                'predicted_memory_mb': prediction.predicted_memory_mb,
                'confidence_score': prediction.confidence_score,
                'profile': self.model_profiles[model_name]
            })
        
        # ìš°ì„ ìˆœìœ„ì— ë”°ë¥¸ ì •ë ¬
        if priority == 'speed':
            recommendations.sort(key=lambda x: x['predicted_time_ms'])
        elif priority == 'memory':
            recommendations.sort(key=lambda x: x['predicted_memory_mb'])
        elif priority == 'accuracy':
            recommendations.sort(key=lambda x: x['profile'].error_rate)
        else:  # balanced
            # ì¢…í•© ì ìˆ˜ ê³„ì‚° (ì‹œê°„, ë©”ëª¨ë¦¬, ì •í™•ë„ì˜ ê°€ì¤‘ í‰ê· )
            for rec in recommendations:
                # ì •ê·œí™” ì ìˆ˜ (0-1)
                time_score = 1.0 / (1.0 + rec['predicted_time_ms'] / 10000.0)
                memory_score = 1.0 / (1.0 + rec['predicted_memory_mb'] / 1000.0)
                accuracy_score = 1.0 - rec['profile'].error_rate
                confidence_score = rec['confidence_score']
                
                rec['overall_score'] = (
                    0.3 * time_score + 
                    0.2 * memory_score + 
                    0.3 * accuracy_score + 
                    0.2 * confidence_score
                )
            
            recommendations.sort(key=lambda x: x['overall_score'], reverse=True)
        
        return recommendations[:3]  # ìƒìœ„ 3ê°œë§Œ ë°˜í™˜
    
    def export_performance_data(self, output_path: str) -> None:
        """ì„±ëŠ¥ ë°ì´í„° ë‚´ë³´ë‚´ê¸°"""
        export_data = {
            'export_timestamp': datetime.now().isoformat(),
            'model_profiles': {name: asdict(profile) for name, profile in self.model_profiles.items()},
            'file_type_rates': self.file_type_rates,
            'prediction_accuracy': self.prediction_accuracy,
            'processing_history_count': len(self.processing_history),
            'recent_metrics': [asdict(metrics) for metrics in list(self.processing_history)[-100:]]
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"ğŸ“Š ì„±ëŠ¥ ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {output_path}")
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ìš”ì•½ ì •ë³´"""
        return {
            'total_models': len(self.model_profiles),
            'supported_file_types': len(self.file_type_rates),
            'processing_history_count': len(self.processing_history),
            'prediction_accuracy': self.prediction_accuracy['current_accuracy'],
            'most_accurate_model': max(
                self.model_profiles.items(), 
                key=lambda x: x[1].confidence_level
            )[0] if self.model_profiles else 'none',
            'fastest_model': min(
                self.model_profiles.items(), 
                key=lambda x: x[1].base_processing_rate
            )[0] if self.model_profiles else 'none'
        }

# ì „ì—­ ì˜ˆì¸¡ ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤
_global_prediction_engine = None

def get_global_prediction_engine() -> IntelligentPredictionEngine:
    """ì „ì—­ ì˜ˆì¸¡ ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _global_prediction_engine
    if _global_prediction_engine is None:
        _global_prediction_engine = IntelligentPredictionEngine()
    return _global_prediction_engine

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    engine = IntelligentPredictionEngine()
    
    # í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡
    prediction = engine.predict_processing_time('qwen3:8b', 'audio/wav', 25.0)
    
    print("ğŸ”® ì§€ëŠ¥í˜• ì˜ˆì¸¡ ê²°ê³¼:")
    print(f"ëª¨ë¸: {prediction.model_name}")
    print(f"íŒŒì¼: {prediction.file_type} ({prediction.file_size_mb}MB)")
    print(f"ì˜ˆìƒ ì²˜ë¦¬ ì‹œê°„: {prediction.predicted_time_ms:.1f}ms")
    print(f"ì˜ˆìƒ ë©”ëª¨ë¦¬ ì‚¬ìš©: {prediction.predicted_memory_mb:.1f}MB")
    print(f"ì‹ ë¢°ë„: {prediction.confidence_score:.2%}")
    print(f"ìµœì í™” ì œì•ˆ: {len(prediction.optimization_suggestions)}ê°œ")
    
    # ëª¨ë¸ ì¶”ì²œ
    recommendations = engine.get_model_recommendations('audio/wav', 25.0, 'balanced')
    print(f"\nğŸ† ì¶”ì²œ ëª¨ë¸ (ìƒìœ„ {len(recommendations)}ê°œ):")
    for i, rec in enumerate(recommendations, 1):
        print(f"{i}. {rec['model_name']} - {rec['predicted_time_ms']:.1f}ms")