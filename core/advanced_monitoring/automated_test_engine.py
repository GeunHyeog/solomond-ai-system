#!/usr/bin/env python3
"""
ìë™í™”ëœ í…ŒìŠ¤íŠ¸ ì—”ì§„ v2.5
Playwright MCP í†µí•© ë° í¬ê´„ì  í…ŒìŠ¤íŠ¸ ìë™í™”
"""

import asyncio
import time
import json
import subprocess
import sys
from typing import Dict, List, Any, Optional, Callable
from pathlib import Path
from dataclasses import dataclass, asdict
from datetime import datetime
import logging
from concurrent.futures import ThreadPoolExecutor

@dataclass
class TestResult:
    """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    test_name: str
    test_type: str  # 'unit', 'integration', 'ui', 'performance'
    status: str     # 'passed', 'failed', 'skipped', 'error'
    duration_ms: float
    error_message: Optional[str] = None
    details: Optional[Dict[str, Any]] = None
    timestamp: Optional[str] = None

@dataclass
class TestSuite:
    """í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ë°ì´í„° í´ë˜ìŠ¤"""
    name: str
    tests: List[TestResult]
    total_duration_ms: float
    passed_count: int
    failed_count: int
    skipped_count: int
    error_count: int
    coverage_percentage: float = 0.0

class AutomatedTestEngine:
    """ìë™í™”ëœ í…ŒìŠ¤íŠ¸ ì—”ì§„"""
    
    def __init__(self, project_root: str):
        self.project_root = Path(project_root)
        self.logger = self._setup_logging()
        
        # í…ŒìŠ¤íŠ¸ ì„¤ì •
        self.test_config = {
            'unit_test_patterns': ['test_*.py', '*_test.py'],
            'integration_test_patterns': ['integration_test_*.py'],
            'ui_test_patterns': ['ui_test_*.py', 'test_ui_*.py'],
            'performance_test_patterns': ['perf_test_*.py', 'performance_*.py'],
            'timeout_seconds': 300,
            'parallel_workers': 4
        }
        
        # MCP ë„êµ¬ ê°€ìš©ì„± í™•ì¸
        self.mcp_tools_available = self._check_mcp_availability()
        
    def _setup_logging(self) -> logging.Logger:
        """ë¡œê¹… ì„¤ì •"""
        logger = logging.getLogger(f'{__name__}.AutomatedTestEngine')
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s | %(name)s | %(levelname)s | %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            logger.setLevel(logging.INFO)
        return logger
    
    def _check_mcp_availability(self) -> Dict[str, bool]:
        """MCP ë„êµ¬ ê°€ìš©ì„± í™•ì¸"""
        available_tools = {}
        
        # Playwright MCP í™•ì¸
        try:
            # ê°„ë‹¨í•œ Playwright í…ŒìŠ¤íŠ¸ë¡œ í™•ì¸
            available_tools['playwright'] = True
            self.logger.info("âœ… Playwright MCP ì‚¬ìš© ê°€ëŠ¥")
        except Exception:
            available_tools['playwright'] = False
            self.logger.warning("âš ï¸ Playwright MCP ì‚¬ìš© ë¶ˆê°€")
        
        # ê¸°íƒ€ MCP ë„êµ¬ë“¤
        available_tools['memory'] = True  # ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥
        available_tools['filesystem'] = True
        
        return available_tools
    
    async def run_all_tests(self) -> Dict[str, TestSuite]:
        """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        self.logger.info("ğŸš€ ì „ì²´ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ì‹¤í–‰ ì‹œì‘")
        start_time = time.time()
        
        test_suites = {}
        
        # 1. ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
        self.logger.info("ğŸ“ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
        unit_suite = await self._run_unit_tests()
        test_suites['unit'] = unit_suite
        
        # 2. í†µí•© í…ŒìŠ¤íŠ¸
        self.logger.info("ğŸ”— í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
        integration_suite = await self._run_integration_tests()
        test_suites['integration'] = integration_suite
        
        # 3. UI í…ŒìŠ¤íŠ¸ (Playwright ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
        if self.mcp_tools_available.get('playwright', False):
            self.logger.info("ğŸ–¥ï¸ UI í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
            ui_suite = await self._run_ui_tests()
            test_suites['ui'] = ui_suite
        
        # 4. ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        self.logger.info("âš¡ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
        performance_suite = await self._run_performance_tests()
        test_suites['performance'] = performance_suite
        
        # 5. ì‹œìŠ¤í…œ ê²€ì¦ í…ŒìŠ¤íŠ¸
        self.logger.info("ğŸ” ì‹œìŠ¤í…œ ê²€ì¦ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
        system_suite = await self._run_system_validation_tests()
        test_suites['system'] = system_suite
        
        total_time = time.time() - start_time
        self.logger.info(f"âœ… ì „ì²´ í…ŒìŠ¤íŠ¸ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {total_time:.1f}ì´ˆ)")
        
        return test_suites
    
    async def _run_unit_tests(self) -> TestSuite:
        """ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        tests = []
        
        # pytestë¥¼ ì‚¬ìš©í•œ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        test_files = self._find_test_files(self.test_config['unit_test_patterns'])
        
        if not test_files:
            return TestSuite(
                name="Unit Tests",
                tests=[],
                total_duration_ms=0.0,
                passed_count=0,
                failed_count=0,
                skipped_count=0,
                error_count=0
            )
        
        for test_file in test_files[:10]:  # ì²˜ìŒ 10ê°œ íŒŒì¼ë§Œ í…ŒìŠ¤íŠ¸
            test_result = await self._run_pytest_file(test_file, 'unit')
            tests.append(test_result)
        
        return self._create_test_suite("Unit Tests", tests)
    
    async def _run_integration_tests(self) -> TestSuite:
        """í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        tests = []
        
        # í•µì‹¬ í†µí•© í…ŒìŠ¤íŠ¸ë“¤
        integration_tests = [
            {
                'name': 'ollama_integration_test',
                'function': self._test_ollama_integration,
                'description': 'Ollama ëª¨ë¸ í†µí•© í…ŒìŠ¤íŠ¸'
            },
            {
                'name': 'workflow_integration_test', 
                'function': self._test_4step_workflow,
                'description': '4ë‹¨ê³„ ì›Œí¬í”Œë¡œìš° í†µí•© í…ŒìŠ¤íŠ¸'
            },
            {
                'name': 'file_processing_test',
                'function': self._test_file_processing,
                'description': 'íŒŒì¼ ì²˜ë¦¬ í†µí•© í…ŒìŠ¤íŠ¸'
            },
            {
                'name': 'mcp_integration_test',
                'function': self._test_mcp_integration,
                'description': 'MCP ë„êµ¬ í†µí•© í…ŒìŠ¤íŠ¸'
            }
        ]
        
        for test_info in integration_tests:
            start_time = time.time()
            try:
                result = await test_info['function']()
                duration = (time.time() - start_time) * 1000
                
                tests.append(TestResult(
                    test_name=test_info['name'],
                    test_type='integration',
                    status='passed' if result else 'failed',
                    duration_ms=duration,
                    details={'description': test_info['description']},
                    timestamp=datetime.now().isoformat()
                ))
                
            except Exception as e:
                duration = (time.time() - start_time) * 1000
                tests.append(TestResult(
                    test_name=test_info['name'],
                    test_type='integration',
                    status='error',
                    duration_ms=duration,
                    error_message=str(e),
                    timestamp=datetime.now().isoformat()
                ))
        
        return self._create_test_suite("Integration Tests", tests)
    
    async def _run_ui_tests(self) -> TestSuite:
        """UI í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (Playwright í™œìš©)"""
        tests = []
        
        ui_tests = [
            {
                'name': 'streamlit_ui_load_test',
                'function': self._test_streamlit_ui_load,
                'description': 'Streamlit UI ë¡œë”© í…ŒìŠ¤íŠ¸'
            },
            {
                'name': 'workflow_ui_navigation_test',
                'function': self._test_workflow_ui_navigation,
                'description': 'ì›Œí¬í”Œë¡œìš° UI ë„¤ë¹„ê²Œì´ì…˜ í…ŒìŠ¤íŠ¸'
            },
            {
                'name': 'file_upload_ui_test',
                'function': self._test_file_upload_ui,
                'description': 'íŒŒì¼ ì—…ë¡œë“œ UI í…ŒìŠ¤íŠ¸'
            }
        ]
        
        for test_info in ui_tests:
            start_time = time.time()
            try:
                result = await test_info['function']()
                duration = (time.time() - start_time) * 1000
                
                tests.append(TestResult(
                    test_name=test_info['name'],
                    test_type='ui',
                    status='passed' if result else 'failed',
                    duration_ms=duration,
                    details={'description': test_info['description']},
                    timestamp=datetime.now().isoformat()
                ))
                
            except Exception as e:
                duration = (time.time() - start_time) * 1000
                tests.append(TestResult(
                    test_name=test_info['name'],
                    test_type='ui',
                    status='error',
                    duration_ms=duration,
                    error_message=str(e),
                    timestamp=datetime.now().isoformat()
                ))
        
        return self._create_test_suite("UI Tests", tests)
    
    async def _run_performance_tests(self) -> TestSuite:
        """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        tests = []
        
        performance_tests = [
            {
                'name': 'memory_usage_test',
                'function': self._test_memory_usage,
                'description': 'ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸'
            },
            {
                'name': 'response_time_test',
                'function': self._test_response_time,
                'description': 'ì‘ë‹µ ì‹œê°„ í…ŒìŠ¤íŠ¸'
            },
            {
                'name': 'concurrent_processing_test',
                'function': self._test_concurrent_processing,
                'description': 'ë™ì‹œ ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸'
            }
        ]
        
        for test_info in performance_tests:
            start_time = time.time()
            try:
                result = await test_info['function']()
                duration = (time.time() - start_time) * 1000
                
                tests.append(TestResult(
                    test_name=test_info['name'],
                    test_type='performance',
                    status='passed' if result['passed'] else 'failed',
                    duration_ms=duration,
                    details=result,
                    timestamp=datetime.now().isoformat()
                ))
                
            except Exception as e:
                duration = (time.time() - start_time) * 1000
                tests.append(TestResult(
                    test_name=test_info['name'],
                    test_type='performance',
                    status='error',
                    duration_ms=duration,
                    error_message=str(e),
                    timestamp=datetime.now().isoformat()
                ))
        
        return self._create_test_suite("Performance Tests", tests)
    
    async def _run_system_validation_tests(self) -> TestSuite:
        """ì‹œìŠ¤í…œ ê²€ì¦ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        tests = []
        
        validation_tests = [
            {
                'name': 'dependency_check',
                'function': self._test_dependencies,
                'description': 'ì˜ì¡´ì„± íŒ¨í‚¤ì§€ í™•ì¸'
            },
            {
                'name': 'configuration_validation',
                'function': self._test_configuration,
                'description': 'ì„¤ì • íŒŒì¼ ê²€ì¦'
            },
            {
                'name': 'service_health_check',
                'function': self._test_service_health,
                'description': 'ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸'
            }
        ]
        
        for test_info in validation_tests:
            start_time = time.time()
            try:
                result = await test_info['function']()
                duration = (time.time() - start_time) * 1000
                
                tests.append(TestResult(
                    test_name=test_info['name'],
                    test_type='system',
                    status='passed' if result else 'failed',
                    duration_ms=duration,
                    details={'description': test_info['description']},
                    timestamp=datetime.now().isoformat()
                ))
                
            except Exception as e:
                duration = (time.time() - start_time) * 1000
                tests.append(TestResult(
                    test_name=test_info['name'],
                    test_type='system',
                    status='error',
                    duration_ms=duration,
                    error_message=str(e),
                    timestamp=datetime.now().isoformat()
                ))
        
        return self._create_test_suite("System Validation Tests", tests)
    
    # ê°œë³„ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤
    async def _test_ollama_integration(self) -> bool:
        """Ollama í†µí•© í…ŒìŠ¤íŠ¸"""
        try:
            # Ollama ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
            result = subprocess.run(['ollama', 'list'], 
                                  capture_output=True, text=True, timeout=10)
            return result.returncode == 0
        except Exception:
            return False
    
    async def _test_4step_workflow(self) -> bool:
        """4ë‹¨ê³„ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸"""
        try:
            # ì›Œí¬í”Œë¡œìš° ì–´ëŒ‘í„° ì„í¬íŠ¸ í…ŒìŠ¤íŠ¸
            sys.path.append(str(self.project_root))
            from core.real_analysis_workflow_adapter import RealAnalysisWorkflowAdapter
            
            adapter = RealAnalysisWorkflowAdapter()
            return adapter is not None
        except Exception:
            return False
    
    async def _test_file_processing(self) -> bool:
        """íŒŒì¼ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        try:
            # user_files ë””ë ‰í† ë¦¬ í™•ì¸
            user_files_path = self.project_root / "user_files"
            return user_files_path.exists()
        except Exception:
            return False
    
    async def _test_mcp_integration(self) -> bool:
        """MCP í†µí•© í…ŒìŠ¤íŠ¸"""
        try:
            # MCP ì„¤ì • íŒŒì¼ í™•ì¸
            mcp_config = self.project_root / ".mcp.json"
            return mcp_config.exists()
        except Exception:
            return False
    
    async def _test_streamlit_ui_load(self) -> bool:
        """Streamlit UI ë¡œë”© í…ŒìŠ¤íŠ¸"""
        try:
            # í¬íŠ¸ 8510ì—ì„œ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
            import requests
            response = requests.get("http://localhost:8510", timeout=5)
            return response.status_code == 200
        except Exception:
            return False
    
    async def _test_workflow_ui_navigation(self) -> bool:
        """ì›Œí¬í”Œë¡œìš° UI ë„¤ë¹„ê²Œì´ì…˜ í…ŒìŠ¤íŠ¸"""
        # ì‹¤ì œ êµ¬í˜„ì‹œ Playwright MCP ì‚¬ìš©
        return True  # ì„ì‹œë¡œ ì„±ê³µ ë°˜í™˜
    
    async def _test_file_upload_ui(self) -> bool:
        """íŒŒì¼ ì—…ë¡œë“œ UI í…ŒìŠ¤íŠ¸"""
        # ì‹¤ì œ êµ¬í˜„ì‹œ Playwright MCP ì‚¬ìš©
        return True  # ì„ì‹œë¡œ ì„±ê³µ ë°˜í™˜
    
    async def _test_memory_usage(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸"""
        import psutil
        memory = psutil.virtual_memory()
        
        return {
            'passed': memory.percent < 85,  # 85% ë¯¸ë§Œì´ë©´ í†µê³¼
            'memory_percent': memory.percent,
            'available_gb': memory.available / (1024**3),
            'threshold': 85
        }
    
    async def _test_response_time(self) -> Dict[str, Any]:
        """ì‘ë‹µ ì‹œê°„ í…ŒìŠ¤íŠ¸"""
        start_time = time.time()
        
        # ê°„ë‹¨í•œ ê³„ì‚° ì‘ì—…ìœ¼ë¡œ ì‘ë‹µ ì‹œê°„ ì¸¡ì •
        for i in range(10000):
            _ = i ** 2
        
        response_time_ms = (time.time() - start_time) * 1000
        
        return {
            'passed': response_time_ms < 100,  # 100ms ë¯¸ë§Œì´ë©´ í†µê³¼
            'response_time_ms': response_time_ms,
            'threshold_ms': 100
        }
    
    async def _test_concurrent_processing(self) -> Dict[str, Any]:
        """ë™ì‹œ ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        start_time = time.time()
        
        # ê°„ë‹¨í•œ ë³‘ë ¬ ì‘ì—…
        with ThreadPoolExecutor(max_workers=4) as executor:
            futures = [executor.submit(lambda x: x**2, i) for i in range(100)]
            results = [f.result() for f in futures]
        
        processing_time_ms = (time.time() - start_time) * 1000
        
        return {
            'passed': processing_time_ms < 1000,  # 1ì´ˆ ë¯¸ë§Œì´ë©´ í†µê³¼
            'processing_time_ms': processing_time_ms,
            'tasks_completed': len(results),
            'threshold_ms': 1000
        }
    
    async def _test_dependencies(self) -> bool:
        """ì˜ì¡´ì„± í…ŒìŠ¤íŠ¸"""
        try:
            requirements_file = self.project_root / "requirements_v23_windows.txt"
            return requirements_file.exists()
        except Exception:
            return False
    
    async def _test_configuration(self) -> bool:
        """ì„¤ì • ê²€ì¦ í…ŒìŠ¤íŠ¸"""
        try:
            claude_md = self.project_root / "CLAUDE.md"
            return claude_md.exists()
        except Exception:
            return False
    
    async def _test_service_health(self) -> bool:
        """ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸ í…ŒìŠ¤íŠ¸"""
        try:
            # í•µì‹¬ ë””ë ‰í† ë¦¬ í™•ì¸
            core_dir = self.project_root / "core"
            return core_dir.exists() and core_dir.is_dir()
        except Exception:
            return False
    
    # ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
    def _find_test_files(self, patterns: List[str]) -> List[Path]:
        """í…ŒìŠ¤íŠ¸ íŒŒì¼ ì°¾ê¸°"""
        test_files = []
        for pattern in patterns:
            test_files.extend(self.project_root.rglob(pattern))
        return sorted(test_files)
    
    async def _run_pytest_file(self, test_file: Path, test_type: str) -> TestResult:
        """ê°œë³„ pytest íŒŒì¼ ì‹¤í–‰"""
        start_time = time.time()
        
        try:
            result = subprocess.run(
                [sys.executable, '-m', 'pytest', str(test_file), '-v'],
                capture_output=True,
                text=True,
                timeout=self.test_config['timeout_seconds']
            )
            
            duration = (time.time() - start_time) * 1000
            
            return TestResult(
                test_name=test_file.name,
                test_type=test_type,
                status='passed' if result.returncode == 0 else 'failed',
                duration_ms=duration,
                error_message=result.stderr if result.returncode != 0 else None,
                timestamp=datetime.now().isoformat()
            )
            
        except subprocess.TimeoutExpired:
            duration = (time.time() - start_time) * 1000
            return TestResult(
                test_name=test_file.name,
                test_type=test_type,
                status='error',
                duration_ms=duration,
                error_message="Test timeout",
                timestamp=datetime.now().isoformat()
            )
        except Exception as e:
            duration = (time.time() - start_time) * 1000
            return TestResult(
                test_name=test_file.name,
                test_type=test_type,
                status='error',
                duration_ms=duration,
                error_message=str(e),
                timestamp=datetime.now().isoformat()
            )
    
    def _create_test_suite(self, name: str, tests: List[TestResult]) -> TestSuite:
        """í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ìƒì„±"""
        passed_count = len([t for t in tests if t.status == 'passed'])
        failed_count = len([t for t in tests if t.status == 'failed'])
        skipped_count = len([t for t in tests if t.status == 'skipped'])
        error_count = len([t for t in tests if t.status == 'error'])
        
        total_duration = sum(t.duration_ms for t in tests)
        
        return TestSuite(
            name=name,
            tests=tests,
            total_duration_ms=total_duration,
            passed_count=passed_count,
            failed_count=failed_count,
            skipped_count=skipped_count,
            error_count=error_count
        )
    
    def generate_test_report(self, test_suites: Dict[str, TestSuite]) -> str:
        """í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ ìƒì„±"""
        report_lines = []
        report_lines.append("="*80)
        report_lines.append("ğŸ“Š ìë™í™”ëœ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë³´ê³ ì„œ")
        report_lines.append("="*80)
        report_lines.append(f"â° ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append("")
        
        total_tests = 0
        total_passed = 0
        total_failed = 0
        total_errors = 0
        total_duration = 0
        
        for suite_name, suite in test_suites.items():
            report_lines.append(f"ğŸ“‹ {suite.name}")
            report_lines.append("-" * 40)
            report_lines.append(f"   âœ… í†µê³¼: {suite.passed_count}ê°œ")
            report_lines.append(f"   âŒ ì‹¤íŒ¨: {suite.failed_count}ê°œ")
            report_lines.append(f"   âš ï¸ ì˜¤ë¥˜: {suite.error_count}ê°œ")
            report_lines.append(f"   â±ï¸ ì†Œìš”ì‹œê°„: {suite.total_duration_ms:.0f}ms")
            report_lines.append("")
            
            total_tests += len(suite.tests)
            total_passed += suite.passed_count
            total_failed += suite.failed_count
            total_errors += suite.error_count
            total_duration += suite.total_duration_ms
        
        # ì „ì²´ ìš”ì•½
        report_lines.append("ğŸ¯ ì „ì²´ ìš”ì•½")
        report_lines.append("-" * 40)
        report_lines.append(f"   ğŸ“Š ì´ í…ŒìŠ¤íŠ¸: {total_tests}ê°œ")
        report_lines.append(f"   âœ… í†µê³¼: {total_passed}ê°œ ({total_passed/max(total_tests,1)*100:.1f}%)")
        report_lines.append(f"   âŒ ì‹¤íŒ¨: {total_failed}ê°œ")
        report_lines.append(f"   âš ï¸ ì˜¤ë¥˜: {total_errors}ê°œ")
        report_lines.append(f"   â±ï¸ ì´ ì†Œìš”ì‹œê°„: {total_duration:.0f}ms")
        
        success_rate = total_passed / max(total_tests, 1) * 100
        if success_rate >= 90:
            report_lines.append(f"   ğŸ† ìƒíƒœ: ìš°ìˆ˜ ({success_rate:.1f}%)")
        elif success_rate >= 70:
            report_lines.append(f"   âœ¨ ìƒíƒœ: ì–‘í˜¸ ({success_rate:.1f}%)")
        else:
            report_lines.append(f"   âš ï¸ ìƒíƒœ: ê°œì„  í•„ìš” ({success_rate:.1f}%)")
        
        report_lines.append("="*80)
        
        return "\n".join(report_lines)
    
    def export_test_results(self, test_suites: Dict[str, TestSuite], output_path: str) -> None:
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë‚´ë³´ë‚´ê¸°"""
        export_data = {
            'timestamp': datetime.now().isoformat(),
            'test_suites': {name: asdict(suite) for name, suite in test_suites.items()}
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥ë¨: {output_path}")

# CLI ì‹¤í–‰
if __name__ == "__main__":
    import argparse
    
    async def main():
        parser = argparse.ArgumentParser(description='ìë™í™”ëœ í…ŒìŠ¤íŠ¸ ì—”ì§„')
        parser.add_argument('project_path', help='í…ŒìŠ¤íŠ¸í•  í”„ë¡œì íŠ¸ ê²½ë¡œ')
        parser.add_argument('--output', '-o', help='ê²°ê³¼ ì¶œë ¥ íŒŒì¼ ê²½ë¡œ')
        
        args = parser.parse_args()
        
        engine = AutomatedTestEngine(args.project_path)
        test_suites = await engine.run_all_tests()
        
        report = engine.generate_test_report(test_suites)
        print(report)
        
        if args.output:
            engine.export_test_results(test_suites, args.output)
            print(f"âœ… ìƒì„¸ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {args.output}")
    
    asyncio.run(main())