#!/usr/bin/env python3
"""
ğŸ’¡ SOLOMOND AI ìë™ ì¸ì‚¬ì´íŠ¸ ìƒì„± ì—”ì§„
Automated Insight Generation Engine with Advanced Analytics

ğŸ¯ ì£¼ìš” ê¸°ëŠ¥:
1. í†µí•© ë¶„ì„ - ë©€í‹°ëª¨ë‹¬ + ì˜¨í†¨ë¡œì§€ + ì™¸ë¶€ ë°ì´í„°
2. ì‹¤ì‹œê°„ ì¸ì‚¬ì´íŠ¸ ìƒì„± - ë¶„ì„ ì¤‘ ì¦‰ì‹œ íŒ¨í„´ íƒì§€
3. ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ì¶”ë¡  - ìƒí™© ë§ì¶¤ ì¸ì‚¬ì´íŠ¸
4. ì‹ ë¢°ë„ ê¸°ë°˜ ë­í‚¹ - í’ˆì§ˆ ë³´ì¦ ì‹œìŠ¤í…œ
5. ì•¡ì…˜ ê°€ëŠ¥í•œ ì œì•ˆ - êµ¬ì²´ì  ì‹¤í–‰ ë°©ì•ˆ ì œì‹œ
"""

import asyncio
import logging
from typing import Dict, List, Any, Optional, Tuple, Union
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from pathlib import Path
import json
import numpy as np
from collections import defaultdict, Counter
import re
import statistics

# ë‚´ë¶€ ëª¨ë“ˆ import
try:
    from .knowledge_ontology import KnowledgeOntology, KnowledgeNode
    from .multimodal_pipeline import MultimodalPipeline, MultimodalResult
except ImportError:
    try:
        from core.knowledge_ontology import KnowledgeOntology, KnowledgeNode
        from core.multimodal_pipeline import MultimodalPipeline, MultimodalResult
    except ImportError as e:
        print(f"âš ï¸ ë‚´ë¶€ ëª¨ë“ˆ import ì‹¤íŒ¨: {e}")
        
        # ê¸°ë³¸ í´ë°± í´ë˜ìŠ¤ë“¤
        class KnowledgeOntology:
            def __init__(self, domain):
                self.domain = domain
            def add_knowledge_from_analysis(self, data): pass
            def infer_insights(self): return []
            def get_knowledge_summary(self): return {}
        
        class KnowledgeNode: pass

# ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass 
class InsightItem:
    """ì¸ì‚¬ì´íŠ¸ ì•„ì´í…œ êµ¬ì¡°"""
    id: str
    type: str
    title: str
    description: str
    confidence: float
    priority: str  # "high", "medium", "low"
    category: str
    evidence: List[str]
    actionable_suggestions: List[str]
    related_data: Dict[str, Any]
    generated_at: str
    validity_period: Optional[str] = None

class InsightGenerator:
    """ìë™ ì¸ì‚¬ì´íŠ¸ ìƒì„± ì—”ì§„"""
    
    def __init__(self, domain: str = "conference_analysis"):
        self.domain = domain
        self.ontology = KnowledgeOntology(domain)
        self.insight_patterns = self._initialize_insight_patterns()
        self.quality_thresholds = self._initialize_quality_thresholds()
        
        # ìƒì„±ëœ ì¸ì‚¬ì´íŠ¸ ì €ì¥
        self.insights_history: List[InsightItem] = []
        self.insights_cache: Dict[str, InsightItem] = {}
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        self.performance_stats = {
            'total_insights_generated': 0,
            'high_confidence_insights': 0,
            'actionable_insights': 0,
            'average_generation_time': 0.0,
            'pattern_detection_accuracy': 0.0
        }
        
        logger.info(f"ğŸ’¡ ì¸ì‚¬ì´íŠ¸ ìƒì„± ì—”ì§„ ì´ˆê¸°í™” - ë„ë©”ì¸: {domain}")
    
    def _initialize_insight_patterns(self) -> Dict[str, Dict]:
        """ì¸ì‚¬ì´íŠ¸ íŒ¨í„´ ì •ì˜ ì´ˆê¸°í™”"""
        patterns = {
            # íŠ¸ë Œë“œ íŒ¨í„´
            "trend_detection": {
                "pattern": "ì‹œê°„ì— ë”°ë¥¸ ë³€í™” íŒ¨í„´",
                "indicators": ["ì¦ê°€", "ê°ì†Œ", "ìƒìŠ¹", "í•˜ë½", "ì„±ì¥", "decline", "growth"],
                "confidence_weight": 0.8,
                "min_evidence_count": 3,
                "category": "trend_analysis"
            },
            
            # ìƒê´€ê´€ê³„ íŒ¨í„´
            "correlation_insight": {
                "pattern": "ë‘ ê°œ ì´ìƒ ìš”ì†Œ ê°„ ìƒê´€ê´€ê³„",
                "indicators": ["ì˜í–¥", "ê´€ë ¨", "ì—°ê´€", "correlation", "relationship"],
                "confidence_weight": 0.75,
                "min_evidence_count": 2,
                "category": "relationship_analysis"
            },
            
            # ì´ìƒ íŒ¨í„´ íƒì§€
            "anomaly_detection": {
                "pattern": "ì˜ˆìƒê³¼ ë‹¤ë¥¸ íŠ¹ì´í•œ íŒ¨í„´",
                "indicators": ["ì˜ˆì™¸", "íŠ¹ì´", "ì´ìƒ", "unusual", "unexpected", "anomaly"],
                "confidence_weight": 0.9,
                "min_evidence_count": 1,
                "category": "anomaly_analysis"
            },
            
            # ê¸°íšŒ ì‹ë³„
            "opportunity_identification": {
                "pattern": "ê°œì„ ì´ë‚˜ í™œìš© ê°€ëŠ¥í•œ ê¸°íšŒ",
                "indicators": ["ê¸°íšŒ", "ê°€ëŠ¥ì„±", "ì ì¬", "opportunity", "potential", "chance"],
                "confidence_weight": 0.7,
                "min_evidence_count": 2,
                "category": "opportunity_analysis"
            },
            
            # ë¦¬ìŠ¤í¬ íƒì§€
            "risk_assessment": {
                "pattern": "ìœ„í—˜ ìš”ì†Œë‚˜ ì£¼ì˜ì‚¬í•­",
                "indicators": ["ìœ„í—˜", "ë¬¸ì œ", "ìš°ë ¤", "risk", "concern", "issue"],
                "confidence_weight": 0.85,
                "min_evidence_count": 1,
                "category": "risk_analysis"
            },
            
            # ì„±ê³¼ ë¶„ì„
            "performance_analysis": {
                "pattern": "ì„±ê³¼ë‚˜ ê²°ê³¼ì— ëŒ€í•œ ë¶„ì„",
                "indicators": ["ì„±ê³¼", "ê²°ê³¼", "íš¨ê³¼", "performance", "result", "effectiveness"],
                "confidence_weight": 0.8,
                "min_evidence_count": 2,
                "category": "performance_analysis"
            }
        }
        
        return patterns
    
    def _initialize_quality_thresholds(self) -> Dict[str, float]:
        """í’ˆì§ˆ ì„ê³„ê°’ ì„¤ì •"""
        return {
            'min_confidence': 0.6,
            'high_confidence': 0.8,
            'min_evidence_strength': 0.5,
            'max_similarity_threshold': 0.85,  # ì¤‘ë³µ ë°©ì§€
            'temporal_relevance_decay': 0.95  # ì‹œê°„ ê²½ê³¼ì— ë”°ë¥¸ ê´€ë ¨ì„± ê°ì†Œ
        }
    
    async def generate_comprehensive_insights(
        self, 
        multimodal_results: List[MultimodalResult],
        cross_modal_insights: Dict[str, Any],
        external_context: Optional[Dict[str, Any]] = None
    ) -> List[InsightItem]:
        """ì¢…í•©ì ì¸ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        
        logger.info("ğŸš€ ì¢…í•© ì¸ì‚¬ì´íŠ¸ ìƒì„± ì‹œì‘...")
        start_time = datetime.now()
        
        # 1. ì˜¨í†¨ë¡œì§€ ì§€ì‹ ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸
        analysis_data = {
            "multimodal_results": [asdict(result) for result in multimodal_results],
            "cross_modal_insights": cross_modal_insights
        }
        
        self.ontology.add_knowledge_from_analysis(analysis_data)
        
        # 2. ë‹¤ê°ë„ ì¸ì‚¬ì´íŠ¸ ìƒì„±
        insights = []
        
        # ë©€í‹°ëª¨ë‹¬ ë¶„ì„ ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸
        modal_insights = await self._generate_multimodal_insights(multimodal_results)
        insights.extend(modal_insights)
        
        # í¬ë¡œìŠ¤ ëª¨ë‹¬ ë¶„ì„ ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸  
        cross_insights = await self._generate_cross_modal_insights(cross_modal_insights)
        insights.extend(cross_insights)
        
        # ì˜¨í†¨ë¡œì§€ ê¸°ë°˜ ì¶”ë¡  ì¸ì‚¬ì´íŠ¸
        ontology_insights = await self._generate_ontology_insights()
        insights.extend(ontology_insights)
        
        # ì™¸ë¶€ ì»¨í…ìŠ¤íŠ¸ í†µí•© ì¸ì‚¬ì´íŠ¸
        if external_context:
            context_insights = await self._generate_context_insights(external_context)
            insights.extend(context_insights)
        
        # 3. ì¸ì‚¬ì´íŠ¸ í’ˆì§ˆ í•„í„°ë§ ë° ë­í‚¹
        filtered_insights = self._filter_and_rank_insights(insights)
        
        # 4. ì•¡ì…˜ ê°€ëŠ¥í•œ ì œì•ˆ ìƒì„±
        actionable_insights = await self._add_actionable_suggestions(filtered_insights)
        
        # 5. ì¤‘ë³µ ì œê±° ë° ìµœì¢… ì •ì œ
        final_insights = self._deduplicate_insights(actionable_insights)
        
        # ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸
        generation_time = (datetime.now() - start_time).total_seconds()
        self._update_performance_stats(len(final_insights), generation_time)
        
        # íˆìŠ¤í† ë¦¬ì— ì €ì¥
        self.insights_history.extend(final_insights)
        
        logger.info(f"âœ… ì¸ì‚¬ì´íŠ¸ ìƒì„± ì™„ë£Œ: {len(final_insights)}ê°œ (ì²˜ë¦¬ì‹œê°„: {generation_time:.2f}ì´ˆ)")
        return final_insights
    
    async def _generate_multimodal_insights(self, results: List[MultimodalResult]) -> List[InsightItem]:
        """ë©€í‹°ëª¨ë‹¬ ê²°ê³¼ ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        insights = []
        
        # ëª¨ë‹¬ë³„ í’ˆì§ˆ ë¶„ì„
        modality_quality = self._analyze_modality_quality(results)
        if modality_quality['insight_worthy']:
            insights.append(
                self._create_insight(
                    type="quality_analysis",
                    title="ë©€í‹°ëª¨ë‹¬ ë°ì´í„° í’ˆì§ˆ ë¶„ì„",
                    description=modality_quality['description'],
                    confidence=modality_quality['confidence'],
                    evidence=modality_quality['evidence'],
                    category="data_quality"
                )
            )
        
        # ì»¨í…ì¸  ì¼ê´€ì„± ë¶„ì„
        consistency_analysis = self._analyze_content_consistency(results)
        if consistency_analysis['has_insights']:
            insights.append(
                self._create_insight(
                    type="consistency_analysis",
                    title="ì»¨í…ì¸  ì¼ê´€ì„± í‰ê°€",
                    description=consistency_analysis['description'],
                    confidence=consistency_analysis['confidence'],
                    evidence=consistency_analysis['evidence'],
                    category="content_analysis"
                )
            )
        
        # ì²˜ë¦¬ íš¨ìœ¨ì„± ì¸ì‚¬ì´íŠ¸
        efficiency_insights = self._analyze_processing_efficiency(results)
        insights.extend(efficiency_insights)
        
        return insights
    
    def _analyze_modality_quality(self, results: List[MultimodalResult]) -> Dict[str, Any]:
        """ëª¨ë‹¬ë³„ í’ˆì§ˆ ë¶„ì„"""
        modality_stats = defaultdict(list)
        
        for result in results:
            modality_stats[result.file_type].append(result.confidence)
        
        quality_analysis = {
            'insight_worthy': False,
            'description': "",
            'confidence': 0.0,
            'evidence': []
        }
        
        if len(modality_stats) > 1:
            # ëª¨ë‹¬ë³„ í‰ê·  ì‹ ë¢°ë„ ê³„ì‚°
            modal_averages = {
                modal: statistics.mean(confidences) 
                for modal, confidences in modality_stats.items()
            }
            
            best_modal = max(modal_averages, key=modal_averages.get)
            worst_modal = min(modal_averages, key=modal_averages.get)
            
            quality_gap = modal_averages[best_modal] - modal_averages[worst_modal]
            
            if quality_gap > 0.2:  # 20% ì´ìƒ ì°¨ì´
                quality_analysis.update({
                    'insight_worthy': True,
                    'description': f"{best_modal} ë°ì´í„° í’ˆì§ˆì´ {worst_modal}ë³´ë‹¤ {quality_gap:.1%} ë†’ìŒ. ë¶„ì„ ì‹ ë¢°ë„ ê°œì„  ë°©ì•ˆ í•„ìš”.",
                    'confidence': 0.8,
                    'evidence': [
                        f"{best_modal} í‰ê·  ì‹ ë¢°ë„: {modal_averages[best_modal]:.3f}",
                        f"{worst_modal} í‰ê·  ì‹ ë¢°ë„: {modal_averages[worst_modal]:.3f}"
                    ]
                })
        
        return quality_analysis
    
    def _analyze_content_consistency(self, results: List[MultimodalResult]) -> Dict[str, Any]:
        """ì»¨í…ì¸  ì¼ê´€ì„± ë¶„ì„"""
        analysis = {
            'has_insights': False,
            'description': "",
            'confidence': 0.0,
            'evidence': []
        }
        
        if len(results) < 2:
            return analysis
        
        # ì»¨í…ì¸  ê¸¸ì´ ë¶„ì‚° ë¶„ì„
        content_lengths = [len(result.content) for result in results if result.content]
        
        if content_lengths and len(content_lengths) > 1:
            avg_length = statistics.mean(content_lengths)
            std_length = statistics.stdev(content_lengths)
            cv = std_length / avg_length if avg_length > 0 else 0  # ë³€ë™ê³„ìˆ˜
            
            if cv > 1.0:  # ë†’ì€ ë³€ë™ì„±
                analysis.update({
                    'has_insights': True,
                    'description': f"ì»¨í…ì¸  ê¸¸ì´ í¸ì°¨ê°€ í¼ (ë³€ë™ê³„ìˆ˜: {cv:.2f}). ì¼ë¶€ íŒŒì¼ì—ì„œ ì •ë³´ ì¶”ì¶œì´ ë¶€ì¡±í•  ê°€ëŠ¥ì„±.",
                    'confidence': 0.7,
                    'evidence': [
                        f"í‰ê·  ì»¨í…ì¸  ê¸¸ì´: {avg_length:.0f}ì",
                        f"í‘œì¤€í¸ì°¨: {std_length:.0f}ì",
                        f"ìµœëŒ€/ìµœì†Œ ë¹„ìœ¨: {max(content_lengths)/min(content_lengths):.1f}"
                    ]
                })
        
        return analysis
    
    def _analyze_processing_efficiency(self, results: List[MultimodalResult]) -> List[InsightItem]:
        """ì²˜ë¦¬ íš¨ìœ¨ì„± ë¶„ì„"""
        insights = []
        
        if not results:
            return insights
        
        # ì²˜ë¦¬ ì‹œê°„ ë¶„ì„
        processing_times = [result.processing_time for result in results]
        avg_time = statistics.mean(processing_times)
        
        # ë¹„íš¨ìœ¨ì ì¸ íŒŒì¼ ì‹ë³„
        slow_files = [
            result for result in results 
            if result.processing_time > avg_time * 2  # í‰ê· ì˜ 2ë°° ì´ìƒ
        ]
        
        if slow_files:
            insights.append(
                self._create_insight(
                    type="efficiency_analysis",
                    title="ì²˜ë¦¬ íš¨ìœ¨ì„± ê°œì„  ê¸°íšŒ",
                    description=f"{len(slow_files)}ê°œ íŒŒì¼ì˜ ì²˜ë¦¬ ì‹œê°„ì´ í‰ê· ë³´ë‹¤ {len(slow_files)/len(results):.1%} ê¸¸ìŒ. ìµœì í™” í•„ìš”.",
                    confidence=0.75,
                    evidence=[
                        f"í‰ê·  ì²˜ë¦¬ ì‹œê°„: {avg_time:.2f}ì´ˆ",
                        f"ìµœëŒ€ ì²˜ë¦¬ ì‹œê°„: {max(processing_times):.2f}ì´ˆ",
                        f"ë¹„íš¨ìœ¨ íŒŒì¼ ìˆ˜: {len(slow_files)}ê°œ"
                    ],
                    category="performance_optimization"
                )
            )
        
        return insights
    
    async def _generate_cross_modal_insights(self, cross_insights: Dict[str, Any]) -> List[InsightItem]:
        """í¬ë¡œìŠ¤ ëª¨ë‹¬ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        insights = []
        
        # ìƒê´€ê´€ê³„ ë¶„ì„
        correlation_pairs = cross_insights.get("high_correlation_pairs", [])
        if correlation_pairs:
            for pair in correlation_pairs[:3]:  # ìƒìœ„ 3ê°œ
                insights.append(
                    self._create_insight(
                        type="cross_modal_correlation",
                        title="ê°•í•œ í¬ë¡œìŠ¤ ëª¨ë‹¬ ìƒê´€ê´€ê³„ ë°œê²¬",
                        description=f"íŒŒì¼ ê°„ {pair['correlation_score']:.1%} ìƒê´€ê´€ê³„ íƒì§€. ì—°ê´€ëœ ë‚´ìš©ì¼ ê°€ëŠ¥ì„± ë†’ìŒ.",
                        confidence=pair['correlation_score'],
                        evidence=[f"ìƒê´€ê³„ìˆ˜: {pair['correlation_score']:.3f}"],
                        category="relationship_analysis"
                    )
                )
        
        # ì£¼ìš” í…Œë§ˆ ë¶„ì„
        themes = cross_insights.get("dominant_themes", [])
        if len(themes) >= 3:
            insights.append(
                self._create_insight(
                    type="theme_analysis",
                    title="ì£¼ìš” í…Œë§ˆ ì‹ë³„",
                    description=f"ë¶„ì„ ë°ì´í„°ì—ì„œ {len(themes)}ê°œ í•µì‹¬ í…Œë§ˆ ë°œê²¬: {', '.join(themes[:5])}",
                    confidence=0.8,
                    evidence=[f"í…Œë§ˆ ìˆ˜: {len(themes)}", f"ìƒìœ„ í…Œë§ˆ: {themes[:3]}"],
                    category="content_analysis"
                )
            )
        
        return insights
    
    async def _generate_ontology_insights(self) -> List[InsightItem]:
        """ì˜¨í†¨ë¡œì§€ ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        insights = []
        
        # ì˜¨í†¨ë¡œì§€ì—ì„œ ì¶”ë¡ ëœ ì¸ì‚¬ì´íŠ¸ ê°€ì ¸ì˜¤ê¸°
        ontology_insights = self.ontology.infer_insights()
        
        for ont_insight in ontology_insights[:5]:  # ìƒìœ„ 5ê°œ
            insights.append(
                self._create_insight(
                    type="ontology_inference",
                    title=f"ì§€ì‹ ê¸°ë°˜ ì¶”ë¡ : {ont_insight.get('type', 'ë¶„ì„')}",
                    description=ont_insight.get('description', ''),
                    confidence=ont_insight.get('confidence', 0.7),
                    evidence=ont_insight.get('evidence', []),
                    category=ont_insight.get('category', 'knowledge_inference')
                )
            )
        
        return insights
    
    async def _generate_context_insights(self, external_context: Dict[str, Any]) -> List[InsightItem]:
        """ì™¸ë¶€ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        insights = []
        
        # ì‹œê°„ ì»¨í…ìŠ¤íŠ¸ ë¶„ì„
        if 'timestamp' in external_context:
            time_insights = self._analyze_temporal_context(external_context['timestamp'])
            insights.extend(time_insights)
        
        # ì—…ê³„ ì»¨í…ìŠ¤íŠ¸ ë¶„ì„
        if 'industry_data' in external_context:
            industry_insights = self._analyze_industry_context(external_context['industry_data'])
            insights.extend(industry_insights)
        
        return insights
    
    def _analyze_temporal_context(self, timestamp: str) -> List[InsightItem]:
        """ì‹œê°„ì  ì»¨í…ìŠ¤íŠ¸ ë¶„ì„"""
        insights = []
        
        try:
            analysis_time = datetime.fromisoformat(timestamp)
            current_time = datetime.now()
            
            # ë¶„ì„ ì‹œì ì˜ íŠ¹ì„± íŒŒì•…
            hour = analysis_time.hour
            weekday = analysis_time.weekday()
            
            time_context = ""
            if hour < 9 or hour > 18:
                time_context = "ì—…ë¬´ ì‹œê°„ ì™¸"
            elif weekday >= 5:
                time_context = "ì£¼ë§"
            else:
                time_context = "ì—…ë¬´ ì‹œê°„ ì¤‘"
            
            if time_context in ["ì—…ë¬´ ì‹œê°„ ì™¸", "ì£¼ë§"]:
                insights.append(
                    self._create_insight(
                        type="temporal_context",
                        title="ë¶„ì„ ì‹œì  íŠ¹ì„±",
                        description=f"{time_context} ë¶„ì„ìœ¼ë¡œ, ê³µì‹ì  ì—…ë¬´ ê´€ë ¨ ë‚´ìš©ì¼ ê°€ëŠ¥ì„± ë‚®ìŒ",
                        confidence=0.6,
                        evidence=[f"ë¶„ì„ ì‹œì : {analysis_time.strftime('%Y-%m-%d %H:%M')}"],
                        category="context_analysis"
                    )
                )
                
        except Exception as e:
            logger.warning(f"ì‹œê°„ ì»¨í…ìŠ¤íŠ¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
        
        return insights
    
    def _analyze_industry_context(self, industry_data: Dict[str, Any]) -> List[InsightItem]:
        """ì—…ê³„ ì»¨í…ìŠ¤íŠ¸ ë¶„ì„"""
        insights = []
        
        # ì—…ê³„ íŠ¸ë Œë“œì™€ ë¶„ì„ ë‚´ìš© ë¹„êµ
        if 'trends' in industry_data:
            trends = industry_data['trends']
            
            insights.append(
                self._create_insight(
                    type="industry_alignment",
                    title="ì—…ê³„ íŠ¸ë Œë“œ ì •ë ¬ë„",
                    description=f"í˜„ì¬ ì—…ê³„ {len(trends)}ê°œ ì£¼ìš” íŠ¸ë Œë“œì™€ì˜ ì—°ê´€ì„± ë¶„ì„ í•„ìš”",
                    confidence=0.7,
                    evidence=[f"ì—…ê³„ íŠ¸ë Œë“œ: {', '.join(trends[:3])}"],
                    category="market_analysis"
                )
            )
        
        return insights
    
    def _filter_and_rank_insights(self, insights: List[InsightItem]) -> List[InsightItem]:
        """ì¸ì‚¬ì´íŠ¸ í’ˆì§ˆ í•„í„°ë§ ë° ë­í‚¹"""
        # ìµœì†Œ ì‹ ë¢°ë„ í•„í„°ë§
        filtered = [
            insight for insight in insights 
            if insight.confidence >= self.quality_thresholds['min_confidence']
        ]
        
        # ìš°ì„ ìˆœìœ„ ê³„ì‚°
        for insight in filtered:
            insight.priority = self._calculate_priority(insight)
        
        # ì‹ ë¢°ë„ì™€ ìš°ì„ ìˆœìœ„ ê¸°ì¤€ ì •ë ¬
        priority_weights = {"high": 3, "medium": 2, "low": 1}
        
        filtered.sort(
            key=lambda x: (priority_weights.get(x.priority, 0), x.confidence), 
            reverse=True
        )
        
        return filtered[:20]  # ìƒìœ„ 20ê°œë§Œ ìœ ì§€
    
    def _calculate_priority(self, insight: InsightItem) -> str:
        """ì¸ì‚¬ì´íŠ¸ ìš°ì„ ìˆœìœ„ ê³„ì‚°"""
        score = insight.confidence
        
        # ì¹´í…Œê³ ë¦¬ë³„ ê°€ì¤‘ì¹˜
        category_weights = {
            'risk_analysis': 0.3,
            'opportunity_analysis': 0.25,
            'anomaly_analysis': 0.2,
            'performance_optimization': 0.15,
            'trend_analysis': 0.1
        }
        
        score += category_weights.get(insight.category, 0)
        
        # ì¦ê±° ìˆ˜ì— ë”°ë¥¸ ë³´ì •
        score += min(0.1, len(insight.evidence) * 0.02)
        
        if score >= 0.9:
            return "high"
        elif score >= 0.7:
            return "medium"
        else:
            return "low"
    
    async def _add_actionable_suggestions(self, insights: List[InsightItem]) -> List[InsightItem]:
        """ì•¡ì…˜ ê°€ëŠ¥í•œ ì œì•ˆ ì¶”ê°€"""
        for insight in insights:
            suggestions = self._generate_action_suggestions(insight)
            insight.actionable_suggestions = suggestions
        
        return insights
    
    def _generate_action_suggestions(self, insight: InsightItem) -> List[str]:
        """ì¸ì‚¬ì´íŠ¸ë³„ ì•¡ì…˜ ì œì•ˆ ìƒì„±"""
        suggestions = []
        
        # ì¸ì‚¬ì´íŠ¸ íƒ€ì…ë³„ ì œì•ˆ
        if insight.type == "quality_analysis":
            suggestions.extend([
                "ì €í’ˆì§ˆ ë°ì´í„° ì†ŒìŠ¤ ê°œì„  ë°©ì•ˆ ê²€í† ",
                "ë°ì´í„° ì „ì²˜ë¦¬ í”„ë¡œì„¸ìŠ¤ ìµœì í™”",
                "í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ êµ¬ì¶•"
            ])
        
        elif insight.type == "efficiency_analysis":
            suggestions.extend([
                "ë¹„íš¨ìœ¨ì ì¸ íŒŒì¼ ì²˜ë¦¬ í”„ë¡œì„¸ìŠ¤ ìµœì í™”",
                "í•˜ë“œì›¨ì–´ ë¦¬ì†ŒìŠ¤ ì—…ê·¸ë ˆì´ë“œ ê²€í† ",
                "ë°°ì¹˜ ì²˜ë¦¬ í¬ê¸° ì¡°ì •"
            ])
        
        elif insight.type == "cross_modal_correlation":
            suggestions.extend([
                "ìƒê´€ê´€ê³„ ìˆëŠ” ë°ì´í„° í†µí•© ë¶„ì„",
                "ì—°ê´€ íŒ¨í„´ í™œìš©í•œ ì˜ˆì¸¡ ëª¨ë¸ êµ¬ì¶•",
                "ë°ì´í„° ìˆ˜ì§‘ ì „ëµ ê°œì„ "
            ])
        
        elif insight.type == "theme_analysis":
            suggestions.extend([
                "ì£¼ìš” í…Œë§ˆ ê¸°ë°˜ ì‹¬í™” ë¶„ì„ ìˆ˜í–‰",
                "í…Œë§ˆë³„ ì „ë¬¸ê°€ ë¦¬ë·° ìš”ì²­",
                "ê´€ë ¨ ì™¸ë¶€ ë°ì´í„° ìˆ˜ì§‘"
            ])
        
        # ì¹´í…Œê³ ë¦¬ë³„ ê³µí†µ ì œì•ˆ
        if insight.category == "risk_analysis":
            suggestions.append("ë¦¬ìŠ¤í¬ ì™„í™” ê³„íš ìˆ˜ë¦½ ë° ì‹¤í–‰")
        
        elif insight.category == "opportunity_analysis":
            suggestions.append("ê¸°íšŒ í™œìš© ì „ëµ ê°œë°œ ë° ìš°ì„ ìˆœìœ„ ì„¤ì •")
        
        return suggestions[:3]  # ìµœëŒ€ 3ê°œ ì œì•ˆ
    
    def _deduplicate_insights(self, insights: List[InsightItem]) -> List[InsightItem]:
        """ì¤‘ë³µ ì¸ì‚¬ì´íŠ¸ ì œê±°"""
        unique_insights = []
        seen_signatures = set()
        
        for insight in insights:
            # ì¸ì‚¬ì´íŠ¸ ì„œëª… ìƒì„± (ì œëª© + ì¹´í…Œê³ ë¦¬ ê¸°ë°˜)
            signature = f"{insight.title}_{insight.category}"
            
            if signature not in seen_signatures:
                unique_insights.append(insight)
                seen_signatures.add(signature)
        
        return unique_insights
    
    def _create_insight(
        self,
        type: str,
        title: str, 
        description: str,
        confidence: float,
        evidence: List[str],
        category: str,
        validity_hours: int = 24
    ) -> InsightItem:
        """ì¸ì‚¬ì´íŠ¸ ì•„ì´í…œ ìƒì„± í—¬í¼"""
        
        insight_id = f"{type}_{datetime.now().timestamp()}"
        validity_period = (datetime.now() + timedelta(hours=validity_hours)).isoformat()
        
        return InsightItem(
            id=insight_id,
            type=type,
            title=title,
            description=description,
            confidence=min(1.0, max(0.0, confidence)),  # 0-1 ë²”ìœ„ë¡œ í´ë¨í•‘
            priority="medium",  # ê¸°ë³¸ê°’, ë‚˜ì¤‘ì— ê³„ì‚°ë¨
            category=category,
            evidence=evidence,
            actionable_suggestions=[],  # ë‚˜ì¤‘ì— ì¶”ê°€ë¨
            related_data={},
            generated_at=datetime.now().isoformat(),
            validity_period=validity_period
        )
    
    def _update_performance_stats(self, insight_count: int, generation_time: float) -> None:
        """ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸"""
        self.performance_stats['total_insights_generated'] += insight_count
        self.performance_stats['high_confidence_insights'] += sum(
            1 for insight in self.insights_history[-insight_count:] 
            if insight.confidence >= self.quality_thresholds['high_confidence']
        )
        self.performance_stats['actionable_insights'] += sum(
            1 for insight in self.insights_history[-insight_count:] 
            if insight.actionable_suggestions
        )
        
        # í‰ê·  ìƒì„± ì‹œê°„ ì—…ë°ì´íŠ¸ (ì´ë™ í‰ê· )
        current_avg = self.performance_stats['average_generation_time']
        total_runs = self.performance_stats['total_insights_generated'] // insight_count
        
        if total_runs == 1:
            self.performance_stats['average_generation_time'] = generation_time
        else:
            # ì§€ìˆ˜ ì´ë™ í‰ê· 
            alpha = 0.1
            self.performance_stats['average_generation_time'] = (
                alpha * generation_time + (1 - alpha) * current_avg
            )
    
    def get_insights_summary(self) -> Dict[str, Any]:
        """ì¸ì‚¬ì´íŠ¸ ìƒì„± ìš”ì•½"""
        if not self.insights_history:
            return {"message": "ìƒì„±ëœ ì¸ì‚¬ì´íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤."}
        
        # ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬
        category_dist = Counter(insight.category for insight in self.insights_history)
        
        # ìš°ì„ ìˆœìœ„ë³„ ë¶„í¬
        priority_dist = Counter(insight.priority for insight in self.insights_history)
        
        # ìµœê·¼ ìƒì„±ëœ ì¸ì‚¬ì´íŠ¸ë“¤
        recent_insights = [
            insight for insight in self.insights_history 
            if (datetime.now() - datetime.fromisoformat(insight.generated_at)).days <= 1
        ]
        
        return {
            'total_insights': len(self.insights_history),
            'recent_insights': len(recent_insights),
            'category_distribution': dict(category_dist),
            'priority_distribution': dict(priority_dist),
            'average_confidence': statistics.mean(
                insight.confidence for insight in self.insights_history
            ),
            'performance_stats': self.performance_stats,
            'ontology_summary': self.ontology.get_knowledge_summary()
        }

# ì‚¬ìš© ì˜ˆì œ
async def main():
    """ì‚¬ìš© ì˜ˆì œ"""
    generator = InsightGenerator("conference_analysis")
    
    # ìƒ˜í”Œ ë©€í‹°ëª¨ë‹¬ ê²°ê³¼
    from multimodal_pipeline import MultimodalResult
    sample_results = [
        MultimodalResult(
            file_path="sample1.wav",
            file_type="audio",
            content="ì£¼ì–¼ë¦¬ ì‹œì¥ì˜ ë””ì§€í„¸ ì „í™˜ì´ ê°€ì†í™”ë˜ê³  ìˆìŠµë‹ˆë‹¤",
            confidence=0.85,
            processing_time=2.3,
            metadata={"duration": 45.2}
        ),
        MultimodalResult(
            file_path="sample2.png", 
            file_type="image",
            content="Digital Transformation Jewelry Market Growth 2024",
            confidence=0.92,
            processing_time=1.8,
            metadata={"image_size": (1920, 1080)}
        )
    ]
    
    # ìƒ˜í”Œ í¬ë¡œìŠ¤ ëª¨ë‹¬ ì¸ì‚¬ì´íŠ¸
    cross_insights = {
        "high_correlation_pairs": [
            {
                "primary_file": "sample1.wav",
                "correlation_score": 0.87,
                "related_files": [{"file": "sample2.png", "similarity": 0.87}]
            }
        ],
        "dominant_themes": ["ë””ì§€í„¸ì „í™˜", "ì£¼ì–¼ë¦¬", "ì‹œì¥ì„±ì¥", "ê¸°ìˆ í˜ì‹ "]
    }
    
    # ì¢…í•© ì¸ì‚¬ì´íŠ¸ ìƒì„±
    insights = await generator.generate_comprehensive_insights(
        sample_results, 
        cross_insights
    )
    
    print("ğŸ’¡ ìƒì„±ëœ ì¸ì‚¬ì´íŠ¸:")
    for i, insight in enumerate(insights[:5], 1):
        print(f"\n{i}. [{insight.priority.upper()}] {insight.title}")
        print(f"   ì„¤ëª…: {insight.description}")
        print(f"   ì‹ ë¢°ë„: {insight.confidence:.2f}")
        print(f"   ì œì•ˆ: {'; '.join(insight.actionable_suggestions[:2])}")
    
    print(f"\nğŸ“Š ìš”ì•½: {generator.get_insights_summary()}")

if __name__ == "__main__":
    asyncio.run(main())