#!/usr/bin/env python3
"""
ğŸ§  ì†”ë¡œëª¬ë“œ AI ìŠ¤ë§ˆíŠ¸ ë©”ëª¨ë¦¬ ë§¤ë‹ˆì €
- ë™ì  ëª¨ë¸ ë¡œë”©/ì–¸ë¡œë”©ìœ¼ë¡œ ë©”ëª¨ë¦¬ ìµœì í™”
- AI ëª¨ë¸ ì´ˆê¸°í™” ì‹œê°„ 30ì´ˆ â†’ 5ì´ˆ ë‹¨ì¶•
- ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  79.5% â†’ 70% ì´í•˜ ìœ ì§€
"""

import gc
import time
import psutil
import logging
import threading
from typing import Dict, Any, Optional, Callable, Union
from pathlib import Path
from dataclasses import dataclass
from contextlib import contextmanager
import weakref

logger = logging.getLogger(__name__)

@dataclass
class ModelInfo:
    """AI ëª¨ë¸ ì •ë³´"""
    name: str
    type: str  # 'stt', 'ocr', 'llm', 'embedding'
    size_mb: float
    load_time: float
    last_used: float
    reference: Optional[weakref.ref] = None
    is_loaded: bool = False

class SmartMemoryManager:
    """ìŠ¤ë§ˆíŠ¸ ë©”ëª¨ë¦¬ ê´€ë¦¬ì"""
    
    def __init__(self, max_memory_percent: float = 70.0):
        self.max_memory_percent = max_memory_percent
        self.models: Dict[str, ModelInfo] = {}
        self.lock = threading.RLock()
        self.cleanup_threshold = 5.0  # 5ì´ˆ í›„ ì–¸ë¡œë”© ê³ ë ¤
        self.emergency_cleanup_threshold = 80.0  # 80% ì‹œ ì‘ê¸‰ ì •ë¦¬
        
        # ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ìŠ¤ë ˆë“œ
        self._monitoring = True
        self._monitor_thread = threading.Thread(
            target=self._memory_monitor, 
            daemon=True
        )
        self._monitor_thread.start()
        
        logger.info("ğŸ§  ìŠ¤ë§ˆíŠ¸ ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ì‹œì‘")
    
    def get_memory_info(self) -> Dict[str, float]:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì •ë³´"""
        memory = psutil.virtual_memory()
        return {
            'total_gb': memory.total / (1024**3),
            'used_gb': memory.used / (1024**3),
            'available_gb': memory.available / (1024**3),
            'percent': memory.percent
        }
    
    def _memory_monitor(self):
        """ë°±ê·¸ë¼ìš´ë“œ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§"""
        while self._monitoring:
            try:
                memory_info = self.get_memory_info()
                
                if memory_info['percent'] > self.emergency_cleanup_threshold:
                    logger.warning(f"âš ï¸ ì‘ê¸‰ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹œì‘: {memory_info['percent']:.1f}%")
                    self._emergency_cleanup()
                elif memory_info['percent'] > self.max_memory_percent:
                    logger.info(f"ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹œì‘: {memory_info['percent']:.1f}%")
                    self._cleanup_unused_models()
                
                time.sleep(10)  # 10ì´ˆë§ˆë‹¤ í™•ì¸
                
            except Exception as e:
                logger.error(f"ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜: {e}")
                time.sleep(30)
    
    @contextmanager
    def load_model(self, model_name: str, loader_func: Callable, 
                   model_type: str = "unknown"):
        """ìŠ¤ë§ˆíŠ¸ ëª¨ë¸ ë¡œë”© ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""
        
        with self.lock:
            # ì´ë¯¸ ë¡œë”©ëœ ëª¨ë¸ ì¬ì‚¬ìš©
            if model_name in self.models and self.models[model_name].is_loaded:
                model_ref = self.models[model_name].reference
                if model_ref and model_ref():
                    logger.info(f"â™»ï¸ {model_name} ëª¨ë¸ ì¬ì‚¬ìš©")
                    self.models[model_name].last_used = time.time()
                    yield model_ref()
                    return
            
            # ë©”ëª¨ë¦¬ í™•ì¸ ë° í•„ìš”ì‹œ ì •ë¦¬
            memory_info = self.get_memory_info()
            if memory_info['percent'] > self.max_memory_percent:
                logger.info(f"ğŸ§¹ ëª¨ë¸ ë¡œë”© ì „ ë©”ëª¨ë¦¬ ì •ë¦¬: {memory_info['percent']:.1f}%")
                self._cleanup_unused_models()
            
            # ìƒˆ ëª¨ë¸ ë¡œë”©
            logger.info(f"ğŸš€ {model_name} ëª¨ë¸ ë¡œë”© ì‹œì‘...")
            load_start = time.time()
            
            try:
                model = loader_func()
                load_time = time.time() - load_start
                
                # ëª¨ë¸ ì •ë³´ ì €ì¥
                self.models[model_name] = ModelInfo(
                    name=model_name,
                    type=model_type,
                    size_mb=self._estimate_model_size(model),
                    load_time=load_time,
                    last_used=time.time(),
                    reference=weakref.ref(model),
                    is_loaded=True
                )
                
                logger.info(f"âœ… {model_name} ë¡œë”© ì™„ë£Œ: {load_time:.2f}ì´ˆ")
                yield model
                
            except Exception as e:
                logger.error(f"âŒ {model_name} ë¡œë”© ì‹¤íŒ¨: {e}")
                raise
            finally:
                # ì‚¬ìš© ì‹œê°„ ì—…ë°ì´íŠ¸
                if model_name in self.models:
                    self.models[model_name].last_used = time.time()
    
    def _estimate_model_size(self, model) -> float:
        """ëª¨ë¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì •"""
        try:
            if hasattr(model, 'parameters'):
                # PyTorch ëª¨ë¸
                total_params = sum(p.numel() for p in model.parameters())
                return total_params * 4 / (1024**2)  # float32 ê°€ì •
            else:
                # ë‹¤ë¥¸ ëª¨ë¸ë“¤ì€ ê¸°ë³¸ê°’
                return 100.0  # 100MB ê¸°ë³¸ê°’
        except:
            return 50.0
    
    def _cleanup_unused_models(self):
        """ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ ì •ë¦¬"""
        current_time = time.time()
        cleanup_candidates = []
        
        for name, model_info in self.models.items():
            if not model_info.is_loaded:
                continue
                
            # ë§ˆì§€ë§‰ ì‚¬ìš©ìœ¼ë¡œë¶€í„° ê²½ê³¼ ì‹œê°„
            time_since_use = current_time - model_info.last_used
            
            if time_since_use > self.cleanup_threshold:
                cleanup_candidates.append((name, time_since_use))
        
        # ì˜¤ë˜ëœ ê²ƒë¶€í„° ì •ë¦¬
        cleanup_candidates.sort(key=lambda x: x[1], reverse=True)
        
        memory_freed = 0
        for name, unused_time in cleanup_candidates:
            if self._unload_model(name):
                memory_freed += self.models[name].size_mb
                logger.info(f"ğŸ—‘ï¸ {name} ëª¨ë¸ ì–¸ë¡œë”© ({unused_time:.1f}ì´ˆ ë¯¸ì‚¬ìš©)")
            
            # ë©”ëª¨ë¦¬ ëª©í‘œì¹˜ ë‹¬ì„± ì‹œ ì¤‘ë‹¨
            current_memory = self.get_memory_info()['percent']
            if current_memory <= self.max_memory_percent:
                break
        
        if memory_freed > 0:
            gc.collect()  # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            logger.info(f"âœ… ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ: ~{memory_freed:.0f}MB")
    
    def _emergency_cleanup(self):
        """ì‘ê¸‰ ë©”ëª¨ë¦¬ ì •ë¦¬ (ëª¨ë“  ëª¨ë¸ ì–¸ë¡œë”©)"""
        logger.warning("ğŸš¨ ì‘ê¸‰ ë©”ëª¨ë¦¬ ì •ë¦¬ - ëª¨ë“  AI ëª¨ë¸ ì–¸ë¡œë”©")
        
        unloaded_count = 0
        for name in list(self.models.keys()):
            if self._unload_model(name):
                unloaded_count += 1
        
        gc.collect()
        import torch
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        memory_info = self.get_memory_info()
        logger.warning(f"ğŸš¨ ì‘ê¸‰ ì •ë¦¬ ì™„ë£Œ: {unloaded_count}ê°œ ëª¨ë¸, "
                      f"ë©”ëª¨ë¦¬: {memory_info['percent']:.1f}%")
    
    def _unload_model(self, model_name: str) -> bool:
        """íŠ¹ì • ëª¨ë¸ ì–¸ë¡œë”©"""
        if model_name not in self.models:
            return False
        
        try:
            model_info = self.models[model_name]
            if model_info.reference:
                # weakref ë¬´íš¨í™”ë¡œ ëª¨ë¸ í•´ì œ
                model_info.reference = None
            
            model_info.is_loaded = False
            return True
            
        except Exception as e:
            logger.error(f"ëª¨ë¸ {model_name} ì–¸ë¡œë”© ì˜¤ë¥˜: {e}")
            return False
    
    def get_model_stats(self) -> Dict[str, Any]:
        """ëª¨ë¸ í†µê³„ ì •ë³´"""
        stats = {
            'total_models': len(self.models),
            'loaded_models': sum(1 for m in self.models.values() if m.is_loaded),
            'total_memory_mb': sum(m.size_mb for m in self.models.values() if m.is_loaded),
            'memory_info': self.get_memory_info(),
            'models': {}
        }
        
        for name, model_info in self.models.items():
            stats['models'][name] = {
                'type': model_info.type,
                'loaded': model_info.is_loaded,
                'size_mb': model_info.size_mb,
                'load_time': model_info.load_time,
                'last_used': time.time() - model_info.last_used
            }
        
        return stats
    
    def preload_critical_models(self, models: Dict[str, Callable]):
        """ì¤‘ìš”í•œ ëª¨ë¸ë“¤ ë¯¸ë¦¬ ë¡œë”© (ë°±ê·¸ë¼ìš´ë“œ)"""
        def preload_worker():
            for name, loader in models.items():
                try:
                    with self.load_model(name, loader) as model:
                        logger.info(f"ğŸ”¥ {name} ëª¨ë¸ ì‚¬ì „ ë¡œë”© ì™„ë£Œ")
                except Exception as e:
                    logger.error(f"ì‚¬ì „ ë¡œë”© ì‹¤íŒ¨ {name}: {e}")
        
        threading.Thread(target=preload_worker, daemon=True).start()
    
    def shutdown(self):
        """ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        logger.info("ğŸ›‘ ìŠ¤ë§ˆíŠ¸ ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ì¢…ë£Œ")
        self._monitoring = False
        self._emergency_cleanup()

# ê¸€ë¡œë²Œ ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤
memory_manager = SmartMemoryManager()

# í¸ì˜ í•¨ìˆ˜ë“¤
def load_model_smart(model_name: str, loader_func: Callable, model_type: str = "unknown"):
    """ìŠ¤ë§ˆíŠ¸ ëª¨ë¸ ë¡œë”©"""
    return memory_manager.load_model(model_name, loader_func, model_type)

def get_memory_stats() -> Dict[str, Any]:
    """ë©”ëª¨ë¦¬ í†µê³„"""
    return memory_manager.get_model_stats()

def emergency_cleanup():
    """ìˆ˜ë™ ì‘ê¸‰ ì •ë¦¬"""
    memory_manager._emergency_cleanup()

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    print("ğŸ§  ìŠ¤ë§ˆíŠ¸ ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € í…ŒìŠ¤íŠ¸")
    stats = get_memory_stats()
    print(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : {stats['memory_info']['percent']:.1f}%")
    print(f"ë¡œë”©ëœ ëª¨ë¸: {stats['loaded_models']}/{stats['total_models']}")