"""
ì†”ë¡œëª¬ë“œ AI ì‹œìŠ¤í…œ - í•œêµ­ì–´ ìµœì¢… í†µí•© ìš”ì•½ ì—”ì§„
ë‹¤êµ­ì–´ ì½˜í…ì¸ ë¥¼ í•œêµ­ì–´ë¡œ í†µí•© ë²ˆì—­ ë° ì£¼ì–¼ë¦¬ íŠ¹í™” ìš”ì•½ ìƒì„± ëª¨ë“ˆ
"""

import asyncio
from typing import Dict, List, Optional, Tuple, Any, Union
import logging
import json
import re
from datetime import datetime
from collections import defaultdict, Counter
import openai
from pathlib import Path

# ë²ˆì—­ ë° ì–¸ì–´ ê°ì§€ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from langdetect import detect, LangDetectError
    LANGDETECT_AVAILABLE = True
except ImportError:
    LANGDETECT_AVAILABLE = False

try:
    from googletrans import Translator
    GOOGLETRANS_AVAILABLE = True
except ImportError:
    GOOGLETRANS_AVAILABLE = False

# ì£¼ì–¼ë¦¬ AI ì—”ì§„ import
from .jewelry_ai_engine import JewelryAIEngine

class KoreanFinalSummarizer:
    """ë‹¤êµ­ì–´ â†’ í•œêµ­ì–´ í†µí•© ìš”ì•½ ìƒì„± í´ë˜ìŠ¤"""
    
    def __init__(self):
        # ì–¸ì–´ ì½”ë“œ ë§¤í•‘
        self.language_mapping = {
            "ko": "í•œêµ­ì–´",
            "en": "ì˜ì–´", 
            "zh": "ì¤‘êµ­ì–´",
            "zh-cn": "ì¤‘êµ­ì–´(ê°„ì²´)",
            "zh-tw": "ì¤‘êµ­ì–´(ë²ˆì²´)",
            "ja": "ì¼ë³¸ì–´",
            "es": "ìŠ¤í˜ì¸ì–´",
            "fr": "í”„ë‘ìŠ¤ì–´",
            "de": "ë…ì¼ì–´",
            "ru": "ëŸ¬ì‹œì•„ì–´"
        }
        
        # ì£¼ì–¼ë¦¬ ì „ë¬¸ ìš©ì–´ í•œêµ­ì–´ ë§¤í•‘
        self.jewelry_term_mapping = {
            # ì˜ì–´ â†’ í•œêµ­ì–´
            "diamond": "ë‹¤ì´ì•„ëª¬ë“œ",
            "ruby": "ë£¨ë¹„",
            "sapphire": "ì‚¬íŒŒì´ì–´", 
            "emerald": "ì—ë©”ë„ë“œ",
            "pearl": "ì§„ì£¼",
            "gold": "ê¸ˆ",
            "silver": "ì€",
            "platinum": "ë°±ê¸ˆ",
            "carat": "ìºëŸ¿",
            "clarity": "íˆ¬ëª…ë„",
            "color": "ì»¬ëŸ¬",
            "cut": "ì»·",
            "certification": "ê°ì •ì„œ",
            "GIA": "GIA",
            "AGS": "AGS",
            "setting": "ì„¸íŒ…",
            "mounting": "ë§ˆìš´íŒ…",
            "prong": "í”„ë¡±",
            "bezel": "ë² ì ¤",
            "wholesale": "ë„ë§¤",
            "retail": "ì†Œë§¤",
            "appraisal": "ê°ì •",
            "gemstone": "ë³´ì„",
            "jewelry": "ì£¼ì–¼ë¦¬",
            "ring": "ë°˜ì§€",
            "necklace": "ëª©ê±¸ì´",
            "earring": "ê·€ê±¸ì´",
            "bracelet": "íŒ”ì°Œ",
            "pendant": "íœë˜íŠ¸",
            
            # ì¤‘êµ­ì–´ â†’ í•œêµ­ì–´ (ê°„ì²´)
            "é’»çŸ³": "ë‹¤ì´ì•„ëª¬ë“œ",
            "çº¢å®çŸ³": "ë£¨ë¹„",
            "è“å®çŸ³": "ì‚¬íŒŒì´ì–´",
            "ç¥–æ¯ç»¿": "ì—ë©”ë„ë“œ",
            "çç ": "ì§„ì£¼",
            "é»„é‡‘": "ê¸ˆ",
            "ç™½é“¶": "ì€",
            "é“‚é‡‘": "ë°±ê¸ˆ",
            "å…‹æ‹‰": "ìºëŸ¿",
            "å‡€åº¦": "íˆ¬ëª…ë„",
            "é¢œè‰²": "ì»¬ëŸ¬",
            "åˆ‡å·¥": "ì»·",
            "è¯ä¹¦": "ê°ì •ì„œ",
            "ç å®": "ì£¼ì–¼ë¦¬",
            "æˆ’æŒ‡": "ë°˜ì§€",
            "é¡¹é“¾": "ëª©ê±¸ì´",
            "è€³ç¯": "ê·€ê±¸ì´",
            
            # ì¼ë³¸ì–´ â†’ í•œêµ­ì–´
            "ãƒ€ã‚¤ãƒ¤ãƒ¢ãƒ³ãƒ‰": "ë‹¤ì´ì•„ëª¬ë“œ",
            "ãƒ«ãƒ“ãƒ¼": "ë£¨ë¹„",
            "ã‚µãƒ•ã‚¡ã‚¤ã‚¢": "ì‚¬íŒŒì´ì–´",
            "ã‚¨ãƒ¡ãƒ©ãƒ«ãƒ‰": "ì—ë©”ë„ë“œ",
            "çœŸç ": "ì§„ì£¼",
            "é‡‘": "ê¸ˆ",
            "éŠ€": "ì€",
            "ãƒ—ãƒ©ãƒãƒŠ": "ë°±ê¸ˆ",
            "ã‚«ãƒ©ãƒƒãƒˆ": "ìºëŸ¿",
            "é€æ˜åº¦": "íˆ¬ëª…ë„",
            "ã‚«ãƒ©ãƒ¼": "ì»¬ëŸ¬",
            "ã‚«ãƒƒãƒˆ": "ì»·",
            "é‘‘å®šæ›¸": "ê°ì •ì„œ",
            "ã‚¸ãƒ¥ã‚¨ãƒªãƒ¼": "ì£¼ì–¼ë¦¬",
            "æŒ‡è¼ª": "ë°˜ì§€",
            "ãƒãƒƒã‚¯ãƒ¬ã‚¹": "ëª©ê±¸ì´",
            "ã‚¤ãƒ¤ãƒªãƒ³ã‚°": "ê·€ê±¸ì´"
        }
        
        # ë²ˆì—­ê¸° ì´ˆê¸°í™”
        self.translator = None
        if GOOGLETRANS_AVAILABLE:
            try:
                self.translator = Translator()
            except Exception as e:
                logging.warning(f"Google Translator ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # ì£¼ì–¼ë¦¬ AI ì—”ì§„
        self.jewelry_ai = JewelryAIEngine()
        
        # ìš”ì•½ ìŠ¤íƒ€ì¼ í…œí”Œë¦¿
        self.summary_styles = {
            "comprehensive": {
                "title": "ì¢…í•© ë¶„ì„ ìš”ì•½",
                "sections": ["í•µì‹¬ ë‚´ìš©", "ì£¼ìš” ë…¼ì ", "ê²°ë¡  ë° ì‹œì‚¬ì ", "ì‹¤í–‰ ë°©ì•ˆ"],
                "tone": "ì „ë¬¸ì ì´ê³  ìƒì„¸í•œ"
            },
            "executive": {
                "title": "ê²½ì˜ì§„ ìš”ì•½",
                "sections": ["í•µì‹¬ ë©”ì‹œì§€", "ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸", "ì˜ì‚¬ê²°ì • í¬ì¸íŠ¸", "ë‹¤ìŒ ë‹¨ê³„"],
                "tone": "ê°„ê²°í•˜ê³  í•µì‹¬ì ì¸"
            },
            "technical": {
                "title": "ê¸°ìˆ ì  ìš”ì•½", 
                "sections": ["ê¸°ìˆ ì  ë‚´ìš©", "ì „ë¬¸ ìš©ì–´ í•´ì„¤", "ê¸°ìˆ ì  ì‹œì‚¬ì ", "ì ìš© ë°©ì•ˆ"],
                "tone": "ê¸°ìˆ ì ì´ê³  ì „ë¬¸ì ì¸"
            },
            "business": {
                "title": "ë¹„ì¦ˆë‹ˆìŠ¤ ìš”ì•½",
                "sections": ["ì‹œì¥ ë™í–¥", "ë¹„ì¦ˆë‹ˆìŠ¤ ê¸°íšŒ", "ìœ„í—˜ ìš”ì†Œ", "ì „ëµì  ì œì•ˆ"],
                "tone": "ë¹„ì¦ˆë‹ˆìŠ¤ ì¤‘ì‹¬ì˜ ì‹¤ìš©ì ì¸"
            }
        }
        
        logging.info("í•œêµ­ì–´ ìµœì¢… í†µí•© ìš”ì•½ ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def process_multilingual_session(self, 
                                         session_data: Dict,
                                         summary_style: str = "comprehensive",
                                         preserve_original: bool = True) -> Dict:
        """
        ë‹¤êµ­ì–´ ì„¸ì…˜ ë°ì´í„°ë¥¼ í•œêµ­ì–´ë¡œ í†µí•© ìš”ì•½
        
        Args:
            session_data: ë‹¤êµ­ì–´ ì„¸ì…˜ ë°ì´í„°
            summary_style: ìš”ì•½ ìŠ¤íƒ€ì¼ ("comprehensive", "executive", "technical", "business")
            preserve_original: ì›ë¬¸ ë³´ì¡´ ì—¬ë¶€
            
        Returns:
            í•œêµ­ì–´ í†µí•© ìš”ì•½ ê²°ê³¼
        """
        try:
            print(f"ğŸ‡°ğŸ‡· ë‹¤êµ­ì–´ â†’ í•œêµ­ì–´ í†µí•© ìš”ì•½ ì‹œì‘")
            
            # 1. ë‹¤êµ­ì–´ ì½˜í…ì¸  ìˆ˜ì§‘ ë° ë¶„ì„
            multilingual_content = await self._collect_multilingual_content(session_data)
            
            # 2. ì–¸ì–´ë³„ ì½˜í…ì¸  ë²ˆì—­
            translated_content = await self._translate_to_korean(multilingual_content)
            
            # 3. ì£¼ì–¼ë¦¬ ìš©ì–´ ì •ê·œí™”
            normalized_content = await self._normalize_jewelry_terms(translated_content)
            
            # 4. ë‚´ìš© í†µí•© ë° ì¤‘ë³µ ì œê±°
            integrated_content = await self._integrate_content(normalized_content)
            
            # 5. í•œêµ­ì–´ ìš”ì•½ ìƒì„±
            korean_summary = await self._generate_korean_summary(
                integrated_content, 
                summary_style,
                session_data.get("session_type", "meeting")
            )
            
            # 6. í’ˆì§ˆ í‰ê°€ ë° í›„ì²˜ë¦¬
            quality_assessment = await self._assess_summary_quality(korean_summary, integrated_content)
            
            # 7. ìµœì¢… ê²°ê³¼ êµ¬ì„±
            result = {
                "success": True,
                "session_info": {
                    "session_id": session_data.get("session_id", "unknown"),
                    "session_type": session_data.get("session_type", "meeting"),
                    "processed_languages": list(multilingual_content.keys()),
                    "total_sources": sum(len(sources) for sources in multilingual_content.values())
                },
                "translation_analysis": {
                    "detected_languages": {lang: len(sources) for lang, sources in multilingual_content.items()},
                    "translation_quality": self._calculate_translation_quality(translated_content),
                    "jewelry_terms_mapped": len(self._get_mapped_terms(normalized_content))
                },
                "korean_summary": korean_summary,
                "quality_assessment": quality_assessment,
                "processing_metadata": {
                    "summary_style": summary_style,
                    "preserve_original": preserve_original,
                    "generated_at": datetime.now().isoformat(),
                    "processing_time": "ì‹¤ì‹œê°„ ê³„ì‚°ë¨"
                }
            }
            
            # ì›ë¬¸ ë³´ì¡´ ì˜µì…˜
            if preserve_original:
                result["original_content"] = {
                    "multilingual_content": multilingual_content,
                    "translated_content": translated_content,
                    "normalized_content": normalized_content
                }
            
            print(f"âœ… í•œêµ­ì–´ í†µí•© ìš”ì•½ ì™„ë£Œ: {len(korean_summary.get('content', ''))}ì")
            return result
            
        except Exception as e:
            logging.error(f"ë‹¤êµ­ì–´ â†’ í•œêµ­ì–´ í†µí•© ìš”ì•½ ì˜¤ë¥˜: {e}")
            return {
                "success": False,
                "error": str(e),
                "session_id": session_data.get("session_id", "unknown")
            }
    
    async def _collect_multilingual_content(self, session_data: Dict) -> Dict[str, List[Dict]]:
        """ë‹¤êµ­ì–´ ì½˜í…ì¸  ìˆ˜ì§‘ ë° ì–¸ì–´ë³„ ë¶„ë¥˜"""
        try:
            multilingual_content = defaultdict(list)
            
            # ê° ì†ŒìŠ¤ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            sources = [
                ("audio", session_data.get("audio_files", [])),
                ("video", session_data.get("video_files", [])), 
                ("documents", session_data.get("document_files", [])),
                ("web", session_data.get("web_urls", []))
            ]
            
            for source_type, source_list in sources:
                for item in source_list:
                    # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    text_content = self._extract_text_from_source(item, source_type)
                    
                    if text_content and len(text_content.strip()) > 10:
                        # ì–¸ì–´ ê°ì§€
                        detected_lang = await self._detect_language(text_content)
                        
                        multilingual_content[detected_lang].append({
                            "source_type": source_type,
                            "content": text_content,
                            "confidence": self._estimate_detection_confidence(text_content, detected_lang),
                            "word_count": len(text_content.split()),
                            "source_info": item.get("filename", item.get("url", "unknown"))
                        })
            
            return dict(multilingual_content)
            
        except Exception as e:
            logging.error(f"ë‹¤êµ­ì–´ ì½˜í…ì¸  ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            return {}
    
    def _extract_text_from_source(self, item: Dict, source_type: str) -> str:
        """ì†ŒìŠ¤ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            if source_type == "audio":
                return item.get("enhanced_text", item.get("text", ""))
            elif source_type == "video":
                return item.get("enhanced_text", item.get("text", ""))
            elif source_type == "documents":
                return item.get("text", item.get("content", ""))
            elif source_type == "web":
                return item.get("content", item.get("text", ""))
            else:
                return item.get("text", item.get("content", ""))
        except Exception as e:
            logging.error(f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì˜¤ë¥˜ ({source_type}): {e}")
            return ""
    
    async def _detect_language(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì–¸ì–´ ê°ì§€"""
        try:
            if not LANGDETECT_AVAILABLE:
                # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹± ë°©ë²•
                return self._simple_language_detection(text)
            
            # langdetect ì‚¬ìš©
            detected = detect(text)
            
            # ì‹ ë¢°ë„ê°€ ë‚®ê±°ë‚˜ ê°ì§€ ì‹¤íŒ¨ ì‹œ íœ´ë¦¬ìŠ¤í‹± ì‚¬ìš©
            if detected == "ca" or len(text) < 50:  # ì¹´íƒˆë¡œë‹ˆì•„ì–´ë¡œ ì˜ëª» ê°ì§€ë˜ëŠ” ê²½ìš°ê°€ ë§ìŒ
                return self._simple_language_detection(text)
            
            return detected
            
        except (LangDetectError, Exception) as e:
            logging.warning(f"ì–¸ì–´ ê°ì§€ ì‹¤íŒ¨: {e}")
            return self._simple_language_detection(text)
    
    def _simple_language_detection(self, text: str) -> str:
        """ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹± ì–¸ì–´ ê°ì§€"""
        try:
            # í•œê¸€ ì²´í¬
            korean_chars = re.findall(r'[ã„±-ã…ê°€-í£]', text)
            if len(korean_chars) > len(text) * 0.3:
                return "ko"
            
            # ì¤‘êµ­ì–´ ì²´í¬ (í•œì)
            chinese_chars = re.findall(r'[\u4e00-\u9fff]', text)
            if len(chinese_chars) > len(text) * 0.3:
                return "zh"
            
            # ì¼ë³¸ì–´ ì²´í¬ (íˆë¼ê°€ë‚˜, ì¹´íƒ€ì¹´ë‚˜)
            japanese_chars = re.findall(r'[\u3040-\u309f\u30a0-\u30ff]', text)
            if len(japanese_chars) > len(text) * 0.2:
                return "ja"
            
            # ì˜ì–´ë¡œ ê¸°ë³¸ ì„¤ì •
            return "en"
            
        except Exception as e:
            logging.error(f"ê°„ë‹¨í•œ ì–¸ì–´ ê°ì§€ ì˜¤ë¥˜: {e}")
            return "en"
    
    def _estimate_detection_confidence(self, text: str, detected_lang: str) -> float:
        """ì–¸ì–´ ê°ì§€ ì‹ ë¢°ë„ ì¶”ì •"""
        try:
            text_length = len(text)
            
            # í…ìŠ¤íŠ¸ ê¸¸ì´ ê¸°ë°˜ ì‹ ë¢°ë„
            if text_length < 20:
                length_confidence = 0.3
            elif text_length < 100:
                length_confidence = 0.7
            else:
                length_confidence = 0.9
            
            # ì–¸ì–´ë³„ íŠ¹ì„± í™•ì¸
            if detected_lang == "ko":
                korean_ratio = len(re.findall(r'[ã„±-ã…ê°€-í£]', text)) / max(len(text), 1)
                lang_confidence = korean_ratio
            elif detected_lang == "zh":
                chinese_ratio = len(re.findall(r'[\u4e00-\u9fff]', text)) / max(len(text), 1)
                lang_confidence = chinese_ratio
            elif detected_lang == "ja":
                japanese_ratio = len(re.findall(r'[\u3040-\u309f\u30a0-\u30ff]', text)) / max(len(text), 1)
                lang_confidence = japanese_ratio
            else:  # ì˜ì–´ ë“±
                ascii_ratio = len(re.findall(r'[a-zA-Z]', text)) / max(len(text), 1)
                lang_confidence = ascii_ratio
            
            return round((length_confidence + lang_confidence) / 2, 3)
            
        except Exception as e:
            logging.error(f"ì–¸ì–´ ê°ì§€ ì‹ ë¢°ë„ ì¶”ì • ì˜¤ë¥˜: {e}")
            return 0.5
    
    async def _translate_to_korean(self, multilingual_content: Dict[str, List[Dict]]) -> Dict[str, List[Dict]]:
        """ë‹¤êµ­ì–´ ì½˜í…ì¸ ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­"""
        try:
            translated_content = {}
            
            for lang, sources in multilingual_content.items():
                if lang == "ko":
                    # ì´ë¯¸ í•œêµ­ì–´ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ìœ ì§€
                    translated_content[lang] = sources
                else:
                    # ë‹¤ë¥¸ ì–¸ì–´ëŠ” í•œêµ­ì–´ë¡œ ë²ˆì—­
                    translated_sources = []
                    
                    for source in sources:
                        original_text = source["content"]
                        
                        # ì£¼ì–¼ë¦¬ ìš©ì–´ ì‚¬ì „ ê¸°ë°˜ ë²ˆì—­
                        translated_text = await self._translate_with_jewelry_context(
                            original_text, lang, "ko"
                        )
                        
                        translated_source = source.copy()
                        translated_source.update({
                            "original_content": original_text,
                            "translated_content": translated_text,
                            "original_language": lang,
                            "translation_method": "jewelry_enhanced"
                        })
                        
                        translated_sources.append(translated_source)
                    
                    translated_content[lang] = translated_sources
            
            return translated_content
            
        except Exception as e:
            logging.error(f"í•œêµ­ì–´ ë²ˆì—­ ì˜¤ë¥˜: {e}")
            return multilingual_content
    
    async def _translate_with_jewelry_context(self, text: str, source_lang: str, target_lang: str) -> str:
        """ì£¼ì–¼ë¦¬ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê³ ë ¤í•œ ë²ˆì—­"""
        try:
            # 1. ì£¼ì–¼ë¦¬ ìš©ì–´ ì‚¬ì „ ë§¤í•‘ ì ìš©
            translated_text = self._apply_jewelry_term_mapping(text)
            
            # 2. Google Translateë¡œ ë‚˜ë¨¸ì§€ ë²ˆì—­
            if self.translator and source_lang != target_lang:
                try:
                    # ê¸´ í…ìŠ¤íŠ¸ëŠ” ì²­í¬ë¡œ ë¶„í• í•˜ì—¬ ë²ˆì—­
                    if len(text) > 5000:
                        chunks = self._split_text_into_chunks(translated_text, 4000)
                        translated_chunks = []
                        
                        for chunk in chunks:
                            translated_chunk = self.translator.translate(
                                chunk, src=source_lang, dest=target_lang
                            ).text
                            translated_chunks.append(translated_chunk)
                        
                        translated_text = " ".join(translated_chunks)
                    else:
                        translated_text = self.translator.translate(
                            translated_text, src=source_lang, dest=target_lang
                        ).text
                        
                except Exception as translate_error:
                    logging.warning(f"Google Translate ì˜¤ë¥˜: {translate_error}")
                    # ë²ˆì—­ ì‹¤íŒ¨ ì‹œ ìš©ì–´ ë§¤í•‘ë§Œ ì ìš©ëœ í…ìŠ¤íŠ¸ ë°˜í™˜
            
            # 3. ë²ˆì—­ í›„ ì£¼ì–¼ë¦¬ ìš©ì–´ ì¬ì •ê·œí™”
            final_text = self._post_process_translation(translated_text)
            
            return final_text
            
        except Exception as e:
            logging.error(f"ì£¼ì–¼ë¦¬ ì»¨í…ìŠ¤íŠ¸ ë²ˆì—­ ì˜¤ë¥˜: {e}")
            return text
    
    def _apply_jewelry_term_mapping(self, text: str) -> str:
        """ì£¼ì–¼ë¦¬ ìš©ì–´ ì‚¬ì „ ë§¤í•‘ ì ìš©"""
        try:
            mapped_text = text
            
            # ìš©ì–´ ë§¤í•‘ ì ìš© (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´)
            for original_term, korean_term in self.jewelry_term_mapping.items():
                # ë‹¨ì–´ ê²½ê³„ë¥¼ ê³ ë ¤í•œ ì¹˜í™˜
                pattern = r'\b' + re.escape(original_term) + r'\b'
                mapped_text = re.sub(pattern, korean_term, mapped_text, flags=re.IGNORECASE)
            
            return mapped_text
            
        except Exception as e:
            logging.error(f"ì£¼ì–¼ë¦¬ ìš©ì–´ ë§¤í•‘ ì˜¤ë¥˜: {e}")
            return text
    
    def _split_text_into_chunks(self, text: str, max_length: int = 4000) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
        try:
            # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
            sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]\s+', text)
            chunks = []
            current_chunk = ""
            
            for sentence in sentences:
                if len(current_chunk + sentence) < max_length:
                    current_chunk += sentence + ". "
                else:
                    if current_chunk:
                        chunks.append(current_chunk.strip())
                    current_chunk = sentence + ". "
            
            if current_chunk:
                chunks.append(current_chunk.strip())
            
            return chunks
            
        except Exception as e:
            logging.error(f"í…ìŠ¤íŠ¸ ì²­í¬ ë¶„í•  ì˜¤ë¥˜: {e}")
            return [text]
    
    def _post_process_translation(self, translated_text: str) -> str:
        """ë²ˆì—­ í›„ì²˜ë¦¬"""
        try:
            # ì¼ë°˜ì ì¸ ë²ˆì—­ ì˜¤ë¥˜ ìˆ˜ì •
            corrections = {
                "ë‹¤ì´ì•„ ëª¬ë“œ": "ë‹¤ì´ì•„ëª¬ë“œ",
                "ë£¨ ë¹„": "ë£¨ë¹„", 
                "ì‚¬íŒŒ ì´ì–´": "ì‚¬íŒŒì´ì–´",
                "ì—ë©”ë„ ë“œ": "ì—ë©”ë„ë“œ",
                "ìº ëŸ¿": "ìºëŸ¿",
                "ì§€ ì•„": "GIA",
                "ì—ì´ì§€ì—ìŠ¤": "AGS",
                "ë³´ì„ë¥˜": "ì£¼ì–¼ë¦¬",
                "ê·€ì¤‘í’ˆ": "ì£¼ì–¼ë¦¬"
            }
            
            processed_text = translated_text
            for wrong, correct in corrections.items():
                processed_text = processed_text.replace(wrong, correct)
            
            # ë¶ˆí•„ìš”í•œ ê³µë°± ì •ë¦¬
            processed_text = re.sub(r'\s+', ' ', processed_text)
            processed_text = processed_text.strip()
            
            return processed_text
            
        except Exception as e:
            logging.error(f"ë²ˆì—­ í›„ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return translated_text
    
    async def _normalize_jewelry_terms(self, translated_content: Dict[str, List[Dict]]) -> Dict[str, List[Dict]]:
        """ì£¼ì–¼ë¦¬ ìš©ì–´ ì •ê·œí™”"""
        try:
            normalized_content = {}
            
            for lang, sources in translated_content.items():
                normalized_sources = []
                
                for source in sources:
                    # ë²ˆì—­ëœ í…ìŠ¤íŠ¸ ë˜ëŠ” ì›ë³¸ í…ìŠ¤íŠ¸ ì‚¬ìš©
                    content = source.get("translated_content", source.get("content", ""))
                    
                    # ì£¼ì–¼ë¦¬ ìš©ì–´ ì •ê·œí™”
                    normalized_text = self._normalize_jewelry_vocabulary(content)
                    
                    normalized_source = source.copy()
                    normalized_source["normalized_content"] = normalized_text
                    normalized_source["jewelry_terms"] = self._extract_jewelry_terms(normalized_text)
                    
                    normalized_sources.append(normalized_source)
                
                normalized_content[lang] = normalized_sources
            
            return normalized_content
            
        except Exception as e:
            logging.error(f"ì£¼ì–¼ë¦¬ ìš©ì–´ ì •ê·œí™” ì˜¤ë¥˜: {e}")
            return translated_content
    
    def _normalize_jewelry_vocabulary(self, text: str) -> str:
        """ì£¼ì–¼ë¦¬ ì–´íœ˜ ì •ê·œí™”"""
        try:
            # ì£¼ì–¼ë¦¬ ì—…ê³„ í‘œì¤€ ìš©ì–´ë¡œ í†µì¼
            vocabulary_mapping = {
                # ë‹¤ì´ì•„ëª¬ë“œ ë“±ê¸‰ ê´€ë ¨
                "íˆ¬ëª…ë„": "í´ë˜ë¦¬í‹°",
                "ëª…ë„": "í´ë˜ë¦¬í‹°", 
                "íˆ¬ëª…ì„±": "í´ë˜ë¦¬í‹°",
                "ìƒ‰ìƒ": "ì»¬ëŸ¬",
                "ìƒ‰ê¹”": "ì»¬ëŸ¬",
                "ì ˆë‹¨": "ì»·",
                "ì»¤íŒ…": "ì»·",
                "ì ˆì‚­": "ì»·",
                "ë¬´ê²Œ": "ìºëŸ¿",
                "ì¤‘ëŸ‰": "ìºëŸ¿",
                
                # ë³´ì„ ì¢…ë¥˜
                "ë‹¤ì´ì•„": "ë‹¤ì´ì•„ëª¬ë“œ",
                "ë‹¤ì´ì•¼ëª¬ë“œ": "ë‹¤ì´ì•„ëª¬ë“œ",
                "í™ì˜¥": "ë£¨ë¹„",
                "ì²­ì˜¥": "ì‚¬íŒŒì´ì–´",
                "ë…¹ì£¼ì„": "ì—ë©”ë„ë“œ",
                
                # ê¸ˆì† ì¢…ë¥˜
                "í™©ê¸ˆ": "ê¸ˆ",
                "ìˆœê¸ˆ": "ê¸ˆ",
                "ë°±ì€": "ì€",
                "ìˆœì€": "ì€", 
                "ë°±ê¸ˆ": "í”Œë˜í‹°ë„˜",
                
                # ì£¼ì–¼ë¦¬ ì¢…ë¥˜
                "ëª©ê±¸ì´": "ë„¤í¬ë¦¬ìŠ¤",
                "ëª©ê±°ë¦¬": "ë„¤í¬ë¦¬ìŠ¤",
                "íŒ”ê±¸ì´": "ë¸Œë ˆì´ìŠ¬ë¦¿",
                "ì†ëª©ê±¸ì´": "ë¸Œë ˆì´ìŠ¬ë¦¿"
            }
            
            normalized_text = text
            for old_term, new_term in vocabulary_mapping.items():
                normalized_text = normalized_text.replace(old_term, new_term)
            
            return normalized_text
            
        except Exception as e:
            logging.error(f"ì£¼ì–¼ë¦¬ ì–´íœ˜ ì •ê·œí™” ì˜¤ë¥˜: {e}")
            return text
    
    def _extract_jewelry_terms(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ ì£¼ì–¼ë¦¬ ìš©ì–´ ì¶”ì¶œ"""
        try:
            jewelry_terms = []
            text_lower = text.lower()
            
            # ì£¼ì–¼ë¦¬ ìš©ì–´ ë¦¬ìŠ¤íŠ¸
            all_terms = list(self.jewelry_term_mapping.values()) + list(self.jewelry_term_mapping.keys())
            
            for term in all_terms:
                if term.lower() in text_lower:
                    jewelry_terms.append(term)
            
            return list(set(jewelry_terms))
            
        except Exception as e:
            logging.error(f"ì£¼ì–¼ë¦¬ ìš©ì–´ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return []
    
    async def _integrate_content(self, normalized_content: Dict[str, List[Dict]]) -> Dict:
        """ì½˜í…ì¸  í†µí•© ë° ì¤‘ë³µ ì œê±°"""
        try:
            # ëª¨ë“  ì†ŒìŠ¤ì˜ ì •ê·œí™”ëœ ì½˜í…ì¸  ìˆ˜ì§‘
            all_content = []
            source_mapping = {}
            
            for lang, sources in normalized_content.items():
                for idx, source in enumerate(sources):
                    content = source.get("normalized_content", "")
                    if content and len(content.strip()) > 20:
                        source_key = f"{lang}_{idx}"
                        all_content.append({
                            "key": source_key,
                            "content": content,
                            "source_type": source.get("source_type", "unknown"),
                            "word_count": len(content.split()),
                            "jewelry_terms": source.get("jewelry_terms", []),
                            "original_language": lang
                        })
                        source_mapping[source_key] = source
            
            # ì¤‘ë³µ ì½˜í…ì¸  ì œê±°
            deduplicated_content = self._remove_duplicate_content(all_content)
            
            # ì£¼ì œë³„ ê·¸ë£¹í™”
            topic_groups = self._group_content_by_topics(deduplicated_content)
            
            # ì‹œê°„ìˆœ ì •ë ¬ (ê°€ëŠ¥í•œ ê²½ìš°)
            chronological_content = self._arrange_chronologically(deduplicated_content)
            
            # í†µí•© í…ìŠ¤íŠ¸ ìƒì„±
            integrated_text = self._merge_content_intelligently(deduplicated_content, topic_groups)
            
            return {
                "integrated_text": integrated_text,
                "total_word_count": len(integrated_text.split()),
                "source_count": len(deduplicated_content),
                "topic_groups": topic_groups,
                "chronological_order": chronological_content,
                "language_distribution": self._calculate_language_distribution(deduplicated_content),
                "jewelry_terms_summary": self._summarize_jewelry_terms(deduplicated_content)
            }
            
        except Exception as e:
            logging.error(f"ì½˜í…ì¸  í†µí•© ì˜¤ë¥˜: {e}")
            return {"integrated_text": "", "error": str(e)}
    
    def _remove_duplicate_content(self, all_content: List[Dict]) -> List[Dict]:
        """ì¤‘ë³µ ì½˜í…ì¸  ì œê±°"""
        try:
            unique_content = []
            seen_hashes = set()
            
            for item in all_content:
                content = item["content"]
                
                # ì½˜í…ì¸  í•´ì‹œ ìƒì„± (ì²« 100ì + ë§ˆì§€ë§‰ 100ì)
                content_hash = hash(content[:100] + content[-100:] if len(content) > 200 else content)
                
                if content_hash not in seen_hashes:
                    seen_hashes.add(content_hash)
                    unique_content.append(item)
                else:
                    # ì¤‘ë³µ ë°œê²¬ ì‹œ ë” ê¸´ ì½˜í…ì¸  ì„ íƒ
                    for i, existing in enumerate(unique_content):
                        if hash(existing["content"][:100] + existing["content"][-100:] if len(existing["content"]) > 200 else existing["content"]) == content_hash:
                            if len(content) > len(existing["content"]):
                                unique_content[i] = item
                            break
            
            return unique_content
            
        except Exception as e:
            logging.error(f"ì¤‘ë³µ ì½˜í…ì¸  ì œê±° ì˜¤ë¥˜: {e}")
            return all_content
    
    def _group_content_by_topics(self, content_list: List[Dict]) -> Dict[str, List[Dict]]:
        """ì£¼ì œë³„ ì½˜í…ì¸  ê·¸ë£¹í™”"""
        try:
            topic_groups = defaultdict(list)
            
            # ì£¼ì œ í‚¤ì›Œë“œ ì •ì˜
            topic_keywords = {
                "product_analysis": ["ì œí’ˆ", "ìƒí’ˆ", "ë‹¤ì´ì•„ëª¬ë“œ", "ë³´ì„", "í’ˆì§ˆ", "ë“±ê¸‰"],
                "market_trends": ["ì‹œì¥", "íŠ¸ë Œë“œ", "ê°€ê²©", "ë™í–¥", "ìˆ˜ìš”", "ê³µê¸‰"],
                "business_strategy": ["ì „ëµ", "ê³„íš", "ëª©í‘œ", "ë°©í–¥", "ì •ì±…", "ì‚¬ì—…"],
                "technology": ["ê¸°ìˆ ", "ê¸°ë²•", "ë°©ë²•", "ê³µì •", "ì œì¡°", "ê°€ê³µ"],
                "certification": ["ê°ì •", "ì¸ì¦", "GIA", "AGS", "ê°ì •ì„œ", "ì¦ëª…"],
                "customer_service": ["ê³ ê°", "ì„œë¹„ìŠ¤", "ìƒë‹´", "ì‘ëŒ€", "ë§Œì¡±", "ê´€ë¦¬"]
            }
            
            for item in content_list:
                content = item["content"].lower()
                max_matches = 0
                best_topic = "general"
                
                for topic, keywords in topic_keywords.items():
                    matches = sum(1 for keyword in keywords if keyword in content)
                    if matches > max_matches:
                        max_matches = matches
                        best_topic = topic
                
                topic_groups[best_topic].append(item)
            
            return dict(topic_groups)
            
        except Exception as e:
            logging.error(f"ì£¼ì œë³„ ê·¸ë£¹í™” ì˜¤ë¥˜: {e}")
            return {"general": content_list}
    
    def _arrange_chronologically(self, content_list: List[Dict]) -> List[Dict]:
        """ì‹œê°„ìˆœ ì •ë ¬"""
        try:
            # ì†ŒìŠ¤ íƒ€ì…ë³„ ìš°ì„ ìˆœìœ„ (ì¼ë°˜ì ì¸ íšŒì˜ ì§„í–‰ ìˆœì„œ)
            type_priority = {
                "audio": 1,  # ì£¼ ë°œí‘œ/íšŒì˜ ë‚´ìš©
                "video": 2,  # ë¹„ë””ì˜¤ ë°œí‘œ
                "documents": 3,  # ë°œí‘œ ìë£Œ
                "web": 4  # ì°¸ê³  ìë£Œ
            }
            
            # ìš°ì„ ìˆœìœ„ì™€ ë‹¨ì–´ ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
            sorted_content = sorted(
                content_list,
                key=lambda x: (type_priority.get(x["source_type"], 5), -x["word_count"])
            )
            
            return sorted_content
            
        except Exception as e:
            logging.error(f"ì‹œê°„ìˆœ ì •ë ¬ ì˜¤ë¥˜: {e}")
            return content_list
    
    def _merge_content_intelligently(self, content_list: List[Dict], topic_groups: Dict) -> str:
        """ì§€ëŠ¥ì  ì½˜í…ì¸  ë³‘í•©"""
        try:
            merged_sections = []
            
            # ì£¼ì œë³„ë¡œ ì½˜í…ì¸  ë³‘í•©
            for topic, items in topic_groups.items():
                if not items:
                    continue
                
                topic_content = []
                for item in items:
                    content = item["content"].strip()
                    if content:
                        topic_content.append(content)
                
                if topic_content:
                    section_text = " ".join(topic_content)
                    merged_sections.append(section_text)
            
            # ì „ì²´ í†µí•© í…ìŠ¤íŠ¸
            integrated_text = "\n\n".join(merged_sections)
            
            # ì¤‘ë³µ ë¬¸ì¥ ì œê±°
            integrated_text = self._remove_duplicate_sentences(integrated_text)
            
            return integrated_text
            
        except Exception as e:
            logging.error(f"ì§€ëŠ¥ì  ì½˜í…ì¸  ë³‘í•© ì˜¤ë¥˜: {e}")
            return " ".join([item["content"] for item in content_list])
    
    def _remove_duplicate_sentences(self, text: str) -> str:
        """ì¤‘ë³µ ë¬¸ì¥ ì œê±°"""
        try:
            sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]\s+', text)
            unique_sentences = []
            seen_sentences = set()
            
            for sentence in sentences:
                sentence = sentence.strip()
                if sentence and len(sentence) > 10:
                    # ë¬¸ì¥ì˜ í•µì‹¬ ë¶€ë¶„ë§Œ í•´ì‹œí™”
                    sentence_key = re.sub(r'\s+', ' ', sentence.lower())[:50]
                    if sentence_key not in seen_sentences:
                        seen_sentences.add(sentence_key)
                        unique_sentences.append(sentence)
            
            return ". ".join(unique_sentences) + "."
            
        except Exception as e:
            logging.error(f"ì¤‘ë³µ ë¬¸ì¥ ì œê±° ì˜¤ë¥˜: {e}")
            return text
    
    def _calculate_language_distribution(self, content_list: List[Dict]) -> Dict:
        """ì–¸ì–´ ë¶„í¬ ê³„ì‚°"""
        try:
            language_stats = defaultdict(lambda: {"count": 0, "word_count": 0})
            
            for item in content_list:
                lang = item.get("original_language", "unknown")
                language_stats[lang]["count"] += 1
                language_stats[lang]["word_count"] += item.get("word_count", 0)
            
            total_sources = len(content_list)
            total_words = sum(item.get("word_count", 0) for item in content_list)
            
            distribution = {}
            for lang, stats in language_stats.items():
                distribution[lang] = {
                    "sources": stats["count"],
                    "source_percentage": round(stats["count"] / total_sources * 100, 1) if total_sources > 0 else 0,
                    "word_count": stats["word_count"],
                    "word_percentage": round(stats["word_count"] / total_words * 100, 1) if total_words > 0 else 0,
                    "language_name": self.language_mapping.get(lang, lang)
                }
            
            return distribution
            
        except Exception as e:
            logging.error(f"ì–¸ì–´ ë¶„í¬ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return {}
    
    def _summarize_jewelry_terms(self, content_list: List[Dict]) -> Dict:
        """ì£¼ì–¼ë¦¬ ìš©ì–´ ìš”ì•½"""
        try:
            all_terms = []
            for item in content_list:
                all_terms.extend(item.get("jewelry_terms", []))
            
            term_frequency = Counter(all_terms)
            
            return {
                "total_unique_terms": len(term_frequency),
                "most_frequent_terms": term_frequency.most_common(10),
                "all_terms": list(term_frequency.keys())
            }
            
        except Exception as e:
            logging.error(f"ì£¼ì–¼ë¦¬ ìš©ì–´ ìš”ì•½ ì˜¤ë¥˜: {e}")
            return {}
    
    async def _generate_korean_summary(self, 
                                     integrated_content: Dict,
                                     summary_style: str,
                                     session_type: str) -> Dict:
        """í•œêµ­ì–´ ìš”ì•½ ìƒì„±"""
        try:
            print(f"ğŸ“ {summary_style} ìŠ¤íƒ€ì¼ í•œêµ­ì–´ ìš”ì•½ ìƒì„± ì¤‘...")
            
            content_text = integrated_content.get("integrated_text", "")
            if not content_text:
                return {"error": "í†µí•© ì½˜í…ì¸ ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤"}
            
            # ìŠ¤íƒ€ì¼ë³„ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            style_config = self.summary_styles.get(summary_style, self.summary_styles["comprehensive"])
            
            # ì£¼ì–¼ë¦¬ AI ì—”ì§„ìœ¼ë¡œ ìš”ì•½ ìƒì„±
            summary_result = await self.jewelry_ai.analyze_jewelry_content(
                content_text,
                analysis_type="summary",
                language="korean",
                style=summary_style
            )
            
            # êµ¬ì¡°í™”ëœ ìš”ì•½ ìƒì„±
            structured_summary = self._create_structured_summary(
                content_text, 
                summary_result,
                style_config,
                session_type,
                integrated_content
            )
            
            return structured_summary
            
        except Exception as e:
            logging.error(f"í•œêµ­ì–´ ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {e}")
            return {"error": str(e)}
    
    def _create_structured_summary(self, 
                                 content_text: str,
                                 ai_summary: Dict,
                                 style_config: Dict,
                                 session_type: str,
                                 integrated_content: Dict) -> Dict:
        """êµ¬ì¡°í™”ëœ ìš”ì•½ ìƒì„±"""
        try:
            # í•µì‹¬ ë‚´ìš© ì¶”ì¶œ
            key_points = self._extract_key_points(content_text)
            
            # ì£¼ì–¼ë¦¬ ê´€ë ¨ ì¸ì‚¬ì´íŠ¸
            jewelry_insights = ai_summary.get("jewelry_insights", {})
            
            # ìš”ì•½ ì„¹ì…˜ë³„ ë‚´ìš© ìƒì„±
            sections = {}
            
            for section in style_config["sections"]:
                if section == "í•µì‹¬ ë‚´ìš©":
                    sections[section] = self._generate_key_content_section(key_points, jewelry_insights)
                elif section == "ì£¼ìš” ë…¼ì ":
                    sections[section] = self._generate_main_points_section(content_text, ai_summary)
                elif section == "ê²°ë¡  ë° ì‹œì‚¬ì ":
                    sections[section] = self._generate_conclusions_section(ai_summary, jewelry_insights)
                elif section == "ì‹¤í–‰ ë°©ì•ˆ":
                    sections[section] = self._generate_action_items_section(ai_summary, session_type)
                elif section == "í•µì‹¬ ë©”ì‹œì§€":
                    sections[section] = self._generate_key_message_section(key_points)
                elif section == "ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸":
                    sections[section] = self._generate_business_impact_section(jewelry_insights)
                elif section == "ì˜ì‚¬ê²°ì • í¬ì¸íŠ¸":
                    sections[section] = self._generate_decision_points_section(ai_summary)
                elif section == "ë‹¤ìŒ ë‹¨ê³„":
                    sections[section] = self._generate_next_steps_section(ai_summary, session_type)
                elif section == "ê¸°ìˆ ì  ë‚´ìš©":
                    sections[section] = self._generate_technical_content_section(content_text)
                elif section == "ì „ë¬¸ ìš©ì–´ í•´ì„¤":
                    sections[section] = self._generate_terminology_section(integrated_content)
                elif section == "ì‹œì¥ ë™í–¥":
                    sections[section] = self._generate_market_trends_section(jewelry_insights)
                elif section == "ë¹„ì¦ˆë‹ˆìŠ¤ ê¸°íšŒ":
                    sections[section] = self._generate_business_opportunities_section(jewelry_insights)
            
            # ì „ì²´ ìš”ì•½ í…ìŠ¤íŠ¸ ìƒì„±
            summary_text = f"# {style_config['title']}\n\n"
            for section_title, section_content in sections.items():
                if section_content:
                    summary_text += f"## {section_title}\n{section_content}\n\n"
            
            return {
                "title": style_config["title"],
                "style": summary_style,
                "tone": style_config["tone"],
                "sections": sections,
                "full_summary": summary_text.strip(),
                "word_count": len(summary_text.split()),
                "key_metrics": {
                    "original_word_count": len(content_text.split()),
                    "compression_ratio": round(len(summary_text.split()) / len(content_text.split()), 3) if content_text else 0,
                    "jewelry_terms_count": len(integrated_content.get("jewelry_terms_summary", {}).get("all_terms", [])),
                    "source_languages": len(integrated_content.get("language_distribution", {}))
                }
            }
            
        except Exception as e:
            logging.error(f"êµ¬ì¡°í™”ëœ ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {e}")
            return {"error": str(e)}
    
    def _extract_key_points(self, content: str) -> List[str]:
        """í•µì‹¬ í¬ì¸íŠ¸ ì¶”ì¶œ"""
        try:
            # ë¬¸ì¥ì„ ë¶„ë¦¬í•˜ê³  ì¤‘ìš”ë„ í‰ê°€
            sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]\s+', content)
            
            key_sentences = []
            for sentence in sentences:
                sentence = sentence.strip()
                if len(sentence) > 20:
                    # ì£¼ì–¼ë¦¬ ìš©ì–´ë‚˜ ì¤‘ìš” í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì¥ ìš°ì„  ì„ íƒ
                    importance_score = 0
                    
                    # ì£¼ì–¼ë¦¬ ìš©ì–´ ê°€ì 
                    for term in self.jewelry_term_mapping.values():
                        if term in sentence:
                            importance_score += 2
                    
                    # ì¤‘ìš” í‚¤ì›Œë“œ ê°€ì 
                    important_keywords = ["ì¤‘ìš”", "í•µì‹¬", "ì£¼ìš”", "ê²°ì •", "ì „ëµ", "ê³„íš", "ëª©í‘œ", "ê²°ê³¼", "ê²°ë¡ "]
                    for keyword in important_keywords:
                        if keyword in sentence:
                            importance_score += 1
                    
                    if importance_score > 0:
                        key_sentences.append((sentence, importance_score))
            
            # ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ 5ê°œ ì„ íƒ
            key_sentences.sort(key=lambda x: x[1], reverse=True)
            return [sentence for sentence, score in key_sentences[:5]]
            
        except Exception as e:
            logging.error(f"í•µì‹¬ í¬ì¸íŠ¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return []
    
    def _generate_key_content_section(self, key_points: List[str], jewelry_insights: Dict) -> str:
        """í•µì‹¬ ë‚´ìš© ì„¹ì…˜ ìƒì„±"""
        try:
            content = []
            
            if key_points:
                content.append("**ì£¼ìš” ë‚´ìš©:**")
                for i, point in enumerate(key_points, 1):
                    content.append(f"{i}. {point}")
            
            if jewelry_insights.get("main_topics"):
                content.append("\n**í•µì‹¬ ì£¼ì œ:**")
                for topic in jewelry_insights["main_topics"][:3]:
                    content.append(f"â€¢ {topic}")
            
            return "\n".join(content) if content else "í•µì‹¬ ë‚´ìš©ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
        except Exception as e:
            logging.error(f"í•µì‹¬ ë‚´ìš© ì„¹ì…˜ ìƒì„± ì˜¤ë¥˜: {e}")
            return "ì„¹ì…˜ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    def _generate_main_points_section(self, content: str, ai_summary: Dict) -> str:
        """ì£¼ìš” ë…¼ì  ì„¹ì…˜ ìƒì„±"""
        try:
            points = []
            
            # AI ìš”ì•½ì—ì„œ ì£¼ìš” ë…¼ì  ì¶”ì¶œ
            if ai_summary.get("key_insights"):
                for insight in ai_summary["key_insights"][:4]:
                    points.append(f"â€¢ {insight}")
            
            if not points:
                # ì§ì ‘ ì¶”ì¶œ
                discussion_keywords = ["ë…¼ì˜", "í† ë¡ ", "ì˜ê²¬", "ì œì•ˆ", "ë¬¸ì œ", "ê³¼ì œ", "ë°©ì•ˆ", "ë°©ë²•"]
                sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]\s+', content)
                
                for sentence in sentences:
                    if any(keyword in sentence for keyword in discussion_keywords) and len(sentence) > 30:
                        points.append(f"â€¢ {sentence}")
                        if len(points) >= 3:
                            break
            
            return "\n".join(points) if points else "ì£¼ìš” ë…¼ì ì„ ì‹ë³„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
        except Exception as e:
            logging.error(f"ì£¼ìš” ë…¼ì  ì„¹ì…˜ ìƒì„± ì˜¤ë¥˜: {e}")
            return "ì„¹ì…˜ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    def _generate_conclusions_section(self, ai_summary: Dict, jewelry_insights: Dict) -> str:
        """ê²°ë¡  ë° ì‹œì‚¬ì  ì„¹ì…˜ ìƒì„±"""
        try:
            conclusions = []
            
            if jewelry_insights.get("business_implications"):
                conclusions.extend(jewelry_insights["business_implications"])
            
            if ai_summary.get("conclusions"):
                conclusions.extend(ai_summary["conclusions"])
            
            if not conclusions:
                conclusions = ["í–¥í›„ ì£¼ì–¼ë¦¬ ì—…ê³„ ë™í–¥ì— ëŒ€í•œ ì§€ì†ì ì¸ ëª¨ë‹ˆí„°ë§ì´ í•„ìš”í•©ë‹ˆë‹¤."]
            
            content = []
            for i, conclusion in enumerate(conclusions[:4], 1):
                content.append(f"{i}. {conclusion}")
            
            return "\n".join(content)
            
        except Exception as e:
            logging.error(f"ê²°ë¡  ì„¹ì…˜ ìƒì„± ì˜¤ë¥˜: {e}")
            return "ê²°ë¡ ì„ ë„ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    def _generate_action_items_section(self, ai_summary: Dict, session_type: str) -> str:
        """ì‹¤í–‰ ë°©ì•ˆ ì„¹ì…˜ ìƒì„±"""
        try:
            actions = []
            
            if ai_summary.get("action_items"):
                actions.extend(ai_summary["action_items"])
            
            # ì„¸ì…˜ íƒ€ì…ë³„ ê¸°ë³¸ ì‹¤í–‰ ë°©ì•ˆ
            if session_type == "seminar":
                actions.append("ì„¸ë¯¸ë‚˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ í•œ í›„ì† êµìœ¡ ê³„íš ìˆ˜ë¦½")
            elif session_type == "meeting":
                actions.append("íšŒì˜ ê²°ì •ì‚¬í•­ì— ëŒ€í•œ êµ¬ì²´ì  ì‹¤í–‰ ê³„íš ìˆ˜ë¦½")
            elif session_type == "conference":
                actions.append("ì»¨í¼ëŸ°ìŠ¤ ì£¼ìš” ë‚´ìš©ì„ ì¡°ì§ ë‚´ ê³µìœ ")
            
            if not actions:
                actions = ["ì£¼ìš” ë‚´ìš©ì— ëŒ€í•œ ì„¸ë¶€ ì‹¤í–‰ ê³„íš ìˆ˜ë¦½ì´ í•„ìš”í•©ë‹ˆë‹¤."]
            
            content = []
            for i, action in enumerate(actions[:4], 1):
                content.append(f"{i}. {action}")
            
            return "\n".join(content)
            
        except Exception as e:
            logging.error(f"ì‹¤í–‰ ë°©ì•ˆ ì„¹ì…˜ ìƒì„± ì˜¤ë¥˜: {e}")
            return "ì‹¤í–‰ ë°©ì•ˆì„ ì œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    def _generate_business_impact_section(self, jewelry_insights: Dict) -> str:
        """ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸ ì„¹ì…˜ ìƒì„±"""
        try:
            impacts = []
            
            if jewelry_insights.get("business_opportunities"):
                impacts.extend(jewelry_insights["business_opportunities"])
            
            if jewelry_insights.get("market_insights"):
                impacts.extend(jewelry_insights["market_insights"])
            
            if not impacts:
                impacts = ["ì£¼ì–¼ë¦¬ ì—…ê³„ ë¹„ì¦ˆë‹ˆìŠ¤ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë©´ë°€íˆ ë¶„ì„í•  í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤."]
            
            content = []
            for impact in impacts[:3]:
                content.append(f"â€¢ {impact}")
            
            return "\n".join(content)
            
        except Exception as e:
            logging.error(f"ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸ ì„¹ì…˜ ìƒì„± ì˜¤ë¥˜: {e}")
            return "ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸ë¥¼ í‰ê°€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    def _calculate_translation_quality(self, translated_content: Dict) -> Dict:
        """ë²ˆì—­ í’ˆì§ˆ ê³„ì‚°"""
        try:
            total_sources = 0
            translation_scores = []
            
            for lang, sources in translated_content.items():
                for source in sources:
                    total_sources += 1
                    if "translated_content" in source:
                        # ê°„ë‹¨í•œ ë²ˆì—­ í’ˆì§ˆ ì¶”ì •
                        original_length = len(source.get("original_content", ""))
                        translated_length = len(source.get("translated_content", ""))
                        
                        # ê¸¸ì´ ê¸°ë°˜ í’ˆì§ˆ ì ìˆ˜ (ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸¸ë©´ í’ˆì§ˆ ì˜ì‹¬)
                        if original_length > 0:
                            length_ratio = translated_length / original_length
                            if 0.5 <= length_ratio <= 2.0:
                                quality_score = 0.8
                            else:
                                quality_score = 0.6
                        else:
                            quality_score = 0.5
                        
                        translation_scores.append(quality_score)
            
            avg_quality = sum(translation_scores) / len(translation_scores) if translation_scores else 0.5
            
            return {
                "average_quality": round(avg_quality, 3),
                "total_translations": len(translation_scores),
                "quality_level": "ë†’ìŒ" if avg_quality >= 0.8 else "ë³´í†µ" if avg_quality >= 0.6 else "ë‚®ìŒ"
            }
            
        except Exception as e:
            logging.error(f"ë²ˆì—­ í’ˆì§ˆ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return {"average_quality": 0.5, "quality_level": "ì•Œ ìˆ˜ ì—†ìŒ"}
    
    def _get_mapped_terms(self, normalized_content: Dict) -> List[str]:
        """ë§¤í•‘ëœ ì£¼ì–¼ë¦¬ ìš©ì–´ ìˆ˜ì§‘"""
        try:
            all_terms = []
            for lang, sources in normalized_content.items():
                for source in sources:
                    all_terms.extend(source.get("jewelry_terms", []))
            
            return list(set(all_terms))
            
        except Exception as e:
            logging.error(f"ë§¤í•‘ëœ ìš©ì–´ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            return []
    
    async def _assess_summary_quality(self, korean_summary: Dict, integrated_content: Dict) -> Dict:
        """ìš”ì•½ í’ˆì§ˆ í‰ê°€"""
        try:
            original_word_count = integrated_content.get("total_word_count", 0)
            summary_word_count = korean_summary.get("word_count", 0)
            
            # ì••ì¶•ë¥  í‰ê°€
            compression_ratio = korean_summary.get("key_metrics", {}).get("compression_ratio", 0)
            
            # ë‚´ìš© ì™„ì„±ë„ í‰ê°€
            sections_completed = len([s for s in korean_summary.get("sections", {}).values() if s and "ì˜¤ë¥˜" not in s])
            total_sections = len(korean_summary.get("sections", {}))
            completeness = sections_completed / total_sections if total_sections > 0 else 0
            
            # ì£¼ì–¼ë¦¬ ìš©ì–´ ë³´ì¡´ë„
            original_terms = len(integrated_content.get("jewelry_terms_summary", {}).get("all_terms", []))
            summary_terms = korean_summary.get("key_metrics", {}).get("jewelry_terms_count", 0)
            term_preservation = min(1.0, summary_terms / original_terms) if original_terms > 0 else 1.0
            
            # ì¢…í•© í’ˆì§ˆ ì ìˆ˜
            quality_score = (completeness * 0.4 + term_preservation * 0.3 + min(1.0, compression_ratio * 5) * 0.3) * 100
            
            return {
                "overall_score": round(quality_score, 1),
                "quality_level": "ìš°ìˆ˜" if quality_score >= 80 else "ì–‘í˜¸" if quality_score >= 60 else "ë³´í†µ",
                "metrics": {
                    "completeness": round(completeness, 3),
                    "term_preservation": round(term_preservation, 3),
                    "compression_efficiency": round(compression_ratio, 3)
                },
                "recommendations": self._generate_quality_recommendations(quality_score, completeness, term_preservation)
            }
            
        except Exception as e:
            logging.error(f"ìš”ì•½ í’ˆì§ˆ í‰ê°€ ì˜¤ë¥˜: {e}")
            return {"overall_score": 50, "quality_level": "í‰ê°€ ë¶ˆê°€"}
    
    def _generate_quality_recommendations(self, quality_score: float, completeness: float, term_preservation: float) -> List[str]:
        """í’ˆì§ˆ ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        try:
            recommendations = []
            
            if quality_score < 70:
                recommendations.append("ì „ì²´ì ì¸ ìš”ì•½ í’ˆì§ˆ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            
            if completeness < 0.8:
                recommendations.append("ìš”ì•½ ì„¹ì…˜ì˜ ì™„ì„±ë„ë¥¼ ë†’ì—¬ì£¼ì„¸ìš”.")
            
            if term_preservation < 0.7:
                recommendations.append("ì£¼ì–¼ë¦¬ ì „ë¬¸ ìš©ì–´ì˜ ë³´ì¡´ì„ ê°œì„ í•´ì£¼ì„¸ìš”.")
            
            if not recommendations:
                recommendations.append("ìš°ìˆ˜í•œ í’ˆì§ˆì˜ ìš”ì•½ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            return recommendations
            
        except Exception as e:
            logging.error(f"í’ˆì§ˆ ê¶Œì¥ì‚¬í•­ ìƒì„± ì˜¤ë¥˜: {e}")
            return ["í’ˆì§ˆ í‰ê°€ë¥¼ ì™„ë£Œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."]

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
_korean_final_summarizer_instance = None

def get_korean_final_summarizer() -> KoreanFinalSummarizer:
    """ì „ì—­ í•œêµ­ì–´ ìµœì¢… ìš”ì•½ê¸° ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _korean_final_summarizer_instance
    if _korean_final_summarizer_instance is None:
        _korean_final_summarizer_instance = KoreanFinalSummarizer()
    return _korean_final_summarizer_instance

# í¸ì˜ í•¨ìˆ˜ë“¤
async def generate_korean_summary(session_data: Dict, **kwargs) -> Dict:
    """í•œêµ­ì–´ ìš”ì•½ ìƒì„± í¸ì˜ í•¨ìˆ˜"""
    summarizer = get_korean_final_summarizer()
    return await summarizer.process_multilingual_session(session_data, **kwargs)

def check_korean_summarizer_support() -> Dict:
    """í•œêµ­ì–´ ìš”ì•½ê¸° ì§€ì› ìƒíƒœ í™•ì¸"""
    return {
        "libraries": {
            "langdetect": LANGDETECT_AVAILABLE,
            "googletrans": GOOGLETRANS_AVAILABLE,
            "jewelry_ai_engine": True
        },
        "features": {
            "language_detection": True,
            "multilingual_translation": GOOGLETRANS_AVAILABLE,
            "jewelry_term_mapping": True,
            "content_integration": True,
            "structured_summary": True
        },
        "supported_languages": list(KoreanFinalSummarizer().language_mapping.keys()),
        "summary_styles": list(KoreanFinalSummarizer().summary_styles.keys()),
        "jewelry_terms_count": len(KoreanFinalSummarizer().jewelry_term_mapping)
    }

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    async def test_korean_summarizer():
        print("í•œêµ­ì–´ ìµœì¢… í†µí•© ìš”ì•½ ì—”ì§„ í…ŒìŠ¤íŠ¸")
        support_info = check_korean_summarizer_support()
        print(f"ì§€ì› ìƒíƒœ: {support_info}")
    
    import asyncio
    asyncio.run(test_korean_summarizer())
