#!/usr/bin/env python3
"""
Ïô∏Î∂Ä AI Î™®Îç∏ ÌÜµÌï© ÏãúÏä§ÌÖú v2.6
OpenAI GPT, Anthropic Claude, Google Gemini API ÌÜµÌï©
"""

import os
import time
import json
import logging
import asyncio
import aiohttp
from typing import Dict, List, Any, Optional, Union, AsyncGenerator
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from pathlib import Path
from enum import Enum
import threading

class AIProvider(Enum):
    """AI Í≥µÍ∏âÏûê"""
    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    GOOGLE = "google"
    COHERE = "cohere"
    HUGGINGFACE = "huggingface"

class TaskType(Enum):
    """ÏûëÏóÖ ÌÉÄÏûÖ"""
    SUMMARIZATION = "summarization"
    TRANSLATION = "translation"
    ANALYSIS = "analysis"
    QA = "question_answering"
    ENHANCEMENT = "enhancement"
    CLASSIFICATION = "classification"
    GENERATION = "generation"

@dataclass
class AIModelConfig:
    """AI Î™®Îç∏ ÏÑ§Ï†ï"""
    provider: AIProvider
    model_name: str
    api_key: Optional[str] = None
    base_url: Optional[str] = None
    max_tokens: int = 4000
    temperature: float = 0.7
    timeout_seconds: int = 30
    rate_limit_rpm: int = 60  # requests per minute
    cost_per_1k_tokens: float = 0.0
    supports_streaming: bool = False
    context_window: int = 4000
    is_enabled: bool = True

@dataclass
class AIRequest:
    """AI ÏöîÏ≤≠"""
    task_type: TaskType
    prompt: str
    system_prompt: Optional[str] = None
    context: Optional[Dict[str, Any]] = None
    max_tokens: Optional[int] = None
    temperature: Optional[float] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    request_id: str = field(default_factory=lambda: f"req_{int(time.time() * 1000)}")

@dataclass
class AIResponse:
    """AI ÏùëÎãµ"""
    request_id: str
    provider: AIProvider
    model_name: str
    content: str
    usage: Dict[str, int] = field(default_factory=dict)
    processing_time_ms: float = 0.0
    cost_estimate: float = 0.0
    success: bool = True
    error_message: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)

class ExternalAIIntegrator:
    """Ïô∏Î∂Ä AI Î™®Îç∏ ÌÜµÌï© Í¥ÄÎ¶¨Ïûê"""
    
    def __init__(self, config_path: Optional[Path] = None):
        self.config_path = config_path or Path.home() / ".solomond_ai" / "ai_configs.json"
        self.lock = threading.RLock()
        self.logger = self._setup_logging()
        
        # AI Î™®Îç∏ ÏÑ§Ï†ï
        self.models: Dict[str, AIModelConfig] = {}
        self.rate_limiters: Dict[str, Dict] = {}
        
        # ÏÇ¨Ïö© ÌÜµÍ≥Ñ
        self.usage_stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'total_tokens_used': 0,
            'total_cost': 0.0,
            'requests_by_provider': {},
            'session_start': datetime.now()
        }
        
        # ÏÑ§Ï†ï Î°úÎìú
        self._load_configurations()
        self._initialize_default_models()
        
        # ÏÇ¨Ïö©Ïûê ÏÑ§Ï†ï Ïó∞Îèô
        try:
            from .user_settings_manager import get_global_settings_manager
            self.settings_manager = get_global_settings_manager()
            self._load_api_keys_from_settings()
        except ImportError:
            self.settings_manager = None
        
        self.logger.info("ü§ñ Ïô∏Î∂Ä AI Î™®Îç∏ ÌÜµÌï© ÏãúÏä§ÌÖú Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
    
    def _setup_logging(self) -> logging.Logger:
        """Î°úÍπÖ ÏÑ§Ï†ï"""
        logger = logging.getLogger(f'{__name__}.{self.__class__.__name__}')
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s | %(name)s | %(levelname)s | %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            logger.setLevel(logging.INFO)
        return logger
    
    def _load_configurations(self) -> None:
        """AI Î™®Îç∏ ÏÑ§Ï†ï Î°úÎìú"""
        try:
            if self.config_path.exists():
                with open(self.config_path, 'r', encoding='utf-8') as f:
                    config_data = json.load(f)
                
                for model_id, model_data in config_data.get('models', {}).items():
                    model_data['provider'] = AIProvider(model_data['provider'])
                    self.models[model_id] = AIModelConfig(**model_data)
                
                self.logger.info(f"üì• AI Î™®Îç∏ ÏÑ§Ï†ï Î°úÎìú: {len(self.models)}Í∞ú Î™®Îç∏")
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è AI Î™®Îç∏ ÏÑ§Ï†ï Î°úÎìú Ïã§Ìå®: {e}")
    
    def _save_configurations(self) -> None:
        """AI Î™®Îç∏ ÏÑ§Ï†ï Ï†ÄÏû•"""
        try:
            # ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±
            self.config_path.parent.mkdir(parents=True, exist_ok=True)
            
            config_data = {
                'models': {},
                'updated_at': datetime.now().isoformat(),
                'version': '2.6'
            }
            
            for model_id, model_config in self.models.items():
                model_data = {
                    'provider': model_config.provider.value,
                    'model_name': model_config.model_name,
                    'api_key': model_config.api_key,
                    'base_url': model_config.base_url,
                    'max_tokens': model_config.max_tokens,
                    'temperature': model_config.temperature,
                    'timeout_seconds': model_config.timeout_seconds,
                    'rate_limit_rpm': model_config.rate_limit_rpm,
                    'cost_per_1k_tokens': model_config.cost_per_1k_tokens,
                    'supports_streaming': model_config.supports_streaming,
                    'context_window': model_config.context_window,
                    'is_enabled': model_config.is_enabled
                }
                config_data['models'][model_id] = model_data
            
            with open(self.config_path, 'w', encoding='utf-8') as f:
                json.dump(config_data, f, indent=2, ensure_ascii=False)
            
            self.logger.debug("üíæ AI Î™®Îç∏ ÏÑ§Ï†ï Ï†ÄÏû• ÏôÑÎ£å")
            
        except Exception as e:
            self.logger.error(f"‚ùå AI Î™®Îç∏ ÏÑ§Ï†ï Ï†ÄÏû• Ïã§Ìå®: {e}")
    
    def _initialize_default_models(self) -> None:
        """Í∏∞Î≥∏ AI Î™®Îç∏ Ï¥àÍ∏∞Ìôî"""
        default_models = [
            # OpenAI GPT Î™®Îç∏Îì§
            AIModelConfig(
                provider=AIProvider.OPENAI,
                model_name="gpt-4o-mini",
                max_tokens=4000,
                temperature=0.7,
                rate_limit_rpm=500,
                cost_per_1k_tokens=0.00015,
                supports_streaming=True,
                context_window=128000,
                is_enabled=True
            ),
            AIModelConfig(
                provider=AIProvider.OPENAI,
                model_name="gpt-4o",
                max_tokens=4000,
                temperature=0.7,
                rate_limit_rpm=500,
                cost_per_1k_tokens=0.005,
                supports_streaming=True,
                context_window=128000,
                is_enabled=True
            ),
            AIModelConfig(
                provider=AIProvider.OPENAI,
                model_name="gpt-3.5-turbo",
                max_tokens=4000,
                temperature=0.7,
                rate_limit_rpm=3500,
                cost_per_1k_tokens=0.0005,
                supports_streaming=True,
                context_window=16385,
                is_enabled=True
            ),
            
            # Anthropic Claude Î™®Îç∏Îì§
            AIModelConfig(
                provider=AIProvider.ANTHROPIC,
                model_name="claude-3-5-sonnet-20241022",
                base_url="https://api.anthropic.com",
                max_tokens=4000,
                temperature=0.7,
                rate_limit_rpm=50,
                cost_per_1k_tokens=0.003,
                supports_streaming=True,
                context_window=200000,
                is_enabled=True
            ),
            AIModelConfig(
                provider=AIProvider.ANTHROPIC,
                model_name="claude-3-5-haiku-20241022",
                base_url="https://api.anthropic.com",
                max_tokens=4000,
                temperature=0.7,
                rate_limit_rpm=50,
                cost_per_1k_tokens=0.00025,
                supports_streaming=True,
                context_window=200000,
                is_enabled=True
            ),
            
            # Google Gemini Î™®Îç∏Îì§
            AIModelConfig(
                provider=AIProvider.GOOGLE,
                model_name="gemini-1.5-flash",
                base_url="https://generativelanguage.googleapis.com",
                max_tokens=8192,
                temperature=0.7,
                rate_limit_rpm=15,
                cost_per_1k_tokens=0.00015,
                supports_streaming=True,
                context_window=1000000,
                is_enabled=True
            ),
            AIModelConfig(
                provider=AIProvider.GOOGLE,
                model_name="gemini-1.5-pro",
                base_url="https://generativelanguage.googleapis.com",
                max_tokens=8192,
                temperature=0.7,
                rate_limit_rpm=2,
                cost_per_1k_tokens=0.0035,
                supports_streaming=True,
                context_window=2000000,
                is_enabled=True
            )
        ]
        
        # Í∏∞Ï°¥Ïóê ÏóÜÎäî Î™®Îç∏Îßå Ï∂îÍ∞Ä
        for model_config in default_models:
            model_id = f"{model_config.provider.value}_{model_config.model_name.replace('-', '_').replace('.', '_')}"
            if model_id not in self.models:
                self.models[model_id] = model_config
                
        self.logger.info(f"üîß Í∏∞Î≥∏ AI Î™®Îç∏ Ï¥àÍ∏∞Ìôî: {len(self.models)}Í∞ú Î™®Îç∏ Îì±Î°ù")
    
    def _load_api_keys_from_settings(self) -> None:
        """ÏÇ¨Ïö©Ïûê ÏÑ§Ï†ïÏóêÏÑú API ÌÇ§ Î°úÎìú"""
        if not self.settings_manager:
            return
        
        try:
            # OpenAI API ÌÇ§
            openai_key = self.settings_manager.get_setting("api.openai_key")
            if openai_key:
                for model_id, model_config in self.models.items():
                    if model_config.provider == AIProvider.OPENAI:
                        model_config.api_key = openai_key
            
            # Anthropic API ÌÇ§
            anthropic_key = self.settings_manager.get_setting("api.anthropic_key")
            if anthropic_key:
                for model_id, model_config in self.models.items():
                    if model_config.provider == AIProvider.ANTHROPIC:
                        model_config.api_key = anthropic_key
            
            # Google API ÌÇ§
            google_key = self.settings_manager.get_setting("api.google_key")
            if google_key:
                for model_id, model_config in self.models.items():
                    if model_config.provider == AIProvider.GOOGLE:
                        model_config.api_key = google_key
            
            self.logger.debug("üîë ÏÇ¨Ïö©Ïûê ÏÑ§Ï†ïÏóêÏÑú API ÌÇ§ Î°úÎìú ÏôÑÎ£å")
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è API ÌÇ§ Î°úÎìú Ïã§Ìå®: {e}")
    
    def add_model(self, model_id: str, model_config: AIModelConfig) -> bool:
        """AI Î™®Îç∏ Ï∂îÍ∞Ä"""
        try:
            with self.lock:
                self.models[model_id] = model_config
                self._save_configurations()
                
                self.logger.info(f"‚ûï AI Î™®Îç∏ Ï∂îÍ∞Ä: {model_id} ({model_config.provider.value})")
                return True
                
        except Exception as e:
            self.logger.error(f"‚ùå AI Î™®Îç∏ Ï∂îÍ∞Ä Ïã§Ìå® {model_id}: {e}")
            return False
    
    def remove_model(self, model_id: str) -> bool:
        """AI Î™®Îç∏ Ï†úÍ±∞"""
        try:
            with self.lock:
                if model_id in self.models:
                    del self.models[model_id]
                    self._save_configurations()
                    
                    self.logger.info(f"‚ûñ AI Î™®Îç∏ Ï†úÍ±∞: {model_id}")
                    return True
                return False
                
        except Exception as e:
            self.logger.error(f"‚ùå AI Î™®Îç∏ Ï†úÍ±∞ Ïã§Ìå® {model_id}: {e}")
            return False
    
    def set_api_key(self, provider: AIProvider, api_key: str) -> bool:
        """API ÌÇ§ ÏÑ§Ï†ï"""
        try:
            with self.lock:
                # Ìï¥Îãπ Í≥µÍ∏âÏûêÏùò Î™®Îì† Î™®Îç∏Ïóê API ÌÇ§ Ï†ÅÏö©
                updated_count = 0
                for model_config in self.models.values():
                    if model_config.provider == provider:
                        model_config.api_key = api_key
                        updated_count += 1
                
                # ÏÇ¨Ïö©Ïûê ÏÑ§Ï†ïÏóêÎèÑ Ï†ÄÏû•
                if self.settings_manager:
                    key_name = f"api.{provider.value}_key"
                    from .user_settings_manager import SettingType, SettingScope
                    self.settings_manager.set_setting(
                        key_name, 
                        api_key,
                        SettingType.SYSTEM,
                        SettingScope.GLOBAL,
                        f"{provider.value.upper()} API ÌÇ§"
                    )
                
                self._save_configurations()
                
                self.logger.info(f"üîë {provider.value.upper()} API ÌÇ§ ÏÑ§Ï†ï: {updated_count}Í∞ú Î™®Îç∏ ÏóÖÎç∞Ïù¥Ìä∏")
                return True
                
        except Exception as e:
            self.logger.error(f"‚ùå API ÌÇ§ ÏÑ§Ï†ï Ïã§Ìå® {provider.value}: {e}")
            return False
    
    def get_available_models(self, task_type: Optional[TaskType] = None) -> List[str]:
        """ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î™®Îç∏ Î™©Î°ù"""
        with self.lock:
            available = []
            
            for model_id, model_config in self.models.items():
                if not model_config.is_enabled or not model_config.api_key:
                    continue
                
                # ÏûëÏóÖ ÌÉÄÏûÖÎ≥Ñ ÌïÑÌÑ∞ÎßÅ (ÌïÑÏöîÏãú ÌôïÏû•)
                if task_type and task_type == TaskType.SUMMARIZATION:
                    # ÏöîÏïΩ ÏûëÏóÖÏóê Ï†ÅÌï©Ìïú Î™®Îç∏Îßå
                    if model_config.max_tokens < 1000:
                        continue
                
                available.append(model_id)
            
            return sorted(available)
    
    def _check_rate_limit(self, model_id: str) -> bool:
        """ÏöîÏ≤≠ ÏÜçÎèÑ Ï†úÌïú ÌôïÏù∏"""
        if model_id not in self.rate_limiters:
            self.rate_limiters[model_id] = {
                'requests': [],
                'last_reset': time.time()
            }
        
        limiter = self.rate_limiters[model_id]
        model_config = self.models[model_id]
        current_time = time.time()
        
        # 1Î∂Ñ Ïù¥Ï†Ñ ÏöîÏ≤≠ Ï†úÍ±∞
        limiter['requests'] = [
            req_time for req_time in limiter['requests']
            if current_time - req_time < 60
        ]
        
        # ÏöîÏ≤≠ Ïàò ÌôïÏù∏
        if len(limiter['requests']) >= model_config.rate_limit_rpm:
            return False
        
        return True
    
    def _record_request(self, model_id: str) -> None:
        """ÏöîÏ≤≠ Í∏∞Î°ù"""
        if model_id not in self.rate_limiters:
            self.rate_limiters[model_id] = {
                'requests': [],
                'last_reset': time.time()
            }
        
        self.rate_limiters[model_id]['requests'].append(time.time())
    
    async def generate_response(self, request: AIRequest, model_id: Optional[str] = None) -> AIResponse:
        """AI ÏùëÎãµ ÏÉùÏÑ±"""
        start_time = time.time()
        
        try:
            # Î™®Îç∏ ÏÑ†ÌÉù
            if model_id is None:
                available_models = self.get_available_models(request.task_type)
                if not available_models:
                    return AIResponse(
                        request_id=request.request_id,
                        provider=AIProvider.OPENAI,  # Í∏∞Î≥∏Í∞í
                        model_name="unavailable",
                        content="",
                        success=False,
                        error_message="ÏÇ¨Ïö© Í∞ÄÎä•Ìïú AI Î™®Îç∏Ïù¥ ÏóÜÏäµÎãàÎã§. API ÌÇ§Î•º ÏÑ§Ï†ïÌï¥Ï£ºÏÑ∏Ïöî."
                    )
                
                # Í∏∞Î≥∏Ï†ÅÏúºÎ°ú Ï≤´ Î≤àÏß∏ ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î™®Îç∏ ÏÑ†ÌÉù
                model_id = available_models[0]
            
            if model_id not in self.models:
                return AIResponse(
                    request_id=request.request_id,
                    provider=AIProvider.OPENAI,
                    model_name=model_id,
                    content="",
                    success=False,
                    error_message=f"Î™®Îç∏ÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§: {model_id}"
                )
            
            model_config = self.models[model_id]
            
            # API ÌÇ§ ÌôïÏù∏
            if not model_config.api_key:
                return AIResponse(
                    request_id=request.request_id,
                    provider=model_config.provider,
                    model_name=model_config.model_name,
                    content="",
                    success=False,
                    error_message=f"{model_config.provider.value.upper()} API ÌÇ§Í∞Ä ÏÑ§Ï†ïÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§."
                )
            
            # ÏöîÏ≤≠ ÏÜçÎèÑ Ï†úÌïú ÌôïÏù∏
            if not self._check_rate_limit(model_id):
                return AIResponse(
                    request_id=request.request_id,
                    provider=model_config.provider,
                    model_name=model_config.model_name,
                    content="",
                    success=False,
                    error_message="ÏöîÏ≤≠ ÏÜçÎèÑ Ï†úÌïúÏóê ÎèÑÎã¨ÌñàÏäµÎãàÎã§. Ïû†Ïãú ÌõÑ Îã§Ïãú ÏãúÎèÑÌï¥Ï£ºÏÑ∏Ïöî."
                )
            
            # ÏöîÏ≤≠ Í∏∞Î°ù
            self._record_request(model_id)
            
            # Í≥µÍ∏âÏûêÎ≥Ñ API Ìò∏Ï∂ú
            if model_config.provider == AIProvider.OPENAI:
                response = await self._call_openai_api(request, model_config)
            elif model_config.provider == AIProvider.ANTHROPIC:
                response = await self._call_anthropic_api(request, model_config)
            elif model_config.provider == AIProvider.GOOGLE:
                response = await self._call_google_api(request, model_config)
            else:
                return AIResponse(
                    request_id=request.request_id,
                    provider=model_config.provider,
                    model_name=model_config.model_name,
                    content="",
                    success=False,
                    error_message=f"ÏßÄÏõêÌïòÏßÄ ÏïäÎäî AI Í≥µÍ∏âÏûê: {model_config.provider.value}"
                )
            
            # Ï≤òÎ¶¨ ÏãúÍ∞Ñ Î∞è ÎπÑÏö© Í≥ÑÏÇ∞
            processing_time = (time.time() - start_time) * 1000
            response.processing_time_ms = processing_time
            
            if response.success and response.usage:
                total_tokens = response.usage.get('total_tokens', 0)
                response.cost_estimate = (total_tokens / 1000) * model_config.cost_per_1k_tokens
            
            # ÌÜµÍ≥Ñ ÏóÖÎç∞Ïù¥Ìä∏
            self._update_usage_stats(response, model_config)
            
            return response
            
        except Exception as e:
            error_response = AIResponse(
                request_id=request.request_id,
                provider=AIProvider.OPENAI,
                model_name="error",
                content="",
                success=False,
                error_message=f"AI ÏöîÏ≤≠ Ï≤òÎ¶¨ Ï§ë Ïò§Î•ò: {str(e)}"
            )
            
            self.logger.error(f"‚ùå AI ÏöîÏ≤≠ Ïã§Ìå® {request.request_id}: {e}")
            return error_response
    
    async def _call_openai_api(self, request: AIRequest, model_config: AIModelConfig) -> AIResponse:
        """OpenAI API Ìò∏Ï∂ú"""
        try:
            # OpenAI ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏÇ¨Ïö©
            try:
                import openai
                client = openai.AsyncOpenAI(api_key=model_config.api_key)
            except ImportError:
                # HTTP ÏöîÏ≤≠ÏúºÎ°ú fallback
                return await self._call_openai_http(request, model_config)
            
            # Î©îÏãúÏßÄ Íµ¨ÏÑ±
            messages = []
            if request.system_prompt:
                messages.append({"role": "system", "content": request.system_prompt})
            messages.append({"role": "user", "content": request.prompt})
            
            # API Ìò∏Ï∂ú
            response = await client.chat.completions.create(
                model=model_config.model_name,
                messages=messages,
                max_tokens=request.max_tokens or model_config.max_tokens,
                temperature=request.temperature or model_config.temperature,
                timeout=model_config.timeout_seconds
            )
            
            # ÏùëÎãµ ÌååÏã±
            content = response.choices[0].message.content
            usage = {
                'prompt_tokens': response.usage.prompt_tokens,
                'completion_tokens': response.usage.completion_tokens,
                'total_tokens': response.usage.total_tokens
            }
            
            return AIResponse(
                request_id=request.request_id,
                provider=model_config.provider,
                model_name=model_config.model_name,
                content=content,
                usage=usage,
                success=True
            )
            
        except Exception as e:
            return AIResponse(
                request_id=request.request_id,
                provider=model_config.provider,
                model_name=model_config.model_name,
                content="",
                success=False,
                error_message=f"OpenAI API Ïò§Î•ò: {str(e)}"
            )
    
    async def _call_openai_http(self, request: AIRequest, model_config: AIModelConfig) -> AIResponse:
        """OpenAI HTTP API Ìò∏Ï∂ú"""
        try:
            headers = {
                "Authorization": f"Bearer {model_config.api_key}",
                "Content-Type": "application/json"
            }
            
            messages = []
            if request.system_prompt:
                messages.append({"role": "system", "content": request.system_prompt})
            messages.append({"role": "user", "content": request.prompt})
            
            payload = {
                "model": model_config.model_name,
                "messages": messages,
                "max_tokens": request.max_tokens or model_config.max_tokens,
                "temperature": request.temperature or model_config.temperature
            }
            
            timeout = aiohttp.ClientTimeout(total=model_config.timeout_seconds)
            
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.post(
                    "https://api.openai.com/v1/chat/completions",
                    headers=headers,
                    json=payload
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        content = data['choices'][0]['message']['content']
                        usage = data.get('usage', {})
                        
                        return AIResponse(
                            request_id=request.request_id,
                            provider=model_config.provider,
                            model_name=model_config.model_name,
                            content=content,
                            usage=usage,
                            success=True
                        )
                    else:
                        error_text = await response.text()
                        return AIResponse(
                            request_id=request.request_id,
                            provider=model_config.provider,
                            model_name=model_config.model_name,
                            content="",
                            success=False,
                            error_message=f"OpenAI HTTP Ïò§Î•ò {response.status}: {error_text}"
                        )
            
        except Exception as e:
            return AIResponse(
                request_id=request.request_id,
                provider=model_config.provider,
                model_name=model_config.model_name,
                content="",
                success=False,
                error_message=f"OpenAI HTTP ÏöîÏ≤≠ Ïò§Î•ò: {str(e)}"
            )
    
    async def _call_anthropic_api(self, request: AIRequest, model_config: AIModelConfig) -> AIResponse:
        """Anthropic Claude API Ìò∏Ï∂ú"""
        try:
            headers = {
                "x-api-key": model_config.api_key,
                "Content-Type": "application/json",
                "anthropic-version": "2023-06-01"
            }
            
            payload = {
                "model": model_config.model_name,
                "max_tokens": request.max_tokens or model_config.max_tokens,
                "temperature": request.temperature or model_config.temperature,
                "messages": [{"role": "user", "content": request.prompt}]
            }
            
            if request.system_prompt:
                payload["system"] = request.system_prompt
            
            timeout = aiohttp.ClientTimeout(total=model_config.timeout_seconds)
            base_url = model_config.base_url or "https://api.anthropic.com"
            
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.post(
                    f"{base_url}/v1/messages",
                    headers=headers,
                    json=payload
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        content = data['content'][0]['text']
                        usage = data.get('usage', {})
                        
                        return AIResponse(
                            request_id=request.request_id,
                            provider=model_config.provider,
                            model_name=model_config.model_name,
                            content=content,
                            usage=usage,
                            success=True
                        )
                    else:
                        error_text = await response.text()
                        return AIResponse(
                            request_id=request.request_id,
                            provider=model_config.provider,
                            model_name=model_config.model_name,
                            content="",
                            success=False,
                            error_message=f"Anthropic API Ïò§Î•ò {response.status}: {error_text}"
                        )
            
        except Exception as e:
            return AIResponse(
                request_id=request.request_id,
                provider=model_config.provider,
                model_name=model_config.model_name,
                content="",
                success=False,
                error_message=f"Anthropic API ÏöîÏ≤≠ Ïò§Î•ò: {str(e)}"
            )
    
    async def _call_google_api(self, request: AIRequest, model_config: AIModelConfig) -> AIResponse:
        """Google Gemini API Ìò∏Ï∂ú"""
        try:
            headers = {
                "Content-Type": "application/json"
            }
            
            # ÌîÑÎ°¨ÌîÑÌä∏ Íµ¨ÏÑ±
            prompt_text = request.prompt
            if request.system_prompt:
                prompt_text = f"{request.system_prompt}\n\n{prompt_text}"
            
            payload = {
                "contents": [{
                    "parts": [{"text": prompt_text}]
                }],
                "generationConfig": {
                    "maxOutputTokens": request.max_tokens or model_config.max_tokens,
                    "temperature": request.temperature or model_config.temperature
                }
            }
            
            timeout = aiohttp.ClientTimeout(total=model_config.timeout_seconds)
            base_url = model_config.base_url or "https://generativelanguage.googleapis.com"
            
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.post(
                    f"{base_url}/v1beta/models/{model_config.model_name}:generateContent?key={model_config.api_key}",
                    headers=headers,
                    json=payload
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        content = data['candidates'][0]['content']['parts'][0]['text']
                        usage = data.get('usageMetadata', {})
                        
                        return AIResponse(
                            request_id=request.request_id,
                            provider=model_config.provider,
                            model_name=model_config.model_name,
                            content=content,
                            usage=usage,
                            success=True
                        )
                    else:
                        error_text = await response.text()
                        return AIResponse(
                            request_id=request.request_id,
                            provider=model_config.provider,
                            model_name=model_config.model_name,
                            content="",
                            success=False,
                            error_message=f"Google API Ïò§Î•ò {response.status}: {error_text}"
                        )
            
        except Exception as e:
            return AIResponse(
                request_id=request.request_id,
                provider=model_config.provider,
                model_name=model_config.model_name,
                content="",
                success=False,
                error_message=f"Google API ÏöîÏ≤≠ Ïò§Î•ò: {str(e)}"
            )
    
    def _update_usage_stats(self, response: AIResponse, model_config: AIModelConfig) -> None:
        """ÏÇ¨Ïö© ÌÜµÍ≥Ñ ÏóÖÎç∞Ïù¥Ìä∏"""
        with self.lock:
            self.usage_stats['total_requests'] += 1
            
            if response.success:
                self.usage_stats['successful_requests'] += 1
                
                # ÌÜ†ÌÅ∞ ÏÇ¨Ïö©Îüâ
                total_tokens = response.usage.get('total_tokens', 0)
                self.usage_stats['total_tokens_used'] += total_tokens
                
                # ÎπÑÏö©
                self.usage_stats['total_cost'] += response.cost_estimate
                
                # Í≥µÍ∏âÏûêÎ≥Ñ ÌÜµÍ≥Ñ
                provider_name = response.provider.value
                if provider_name not in self.usage_stats['requests_by_provider']:
                    self.usage_stats['requests_by_provider'][provider_name] = 0
                self.usage_stats['requests_by_provider'][provider_name] += 1
                
            else:
                self.usage_stats['failed_requests'] += 1
    
    def get_usage_stats(self) -> Dict[str, Any]:
        """ÏÇ¨Ïö© ÌÜµÍ≥Ñ Î∞òÌôò"""
        with self.lock:
            stats = self.usage_stats.copy()
            stats['session_duration_minutes'] = (datetime.now() - stats['session_start']).total_seconds() / 60
            stats['success_rate'] = (
                stats['successful_requests'] / stats['total_requests'] * 100
                if stats['total_requests'] > 0 else 0
            )
            return stats
    
    def get_model_info(self, model_id: str) -> Optional[Dict[str, Any]]:
        """Î™®Îç∏ Ï†ïÎ≥¥ Î∞òÌôò"""
        if model_id not in self.models:
            return None
        
        model_config = self.models[model_id]
        
        return {
            'model_id': model_id,
            'provider': model_config.provider.value,
            'model_name': model_config.model_name,
            'max_tokens': model_config.max_tokens,
            'context_window': model_config.context_window,
            'cost_per_1k_tokens': model_config.cost_per_1k_tokens,
            'rate_limit_rpm': model_config.rate_limit_rpm,
            'supports_streaming': model_config.supports_streaming,
            'is_enabled': model_config.is_enabled,
            'has_api_key': bool(model_config.api_key)
        }
    
    def test_model_connection(self, model_id: str) -> Dict[str, Any]:
        """Î™®Îç∏ Ïó∞Í≤∞ ÌÖåÏä§Ìä∏"""
        if model_id not in self.models:
            return {
                'success': False,
                'error': f'Î™®Îç∏ÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§: {model_id}'
            }
        
        try:
            # Í∞ÑÎã®Ìïú ÌÖåÏä§Ìä∏ ÏöîÏ≤≠
            test_request = AIRequest(
                task_type=TaskType.GENERATION,
                prompt="ÏïàÎÖïÌïòÏÑ∏Ïöî! Í∞ÑÎã®Ìûà Ïù∏ÏÇ¨ÎßêÎ°ú ÎãµÎ≥ÄÌï¥Ï£ºÏÑ∏Ïöî.",
                max_tokens=50,
                temperature=0.7
            )
            
            # ÎèôÍ∏∞ Î∞©ÏãùÏúºÎ°ú ÌÖåÏä§Ìä∏ (asyncio Ïù¥Î≤§Ìä∏ Î£®ÌîÑ ÌôïÏù∏)
            try:
                loop = asyncio.get_event_loop()
                if loop.is_running():
                    # Ïù¥ÎØ∏ Ïã§Ìñâ Ï§ëÏù∏ Î£®ÌîÑÍ∞Ä ÏûàÏúºÎ©¥ ÏÉà Ïä§Î†àÎìúÏóêÏÑú Ïã§Ìñâ
                    import concurrent.futures
                    with concurrent.futures.ThreadPoolExecutor() as executor:
                        future = executor.submit(asyncio.run, self.generate_response(test_request, model_id))
                        response = future.result(timeout=30)
                else:
                    response = asyncio.run(self.generate_response(test_request, model_id))
            except RuntimeError:
                # Ïù¥Î≤§Ìä∏ Î£®ÌîÑÍ∞Ä ÏóÜÏúºÎ©¥ ÏÉàÎ°ú ÏÉùÏÑ±
                response = asyncio.run(self.generate_response(test_request, model_id))
            
            if response.success:
                return {
                    'success': True,
                    'model_name': response.model_name,
                    'response_preview': response.content[:100] + "..." if len(response.content) > 100 else response.content,
                    'processing_time_ms': response.processing_time_ms,
                    'tokens_used': response.usage.get('total_tokens', 0),
                    'cost_estimate': response.cost_estimate
                }
            else:
                return {
                    'success': False,
                    'error': response.error_message
                }
                
        except Exception as e:
            return {
                'success': False,
                'error': f'Ïó∞Í≤∞ ÌÖåÏä§Ìä∏ Ï§ë Ïò§Î•ò: {str(e)}'
            }
    
    def cleanup(self) -> None:
        """Î¶¨ÏÜåÏä§ Ï†ïÎ¶¨"""
        with self.lock:
            # ÏÑ§Ï†ï Ï†ÄÏû•
            self._save_configurations()
            
            # ÌÜµÍ≥Ñ Î¶¨ÏÖã
            self.usage_stats = {
                'total_requests': 0,
                'successful_requests': 0,
                'failed_requests': 0,
                'total_tokens_used': 0,
                'total_cost': 0.0,
                'requests_by_provider': {},
                'session_start': datetime.now()
            }
            
            self.logger.info("üßπ Ïô∏Î∂Ä AI Î™®Îç∏ ÌÜµÌï© ÏãúÏä§ÌÖú Ï†ïÎ¶¨ ÏôÑÎ£å")

# Ï†ÑÏó≠ Ïô∏Î∂Ä AI ÌÜµÌï© Í¥ÄÎ¶¨Ïûê
_global_ai_integrator = None
_global_integrator_lock = threading.Lock()

def get_global_ai_integrator() -> ExternalAIIntegrator:
    """Ï†ÑÏó≠ Ïô∏Î∂Ä AI ÌÜµÌï© Í¥ÄÎ¶¨Ïûê Í∞ÄÏ†∏Ïò§Í∏∞"""
    global _global_ai_integrator
    
    with _global_integrator_lock:
        if _global_ai_integrator is None:
            _global_ai_integrator = ExternalAIIntegrator()
        return _global_ai_integrator

# Ìé∏Ïùò Ìï®ÏàòÎì§
async def generate_ai_response(prompt: str, task_type: TaskType = TaskType.GENERATION,
                              system_prompt: Optional[str] = None, model_id: Optional[str] = None) -> AIResponse:
    """AI ÏùëÎãµ ÏÉùÏÑ± (Ìé∏Ïùò Ìï®Ïàò)"""
    integrator = get_global_ai_integrator()
    
    request = AIRequest(
        task_type=task_type,
        prompt=prompt,
        system_prompt=system_prompt
    )
    
    return await integrator.generate_response(request, model_id)

def set_ai_api_key(provider: str, api_key: str) -> bool:
    """AI API ÌÇ§ ÏÑ§Ï†ï (Ìé∏Ïùò Ìï®Ïàò)"""
    integrator = get_global_ai_integrator()
    
    try:
        provider_enum = AIProvider(provider.lower())
        return integrator.set_api_key(provider_enum, api_key)
    except ValueError:
        return False

def get_ai_usage_stats() -> Dict[str, Any]:
    """AI ÏÇ¨Ïö© ÌÜµÍ≥Ñ Ï°∞Ìöå (Ìé∏Ïùò Ìï®Ïàò)"""
    integrator = get_global_ai_integrator()
    return integrator.get_usage_stats()

# ÏÇ¨Ïö© ÏòàÏãú
if __name__ == "__main__":
    async def test_ai_integrator():
        integrator = ExternalAIIntegrator()
        
        # API ÌÇ§ ÏÑ§Ï†ï (Ïã§Ï†ú ÏÇ¨Ïö© Ïãú ÌôòÍ≤ΩÎ≥ÄÏàòÎÇò ÏÑ§Ï†ï ÌååÏùºÏóêÏÑú Î°úÎìú)
        # integrator.set_api_key(AIProvider.OPENAI, "your-openai-api-key")
        
        # ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î™®Îç∏ ÌôïÏù∏
        models = integrator.get_available_models()
        print(f"ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î™®Îç∏: {models}")
        
        if models:
            # ÌÖåÏä§Ìä∏ ÏöîÏ≤≠
            request = AIRequest(
                task_type=TaskType.SUMMARIZATION,
                prompt="Îã§Ïùå ÌÖçÏä§Ìä∏Î•º ÏöîÏïΩÌï¥Ï£ºÏÑ∏Ïöî: Ïù∏Í≥µÏßÄÎä•ÏùÄ Ïª¥Ìì®ÌÑ∞ ÏãúÏä§ÌÖúÏù¥ Ïù∏Í∞ÑÏùò ÏßÄÎä•ÏùÑ Î™®Î∞©ÌïòÏó¨ ÌïôÏäµ, Ï∂îÎ°†, Î¨∏Ï†ú Ìï¥Í≤∞ Îì±Ïùò ÏûëÏóÖÏùÑ ÏàòÌñâÌï† Ïàò ÏûàÎèÑÎ°ù ÌïòÎäî Í∏∞Ïà†ÏûÖÎãàÎã§.",
                system_prompt="ÎãπÏã†ÏùÄ Ï†ÑÎ¨∏Ï†ÅÏù∏ ÏöîÏïΩ Ïñ¥ÏãúÏä§ÌÑ¥Ìä∏ÏûÖÎãàÎã§. ÌïµÏã¨ ÎÇ¥Ïö©ÏùÑ Í∞ÑÍ≤∞ÌïòÍ≤å Ï†ïÎ¶¨Ìï¥Ï£ºÏÑ∏Ïöî."
            )
            
            # ÏùëÎãµ ÏÉùÏÑ±
            response = await integrator.generate_response(request, models[0])
            
            print(f"\nÏùëÎãµ ÏÑ±Í≥µ: {response.success}")
            if response.success:
                print(f"Î™®Îç∏: {response.model_name}")
                print(f"ÏùëÎãµ: {response.content}")
                print(f"ÌÜ†ÌÅ∞ ÏÇ¨Ïö©: {response.usage}")
                print(f"Ï≤òÎ¶¨ ÏãúÍ∞Ñ: {response.processing_time_ms:.1f}ms")
                print(f"ÏòàÏÉÅ ÎπÑÏö©: ${response.cost_estimate:.6f}")
            else:
                print(f"Ïò§Î•ò: {response.error_message}")
        
        # ÏÇ¨Ïö© ÌÜµÍ≥Ñ
        stats = integrator.get_usage_stats()
        print(f"\nÏÇ¨Ïö© ÌÜµÍ≥Ñ: {stats}")
        
        # Ï†ïÎ¶¨
        integrator.cleanup()
    
    # ÌÖåÏä§Ìä∏ Ïã§Ìñâ
    print("ü§ñ Ïô∏Î∂Ä AI Î™®Îç∏ ÌÜµÌï© ÏãúÏä§ÌÖú ÌÖåÏä§Ìä∏ ÏãúÏûë")
    asyncio.run(test_ai_integrator())
    print("‚úÖ ÌÖåÏä§Ìä∏ ÏôÑÎ£å!")