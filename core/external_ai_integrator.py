#!/usr/bin/env python3
"""
ì™¸ë¶€ AI ëª¨ë¸ í†µí•© ì‹œìŠ¤í…œ v2.6
OpenAI GPT, Anthropic Claude, Google Gemini API í†µí•©
"""

import os
import time
import json
import logging
import asyncio
import aiohttp
from typing import Dict, List, Any, Optional, Union, AsyncGenerator
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from pathlib import Path
from enum import Enum
import threading

class AIProvider(Enum):
    """AI ê³µê¸‰ì"""
    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    GOOGLE = "google"
    COHERE = "cohere"
    HUGGINGFACE = "huggingface"

class TaskType(Enum):
    """ì‘ì—… íƒ€ì…"""
    SUMMARIZATION = "summarization"
    TRANSLATION = "translation"
    ANALYSIS = "analysis"
    QA = "question_answering"
    ENHANCEMENT = "enhancement"
    CLASSIFICATION = "classification"
    GENERATION = "generation"

@dataclass
class AIModelConfig:
    """AI ëª¨ë¸ ì„¤ì •"""
    provider: AIProvider
    model_name: str
    api_key: Optional[str] = None
    base_url: Optional[str] = None
    max_tokens: int = 4000
    temperature: float = 0.7
    timeout_seconds: int = 30
    rate_limit_rpm: int = 60  # requests per minute
    cost_per_1k_tokens: float = 0.0
    supports_streaming: bool = False
    context_window: int = 4000
    is_enabled: bool = True

@dataclass
class AIRequest:
    """AI ìš”ì²­"""
    task_type: TaskType
    prompt: str
    system_prompt: Optional[str] = None
    context: Optional[Dict[str, Any]] = None
    max_tokens: Optional[int] = None
    temperature: Optional[float] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    request_id: str = field(default_factory=lambda: f"req_{int(time.time() * 1000)}")

@dataclass
class AIResponse:
    """AI ì‘ë‹µ"""
    request_id: str
    provider: AIProvider
    model_name: str
    content: str
    usage: Dict[str, int] = field(default_factory=dict)
    processing_time_ms: float = 0.0
    cost_estimate: float = 0.0
    success: bool = True
    error_message: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)

class ExternalAIIntegrator:
    """ì™¸ë¶€ AI ëª¨ë¸ í†µí•© ê´€ë¦¬ì"""
    
    def __init__(self, config_path: Optional[Path] = None):
        self.config_path = config_path or Path.home() / ".solomond_ai" / "ai_configs.json"
        self.lock = threading.RLock()
        self.logger = self._setup_logging()
        
        # AI ëª¨ë¸ ì„¤ì •
        self.models: Dict[str, AIModelConfig] = {}
        self.rate_limiters: Dict[str, Dict] = {}
        
        # ì‚¬ìš© í†µê³„
        self.usage_stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'total_tokens_used': 0,
            'total_cost': 0.0,
            'requests_by_provider': {},
            'session_start': datetime.now()
        }
        
        # ì„¤ì • ë¡œë“œ
        self._load_configurations()
        self._initialize_default_models()
        
        # ì‚¬ìš©ì ì„¤ì • ì—°ë™
        try:
            from .user_settings_manager import get_global_settings_manager
            self.settings_manager = get_global_settings_manager()
            self._load_api_keys_from_settings()
        except ImportError:
            self.settings_manager = None
        
        self.logger.info("ğŸ¤– ì™¸ë¶€ AI ëª¨ë¸ í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _setup_logging(self) -> logging.Logger:
        """ë¡œê¹… ì„¤ì •"""
        logger = logging.getLogger(f'{__name__}.{self.__class__.__name__}')
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s | %(name)s | %(levelname)s | %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            logger.setLevel(logging.INFO)
        return logger
    
    def _load_configurations(self) -> None:
        """AI ëª¨ë¸ ì„¤ì • ë¡œë“œ"""
        try:
            if self.config_path.exists():
                with open(self.config_path, 'r', encoding='utf-8') as f:
                    config_data = json.load(f)
                
                for model_id, model_data in config_data.get('models', {}).items():
                    model_data['provider'] = AIProvider(model_data['provider'])
                    self.models[model_id] = AIModelConfig(**model_data)
                
                self.logger.info(f"ğŸ“¥ AI ëª¨ë¸ ì„¤ì • ë¡œë“œ: {len(self.models)}ê°œ ëª¨ë¸")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ AI ëª¨ë¸ ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def _save_configurations(self) -> None:
        """AI ëª¨ë¸ ì„¤ì • ì €ì¥"""
        try:
            # ë””ë ‰í† ë¦¬ ìƒì„±
            self.config_path.parent.mkdir(parents=True, exist_ok=True)
            
            config_data = {
                'models': {},
                'updated_at': datetime.now().isoformat(),
                'version': '2.6'
            }
            
            for model_id, model_config in self.models.items():
                model_data = {
                    'provider': model_config.provider.value,
                    'model_name': model_config.model_name,
                    'api_key': model_config.api_key,
                    'base_url': model_config.base_url,
                    'max_tokens': model_config.max_tokens,
                    'temperature': model_config.temperature,
                    'timeout_seconds': model_config.timeout_seconds,
                    'rate_limit_rpm': model_config.rate_limit_rpm,
                    'cost_per_1k_tokens': model_config.cost_per_1k_tokens,
                    'supports_streaming': model_config.supports_streaming,
                    'context_window': model_config.context_window,
                    'is_enabled': model_config.is_enabled
                }
                config_data['models'][model_id] = model_data
            
            with open(self.config_path, 'w', encoding='utf-8') as f:
                json.dump(config_data, f, indent=2, ensure_ascii=False)
            
            self.logger.debug("ğŸ’¾ AI ëª¨ë¸ ì„¤ì • ì €ì¥ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ AI ëª¨ë¸ ì„¤ì • ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _initialize_default_models(self) -> None:
        """ê¸°ë³¸ AI ëª¨ë¸ ì´ˆê¸°í™”"""
        default_models = [
            # OpenAI GPT ëª¨ë¸ë“¤
            AIModelConfig(
                provider=AIProvider.OPENAI,
                model_name="gpt-4o-mini",
                max_tokens=4000,
                temperature=0.7,
                rate_limit_rpm=500,
                cost_per_1k_tokens=0.00015,
                supports_streaming=True,
                context_window=128000,
                is_enabled=True
            ),
            AIModelConfig(
                provider=AIProvider.OPENAI,
                model_name="gpt-4o",
                max_tokens=4000,
                temperature=0.7,
                rate_limit_rpm=500,
                cost_per_1k_tokens=0.005,
                supports_streaming=True,
                context_window=128000,
                is_enabled=True
            ),
            AIModelConfig(
                provider=AIProvider.OPENAI,
                model_name="gpt-3.5-turbo",
                max_tokens=4000,
                temperature=0.7,
                rate_limit_rpm=3500,
                cost_per_1k_tokens=0.0005,
                supports_streaming=True,
                context_window=16385,
                is_enabled=True
            ),
            
            # Anthropic Claude ëª¨ë¸ë“¤
            AIModelConfig(
                provider=AIProvider.ANTHROPIC,
                model_name="claude-3-5-sonnet-20241022",
                base_url="https://api.anthropic.com",
                max_tokens=4000,
                temperature=0.7,
                rate_limit_rpm=50,
                cost_per_1k_tokens=0.003,
                supports_streaming=True,
                context_window=200000,
                is_enabled=True
            ),
            AIModelConfig(
                provider=AIProvider.ANTHROPIC,
                model_name="claude-3-5-haiku-20241022",
                base_url="https://api.anthropic.com",
                max_tokens=4000,
                temperature=0.7,
                rate_limit_rpm=50,
                cost_per_1k_tokens=0.00025,
                supports_streaming=True,
                context_window=200000,
                is_enabled=True
            ),
            
            # Google Gemini ëª¨ë¸ë“¤
            AIModelConfig(
                provider=AIProvider.GOOGLE,
                model_name="gemini-1.5-flash",
                base_url="https://generativelanguage.googleapis.com",
                max_tokens=8192,
                temperature=0.7,
                rate_limit_rpm=15,
                cost_per_1k_tokens=0.00015,
                supports_streaming=True,
                context_window=1000000,
                is_enabled=True
            ),
            AIModelConfig(
                provider=AIProvider.GOOGLE,
                model_name="gemini-1.5-pro",
                base_url="https://generativelanguage.googleapis.com",
                max_tokens=8192,
                temperature=0.7,
                rate_limit_rpm=2,
                cost_per_1k_tokens=0.0035,
                supports_streaming=True,
                context_window=2000000,
                is_enabled=True
            )
        ]
        
        # ê¸°ì¡´ì— ì—†ëŠ” ëª¨ë¸ë§Œ ì¶”ê°€
        for model_config in default_models:
            model_id = f"{model_config.provider.value}_{model_config.model_name.replace('-', '_').replace('.', '_')}"
            if model_id not in self.models:
                self.models[model_id] = model_config
                
        self.logger.info(f"ğŸ”§ ê¸°ë³¸ AI ëª¨ë¸ ì´ˆê¸°í™”: {len(self.models)}ê°œ ëª¨ë¸ ë“±ë¡")
    
    def _load_api_keys_from_settings(self) -> None:
        """ì‚¬ìš©ì ì„¤ì •ì—ì„œ API í‚¤ ë¡œë“œ"""
        if not self.settings_manager:
            return
        
        try:
            # OpenAI API í‚¤
            openai_key = self.settings_manager.get_setting("api.openai_key")
            if openai_key:
                for model_id, model_config in self.models.items():
                    if model_config.provider == AIProvider.OPENAI:
                        model_config.api_key = openai_key
            
            # Anthropic API í‚¤
            anthropic_key = self.settings_manager.get_setting("api.anthropic_key")
            if anthropic_key:
                for model_id, model_config in self.models.items():
                    if model_config.provider == AIProvider.ANTHROPIC:
                        model_config.api_key = anthropic_key
            
            # Google API í‚¤
            google_key = self.settings_manager.get_setting("api.google_key")
            if google_key:
                for model_id, model_config in self.models.items():
                    if model_config.provider == AIProvider.GOOGLE:
                        model_config.api_key = google_key
            
            self.logger.debug("ğŸ”‘ ì‚¬ìš©ì ì„¤ì •ì—ì„œ API í‚¤ ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ API í‚¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def add_model(self, model_id: str, model_config: AIModelConfig) -> bool:
        """AI ëª¨ë¸ ì¶”ê°€"""
        try:
            with self.lock:
                self.models[model_id] = model_config
                self._save_configurations()
                
                self.logger.info(f"â• AI ëª¨ë¸ ì¶”ê°€: {model_id} ({model_config.provider.value})")
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ AI ëª¨ë¸ ì¶”ê°€ ì‹¤íŒ¨ {model_id}: {e}")
            return False
    
    def remove_model(self, model_id: str) -> bool:
        """AI ëª¨ë¸ ì œê±°"""
        try:
            with self.lock:
                if model_id in self.models:
                    del self.models[model_id]
                    self._save_configurations()
                    
                    self.logger.info(f"â– AI ëª¨ë¸ ì œê±°: {model_id}")
                    return True
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ AI ëª¨ë¸ ì œê±° ì‹¤íŒ¨ {model_id}: {e}")
            return False
    
    def set_api_key(self, provider: AIProvider, api_key: str) -> bool:
        """API í‚¤ ì„¤ì •"""
        try:
            with self.lock:
                # í•´ë‹¹ ê³µê¸‰ìì˜ ëª¨ë“  ëª¨ë¸ì— API í‚¤ ì ìš©
                updated_count = 0
                for model_config in self.models.values():
                    if model_config.provider == provider:
                        model_config.api_key = api_key
                        updated_count += 1
                
                # ì‚¬ìš©ì ì„¤ì •ì—ë„ ì €ì¥
                if self.settings_manager:
                    key_name = f"api.{provider.value}_key"
                    from .user_settings_manager import SettingType, SettingScope
                    self.settings_manager.set_setting(
                        key_name, 
                        api_key,
                        SettingType.SYSTEM,
                        SettingScope.GLOBAL,
                        f"{provider.value.upper()} API í‚¤"
                    )
                
                self._save_configurations()
                
                self.logger.info(f"ğŸ”‘ {provider.value.upper()} API í‚¤ ì„¤ì •: {updated_count}ê°œ ëª¨ë¸ ì—…ë°ì´íŠ¸")
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ API í‚¤ ì„¤ì • ì‹¤íŒ¨ {provider.value}: {e}")
            return False
    
    def get_available_models(self, task_type: Optional[TaskType] = None) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡"""
        with self.lock:
            available = []
            
            for model_id, model_config in self.models.items():
                if not model_config.is_enabled or not model_config.api_key:
                    continue
                
                # ì‘ì—… íƒ€ì…ë³„ í•„í„°ë§ (í•„ìš”ì‹œ í™•ì¥)
                if task_type and task_type == TaskType.SUMMARIZATION:
                    # ìš”ì•½ ì‘ì—…ì— ì í•©í•œ ëª¨ë¸ë§Œ
                    if model_config.max_tokens < 1000:
                        continue
                
                available.append(model_id)
            
            return sorted(available)
    
    def _check_rate_limit(self, model_id: str) -> bool:
        """ìš”ì²­ ì†ë„ ì œí•œ í™•ì¸"""
        if model_id not in self.rate_limiters:
            self.rate_limiters[model_id] = {
                'requests': [],
                'last_reset': time.time()
            }
        
        limiter = self.rate_limiters[model_id]
        model_config = self.models[model_id]
        current_time = time.time()
        
        # 1ë¶„ ì´ì „ ìš”ì²­ ì œê±°
        limiter['requests'] = [
            req_time for req_time in limiter['requests']
            if current_time - req_time < 60
        ]
        
        # ìš”ì²­ ìˆ˜ í™•ì¸
        if len(limiter['requests']) >= model_config.rate_limit_rpm:
            return False
        
        return True
    
    def _record_request(self, model_id: str) -> None:
        """ìš”ì²­ ê¸°ë¡"""
        if model_id not in self.rate_limiters:
            self.rate_limiters[model_id] = {
                'requests': [],
                'last_reset': time.time()
            }
        
        self.rate_limiters[model_id]['requests'].append(time.time())
    
    async def generate_response(self, request: AIRequest, model_id: Optional[str] = None) -> AIResponse:
        """AI ì‘ë‹µ ìƒì„±"""
        start_time = time.time()
        
        try:
            # ëª¨ë¸ ì„ íƒ
            if model_id is None:
                available_models = self.get_available_models(request.task_type)
                if not available_models:
                    return AIResponse(
                        request_id=request.request_id,
                        provider=AIProvider.OPENAI,  # ê¸°ë³¸ê°’
                        model_name="unavailable",
                        content="",
                        success=False,
                        error_message="ì‚¬ìš© ê°€ëŠ¥í•œ AI ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”."
                    )
                
                # ê¸°ë³¸ì ìœ¼ë¡œ ì²« ë²ˆì§¸ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì„ íƒ
                model_id = available_models[0]
            
            if model_id not in self.models:
                return AIResponse(
                    request_id=request.request_id,
                    provider=AIProvider.OPENAI,
                    model_name=model_id,
                    content="",
                    success=False,
                    error_message=f"ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_id}"
                )
            
            model_config = self.models[model_id]
            
            # API í‚¤ í™•ì¸
            if not model_config.api_key:
                return AIResponse(
                    request_id=request.request_id,
                    provider=model_config.provider,
                    model_name=model_config.model_name,
                    content="",
                    success=False,
                    error_message=f"{model_config.provider.value.upper()} API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
                )
            
            # ìš”ì²­ ì†ë„ ì œí•œ í™•ì¸
            if not self._check_rate_limit(model_id):
                return AIResponse(
                    request_id=request.request_id,
                    provider=model_config.provider,
                    model_name=model_config.model_name,
                    content="",
                    success=False,
                    error_message="ìš”ì²­ ì†ë„ ì œí•œì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                )
            
            # ìš”ì²­ ê¸°ë¡
            self._record_request(model_id)
            
            # ê³µê¸‰ìë³„ API í˜¸ì¶œ
            if model_config.provider == AIProvider.OPENAI:
                response = await self._call_openai_api(request, model_config)
            elif model_config.provider == AIProvider.ANTHROPIC:
                response = await self._call_anthropic_api(request, model_config)
            elif model_config.provider == AIProvider.GOOGLE:
                response = await self._call_google_api(request, model_config)
            else:
                return AIResponse(
                    request_id=request.request_id,
                    provider=model_config.provider,
                    model_name=model_config.model_name,
                    content="",
                    success=False,
                    error_message=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” AI ê³µê¸‰ì: {model_config.provider.value}"
                )
            
            # ì²˜ë¦¬ ì‹œê°„ ë° ë¹„ìš© ê³„ì‚°
            processing_time = (time.time() - start_time) * 1000
            response.processing_time_ms = processing_time
            
            if response.success and response.usage:
                total_tokens = response.usage.get('total_tokens', 0)
                response.cost_estimate = (total_tokens / 1000) * model_config.cost_per_1k_tokens
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self._update_usage_stats(response, model_config)
            
            return response
            
        except Exception as e:
            error_response = AIResponse(
                request_id=request.request_id,
                provider=AIProvider.OPENAI,
                model_name="error",
                content="",
                success=False,
                error_message=f"AI ìš”ì²­ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}"
            )
            
            self.logger.error(f"âŒ AI ìš”ì²­ ì‹¤íŒ¨ {request.request_id}: {e}")
            return error_response
    
    async def _call_openai_api(self, request: AIRequest, model_config: AIModelConfig) -> AIResponse:
        """OpenAI API í˜¸ì¶œ"""
        try:
            # OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©
            try:
                import openai
                client = openai.AsyncOpenAI(api_key=model_config.api_key)
            except ImportError:
                # HTTP ìš”ì²­ìœ¼ë¡œ fallback
                return await self._call_openai_http(request, model_config)
            
            # ë©”ì‹œì§€ êµ¬ì„±
            messages = []
            if request.system_prompt:
                messages.append({"role": "system", "content": request.system_prompt})
            messages.append({"role": "user", "content": request.prompt})
            
            # API í˜¸ì¶œ
            response = await client.chat.completions.create(
                model=model_config.model_name,
                messages=messages,
                max_tokens=request.max_tokens or model_config.max_tokens,
                temperature=request.temperature or model_config.temperature,
                timeout=model_config.timeout_seconds
            )
            
            # ì‘ë‹µ íŒŒì‹±
            content = response.choices[0].message.content
            usage = {
                'prompt_tokens': response.usage.prompt_tokens,
                'completion_tokens': response.usage.completion_tokens,
                'total_tokens': response.usage.total_tokens
            }
            
            return AIResponse(
                request_id=request.request_id,
                provider=model_config.provider,
                model_name=model_config.model_name,
                content=content,
                usage=usage,
                success=True
            )
            
        except Exception as e:
            return AIResponse(
                request_id=request.request_id,
                provider=model_config.provider,
                model_name=model_config.model_name,
                content="",
                success=False,
                error_message=f"OpenAI API ì˜¤ë¥˜: {str(e)}"
            )
    
    async def _call_openai_http(self, request: AIRequest, model_config: AIModelConfig) -> AIResponse:
        """OpenAI HTTP API í˜¸ì¶œ"""
        try:
            headers = {
                "Authorization": f"Bearer {model_config.api_key}",
                "Content-Type": "application/json"
            }
            
            messages = []
            if request.system_prompt:
                messages.append({"role": "system", "content": request.system_prompt})
            messages.append({"role": "user", "content": request.prompt})
            
            payload = {
                "model": model_config.model_name,
                "messages": messages,
                "max_tokens": request.max_tokens or model_config.max_tokens,
                "temperature": request.temperature or model_config.temperature
            }
            
            timeout = aiohttp.ClientTimeout(total=model_config.timeout_seconds)
            
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.post(
                    "https://api.openai.com/v1/chat/completions",
                    headers=headers,
                    json=payload
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        content = data['choices'][0]['message']['content']
                        usage = data.get('usage', {})
                        
                        return AIResponse(
                            request_id=request.request_id,
                            provider=model_config.provider,
                            model_name=model_config.model_name,
                            content=content,
                            usage=usage,
                            success=True
                        )
                    else:
                        error_text = await response.text()
                        return AIResponse(
                            request_id=request.request_id,
                            provider=model_config.provider,
                            model_name=model_config.model_name,
                            content="",
                            success=False,
                            error_message=f"OpenAI HTTP ì˜¤ë¥˜ {response.status}: {error_text}"
                        )
            
        except Exception as e:
            return AIResponse(
                request_id=request.request_id,
                provider=model_config.provider,
                model_name=model_config.model_name,
                content="",
                success=False,
                error_message=f"OpenAI HTTP ìš”ì²­ ì˜¤ë¥˜: {str(e)}"
            )
    
    async def _call_anthropic_api(self, request: AIRequest, model_config: AIModelConfig) -> AIResponse:
        """Anthropic Claude API í˜¸ì¶œ"""
        try:
            headers = {
                "x-api-key": model_config.api_key,
                "Content-Type": "application/json",
                "anthropic-version": "2023-06-01"
            }
            
            payload = {
                "model": model_config.model_name,
                "max_tokens": request.max_tokens or model_config.max_tokens,
                "temperature": request.temperature or model_config.temperature,
                "messages": [{"role": "user", "content": request.prompt}]
            }
            
            if request.system_prompt:
                payload["system"] = request.system_prompt
            
            timeout = aiohttp.ClientTimeout(total=model_config.timeout_seconds)
            base_url = model_config.base_url or "https://api.anthropic.com"
            
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.post(
                    f"{base_url}/v1/messages",
                    headers=headers,
                    json=payload
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        content = data['content'][0]['text']
                        usage = data.get('usage', {})
                        
                        return AIResponse(
                            request_id=request.request_id,
                            provider=model_config.provider,
                            model_name=model_config.model_name,
                            content=content,
                            usage=usage,
                            success=True
                        )
                    else:
                        error_text = await response.text()
                        return AIResponse(
                            request_id=request.request_id,
                            provider=model_config.provider,
                            model_name=model_config.model_name,
                            content="",
                            success=False,
                            error_message=f"Anthropic API ì˜¤ë¥˜ {response.status}: {error_text}"
                        )
            
        except Exception as e:
            return AIResponse(
                request_id=request.request_id,
                provider=model_config.provider,
                model_name=model_config.model_name,
                content="",
                success=False,
                error_message=f"Anthropic API ìš”ì²­ ì˜¤ë¥˜: {str(e)}"
            )
    
    async def _call_google_api(self, request: AIRequest, model_config: AIModelConfig) -> AIResponse:
        """Google Gemini API í˜¸ì¶œ"""
        try:
            headers = {
                "Content-Type": "application/json"
            }
            
            # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt_text = request.prompt
            if request.system_prompt:
                prompt_text = f"{request.system_prompt}\n\n{prompt_text}"
            
            payload = {
                "contents": [{
                    "parts": [{"text": prompt_text}]
                }],
                "generationConfig": {
                    "maxOutputTokens": request.max_tokens or model_config.max_tokens,
                    "temperature": request.temperature or model_config.temperature
                }
            }
            
            timeout = aiohttp.ClientTimeout(total=model_config.timeout_seconds)
            base_url = model_config.base_url or "https://generativelanguage.googleapis.com"
            
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.post(
                    f"{base_url}/v1beta/models/{model_config.model_name}:generateContent?key={model_config.api_key}",
                    headers=headers,
                    json=payload
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        content = data['candidates'][0]['content']['parts'][0]['text']
                        usage = data.get('usageMetadata', {})
                        
                        return AIResponse(
                            request_id=request.request_id,
                            provider=model_config.provider,
                            model_name=model_config.model_name,
                            content=content,
                            usage=usage,
                            success=True
                        )
                    else:
                        error_text = await response.text()
                        return AIResponse(
                            request_id=request.request_id,
                            provider=model_config.provider,
                            model_name=model_config.model_name,
                            content="",
                            success=False,
                            error_message=f"Google API ì˜¤ë¥˜ {response.status}: {error_text}"
                        )
            
        except Exception as e:
            return AIResponse(
                request_id=request.request_id,
                provider=model_config.provider,
                model_name=model_config.model_name,
                content="",
                success=False,
                error_message=f"Google API ìš”ì²­ ì˜¤ë¥˜: {str(e)}"
            )
    
    def _update_usage_stats(self, response: AIResponse, model_config: AIModelConfig) -> None:
        """ì‚¬ìš© í†µê³„ ì—…ë°ì´íŠ¸"""
        with self.lock:
            self.usage_stats['total_requests'] += 1
            
            if response.success:
                self.usage_stats['successful_requests'] += 1
                
                # í† í° ì‚¬ìš©ëŸ‰
                total_tokens = response.usage.get('total_tokens', 0)
                self.usage_stats['total_tokens_used'] += total_tokens
                
                # ë¹„ìš©
                self.usage_stats['total_cost'] += response.cost_estimate
                
                # ê³µê¸‰ìë³„ í†µê³„
                provider_name = response.provider.value
                if provider_name not in self.usage_stats['requests_by_provider']:
                    self.usage_stats['requests_by_provider'][provider_name] = 0
                self.usage_stats['requests_by_provider'][provider_name] += 1
                
            else:
                self.usage_stats['failed_requests'] += 1
    
    def get_usage_stats(self) -> Dict[str, Any]:
        """ì‚¬ìš© í†µê³„ ë°˜í™˜"""
        with self.lock:
            stats = self.usage_stats.copy()
            stats['session_duration_minutes'] = (datetime.now() - stats['session_start']).total_seconds() / 60
            stats['success_rate'] = (
                stats['successful_requests'] / stats['total_requests'] * 100
                if stats['total_requests'] > 0 else 0
            )
            return stats
    
    def get_model_info(self, model_id: str) -> Optional[Dict[str, Any]]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        if model_id not in self.models:
            return None
        
        model_config = self.models[model_id]
        
        return {
            'model_id': model_id,
            'provider': model_config.provider.value,
            'model_name': model_config.model_name,
            'max_tokens': model_config.max_tokens,
            'context_window': model_config.context_window,
            'cost_per_1k_tokens': model_config.cost_per_1k_tokens,
            'rate_limit_rpm': model_config.rate_limit_rpm,
            'supports_streaming': model_config.supports_streaming,
            'is_enabled': model_config.is_enabled,
            'has_api_key': bool(model_config.api_key)
        }
    
    def test_model_connection(self, model_id: str) -> Dict[str, Any]:
        """ëª¨ë¸ ì—°ê²° í…ŒìŠ¤íŠ¸"""
        if model_id not in self.models:
            return {
                'success': False,
                'error': f'ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_id}'
            }
        
        try:
            # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ìš”ì²­
            test_request = AIRequest(
                task_type=TaskType.GENERATION,
                prompt="ì•ˆë…•í•˜ì„¸ìš”! ê°„ë‹¨íˆ ì¸ì‚¬ë§ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.",
                max_tokens=50,
                temperature=0.7
            )
            
            # ë™ê¸° ë°©ì‹ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ (asyncio ì´ë²¤íŠ¸ ë£¨í”„ í™•ì¸)
            try:
                loop = asyncio.get_event_loop()
                if loop.is_running():
                    # ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ë£¨í”„ê°€ ìˆìœ¼ë©´ ìƒˆ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
                    import concurrent.futures
                    with concurrent.futures.ThreadPoolExecutor() as executor:
                        future = executor.submit(asyncio.run, self.generate_response(test_request, model_id))
                        response = future.result(timeout=30)
                else:
                    response = asyncio.run(self.generate_response(test_request, model_id))
            except RuntimeError:
                # ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
                response = asyncio.run(self.generate_response(test_request, model_id))
            
            if response.success:
                return {
                    'success': True,
                    'model_name': response.model_name,
                    'response_preview': response.content[:100] + "..." if len(response.content) > 100 else response.content,
                    'processing_time_ms': response.processing_time_ms,
                    'tokens_used': response.usage.get('total_tokens', 0),
                    'cost_estimate': response.cost_estimate
                }
            else:
                return {
                    'success': False,
                    'error': response.error_message
                }
                
        except Exception as e:
            return {
                'success': False,
                'error': f'ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {str(e)}'
            }
    
    def cleanup(self) -> None:
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        with self.lock:
            # ì„¤ì • ì €ì¥
            self._save_configurations()
            
            # í†µê³„ ë¦¬ì…‹
            self.usage_stats = {
                'total_requests': 0,
                'successful_requests': 0,
                'failed_requests': 0,
                'total_tokens_used': 0,
                'total_cost': 0.0,
                'requests_by_provider': {},
                'session_start': datetime.now()
            }
            
            self.logger.info("ğŸ§¹ ì™¸ë¶€ AI ëª¨ë¸ í†µí•© ì‹œìŠ¤í…œ ì •ë¦¬ ì™„ë£Œ")

# ì „ì—­ ì™¸ë¶€ AI í†µí•© ê´€ë¦¬ì
_global_ai_integrator = None
_global_integrator_lock = threading.Lock()

def get_global_ai_integrator() -> ExternalAIIntegrator:
    """ì „ì—­ ì™¸ë¶€ AI í†µí•© ê´€ë¦¬ì ê°€ì ¸ì˜¤ê¸°"""
    global _global_ai_integrator
    
    with _global_integrator_lock:
        if _global_ai_integrator is None:
            _global_ai_integrator = ExternalAIIntegrator()
        return _global_ai_integrator

# í¸ì˜ í•¨ìˆ˜ë“¤
async def generate_ai_response(prompt: str, task_type: TaskType = TaskType.GENERATION,
                              system_prompt: Optional[str] = None, model_id: Optional[str] = None) -> AIResponse:
    """AI ì‘ë‹µ ìƒì„± (í¸ì˜ í•¨ìˆ˜)"""
    integrator = get_global_ai_integrator()
    
    request = AIRequest(
        task_type=task_type,
        prompt=prompt,
        system_prompt=system_prompt
    )
    
    return await integrator.generate_response(request, model_id)

def set_ai_api_key(provider: str, api_key: str) -> bool:
    """AI API í‚¤ ì„¤ì • (í¸ì˜ í•¨ìˆ˜)"""
    integrator = get_global_ai_integrator()
    
    try:
        provider_enum = AIProvider(provider.lower())
        return integrator.set_api_key(provider_enum, api_key)
    except ValueError:
        return False

def get_ai_usage_stats() -> Dict[str, Any]:
    """AI ì‚¬ìš© í†µê³„ ì¡°íšŒ (í¸ì˜ í•¨ìˆ˜)"""
    integrator = get_global_ai_integrator()
    return integrator.get_usage_stats()

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    async def test_ai_integrator():
        integrator = ExternalAIIntegrator()
        
        # API í‚¤ ì„¤ì • (ì‹¤ì œ ì‚¬ìš© ì‹œ í™˜ê²½ë³€ìˆ˜ë‚˜ ì„¤ì • íŒŒì¼ì—ì„œ ë¡œë“œ)
        # integrator.set_api_key(AIProvider.OPENAI, "your-openai-api-key")
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ í™•ì¸
        models = integrator.get_available_models()
        print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {models}")
        
        if models:
            # í…ŒìŠ¤íŠ¸ ìš”ì²­
            request = AIRequest(
                task_type=TaskType.SUMMARIZATION,
                prompt="ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”: ì¸ê³µì§€ëŠ¥ì€ ì»´í“¨í„° ì‹œìŠ¤í…œì´ ì¸ê°„ì˜ ì§€ëŠ¥ì„ ëª¨ë°©í•˜ì—¬ í•™ìŠµ, ì¶”ë¡ , ë¬¸ì œ í•´ê²° ë“±ì˜ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.",
                system_prompt="ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ ìš”ì•½ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. í•µì‹¬ ë‚´ìš©ì„ ê°„ê²°í•˜ê²Œ ì •ë¦¬í•´ì£¼ì„¸ìš”."
            )
            
            # ì‘ë‹µ ìƒì„±
            response = await integrator.generate_response(request, models[0])
            
            print(f"\nì‘ë‹µ ì„±ê³µ: {response.success}")
            if response.success:
                print(f"ëª¨ë¸: {response.model_name}")
                print(f"ì‘ë‹µ: {response.content}")
                print(f"í† í° ì‚¬ìš©: {response.usage}")
                print(f"ì²˜ë¦¬ ì‹œê°„: {response.processing_time_ms:.1f}ms")
                print(f"ì˜ˆìƒ ë¹„ìš©: ${response.cost_estimate:.6f}")
            else:
                print(f"ì˜¤ë¥˜: {response.error_message}")
        
        # ì‚¬ìš© í†µê³„
        stats = integrator.get_usage_stats()
        print(f"\nì‚¬ìš© í†µê³„: {stats}")
        
        # ì •ë¦¬
        integrator.cleanup()
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    print("ğŸ¤– ì™¸ë¶€ AI ëª¨ë¸ í†µí•© ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    asyncio.run(test_ai_integrator())
    print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")