#!/usr/bin/env python3
"""
ì‹¤ì‹œê°„ ì¶”ì  í†µí•© í™”ì ë¶„ë¦¬ ì‹œìŠ¤í…œ
ì˜¤ë””ì˜¤ì—ì„œ í™”ìë¥¼ ë¶„ë¦¬í•˜ê³  ê° í™”ìë³„ ë°œì–¸ ë‚´ìš©ì„ STTì™€ ë§¤í•‘í•˜ì—¬ ì œê³µ
"""

import os
import time
import tempfile
import logging
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime, timedelta
from pathlib import Path
import json

# ì˜¤ë””ì˜¤ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
try:
    import librosa
    import numpy as np
    LIBROSA_AVAILABLE = True
except ImportError:
    LIBROSA_AVAILABLE = False

try:
    from sklearn.cluster import KMeans
    from sklearn.preprocessing import StandardScaler
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False

try:
    import webrtcvad
    WEBRTC_AVAILABLE = True
except ImportError:
    WEBRTC_AVAILABLE = False

class RealtimeSpeakerDiarization:
    """ì‹¤ì‹œê°„ ì¶”ì  í†µí•© í™”ì ë¶„ë¦¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.logger = self._setup_logging()
        
        # ì‹¤ì‹œê°„ ì¶”ì  ì‹œìŠ¤í…œ ë¡œë“œ - ê°•í™”ëœ ëª¨ë“ˆ ê²€ìƒ‰ ì‹œìŠ¤í…œ
        try:
            import sys
            import os
            from pathlib import Path
            
            # í˜„ì¬ íŒŒì¼ì˜ ë””ë ‰í† ë¦¬
            current_dir = Path(__file__).parent
            
            # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬
            project_root = current_dir.parent
            
            # sys.pathì— í•„ìš”í•œ ê²½ë¡œë“¤ ì¶”ê°€
            paths_to_add = [
                str(current_dir),
                str(project_root),
                str(project_root / "core")
            ]
            
            for path in paths_to_add:
                if path not in sys.path:
                    sys.path.insert(0, path)
            
            # ë™ì  ì„í¬íŠ¸ë¡œ ëª¨ë“ˆ ë¡œë“œ
            import importlib.util
            
            # realtime_progress_tracker ëª¨ë“ˆ ë¡œë“œ
            tracker_path = current_dir / "realtime_progress_tracker.py"
            if tracker_path.exists():
                spec = importlib.util.spec_from_file_location("realtime_progress_tracker", tracker_path)
                tracker_module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(tracker_module)
                self.progress_tracker = getattr(tracker_module, 'global_progress_tracker', None)
            else:
                self.progress_tracker = None
            
            # mcp_auto_problem_solver ëª¨ë“ˆ ë¡œë“œ
            solver_path = current_dir / "mcp_auto_problem_solver.py"
            if solver_path.exists():
                spec = importlib.util.spec_from_file_location("mcp_auto_problem_solver", solver_path)
                solver_module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(solver_module)
                self.problem_solver = getattr(solver_module, 'global_mcp_solver', None)
            else:
                self.problem_solver = None
            
            # ë¡œë“œ ì„±ê³µ ì—¬ë¶€ í™•ì¸
            if self.progress_tracker is not None and self.problem_solver is not None:
                self.realtime_tracking_available = True
                self.logger.info("ğŸ¯ ì‹¤ì‹œê°„ ì¶”ì  ì‹œìŠ¤í…œ ì—°ë™ ì™„ë£Œ")
            else:
                self.realtime_tracking_available = False
                self.logger.warning("ì¼ë¶€ ì‹¤ì‹œê°„ ì¶”ì  ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨")
        except ImportError as e:
            self.progress_tracker = None
            self.problem_solver = None
            self.realtime_tracking_available = False
            self.logger.warning(f"ì‹¤ì‹œê°„ ì¶”ì  ì‹œìŠ¤í…œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # í™”ì ì‹ë³„ ì‹œìŠ¤í…œ ë¡œë“œ - ë™ì¼í•œ ë™ì  ë¡œë”© ì‹œìŠ¤í…œ ì‚¬ìš©
        try:
            # current_dirì´ ì´ë¯¸ ì •ì˜ë˜ì–´ ìˆì§€ ì•Šì€ ê²½ìš°ë¥¼ ëŒ€ë¹„
            if 'current_dir' not in locals():
                current_dir = Path(__file__).parent
                
            speaker_id_path = current_dir / "speaker_identification.py"
            if speaker_id_path.exists():
                spec = importlib.util.spec_from_file_location("speaker_identification", speaker_id_path)
                speaker_module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(speaker_module)
                self.speaker_identifier = getattr(speaker_module, 'global_speaker_identifier', None)
                
                if self.speaker_identifier is not None:
                    self.speaker_identification_available = True
                    self.logger.info("ğŸ­ í™”ì ì‹ë³„ ì‹œìŠ¤í…œ ì—°ë™ ì™„ë£Œ")
                else:
                    self.speaker_identification_available = False
                    self.logger.warning("í™”ì ì‹ë³„ ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤ ë¡œë“œ ì‹¤íŒ¨")
            else:
                self.speaker_identifier = None
                self.speaker_identification_available = False
                self.logger.warning("speaker_identification.py íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        except ImportError as e:
            self.speaker_identifier = None
            self.speaker_identification_available = False
            self.logger.warning(f"í™”ì ì‹ë³„ ì‹œìŠ¤í…œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # ì˜¤ë””ì˜¤ ë¶„ì„ ì„¤ì •
        self.sample_rate = 16000
        self.frame_duration = 0.03  # 30ms í”„ë ˆì„
        self.hop_length = int(self.sample_rate * self.frame_duration)
        
        # í™”ì ë¶„ë¦¬ ì„¤ì •
        self.max_speakers = 6  # ìµœëŒ€ í™”ì ìˆ˜
        self.min_segment_duration = 1.0  # ìµœì†Œ ë°œì–¸ ì§€ì† ì‹œê°„ (ì´ˆ)
        self.energy_threshold = 0.01  # ìŒì„± í™œë™ ê°ì§€ ì„ê³„ê°’
        
        # VAD (Voice Activity Detection) ì„¤ì •
        if WEBRTC_AVAILABLE:
            self.vad = webrtcvad.Vad(2)  # ì¤‘ê°„ ë¯¼ê°ë„
            self.vad_available = True
        else:
            self.vad = None
            self.vad_available = False
        
        self._check_dependencies()
        self.logger.info("ğŸ¤ ì‹¤ì‹œê°„ í™”ì ë¶„ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _setup_logging(self) -> logging.Logger:
        """ë¡œê¹… ì„¤ì •"""
        logger = logging.getLogger(f'{__name__}.RealtimeSpeakerDiarization')
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            logger.setLevel(logging.INFO)
        return logger
    
    def _check_dependencies(self):
        """ì˜ì¡´ì„± í™•ì¸"""
        deps_status = {
            "librosa": LIBROSA_AVAILABLE,
            "sklearn": SKLEARN_AVAILABLE, 
            "webrtcvad": WEBRTC_AVAILABLE,
            "realtime_tracking": self.realtime_tracking_available,
            "speaker_identification": self.speaker_identification_available
        }
        
        self.logger.info(f"ğŸ“¦ ì˜ì¡´ì„± ìƒíƒœ: {deps_status}")
        
        if not LIBROSA_AVAILABLE:
            self.logger.warning("âš ï¸ librosa ë¯¸ì„¤ì¹˜ - ì˜¤ë””ì˜¤ ì²˜ë¦¬ ê¸°ëŠ¥ ì œí•œ")
        if not SKLEARN_AVAILABLE:
            self.logger.warning("âš ï¸ sklearn ë¯¸ì„¤ì¹˜ - í™”ì í´ëŸ¬ìŠ¤í„°ë§ ê¸°ëŠ¥ ì œí•œ")
        if not WEBRTC_AVAILABLE:
            self.logger.warning("âš ï¸ webrtcvad ë¯¸ì„¤ì¹˜ - ìŒì„± í™œë™ ê°ì§€ ê¸°ëŠ¥ ì œí•œ")
    
    def analyze_speakers_in_audio(self, audio_file: str, transcript: str = "", 
                                progress_container=None) -> Dict[str, Any]:
        """ì˜¤ë””ì˜¤ íŒŒì¼ì—ì„œ í™”ì ë¶„ë¦¬ ë° ë°œì–¸ ë§¤í•‘"""
        
        if self.realtime_tracking_available and progress_container:
            self.progress_tracker.update_progress_with_time(
                "ğŸ¤ í™”ì ë¶„ë¦¬ ë¶„ì„ ì‹œì‘",
                f"íŒŒì¼: {os.path.basename(audio_file)}"
            )
        
        start_time = time.time()
        
        try:
            # 1. ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ ë° ë¡œë“œ
            if self.realtime_tracking_available and progress_container:
                self.progress_tracker.update_progress_with_time(
                    "ğŸ“Š ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë“œ ë° ì „ì²˜ë¦¬",
                    "ìŒì„± ì‹ í˜¸ ë¶„ì„ ì¤‘..."
                )
            
            audio_data, segments = self._load_and_preprocess_audio(audio_file)
            
            if audio_data is None:
                return self._create_error_result("ì˜¤ë””ì˜¤ ë¡œë“œ ì‹¤íŒ¨", audio_file)
            
            # 2. ìŒì„± í™œë™ ê°ì§€ (VAD)
            if self.realtime_tracking_available and progress_container:
                self.progress_tracker.update_progress_with_time(
                    "ğŸ”Š ìŒì„± í™œë™ êµ¬ê°„ ê°ì§€",
                    f"ì´ {len(audio_data)/self.sample_rate:.1f}ì´ˆ ë¶„ì„ ì¤‘..."
                )
            
            voice_segments = self._detect_voice_activity(audio_data)
            
            # 3. í™”ì íŠ¹ì„± ì¶”ì¶œ
            if self.realtime_tracking_available and progress_container:
                self.progress_tracker.update_progress_with_time(
                    "ğŸ¯ í™”ì íŠ¹ì„± ì¶”ì¶œ",
                    f"{len(voice_segments)}ê°œ ìŒì„± êµ¬ê°„ ë¶„ì„..."
                )
            
            speaker_features = self._extract_speaker_features(audio_data, voice_segments)
            
            # 4. í™”ì í´ëŸ¬ìŠ¤í„°ë§
            if self.realtime_tracking_available and progress_container:
                self.progress_tracker.update_progress_with_time(
                    "ğŸ‘¥ í™”ì í´ëŸ¬ìŠ¤í„°ë§",
                    "ìœ ì‚¬í•œ ìŒì„± íŠ¹ì„± ê·¸ë£¹í•‘..."
                )
            
            speaker_labels, optimal_speakers = self._cluster_speakers(speaker_features)
            
            # 5. í™”ìë³„ ì‹œê°„ êµ¬ê°„ ìƒì„±
            if self.realtime_tracking_available and progress_container:
                self.progress_tracker.update_progress_with_time(
                    "â° í™”ìë³„ ì‹œê°„ êµ¬ê°„ ìƒì„±",
                    f"{optimal_speakers}ëª… í™”ì êµ¬ê°„ ë§¤í•‘..."
                )
            
            speaker_timeline = self._create_speaker_timeline(voice_segments, speaker_labels)
            
            # 6. STT ê²°ê³¼ì™€ í™”ì ë§¤í•‘
            if self.realtime_tracking_available and progress_container:
                self.progress_tracker.update_progress_with_time(
                    "ğŸ“ í™”ìë³„ ë°œì–¸ ë‚´ìš© ë§¤í•‘",
                    "STT ê²°ê³¼ì™€ í™”ì ì‹œê°„ëŒ€ ë§¤ì¹­..."
                )
            
            speaker_statements = self._map_transcript_to_speakers(transcript, speaker_timeline)
            
            # 7. í™”ì ì‹ë³„ ë° ì—­í•  ë¶„ì„
            if self.realtime_tracking_available and progress_container:
                self.progress_tracker.update_progress_with_time(
                    "ğŸ­ í™”ì ì‹ë³„ ë° ì—­í•  ë¶„ì„",
                    "í…ìŠ¤íŠ¸ ê¸°ë°˜ í™”ì ì •ë³´ ì¶”ì¶œ..."
                )
            
            speaker_identification = self._identify_speakers_in_statements(speaker_statements, transcript)
            
            # 8. ê²°ê³¼ í†µí•© ë° ì‚¬ìš©ì ì¹œí™”ì  ìš”ì•½ ìƒì„±
            processing_time = time.time() - start_time
            
            result = {
                "status": "success",
                "audio_file": audio_file,
                "processing_time": round(processing_time, 2),
                "speaker_count": optimal_speakers,
                "total_duration": len(audio_data) / self.sample_rate,
                "voice_activity_ratio": self._calculate_voice_activity_ratio(voice_segments, len(audio_data)),
                "speaker_timeline": speaker_timeline,
                "speaker_statements": speaker_statements,
                "speaker_identification": speaker_identification,
                "analysis_quality": self._assess_analysis_quality(speaker_features, voice_segments),
                "user_summary": self._create_user_friendly_summary(speaker_statements, speaker_identification, optimal_speakers),
                "detailed_breakdown": self._create_detailed_breakdown(speaker_statements, speaker_timeline),
                "timestamp": datetime.now().isoformat()
            }
            
            if self.realtime_tracking_available and progress_container:
                self.progress_tracker.update_progress_with_time(
                    f"âœ… í™”ì ë¶„ë¦¬ ì™„ë£Œ: {optimal_speakers}ëª… ì‹ë³„",
                    f"ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ"
                )
            
            self.logger.info(f"ğŸ‰ í™”ì ë¶„ë¦¬ ë¶„ì„ ì™„ë£Œ: {optimal_speakers}ëª…, {processing_time:.2f}ì´ˆ")
            return result
            
        except Exception as e:
            processing_time = time.time() - start_time
            error_msg = f"í™”ì ë¶„ë¦¬ ë¶„ì„ ì˜¤ë¥˜: {str(e)}"
            self.logger.error(error_msg)
            
            # MCP ìë™ ë¬¸ì œ í•´ê²° ì‹œë„
            if self.realtime_tracking_available and self.problem_solver:
                try:
                    problem_result = self.problem_solver.detect_and_solve_problems(
                        memory_usage_mb=100,
                        processing_time=processing_time,
                        file_info={'name': audio_file, 'size_mb': 0},
                        error_message=str(e)
                    )
                    
                    if problem_result['solutions_found']:
                        self.logger.info(f"ìë™ í•´ê²°ì±… {len(problem_result['solutions_found'])}ê°œ ë°œê²¬")
                        if progress_container:
                            with progress_container.container():
                                import streamlit as st
                                st.warning("í™”ì ë¶„ë¦¬ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ìë™ í•´ê²°ì±…ì„ í™•ì¸í•˜ì„¸ìš”:")
                                for i, solution in enumerate(problem_result['solutions_found'][:2], 1):
                                    st.write(f"{i}. {solution.get('title', 'í•´ê²°ì±…')}")
                                    if solution.get('url'):
                                        st.write(f"   ì°¸ê³ : {solution['url']}")
                except Exception as mcp_error:
                    self.logger.warning(f"MCP ìë™ ë¬¸ì œ í•´ê²° ì‹¤íŒ¨: {mcp_error}")
            
            return self._create_error_result(error_msg, audio_file, processing_time)
    
    def _load_and_preprocess_audio(self, audio_file: str) -> Tuple[Optional[np.ndarray], Optional[List]]:
        """ì˜¤ë””ì˜¤ ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        
        if not LIBROSA_AVAILABLE:
            self.logger.error("librosaê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ì˜¤ë””ì˜¤ ì²˜ë¦¬ ë¶ˆê°€")
            return None, None
        
        try:
            # ì˜¤ë””ì˜¤ ë¡œë“œ
            y, sr = librosa.load(audio_file, sr=self.sample_rate)
            
            # ë¬´ìŒ ì œê±° ë° ì •ê·œí™”
            y_trimmed, _ = librosa.effects.trim(y, top_db=20)
            y_normalized = librosa.util.normalize(y_trimmed)
            
            # í”„ë ˆì„ ë‹¨ìœ„ë¡œ ë¶„í• 
            frames = librosa.util.frame(y_normalized, frame_length=self.hop_length, 
                                      hop_length=self.hop_length, axis=0)
            
            return y_normalized, frames
            
        except Exception as e:
            self.logger.error(f"ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return None, None
    
    def _detect_voice_activity(self, audio_data: np.ndarray) -> List[Dict[str, float]]:
        """ìŒì„± í™œë™ êµ¬ê°„ ê°ì§€"""
        
        voice_segments = []
        
        if self.vad_available and WEBRTC_AVAILABLE:
            # WebRTC VAD ì‚¬ìš©
            frame_duration = 0.03  # 30ms
            frame_size = int(self.sample_rate * frame_duration)
            
            current_segment_start = None
            
            for i in range(0, len(audio_data) - frame_size, frame_size):
                frame = audio_data[i:i + frame_size]
                
                # 16-bit PCMìœ¼ë¡œ ë³€í™˜
                frame_int16 = (frame * 32767).astype(np.int16)
                frame_bytes = frame_int16.tobytes()
                
                try:
                    is_speech = self.vad.is_speech(frame_bytes, self.sample_rate)
                    time_position = i / self.sample_rate
                    
                    if is_speech and current_segment_start is None:
                        current_segment_start = time_position
                    elif not is_speech and current_segment_start is not None:
                        # ìŒì„± êµ¬ê°„ ì¢…ë£Œ
                        duration = time_position - current_segment_start
                        if duration >= self.min_segment_duration:
                            voice_segments.append({
                                "start": current_segment_start,
                                "end": time_position,
                                "duration": duration
                            })
                        current_segment_start = None
                        
                except Exception as e:
                    # VAD ì˜¤ë¥˜ì‹œ ì—ë„ˆì§€ ê¸°ë°˜ fallback
                    pass
            
            # ë§ˆì§€ë§‰ êµ¬ê°„ ì²˜ë¦¬
            if current_segment_start is not None:
                final_time = len(audio_data) / self.sample_rate
                duration = final_time - current_segment_start
                if duration >= self.min_segment_duration:
                    voice_segments.append({
                        "start": current_segment_start,
                        "end": final_time,
                        "duration": duration
                    })
        
        # VADê°€ ì—†ê±°ë‚˜ ì‹¤íŒ¨í•œ ê²½ìš° ì—ë„ˆì§€ ê¸°ë°˜ ê°ì§€
        if not voice_segments:
            voice_segments = self._energy_based_vad(audio_data)
        
        return voice_segments
    
    def _energy_based_vad(self, audio_data: np.ndarray) -> List[Dict[str, float]]:
        """ì—ë„ˆì§€ ê¸°ë°˜ ìŒì„± í™œë™ ê°ì§€"""
        
        if not LIBROSA_AVAILABLE:
            return []
        
        # ì§§ì€ í”„ë ˆì„ìœ¼ë¡œ ì—ë„ˆì§€ ê³„ì‚°
        frame_length = int(self.sample_rate * 0.025)  # 25ms
        hop_length = int(self.sample_rate * 0.01)     # 10ms
        
        # ìŠ¤í™íŠ¸ëŸ´ ì„¼íŠ¸ë¡œì´ë“œì™€ ì˜êµì°¨ìœ¨ ê³„ì‚°
        energy = librosa.feature.rms(y=audio_data, frame_length=frame_length, 
                                   hop_length=hop_length)[0]
        
        # ì„ê³„ê°’ ì´ìƒì¸ êµ¬ê°„ ì°¾ê¸°
        threshold = np.percentile(energy, 30)  # í•˜ìœ„ 30% ì œê±°
        voice_mask = energy > threshold
        
        # ì—°ì† êµ¬ê°„ ì°¾ê¸°
        voice_segments = []
        in_voice = False
        start_time = 0
        
        for i, is_voice in enumerate(voice_mask):
            time_position = i * hop_length / self.sample_rate
            
            if is_voice and not in_voice:
                start_time = time_position
                in_voice = True
            elif not is_voice and in_voice:
                duration = time_position - start_time
                if duration >= self.min_segment_duration:
                    voice_segments.append({
                        "start": start_time,
                        "end": time_position,
                        "duration": duration
                    })
                in_voice = False
        
        # ë§ˆì§€ë§‰ êµ¬ê°„ ì²˜ë¦¬
        if in_voice:
            final_time = len(audio_data) / self.sample_rate
            duration = final_time - start_time
            if duration >= self.min_segment_duration:
                voice_segments.append({
                    "start": start_time,
                    "end": final_time,
                    "duration": duration
                })
        
        return voice_segments
    
    def _extract_speaker_features(self, audio_data: np.ndarray, voice_segments: List[Dict]) -> List[np.ndarray]:
        """ê° ìŒì„± êµ¬ê°„ì—ì„œ í™”ì íŠ¹ì„± ì¶”ì¶œ"""
        
        if not LIBROSA_AVAILABLE:
            return []
        
        features = []
        
        for segment in voice_segments:
            start_sample = int(segment["start"] * self.sample_rate)
            end_sample = int(segment["end"] * self.sample_rate)
            segment_audio = audio_data[start_sample:end_sample]
            
            if len(segment_audio) < self.sample_rate * 0.5:  # 0.5ì´ˆ ë¯¸ë§Œì€ ìŠ¤í‚µ
                continue
            
            # MFCC íŠ¹ì„± ì¶”ì¶œ (í™”ì ì‹ë³„ì— ìœ ìš©)
            mfccs = librosa.feature.mfcc(y=segment_audio, sr=self.sample_rate, n_mfcc=13)
            mfcc_mean = np.mean(mfccs, axis=1)
            mfcc_std = np.std(mfccs, axis=1)
            
            # ìŠ¤í™íŠ¸ëŸ´ íŠ¹ì„± ì¶”ì¶œ
            spectral_centroids = librosa.feature.spectral_centroid(y=segment_audio, sr=self.sample_rate)
            spectral_rolloff = librosa.feature.spectral_rolloff(y=segment_audio, sr=self.sample_rate)
            zero_crossing_rate = librosa.feature.zero_crossing_rate(segment_audio)
            
            # í”¼ì¹˜ íŠ¹ì„± (ê¸°ë³¸ ì£¼íŒŒìˆ˜)
            pitches, magnitudes = librosa.piptrack(y=segment_audio, sr=self.sample_rate)
            pitch_mean = np.mean(pitches[pitches > 0]) if np.any(pitches > 0) else 0
            
            # íŠ¹ì„± ë²¡í„° ìƒì„±
            feature_vector = np.concatenate([
                mfcc_mean,
                mfcc_std,
                [np.mean(spectral_centroids)],
                [np.mean(spectral_rolloff)],
                [np.mean(zero_crossing_rate)],
                [pitch_mean]
            ])
            
            features.append(feature_vector)
        
        return features
    
    def _cluster_speakers(self, features: List[np.ndarray]) -> Tuple[List[int], int]:
        """í™”ì íŠ¹ì„±ì„ í´ëŸ¬ìŠ¤í„°ë§í•˜ì—¬ í™”ì ë¶„ë¦¬"""
        
        if not SKLEARN_AVAILABLE or not features:
            # ê¸°ë³¸ ë™ì‘: ëª¨ë“  êµ¬ê°„ì„ ë‹¨ì¼ í™”ìë¡œ ì²˜ë¦¬
            return [0] * len(features), 1
        
        features_array = np.array(features)
        
        # íŠ¹ì„± ì •ê·œí™”
        scaler = StandardScaler()
        features_normalized = scaler.fit_transform(features_array)
        
        # ìµœì ì˜ í´ëŸ¬ìŠ¤í„° ìˆ˜ ì°¾ê¸° (Elbow method)
        best_k = 1
        min_inertia = float('inf')
        
        for k in range(1, min(self.max_speakers + 1, len(features) + 1)):
            if k > len(features):
                break
                
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
            labels = kmeans.fit_predict(features_normalized)
            
            if k == 1:
                best_k = 1
                best_labels = labels
            else:
                # ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´ë‚˜ inertiaë¡œ ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ê²°ì •
                inertia = kmeans.inertia_
                if inertia < min_inertia and k <= 4:  # ì¼ë°˜ì ìœ¼ë¡œ 4ëª… ì´í•˜
                    min_inertia = inertia
                    best_k = k
                    best_labels = labels
        
        return best_labels.tolist(), best_k
    
    def _create_speaker_timeline(self, voice_segments: List[Dict], speaker_labels: List[int]) -> List[Dict]:
        """í™”ìë³„ ì‹œê°„ë¼ì¸ ìƒì„±"""
        
        timeline = []
        
        for i, (segment, label) in enumerate(zip(voice_segments, speaker_labels)):
            timeline.append({
                "segment_id": i + 1,
                "speaker_id": f"SPEAKER_{label:02d}",
                "start_time": segment["start"],
                "end_time": segment["end"],
                "duration": segment["duration"],
                "start_formatted": self._format_time(segment["start"]),
                "end_formatted": self._format_time(segment["end"]),
                "duration_formatted": f"{segment['duration']:.1f}ì´ˆ"
            })
        
        return timeline
    
    def _map_transcript_to_speakers(self, transcript: str, speaker_timeline: List[Dict]) -> Dict[str, List[Dict]]:
        """STT ê²°ê³¼ë¥¼ í™”ìë³„ë¡œ ë§¤í•‘"""
        
        if not transcript or not speaker_timeline:
            return {}
        
        # ê°„ë‹¨í•œ ì‹œê°„ ê¸°ë°˜ ë§¤í•‘ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ì•Œê³ ë¦¬ì¦˜ í•„ìš”)
        sentences = self._split_transcript_by_sentences(transcript)
        total_duration = speaker_timeline[-1]["end_time"] if speaker_timeline else 0
        
        speaker_statements = {}
        sentence_duration = total_duration / len(sentences) if sentences else 0
        
        sentence_index = 0
        for segment in speaker_timeline:
            speaker_id = segment["speaker_id"]
            
            if speaker_id not in speaker_statements:
                speaker_statements[speaker_id] = []
            
            # í•´ë‹¹ ì‹œê°„ëŒ€ì— í•´ë‹¹í•˜ëŠ” ë¬¸ì¥ë“¤ ì°¾ê¸°
            segment_start = segment["start_time"]
            segment_end = segment["end_time"]
            
            # ê°„ë‹¨í•œ ë§¤í•‘: ì‹œê°„ ë¹„ìœ¨ë¡œ ë¬¸ì¥ í• ë‹¹
            start_sentence_idx = int((segment_start / total_duration) * len(sentences))
            end_sentence_idx = int((segment_end / total_duration) * len(sentences))
            
            start_sentence_idx = max(0, min(start_sentence_idx, len(sentences) - 1))
            end_sentence_idx = max(start_sentence_idx, min(end_sentence_idx, len(sentences)))
            
            segment_sentences = sentences[start_sentence_idx:end_sentence_idx + 1]
            
            if segment_sentences:
                speaker_statements[speaker_id].append({
                    "segment_id": segment["segment_id"],
                    "start_time": segment["start_formatted"],
                    "end_time": segment["end_formatted"],
                    "duration": segment["duration_formatted"],
                    "content": " ".join(segment_sentences),
                    "sentence_count": len(segment_sentences)
                })
        
        return speaker_statements
    
    def _identify_speakers_in_statements(self, speaker_statements: Dict, full_transcript: str) -> Dict[str, Any]:
        """í™”ìë³„ ë°œì–¸ì—ì„œ í™”ì ì •ë³´ ì‹ë³„"""
        
        if not self.speaker_identification_available:
            return {"status": "speaker_identification_unavailable"}
        
        try:
            # ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ í™”ì ì‹ë³„
            identification_result = self.speaker_identifier.analyze_speakers(full_transcript)
            
            # ê° í™”ìë³„ ë°œì–¸ ë‚´ìš© ë¶„ì„
            speaker_details = {}
            
            for speaker_id, statements in speaker_statements.items():
                combined_text = " ".join([stmt["content"] for stmt in statements])
                
                # ê°œë³„ í™”ì ë¶„ì„
                individual_analysis = self.speaker_identifier.analyze_speakers(combined_text)
                
                speaker_details[speaker_id] = {
                    "total_statements": len(statements),
                    "total_content_length": len(combined_text),
                    "identified_names": individual_analysis.get("identified_speakers", []),
                    "expert_roles": individual_analysis.get("expert_roles", {}),
                    "key_statements": individual_analysis.get("key_statements", {}),
                    "analysis_confidence": individual_analysis.get("analysis_confidence", 0.0)
                }
            
            return {
                "status": "success",
                "global_analysis": identification_result,
                "speaker_details": speaker_details
            }
            
        except Exception as e:
            self.logger.error(f"í™”ì ì‹ë³„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"status": "error", "error": str(e)}
    
    def _split_transcript_by_sentences(self, transcript: str) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• """
        
        import re
        
        # í•œêµ­ì–´ ë¬¸ì¥ ë¶„í• 
        sentences = re.split(r'[.!?]\s+', transcript)
        sentences = [s.strip() for s in sentences if len(s.strip()) > 5]  # ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ ì œì™¸
        
        return sentences
    
    def _calculate_voice_activity_ratio(self, voice_segments: List[Dict], total_samples: int) -> float:
        """ìŒì„± í™œë™ ë¹„ìœ¨ ê³„ì‚°"""
        
        total_voice_duration = sum(segment["duration"] for segment in voice_segments)
        total_duration = total_samples / self.sample_rate
        
        return round(total_voice_duration / total_duration, 3) if total_duration > 0 else 0.0
    
    def _assess_analysis_quality(self, features: List[np.ndarray], voice_segments: List[Dict]) -> Dict[str, Any]:
        """ë¶„ì„ í’ˆì§ˆ í‰ê°€"""
        
        quality_score = 0.0
        quality_factors = []
        
        # ìŒì„± êµ¬ê°„ ìˆ˜ í‰ê°€
        if len(voice_segments) >= 3:
            quality_score += 0.3
            quality_factors.append("ì¶©ë¶„í•œ ìŒì„± êµ¬ê°„ ê°ì§€")
        elif len(voice_segments) >= 1:
            quality_score += 0.1
            quality_factors.append("ìµœì†Œ ìŒì„± êµ¬ê°„ ê°ì§€")
        
        # íŠ¹ì„± ì¶”ì¶œ í’ˆì§ˆ í‰ê°€
        if len(features) >= 3:
            quality_score += 0.3
            quality_factors.append("ì¶©ë¶„í•œ í™”ì íŠ¹ì„± ì¶”ì¶œ")
        
        # ì´ ìŒì„± ê¸¸ì´ í‰ê°€
        total_voice_duration = sum(segment["duration"] for segment in voice_segments)
        if total_voice_duration >= 30:
            quality_score += 0.2
            quality_factors.append("ì¶©ë¶„í•œ ìŒì„± ê¸¸ì´")
        elif total_voice_duration >= 10:
            quality_score += 0.1
            quality_factors.append("ìµœì†Œ ìŒì„± ê¸¸ì´")
        
        # ì˜ì¡´ì„± ìƒíƒœ í‰ê°€
        if LIBROSA_AVAILABLE and SKLEARN_AVAILABLE:
            quality_score += 0.2
            quality_factors.append("í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ê°€ëŠ¥")
        
        return {
            "quality_score": round(min(quality_score, 1.0), 2),
            "quality_level": "ë†’ìŒ" if quality_score >= 0.8 else "ì¤‘ê°„" if quality_score >= 0.5 else "ë‚®ìŒ",
            "quality_factors": quality_factors
        }
    
    def _create_user_friendly_summary(self, speaker_statements: Dict, speaker_identification: Dict, 
                                    speaker_count: int) -> str:
        """ì‚¬ìš©ì ì¹œí™”ì  ìš”ì•½ ìƒì„±"""
        
        summary_parts = []
        
        # ê¸°ë³¸ ì •ë³´
        summary_parts.append(f"ğŸ¤ **í™”ì ë¶„ë¦¬ ê²°ê³¼: {speaker_count}ëª… ê°ì§€**\n")
        
        # ê° í™”ìë³„ ìš”ì•½
        for speaker_id, statements in speaker_statements.items():
            speaker_num = speaker_id.replace("SPEAKER_", "").lstrip("0") or "1"
            total_statements = len(statements)
            total_duration = sum(float(stmt["duration"].replace("ì´ˆ", "")) for stmt in statements)
            
            summary_parts.append(f"ğŸ‘¤ **í™”ì {speaker_num}**")
            summary_parts.append(f"   ğŸ“ ë°œì–¸ êµ¬ê°„: {total_statements}ê°œ")
            summary_parts.append(f"   â±ï¸ ì´ ë°œì–¸ ì‹œê°„: {total_duration:.1f}ì´ˆ")
            
            # ì£¼ìš” ë°œì–¸ ìƒ˜í”Œ
            if statements:
                first_statement = statements[0]["content"][:100]
                summary_parts.append(f"   ğŸ’¬ ì£¼ìš” ë°œì–¸: \"{first_statement}...\"")
            
            # ì‹ë³„ëœ ì •ë³´ (ìˆëŠ” ê²½ìš°)
            if speaker_identification.get("status") == "success":
                speaker_details = speaker_identification.get("speaker_details", {}).get(speaker_id, {})
                identified_names = speaker_details.get("identified_names", [])
                if identified_names:
                    name = identified_names[0].get("name", "")
                    summary_parts.append(f"   ğŸ·ï¸ ì‹ë³„ëœ ì´ë¦„: {name}")
            
            summary_parts.append("")
        
        # ì „ì²´ ëŒ€í™” íŠ¹ì„±
        if speaker_count > 1:
            summary_parts.append(f"ğŸ—£ï¸ **ëŒ€í™” íŠ¹ì„±**: {speaker_count}ëª… ê°„ì˜ ëŒ€í™”ë¡œ ë¶„ì„ë¨")
        else:
            summary_parts.append("ğŸ¤ **ë°œí‘œ íŠ¹ì„±**: ë‹¨ì¼ í™”ì ë°œí‘œë¡œ ë¶„ì„ë¨")
        
        return "\\n".join(summary_parts)
    
    def _create_detailed_breakdown(self, speaker_statements: Dict, speaker_timeline: List[Dict]) -> List[Dict]:
        """ìƒì„¸ ë¶„ì„ ê²°ê³¼ ìƒì„±"""
        
        breakdown = []
        
        for segment in speaker_timeline:
            speaker_id = segment["speaker_id"]
            segment_id = segment["segment_id"]
            
            # í•´ë‹¹ ì„¸ê·¸ë¨¼íŠ¸ì˜ ë°œì–¸ ë‚´ìš© ì°¾ê¸°
            segment_content = ""
            if speaker_id in speaker_statements:
                for stmt in speaker_statements[speaker_id]:
                    if stmt["segment_id"] == segment_id:
                        segment_content = stmt["content"]
                        break
            
            breakdown.append({
                "segment_id": segment_id,
                "speaker_id": speaker_id,
                "time_range": f"{segment['start_formatted']} - {segment['end_formatted']}",
                "duration": segment["duration_formatted"],
                "content": segment_content,
                "content_length": len(segment_content),
                "word_count": len(segment_content.split()) if segment_content else 0
            })
        
        return breakdown
    
    def _format_time(self, seconds: float) -> str:
        """ì‹œê°„ì„ ì½ê¸° ì‰¬ìš´ í˜•íƒœë¡œ í¬ë§·"""
        
        if seconds < 60:
            return f"{seconds:.1f}ì´ˆ"
        elif seconds < 3600:
            minutes = int(seconds // 60)
            secs = seconds % 60
            return f"{minutes}ë¶„ {secs:.1f}ì´ˆ"
        else:
            hours = int(seconds // 3600)
            minutes = int((seconds % 3600) // 60)
            secs = seconds % 60
            return f"{hours}ì‹œê°„ {minutes}ë¶„ {secs:.1f}ì´ˆ"
    
    def _create_error_result(self, error_msg: str, audio_file: str, processing_time: float = 0) -> Dict[str, Any]:
        """ì˜¤ë¥˜ ê²°ê³¼ ìƒì„±"""
        
        return {
            "status": "error",
            "error": error_msg,
            "audio_file": audio_file,
            "processing_time": processing_time,
            "speaker_count": 0,
            "user_summary": f"âŒ í™”ì ë¶„ë¦¬ ë¶„ì„ ì‹¤íŒ¨: {error_msg}",
            "timestamp": datetime.now().isoformat(),
            "dependencies_status": {
                "librosa": LIBROSA_AVAILABLE,
                "sklearn": SKLEARN_AVAILABLE,
                "webrtcvad": WEBRTC_AVAILABLE
            }
        }
    
    def get_installation_guide(self) -> Dict[str, Any]:
        """ì„¤ì¹˜ ê°€ì´ë“œ ì œê³µ"""
        
        missing_packages = []
        
        if not LIBROSA_AVAILABLE:
            missing_packages.append({
                "package": "librosa",
                "command": "pip install librosa",
                "purpose": "ì˜¤ë””ì˜¤ ì‹ í˜¸ ì²˜ë¦¬ ë° íŠ¹ì„± ì¶”ì¶œ"
            })
        
        if not SKLEARN_AVAILABLE:
            missing_packages.append({
                "package": "scikit-learn",
                "command": "pip install scikit-learn",
                "purpose": "í™”ì íŠ¹ì„± í´ëŸ¬ìŠ¤í„°ë§"
            })
        
        if not WEBRTC_AVAILABLE:
            missing_packages.append({
                "package": "webrtcvad",
                "command": "pip install webrtcvad",
                "purpose": "ìŒì„± í™œë™ ê°ì§€ (VAD)"
            })
        
        return {
            "available": LIBROSA_AVAILABLE and SKLEARN_AVAILABLE,
            "missing_packages": missing_packages,
            "install_all": "pip install librosa scikit-learn webrtcvad",
            "realtime_tracking": self.realtime_tracking_available,
            "speaker_identification": self.speaker_identification_available,
            "recommended_setup": [
                "1. pip install librosa scikit-learn webrtcvad",
                "2. ì‹¤ì‹œê°„ ì¶”ì  ì‹œìŠ¤í…œ í™œì„±í™” í™•ì¸",
                "3. í™”ì ì‹ë³„ ì‹œìŠ¤í…œ ì—°ë™ í™•ì¸",
                "4. í…ŒìŠ¤íŠ¸ ì˜¤ë””ì˜¤ë¡œ ê¸°ëŠ¥ ê²€ì¦"
            ]
        }


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
global_speaker_diarization = RealtimeSpeakerDiarization()

def analyze_speakers_realtime(audio_file: str, transcript: str = "", progress_container=None) -> Dict[str, Any]:
    """ì‹¤ì‹œê°„ ì¶”ì  í™”ì ë¶„ë¦¬ ë¶„ì„ í•¨ìˆ˜"""
    return global_speaker_diarization.analyze_speakers_in_audio(audio_file, transcript, progress_container)