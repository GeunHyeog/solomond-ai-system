"""
ì†”ë¡œëª¬ë“œ AI ì‹œìŠ¤í…œ - í˜„ì¥ í’ˆì§ˆ ë¶„ì„ ì—”ì§„
í˜„ì¥ì—ì„œ ì´¬ì˜í•œ ì˜¤ë””ì˜¤/ë¹„ë””ì˜¤ì˜ ë…¸ì´ì¦ˆ ë¶„ì„ ë° í’ˆì§ˆ í‰ê°€ ëª¨ë“ˆ
"""

import numpy as np
import librosa
import scipy.signal
from typing import Dict, List, Optional, Tuple, Any
import logging
import asyncio
from concurrent.futures import ThreadPoolExecutor
import tempfile
import os
from pathlib import Path
import json
from datetime import datetime

# ì˜¤ë””ì˜¤ ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    import soundfile as sf
    SOUNDFILE_AVAILABLE = True
except ImportError:
    SOUNDFILE_AVAILABLE = False

try:
    import webrtcvad
    WEBRTCVAD_AVAILABLE = True
except ImportError:
    WEBRTCVAD_AVAILABLE = False

class FieldQualityAnalyzer:
    """í˜„ì¥ ì˜¤ë””ì˜¤/ë¹„ë””ì˜¤ í’ˆì§ˆ ë¶„ì„ í´ë˜ìŠ¤"""
    
    def __init__(self):
        # í’ˆì§ˆ í‰ê°€ ê¸°ì¤€ê°’ë“¤
        self.quality_thresholds = {
            "excellent": {"snr": 25, "clarity": 0.9, "noise": 0.1},
            "good": {"snr": 20, "clarity": 0.8, "noise": 0.2},
            "fair": {"snr": 15, "clarity": 0.7, "noise": 0.3},
            "poor": {"snr": 10, "clarity": 0.6, "noise": 0.4}
        }
        
        # ë…¸ì´ì¦ˆ ìœ í˜•ë³„ íŠ¹ì„±
        self.noise_signatures = {
            "air_conditioning": {"freq_range": (100, 1000), "pattern": "constant"},
            "crowd_noise": {"freq_range": (200, 4000), "pattern": "variable"},
            "electronic_hum": {"freq_range": (50, 60), "pattern": "constant"},
            "traffic": {"freq_range": (50, 2000), "pattern": "variable"},
            "wind": {"freq_range": (10, 500), "pattern": "variable"},
            "microphone_handling": {"freq_range": (1, 200), "pattern": "burst"}
        }
        
        # VAD (Voice Activity Detection) ì´ˆê¸°í™”
        self.vad = None
        if WEBRTCVAD_AVAILABLE:
            try:
                self.vad = webrtcvad.Vad(2)  # ì¤‘ê°„ ê°•ë„
            except Exception as e:
                logging.warning(f"WebRTC VAD ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # ìŠ¤ë ˆë“œ í’€ executor
        self.executor = ThreadPoolExecutor(max_workers=2)
        
        logging.info("í˜„ì¥ í’ˆì§ˆ ë¶„ì„ ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def analyze_audio_quality(self, 
                                  audio_data: bytes, 
                                  filename: str,
                                  sample_rate: int = None) -> Dict:
        """
        í˜„ì¥ ì˜¤ë””ì˜¤ í’ˆì§ˆ ì¢…í•© ë¶„ì„
        
        Args:
            audio_data: ì˜¤ë””ì˜¤ ë°”ì´ë„ˆë¦¬ ë°ì´í„°
            filename: íŒŒì¼ëª…
            sample_rate: ìƒ˜í”Œë§ ë ˆì´íŠ¸ (Noneì´ë©´ ìë™ ê°ì§€)
            
        Returns:
            í’ˆì§ˆ ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        try:
            print(f"ğŸ”Š í˜„ì¥ ì˜¤ë””ì˜¤ í’ˆì§ˆ ë¶„ì„ ì‹œì‘: {filename}")
            
            # ì˜¤ë””ì˜¤ ë°ì´í„° ë¡œë“œ
            audio_array, sr = await self._load_audio_data(audio_data, sample_rate)
            
            if audio_array is None:
                return {
                    "success": False,
                    "error": "ì˜¤ë””ì˜¤ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨",
                    "filename": filename
                }
            
            # ë³‘ë ¬ë¡œ ì—¬ëŸ¬ ë¶„ì„ ìˆ˜í–‰
            analysis_tasks = [
                self._analyze_snr(audio_array, sr),
                self._analyze_noise_characteristics(audio_array, sr),
                self._analyze_speech_clarity(audio_array, sr),
                self._detect_noise_types(audio_array, sr),
                self._analyze_volume_levels(audio_array, sr)
            ]
            
            results = await asyncio.gather(*analysis_tasks, return_exceptions=True)
            
            # ê²°ê³¼ í†µí•©
            snr_result = results[0] if not isinstance(results[0], Exception) else {}
            noise_result = results[1] if not isinstance(results[1], Exception) else {}
            clarity_result = results[2] if not isinstance(results[2], Exception) else {}
            noise_types = results[3] if not isinstance(results[3], Exception) else {}
            volume_result = results[4] if not isinstance(results[4], Exception) else {}
            
            # ì „ì²´ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
            overall_score = self._calculate_overall_quality_score(
                snr_result, noise_result, clarity_result, volume_result
            )
            
            # ê°œì„  ì œì•ˆ ìƒì„±
            recommendations = self._generate_improvement_recommendations(
                snr_result, noise_result, clarity_result, noise_types, volume_result
            )
            
            # ìµœì¢… ê²°ê³¼ êµ¬ì„±
            result = {
                "success": True,
                "filename": filename,
                "audio_info": {
                    "duration": len(audio_array) / sr,
                    "sample_rate": sr,
                    "channels": 1 if len(audio_array.shape) == 1 else audio_array.shape[1],
                    "file_size_mb": round(len(audio_data) / (1024 * 1024), 2)
                },
                "quality_analysis": {
                    "overall_score": overall_score,
                    "snr_analysis": snr_result,
                    "noise_analysis": noise_result,
                    "clarity_analysis": clarity_result,
                    "volume_analysis": volume_result,
                    "detected_noise_types": noise_types
                },
                "quality_assessment": self._assess_quality_level(overall_score),
                "recommendations": recommendations,
                "analyzed_at": datetime.now().isoformat()
            }
            
            print(f"âœ… í’ˆì§ˆ ë¶„ì„ ì™„ë£Œ: ì „ì²´ ì ìˆ˜ {overall_score}/100")
            return result
            
        except Exception as e:
            logging.error(f"í˜„ì¥ ì˜¤ë””ì˜¤ í’ˆì§ˆ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {
                "success": False,
                "error": str(e),
                "filename": filename
            }
    
    async def _load_audio_data(self, audio_data: bytes, target_sr: int = None) -> Tuple[np.ndarray, int]:
        """ì˜¤ë””ì˜¤ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        try:
            # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
            with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_file:
                temp_file.write(audio_data)
                temp_path = temp_file.name
            
            try:
                # librosaë¡œ ì˜¤ë””ì˜¤ ë¡œë“œ
                audio_array, sr = librosa.load(temp_path, sr=target_sr, mono=True)
                
                # ì •ê·œí™”
                if np.max(np.abs(audio_array)) > 0:
                    audio_array = audio_array / np.max(np.abs(audio_array))
                
                return audio_array, sr
                
            finally:
                # ì„ì‹œ íŒŒì¼ ì •ë¦¬
                try:
                    os.unlink(temp_path)
                except:
                    pass
                    
        except Exception as e:
            logging.error(f"ì˜¤ë””ì˜¤ ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}")
            return None, None
    
    async def _analyze_snr(self, audio: np.ndarray, sr: int) -> Dict:
        """ì‹ í˜¸ ëŒ€ ì¡ìŒë¹„ (SNR) ë¶„ì„"""
        try:
            # VADë¥¼ ì‚¬ìš©í•˜ì—¬ ìŒì„± êµ¬ê°„ê³¼ ë¹„ìŒì„± êµ¬ê°„ ë¶„ë¦¬
            voice_segments, noise_segments = await self._separate_voice_noise(audio, sr)
            
            if len(voice_segments) == 0 or len(noise_segments) == 0:
                # VAD ì‹¤íŒ¨ ì‹œ ê°„ë‹¨í•œ ì—ë„ˆì§€ ê¸°ë°˜ ë¶„ì„
                return await self._analyze_snr_energy_based(audio, sr)
            
            # ìŒì„± ì‹ í˜¸ ì „ë ¥ ê³„ì‚°
            voice_power = np.mean([np.mean(segment**2) for segment in voice_segments])
            
            # ë…¸ì´ì¦ˆ ì‹ í˜¸ ì „ë ¥ ê³„ì‚°
            noise_power = np.mean([np.mean(segment**2) for segment in noise_segments])
            
            # SNR ê³„ì‚° (dB)
            if noise_power > 0:
                snr_db = 10 * np.log10(voice_power / noise_power)
            else:
                snr_db = 60  # ë…¸ì´ì¦ˆê°€ ê±°ì˜ ì—†ëŠ” ê²½ìš°
            
            # SNR í’ˆì§ˆ í‰ê°€
            if snr_db >= 25:
                snr_quality = "excellent"
            elif snr_db >= 20:
                snr_quality = "good"
            elif snr_db >= 15:
                snr_quality = "fair"
            else:
                snr_quality = "poor"
            
            return {
                "snr_db": round(snr_db, 2),
                "voice_power": round(voice_power, 6),
                "noise_power": round(noise_power, 6),
                "quality_level": snr_quality,
                "voice_segments_count": len(voice_segments),
                "noise_segments_count": len(noise_segments)
            }
            
        except Exception as e:
            logging.error(f"SNR ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {"error": str(e)}
    
    async def _analyze_snr_energy_based(self, audio: np.ndarray, sr: int) -> Dict:
        """ì—ë„ˆì§€ ê¸°ë°˜ ê°„ë‹¨í•œ SNR ë¶„ì„"""
        try:
            # í”„ë ˆì„ ë‹¨ìœ„ë¡œ ì—ë„ˆì§€ ê³„ì‚°
            frame_length = int(0.025 * sr)  # 25ms í”„ë ˆì„
            hop_length = int(0.01 * sr)     # 10ms í™‰
            
            # ì—ë„ˆì§€ ê³„ì‚°
            energy = []
            for i in range(0, len(audio) - frame_length, hop_length):
                frame = audio[i:i + frame_length]
                energy.append(np.sum(frame**2))
            
            energy = np.array(energy)
            
            # ìƒìœ„ 30%ë¥¼ ìŒì„±, í•˜ìœ„ 30%ë¥¼ ë…¸ì´ì¦ˆë¡œ ê°€ì •
            sorted_energy = np.sort(energy)
            top_30_percent = sorted_energy[int(0.7 * len(sorted_energy)):]
            bottom_30_percent = sorted_energy[:int(0.3 * len(sorted_energy))]
            
            voice_power = np.mean(top_30_percent)
            noise_power = np.mean(bottom_30_percent)
            
            if noise_power > 0:
                snr_db = 10 * np.log10(voice_power / noise_power)
            else:
                snr_db = 60
            
            return {
                "snr_db": round(snr_db, 2),
                "voice_power": round(voice_power, 6),
                "noise_power": round(noise_power, 6),
                "quality_level": "good" if snr_db >= 20 else "fair" if snr_db >= 15 else "poor",
                "method": "energy_based"
            }
            
        except Exception as e:
            logging.error(f"ì—ë„ˆì§€ ê¸°ë°˜ SNR ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {"error": str(e)}
    
    async def _separate_voice_noise(self, audio: np.ndarray, sr: int) -> Tuple[List[np.ndarray], List[np.ndarray]]:
        """ìŒì„±ê³¼ ë…¸ì´ì¦ˆ êµ¬ê°„ ë¶„ë¦¬"""
        voice_segments = []
        noise_segments = []
        
        try:
            if self.vad and WEBRTCVAD_AVAILABLE:
                # WebRTC VAD ì‚¬ìš©
                frame_duration = 30  # ms
                frame_length = int(sr * frame_duration / 1000)
                
                for i in range(0, len(audio) - frame_length, frame_length):
                    frame = audio[i:i + frame_length]
                    
                    # 16kHz, 16-bit PCMìœ¼ë¡œ ë³€í™˜
                    frame_16k = librosa.resample(frame, orig_sr=sr, target_sr=16000)
                    frame_bytes = (frame_16k * 32767).astype(np.int16).tobytes()
                    
                    try:
                        is_speech = self.vad.is_speech(frame_bytes, 16000)
                        if is_speech:
                            voice_segments.append(frame)
                        else:
                            noise_segments.append(frame)
                    except:
                        # VAD ì‹¤íŒ¨ ì‹œ ì—ë„ˆì§€ ê¸°ë°˜ìœ¼ë¡œ ë¶„ë¥˜
                        energy = np.sum(frame**2)
                        if energy > np.mean(audio**2):
                            voice_segments.append(frame)
                        else:
                            noise_segments.append(frame)
            else:
                # ê°„ë‹¨í•œ ì—ë„ˆì§€ ê¸°ë°˜ ë¶„ë¥˜
                frame_length = int(0.03 * sr)  # 30ms í”„ë ˆì„
                threshold = np.mean(audio**2) * 2
                
                for i in range(0, len(audio) - frame_length, frame_length):
                    frame = audio[i:i + frame_length]
                    energy = np.sum(frame**2)
                    
                    if energy > threshold:
                        voice_segments.append(frame)
                    else:
                        noise_segments.append(frame)
        
        except Exception as e:
            logging.error(f"ìŒì„±/ë…¸ì´ì¦ˆ ë¶„ë¦¬ ì˜¤ë¥˜: {e}")
        
        return voice_segments, noise_segments
    
    async def _analyze_noise_characteristics(self, audio: np.ndarray, sr: int) -> Dict:
        """ë…¸ì´ì¦ˆ íŠ¹ì„± ë¶„ì„"""
        try:
            # ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ê³„ì‚°
            stft = librosa.stft(audio, n_fft=2048, hop_length=512)
            magnitude = np.abs(stft)
            
            # ì£¼íŒŒìˆ˜ë³„ ë…¸ì´ì¦ˆ ë ˆë²¨ ë¶„ì„
            freq_bins = librosa.fft_frequencies(sr=sr, n_fft=2048)
            noise_profile = np.mean(magnitude, axis=1)
            
            # ë…¸ì´ì¦ˆ ì§‘ì¤‘ ì£¼íŒŒìˆ˜ ëŒ€ì—­ ì°¾ê¸°
            peak_freq_idx = np.argmax(noise_profile)
            peak_frequency = freq_bins[peak_freq_idx]
            
            # ë…¸ì´ì¦ˆ ì¼ê´€ì„± ë¶„ì„ (ì‹œê°„ì— ë”°ë¥¸ ë³€í™”)
            temporal_variance = np.var(magnitude, axis=1)
            consistency_score = 1 - (np.mean(temporal_variance) / np.max(temporal_variance))
            
            # ì „ì²´ ë…¸ì´ì¦ˆ ë ˆë²¨
            overall_noise_level = np.mean(noise_profile)
            
            return {
                "peak_frequency": round(peak_frequency, 2),
                "overall_noise_level": round(overall_noise_level, 6),
                "consistency_score": round(consistency_score, 3),
                "frequency_distribution": {
                    "low_freq": round(np.mean(noise_profile[:len(noise_profile)//4]), 6),
                    "mid_freq": round(np.mean(noise_profile[len(noise_profile)//4:3*len(noise_profile)//4]), 6),
                    "high_freq": round(np.mean(noise_profile[3*len(noise_profile)//4:]), 6)
                }
            }
            
        except Exception as e:
            logging.error(f"ë…¸ì´ì¦ˆ íŠ¹ì„± ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {"error": str(e)}
    
    async def _analyze_speech_clarity(self, audio: np.ndarray, sr: int) -> Dict:
        """ìŒì„± ëª…í™•ë„ ë¶„ì„"""
        try:
            # ìŠ¤í™íŠ¸ëŸ¼ ì¤‘ì‹¬ì  ê³„ì‚° (ìŒì„± ëª…í™•ë„ ì§€í‘œ)
            spectral_centroids = librosa.feature.spectral_centroid(y=audio, sr=sr)[0]
            mean_centroid = np.mean(spectral_centroids)
            
            # ìŠ¤í™íŠ¸ëŸ¼ ë¡¤ì˜¤í”„ ê³„ì‚°
            spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr)[0]
            mean_rolloff = np.mean(spectral_rolloff)
            
            # ì œë¡œ í¬ë¡œì‹± ë ˆì´íŠ¸ (ìŒì„± í’ˆì§ˆ ì§€í‘œ)
            zcr = librosa.feature.zero_crossing_rate(audio)[0]
            mean_zcr = np.mean(zcr)
            
            # MFCC ê¸°ë°˜ ìŒì„± í’ˆì§ˆ í‰ê°€
            mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)
            mfcc_variance = np.var(mfcc, axis=1)
            speech_quality = 1 - (np.mean(mfcc_variance) / np.max(mfcc_variance))
            
            # ëª…í™•ë„ ì ìˆ˜ ê³„ì‚° (0-1)
            clarity_score = min(1.0, (speech_quality + (1 - mean_zcr)) / 2)
            
            return {
                "clarity_score": round(clarity_score, 3),
                "spectral_centroid": round(mean_centroid, 2),
                "spectral_rolloff": round(mean_rolloff, 2),
                "zero_crossing_rate": round(mean_zcr, 4),
                "speech_quality": round(speech_quality, 3)
            }
            
        except Exception as e:
            logging.error(f"ìŒì„± ëª…í™•ë„ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {"error": str(e)}
    
    async def _detect_noise_types(self, audio: np.ndarray, sr: int) -> Dict:
        """ë…¸ì´ì¦ˆ ìœ í˜• ê°ì§€"""
        detected_noises = {}
        
        try:
            # ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ê³„ì‚°
            stft = librosa.stft(audio, n_fft=2048, hop_length=512)
            magnitude = np.abs(stft)
            freq_bins = librosa.fft_frequencies(sr=sr, n_fft=2048)
            
            for noise_type, signature in self.noise_signatures.items():
                freq_range = signature["freq_range"]
                pattern = signature["pattern"]
                
                # í•´ë‹¹ ì£¼íŒŒìˆ˜ ëŒ€ì—­ì˜ ì—ë„ˆì§€ ì¶”ì¶œ
                freq_mask = (freq_bins >= freq_range[0]) & (freq_bins <= freq_range[1])
                if np.any(freq_mask):
                    band_energy = np.mean(magnitude[freq_mask], axis=0)
                    
                    # íŒ¨í„´ ë¶„ì„
                    if pattern == "constant":
                        # ì¼ì •í•œ íŒ¨í„´ì¸ì§€ í™•ì¸
                        variance = np.var(band_energy)
                        consistency = 1 - (variance / np.mean(band_energy) if np.mean(band_energy) > 0 else 1)
                        confidence = min(1.0, consistency * np.mean(band_energy) * 1000)
                    else:  # variable
                        # ë³€ë™í•˜ëŠ” íŒ¨í„´ì¸ì§€ í™•ì¸
                        variance = np.var(band_energy)
                        variability = variance / np.mean(band_energy) if np.mean(band_energy) > 0 else 0
                        confidence = min(1.0, variability * np.mean(band_energy) * 1000)
                    
                    if confidence > 0.3:  # ì„ê³„ê°’ ì´ìƒì¼ ë•Œë§Œ ê°ì§€ë¡œ íŒë‹¨
                        detected_noises[noise_type] = {
                            "confidence": round(confidence, 3),
                            "avg_energy": round(np.mean(band_energy), 6),
                            "frequency_range": freq_range
                        }
            
            return detected_noises
            
        except Exception as e:
            logging.error(f"ë…¸ì´ì¦ˆ ìœ í˜• ê°ì§€ ì˜¤ë¥˜: {e}")
            return {"error": str(e)}
    
    async def _analyze_volume_levels(self, audio: np.ndarray, sr: int) -> Dict:
        """ë³¼ë¥¨ ë ˆë²¨ ë¶„ì„"""
        try:
            # RMS ì—ë„ˆì§€ ê³„ì‚°
            rms_energy = librosa.feature.rms(y=audio)[0]
            
            # dBë¡œ ë³€í™˜
            rms_db = 20 * np.log10(rms_energy + 1e-6)  # ë¡œê·¸ ê³„ì‚° ì‹œ 0 ë°©ì§€
            
            # í†µê³„ ê³„ì‚°
            mean_db = np.mean(rms_db)
            max_db = np.max(rms_db)
            min_db = np.min(rms_db)
            dynamic_range = max_db - min_db
            
            # í´ë¦¬í•‘ ê°ì§€
            clipping_ratio = np.sum(np.abs(audio) > 0.95) / len(audio)
            
            # ë³¼ë¥¨ ì¼ê´€ì„±
            volume_consistency = 1 - (np.std(rms_db) / abs(mean_db) if abs(mean_db) > 0 else 1)
            
            return {
                "mean_db": round(mean_db, 2),
                "max_db": round(max_db, 2),
                "min_db": round(min_db, 2),
                "dynamic_range": round(dynamic_range, 2),
                "clipping_ratio": round(clipping_ratio, 4),
                "volume_consistency": round(volume_consistency, 3)
            }
            
        except Exception as e:
            logging.error(f"ë³¼ë¥¨ ë ˆë²¨ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {"error": str(e)}
    
    def _calculate_overall_quality_score(self, 
                                       snr_result: Dict, 
                                       noise_result: Dict, 
                                       clarity_result: Dict, 
                                       volume_result: Dict) -> int:
        """ì „ì²´ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (0-100)"""
        try:
            score = 0
            weight_sum = 0
            
            # SNR ì ìˆ˜ (ê°€ì¤‘ì¹˜: 40%)
            if "snr_db" in snr_result:
                snr_score = min(100, max(0, (snr_result["snr_db"] + 10) * 2.5))  # -10dB~30dB -> 0~100
                score += snr_score * 0.4
                weight_sum += 0.4
            
            # ëª…í™•ë„ ì ìˆ˜ (ê°€ì¤‘ì¹˜: 30%)
            if "clarity_score" in clarity_result:
                clarity_score = clarity_result["clarity_score"] * 100
                score += clarity_score * 0.3
                weight_sum += 0.3
            
            # ë³¼ë¥¨ ì¼ê´€ì„± (ê°€ì¤‘ì¹˜: 20%)
            if "volume_consistency" in volume_result:
                volume_score = volume_result["volume_consistency"] * 100
                score += volume_score * 0.2
                weight_sum += 0.2
            
            # ë…¸ì´ì¦ˆ ë ˆë²¨ (ê°€ì¤‘ì¹˜: 10%)
            if "overall_noise_level" in noise_result:
                # ë…¸ì´ì¦ˆê°€ ì ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜
                noise_score = max(0, 100 - (noise_result["overall_noise_level"] * 10000))
                score += noise_score * 0.1
                weight_sum += 0.1
            
            # ê°€ì¤‘ í‰ê·  ê³„ì‚°
            if weight_sum > 0:
                final_score = int(score / weight_sum)
            else:
                final_score = 50  # ê¸°ë³¸ê°’
            
            return max(0, min(100, final_score))
            
        except Exception as e:
            logging.error(f"ì „ì²´ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return 50
    
    def _assess_quality_level(self, score: int) -> Dict:
        """í’ˆì§ˆ ì ìˆ˜ë¥¼ ë ˆë²¨ë¡œ ë³€í™˜"""
        if score >= 85:
            return {
                "level": "excellent",
                "description": "ë§¤ìš° ìš°ìˆ˜í•œ í’ˆì§ˆ",
                "color": "green",
                "icon": "ğŸŸ¢"
            }
        elif score >= 70:
            return {
                "level": "good", 
                "description": "ì–‘í˜¸í•œ í’ˆì§ˆ",
                "color": "lightgreen",
                "icon": "ğŸŸ¡"
            }
        elif score >= 50:
            return {
                "level": "fair",
                "description": "ë³´í†µ í’ˆì§ˆ",
                "color": "orange", 
                "icon": "ğŸŸ "
            }
        else:
            return {
                "level": "poor",
                "description": "í’ˆì§ˆ ê°œì„  í•„ìš”",
                "color": "red",
                "icon": "ğŸ”´"
            }
    
    def _generate_improvement_recommendations(self, 
                                            snr_result: Dict,
                                            noise_result: Dict, 
                                            clarity_result: Dict,
                                            noise_types: Dict,
                                            volume_result: Dict) -> List[Dict]:
        """í’ˆì§ˆ ê°œì„  ì œì•ˆ ìƒì„±"""
        recommendations = []
        
        try:
            # SNR ê¸°ë°˜ ì œì•ˆ
            if "snr_db" in snr_result:
                snr = snr_result["snr_db"]
                if snr < 15:
                    recommendations.append({
                        "type": "snr",
                        "priority": "high",
                        "issue": f"ì‹ í˜¸ ëŒ€ ì¡ìŒë¹„ê°€ ë‚®ìŠµë‹ˆë‹¤ ({snr:.1f}dB)",
                        "solution": "ë§ˆì´í¬ë¥¼ í™”ìì—ê²Œ ë” ê°€ê¹Œì´ ë°°ì¹˜í•˜ê±°ë‚˜ ë” ì¡°ìš©í•œ í™˜ê²½ì—ì„œ ë…¹ìŒí•˜ì„¸ìš”",
                        "icon": "ğŸ¤"
                    })
                elif snr < 20:
                    recommendations.append({
                        "type": "snr",
                        "priority": "medium",
                        "issue": f"ë°°ê²½ ë…¸ì´ì¦ˆê°€ ë‹¤ì†Œ ìˆìŠµë‹ˆë‹¤ ({snr:.1f}dB)",
                        "solution": "ê°€ëŠ¥í•˜ë©´ ë” ì¡°ìš©í•œ í™˜ê²½ì—ì„œ ë…¹ìŒí•˜ì‹œê¸° ë°”ëë‹ˆë‹¤",
                        "icon": "ğŸ”‡"
                    })
            
            # ë…¸ì´ì¦ˆ ìœ í˜•ë³„ ì œì•ˆ
            for noise_type, detection in noise_types.items():
                if detection["confidence"] > 0.5:
                    if noise_type == "air_conditioning":
                        recommendations.append({
                            "type": "noise",
                            "priority": "medium",
                            "issue": "ì—ì–´ì»¨ ì†ŒìŒì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤",
                            "solution": "ì—ì–´ì»¨ì„ ì¼ì‹œì ìœ¼ë¡œ ë„ê±°ë‚˜ ë” ë©€ë¦¬ ë–¨ì–´ì§„ ê³³ì—ì„œ ë…¹ìŒí•˜ì„¸ìš”",
                            "icon": "â„ï¸"
                        })
                    elif noise_type == "crowd_noise":
                        recommendations.append({
                            "type": "noise",
                            "priority": "high",
                            "issue": "ì‚¬ëŒë“¤ì˜ ëŒ€í™” ì†ŒìŒì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤",
                            "solution": "ë” ì¡°ìš©í•œ ê³µê°„ìœ¼ë¡œ ì´ë™í•˜ê±°ë‚˜ ì§€í–¥ì„± ë§ˆì´í¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”",
                            "icon": "ğŸ‘¥"
                        })
                    elif noise_type == "electronic_hum":
                        recommendations.append({
                            "type": "noise",
                            "priority": "medium",
                            "issue": "ì „ìê¸°ê¸° í—˜(hum) ë…¸ì´ì¦ˆê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤",
                            "solution": "ì „ìê¸°ê¸°ì—ì„œ ë©€ë¦¬ ë–¨ì–´ì ¸ ë…¹ìŒí•˜ê±°ë‚˜ ë‹¤ë¥¸ ì „ì›ì„ ì‚¬ìš©í•˜ì„¸ìš”",
                            "icon": "âš¡"
                        })
            
            # ëª…í™•ë„ ê¸°ë°˜ ì œì•ˆ
            if "clarity_score" in clarity_result:
                clarity = clarity_result["clarity_score"]
                if clarity < 0.6:
                    recommendations.append({
                        "type": "clarity",
                        "priority": "high",
                        "issue": f"ìŒì„± ëª…í™•ë„ê°€ ë‚®ìŠµë‹ˆë‹¤ ({clarity:.2f})",
                        "solution": "í™”ìê°€ ë” í¬ê³  ëª…í™•í•˜ê²Œ ë°œìŒí•˜ë„ë¡ ìš”ì²­í•˜ì„¸ìš”",
                        "icon": "ğŸ—£ï¸"
                    })
            
            # ë³¼ë¥¨ ê¸°ë°˜ ì œì•ˆ
            if "mean_db" in volume_result:
                mean_db = volume_result["mean_db"]
                if mean_db < -30:
                    recommendations.append({
                        "type": "volume",
                        "priority": "high",
                        "issue": f"ë…¹ìŒ ë³¼ë¥¨ì´ ë„ˆë¬´ ë‚®ìŠµë‹ˆë‹¤ ({mean_db:.1f}dB)",
                        "solution": "ë§ˆì´í¬ ê²Œì¸ì„ ë†’ì´ê±°ë‚˜ í™”ìì—ê²Œ ë” ê°€ê¹Œì´ ê°€ì„¸ìš”",
                        "icon": "ğŸ”Š"
                    })
                elif volume_result.get("clipping_ratio", 0) > 0.01:
                    recommendations.append({
                        "type": "volume",
                        "priority": "medium",
                        "issue": "ì˜¤ë””ì˜¤ í´ë¦¬í•‘ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤",
                        "solution": "ë…¹ìŒ ë ˆë²¨ì„ ë‚®ì¶”ì–´ ì™œê³¡ì„ ë°©ì§€í•˜ì„¸ìš”",
                        "icon": "ğŸ“‰"
                    })
            
            # ì¼ë°˜ì ì¸ ì œì•ˆ (í’ˆì§ˆì´ ì „ë°˜ì ìœ¼ë¡œ ë‚®ì€ ê²½ìš°)
            if len(recommendations) >= 3:
                recommendations.append({
                    "type": "general",
                    "priority": "medium",
                    "issue": "ì „ë°˜ì ì¸ ë…¹ìŒ í’ˆì§ˆ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤",
                    "solution": "ì¡°ìš©í•œ í™˜ê²½ì—ì„œ ê³ í’ˆì§ˆ ë§ˆì´í¬ë¡œ ì¬ë…¹ìŒì„ ê¶Œì¥í•©ë‹ˆë‹¤",
                    "icon": "ğŸ¯"
                })
            
            return recommendations[:5]  # ìµœëŒ€ 5ê°œ ì œì•ˆ
            
        except Exception as e:
            logging.error(f"ê°œì„  ì œì•ˆ ìƒì„± ì˜¤ë¥˜: {e}")
            return [{
                "type": "error",
                "priority": "low",
                "issue": "ê°œì„  ì œì•ˆ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ",
                "solution": "ìˆ˜ë™ìœ¼ë¡œ ì˜¤ë””ì˜¤ í’ˆì§ˆì„ í™•ì¸í•´ ì£¼ì„¸ìš”",
                "icon": "âš ï¸"
            }]

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
_field_quality_analyzer_instance = None

def get_field_quality_analyzer() -> FieldQualityAnalyzer:
    """ì „ì—­ í˜„ì¥ í’ˆì§ˆ ë¶„ì„ê¸° ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _field_quality_analyzer_instance
    if _field_quality_analyzer_instance is None:
        _field_quality_analyzer_instance = FieldQualityAnalyzer()
    return _field_quality_analyzer_instance

# í¸ì˜ í•¨ìˆ˜ë“¤
async def analyze_field_audio_quality(audio_data: bytes, filename: str, **kwargs) -> Dict:
    """í˜„ì¥ ì˜¤ë””ì˜¤ í’ˆì§ˆ ë¶„ì„ í¸ì˜ í•¨ìˆ˜"""
    analyzer = get_field_quality_analyzer()
    return await analyzer.analyze_audio_quality(audio_data, filename, **kwargs)

def check_quality_analyzer_support() -> Dict:
    """í’ˆì§ˆ ë¶„ì„ê¸° ì§€ì› ìƒíƒœ í™•ì¸"""
    return {
        "libraries": {
            "librosa": True,  # í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬
            "soundfile": SOUNDFILE_AVAILABLE,
            "webrtcvad": WEBRTCVAD_AVAILABLE,
            "numpy": True,
            "scipy": True
        },
        "features": {
            "snr_analysis": True,
            "noise_detection": True,
            "clarity_analysis": True,
            "volume_analysis": True,
            "vad_support": WEBRTCVAD_AVAILABLE
        },
        "noise_types": list(FieldQualityAnalyzer().noise_signatures.keys()),
        "quality_levels": ["excellent", "good", "fair", "poor"]
    }

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    async def test_analyzer():
        print("í˜„ì¥ í’ˆì§ˆ ë¶„ì„ ì—”ì§„ í…ŒìŠ¤íŠ¸")
        support_info = check_quality_analyzer_support()
        print(f"ì§€ì› ìƒíƒœ: {support_info}")
        
        # í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ ì˜¤ë””ì˜¤ ìƒì„±
        import numpy as np
        sample_rate = 22050
        duration = 3.0
        t = np.linspace(0, duration, int(sample_rate * duration))
        audio_test = 0.5 * np.sin(2 * np.pi * 440 * t)  # 440Hz ì‚¬ì¸íŒŒ
        
        # ë°”ì´íŠ¸ë¡œ ë³€í™˜ (ë”ë¯¸ í…ŒìŠ¤íŠ¸)
        audio_bytes = (audio_test * 32767).astype(np.int16).tobytes()
        
        result = await analyze_field_audio_quality(audio_bytes, "test.wav")
        print(f"í…ŒìŠ¤íŠ¸ ê²°ê³¼: {result.get('success', False)}")
    
    import asyncio
    asyncio.run(test_analyzer())
