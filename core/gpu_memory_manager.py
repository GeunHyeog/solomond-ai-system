#!/usr/bin/env python3
"""
GPU ë©”ëª¨ë¦¬ ê´€ë¦¬ ì‹œìŠ¤í…œ
ë™ì  í• ë‹¹, CPU í´ë°±, ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ë° ìµœì í™”
"""

import os
import gc
import logging
import psutil
import time
from typing import Dict, Any, Optional, Callable, List
from dataclasses import dataclass
from enum import Enum
import threading
import warnings

# GPU ê´€ë ¨ ì„í¬íŠ¸ (ì„ íƒì )
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

try:
    import nvidia_ml_py3 as nvml
    NVIDIA_ML_AVAILABLE = True
except ImportError:
    NVIDIA_ML_AVAILABLE = False

class ComputeMode(Enum):
    """ì—°ì‚° ëª¨ë“œ"""
    AUTO = "auto"          # ìë™ ì„ íƒ
    GPU_ONLY = "gpu_only"  # GPU ì „ìš©
    CPU_ONLY = "cpu_only"  # CPU ì „ìš©
    HYBRID = "hybrid"      # í•˜ì´ë¸Œë¦¬ë“œ

@dataclass
class MemoryInfo:
    """ë©”ëª¨ë¦¬ ì •ë³´ ë°ì´í„°í´ë˜ìŠ¤"""
    total_mb: float
    available_mb: float
    used_mb: float
    usage_percent: float
    device_name: str = "Unknown"

@dataclass
class GPUInfo:
    """GPU ì •ë³´ ë°ì´í„°í´ë˜ìŠ¤"""
    device_id: int
    name: str
    memory_total_mb: float
    memory_free_mb: float
    memory_used_mb: float
    utilization_percent: float
    temperature_c: Optional[float] = None
    power_draw_w: Optional[float] = None

class GPUMemoryManager:
    """GPU ë©”ëª¨ë¦¬ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, 
                 memory_threshold_mb: float = 1000,  # ìµœì†Œ í•„ìš” ë©”ëª¨ë¦¬ (MB)
                 cpu_fallback_enabled: bool = True,
                 monitor_interval: float = 30.0):    # ëª¨ë‹ˆí„°ë§ ì£¼ê¸° (ì´ˆ)
        
        self.logger = self._setup_logging()
        self.memory_threshold_mb = memory_threshold_mb
        self.cpu_fallback_enabled = cpu_fallback_enabled
        self.monitor_interval = monitor_interval
        
        # ìƒíƒœ ê´€ë¦¬
        self.current_mode = ComputeMode.AUTO
        self.gpu_available = False
        self.gpu_info_list: List[GPUInfo] = []
        self.selected_gpu_id = 0
        
        # ëª¨ë‹ˆí„°ë§ ê´€ë ¨
        self.monitoring_active = False
        self.monitor_thread = None
        self.memory_history = []
        
        # ì´ˆê¸°í™”
        self._initialize_gpu_detection()
        self._set_initial_mode()
        
        # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
        self._configure_environment()
    
    def _setup_logging(self) -> logging.Logger:
        """ë¡œê¹… ì„¤ì •"""
        logger = logging.getLogger(f'{__name__}.GPUMemoryManager')
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - ğŸ–¥ï¸  GPU - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            logger.setLevel(logging.INFO)
        return logger
    
    def _initialize_gpu_detection(self):
        """GPU ê°ì§€ ë° ì´ˆê¸°í™”"""
        self.logger.info("ğŸ” GPU ê°ì§€ ì‹œì‘...")
        
        # NVIDIA GPU ê°ì§€
        if NVIDIA_ML_AVAILABLE:
            try:
                nvml.nvmlInit()
                device_count = nvml.nvmlDeviceGetCount()
                
                for i in range(device_count):
                    handle = nvml.nvmlDeviceGetHandleByIndex(i)
                    gpu_info = self._get_gpu_info(handle, i)
                    self.gpu_info_list.append(gpu_info)
                
                if self.gpu_info_list:
                    self.gpu_available = True
                    self.selected_gpu_id = self._select_best_gpu()
                    
                    self.logger.info(f"âœ… GPU ê°ì§€ ì™„ë£Œ: {len(self.gpu_info_list)}ê°œ")
                    for i, gpu in enumerate(self.gpu_info_list):
                        self.logger.info(f"   GPU {i}: {gpu.name} ({gpu.memory_total_mb:.0f}MB)")
                    self.logger.info(f"ğŸ¯ ì„ íƒëœ GPU: {self.selected_gpu_id}")
                else:
                    self.logger.info("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ GPU ì—†ìŒ")
                    
            except Exception as e:
                self.logger.warning(f"âš ï¸ NVIDIA GPU ê°ì§€ ì‹¤íŒ¨: {e}")
        
        # PyTorch GPU ê°ì§€ (ë³´ì¡°)
        elif TORCH_AVAILABLE and torch.cuda.is_available():
            try:
                gpu_count = torch.cuda.device_count()
                for i in range(gpu_count):
                    gpu_name = torch.cuda.get_device_name(i)
                    gpu_memory = torch.cuda.get_device_properties(i).total_memory / (1024**2)  # MB
                    
                    gpu_info = GPUInfo(
                        device_id=i,
                        name=gpu_name,
                        memory_total_mb=gpu_memory,
                        memory_free_mb=gpu_memory * 0.9,  # ì¶”ì •ì¹˜
                        memory_used_mb=gpu_memory * 0.1,
                        utilization_percent=0.0
                    )
                    self.gpu_info_list.append(gpu_info)
                
                if self.gpu_info_list:
                    self.gpu_available = True
                    self.selected_gpu_id = 0
                    self.logger.info(f"âœ… PyTorch GPU ê°ì§€: {gpu_count}ê°œ")
            
            except Exception as e:
                self.logger.warning(f"âš ï¸ PyTorch GPU ê°ì§€ ì‹¤íŒ¨: {e}")
        
        else:
            self.logger.info("â„¹ï¸ GPU ê°ì§€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ (nvidia-ml-py3, torch)")
    
    def _get_gpu_info(self, handle, device_id: int) -> GPUInfo:
        """GPU ì •ë³´ ìˆ˜ì§‘"""
        try:
            # ê¸°ë³¸ ì •ë³´
            name = nvml.nvmlDeviceGetName(handle).decode('utf-8')
            memory_info = nvml.nvmlDeviceGetMemoryInfo(handle)
            utilization = nvml.nvmlDeviceGetUtilizationRates(handle)
            
            # ì„ íƒì  ì •ë³´
            try:
                temperature = nvml.nvmlDeviceGetTemperature(handle, nvml.NVML_TEMPERATURE_GPU)
            except:
                temperature = None
            
            try:
                power_draw = nvml.nvmlDeviceGetPowerUsage(handle) / 1000.0  # mW -> W
            except:
                power_draw = None
            
            return GPUInfo(
                device_id=device_id,
                name=name,
                memory_total_mb=memory_info.total / (1024**2),
                memory_free_mb=memory_info.free / (1024**2),
                memory_used_mb=memory_info.used / (1024**2),
                utilization_percent=utilization.gpu,
                temperature_c=temperature,
                power_draw_w=power_draw
            )
            
        except Exception as e:
            self.logger.warning(f"GPU {device_id} ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return GPUInfo(
                device_id=device_id,
                name="Unknown GPU",
                memory_total_mb=0,
                memory_free_mb=0,
                memory_used_mb=0,
                utilization_percent=0
            )
    
    def _select_best_gpu(self) -> int:
        """ìµœì  GPU ì„ íƒ"""
        if not self.gpu_info_list:
            return 0
        
        # ì—¬ìœ  ë©”ëª¨ë¦¬ê°€ ê°€ì¥ ë§ì€ GPU ì„ íƒ
        best_gpu_id = 0
        max_free_memory = 0
        
        for gpu in self.gpu_info_list:
            if gpu.memory_free_mb > max_free_memory:
                max_free_memory = gpu.memory_free_mb
                best_gpu_id = gpu.device_id
        
        return best_gpu_id
    
    def _set_initial_mode(self):
        """ì´ˆê¸° ì—°ì‚° ëª¨ë“œ ì„¤ì •"""
        if self.gpu_available:
            # GPU ë©”ëª¨ë¦¬ ì¶©ë¶„ì„± í™•ì¸
            selected_gpu = self.gpu_info_list[self.selected_gpu_id]
            if selected_gpu.memory_free_mb >= self.memory_threshold_mb:
                self.current_mode = ComputeMode.GPU_ONLY
                self.logger.info(f"ğŸš€ ì´ˆê¸° ëª¨ë“œ: GPU ì‚¬ìš© (ì—¬ìœ  ë©”ëª¨ë¦¬: {selected_gpu.memory_free_mb:.0f}MB)")
            else:
                self.current_mode = ComputeMode.CPU_ONLY
                self.logger.info(f"ğŸ”„ ì´ˆê¸° ëª¨ë“œ: CPU ì‚¬ìš© (GPU ë©”ëª¨ë¦¬ ë¶€ì¡±: {selected_gpu.memory_free_mb:.0f}MB)")
        else:
            self.current_mode = ComputeMode.CPU_ONLY
            self.logger.info("ğŸ’» ì´ˆê¸° ëª¨ë“œ: CPU ì‚¬ìš© (GPU ì—†ìŒ)")
    
    def _configure_environment(self):
        """í™˜ê²½ ë³€ìˆ˜ ì„¤ì •"""
        if self.current_mode == ComputeMode.CPU_ONLY:
            # CPU ëª¨ë“œ ê°•ì œ
            os.environ['CUDA_VISIBLE_DEVICES'] = ''
            os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'
            self.logger.info("âš™ï¸ CPU ëª¨ë“œë¡œ í™˜ê²½ ì„¤ì • ì™„ë£Œ")
            
        elif self.current_mode == ComputeMode.GPU_ONLY:
            # ì„ íƒëœ GPUë§Œ ì‚¬ìš©
            os.environ['CUDA_VISIBLE_DEVICES'] = str(self.selected_gpu_id)
            os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'
            self.logger.info(f"âš™ï¸ GPU {self.selected_gpu_id} ëª¨ë“œë¡œ í™˜ê²½ ì„¤ì • ì™„ë£Œ")
    
    def get_recommended_mode(self, required_memory_mb: float = None) -> ComputeMode:
        """ê¶Œì¥ ì—°ì‚° ëª¨ë“œ ë°˜í™˜"""
        if required_memory_mb is None:
            required_memory_mb = self.memory_threshold_mb
        
        if not self.gpu_available:
            return ComputeMode.CPU_ONLY
        
        # í˜„ì¬ GPU ìƒíƒœ ì—…ë°ì´íŠ¸
        self._update_gpu_info()
        
        # ì„ íƒëœ GPU ë©”ëª¨ë¦¬ í™•ì¸
        selected_gpu = self.gpu_info_list[self.selected_gpu_id]
        
        if selected_gpu.memory_free_mb >= required_memory_mb:
            return ComputeMode.GPU_ONLY
        elif self.cpu_fallback_enabled:
            return ComputeMode.CPU_ONLY
        else:
            return ComputeMode.GPU_ONLY  # ê°•ì œ GPU ì‚¬ìš©
    
    def switch_mode(self, target_mode: ComputeMode, required_memory_mb: float = None) -> bool:
        """ì—°ì‚° ëª¨ë“œ ì „í™˜"""
        if target_mode == self.current_mode:
            return True
        
        self.logger.info(f"ğŸ”„ ëª¨ë“œ ì „í™˜: {self.current_mode.value} â†’ {target_mode.value}")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        self._cleanup_memory()
        
        # ëª¨ë“œë³„ ì„¤ì •
        if target_mode == ComputeMode.CPU_ONLY:
            os.environ['CUDA_VISIBLE_DEVICES'] = ''
            
        elif target_mode == ComputeMode.GPU_ONLY:
            if not self.gpu_available:
                self.logger.warning("âš ï¸ GPU ì—†ìŒ, CPU ëª¨ë“œ ìœ ì§€")
                return False
            
            # GPU ë©”ëª¨ë¦¬ í™•ì¸
            recommended_mode = self.get_recommended_mode(required_memory_mb)
            if recommended_mode != ComputeMode.GPU_ONLY:
                self.logger.warning("âš ï¸ GPU ë©”ëª¨ë¦¬ ë¶€ì¡±, CPU ëª¨ë“œë¡œ í´ë°±")
                target_mode = ComputeMode.CPU_ONLY
                os.environ['CUDA_VISIBLE_DEVICES'] = ''
            else:
                os.environ['CUDA_VISIBLE_DEVICES'] = str(self.selected_gpu_id)
        
        self.current_mode = target_mode
        
        # PyTorch ìºì‹œ ì •ë¦¬ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
        if TORCH_AVAILABLE:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
        
        self.logger.info(f"âœ… ëª¨ë“œ ì „í™˜ ì™„ë£Œ: {self.current_mode.value}")
        return True
    
    def _cleanup_memory(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        # Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        gc.collect()
        
        # PyTorch ìºì‹œ ì •ë¦¬
        if TORCH_AVAILABLE and torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        self.logger.info("ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
    
    def _update_gpu_info(self):
        """GPU ì •ë³´ ì—…ë°ì´íŠ¸"""
        if not NVIDIA_ML_AVAILABLE or not self.gpu_available:
            return
        
        try:
            for i, gpu_info in enumerate(self.gpu_info_list):
                handle = nvml.nvmlDeviceGetHandleByIndex(gpu_info.device_id)
                updated_info = self._get_gpu_info(handle, gpu_info.device_id)
                self.gpu_info_list[i] = updated_info
                
        except Exception as e:
            self.logger.warning(f"GPU ì •ë³´ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    def get_memory_info(self) -> Dict[str, MemoryInfo]:
        """ì‹œìŠ¤í…œ ë° GPU ë©”ëª¨ë¦¬ ì •ë³´ ë°˜í™˜"""
        info = {}
        
        # ì‹œìŠ¤í…œ RAM ì •ë³´
        ram = psutil.virtual_memory()
        info['system_ram'] = MemoryInfo(
            total_mb=ram.total / (1024**2),
            available_mb=ram.available / (1024**2),
            used_mb=ram.used / (1024**2),
            usage_percent=ram.percent,
            device_name="System RAM"
        )
        
        # GPU ë©”ëª¨ë¦¬ ì •ë³´
        if self.gpu_available:
            self._update_gpu_info()
            for gpu in self.gpu_info_list:
                info[f'gpu_{gpu.device_id}'] = MemoryInfo(
                    total_mb=gpu.memory_total_mb,
                    available_mb=gpu.memory_free_mb,
                    used_mb=gpu.memory_used_mb,
                    usage_percent=(gpu.memory_used_mb / gpu.memory_total_mb) * 100,
                    device_name=gpu.name
                )
        
        return info
    
    def start_monitoring(self) -> bool:
        """ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        if self.monitoring_active:
            return True
        
        self.monitoring_active = True
        self.monitor_thread = threading.Thread(target=self._monitor_loop, daemon=True)
        self.monitor_thread.start()
        
        self.logger.info(f"ğŸ“Š ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì‹œì‘ (ì£¼ê¸°: {self.monitor_interval}ì´ˆ)")
        return True
    
    def stop_monitoring(self):
        """ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        if not self.monitoring_active:
            return
        
        self.monitoring_active = False
        if self.monitor_thread and self.monitor_thread.is_alive():
            self.monitor_thread.join(timeout=5)
        
        self.logger.info("ğŸ“Š ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€")
    
    def _monitor_loop(self):
        """ëª¨ë‹ˆí„°ë§ ë£¨í”„"""
        while self.monitoring_active:
            try:
                memory_info = self.get_memory_info()
                
                # íˆìŠ¤í† ë¦¬ì— ì¶”ê°€ (ìµœê·¼ 100ê°œë§Œ ìœ ì§€)
                self.memory_history.append({
                    'timestamp': time.time(),
                    'memory_info': memory_info
                })
                
                if len(self.memory_history) > 100:
                    self.memory_history = self.memory_history[-100:]
                
                # ë©”ëª¨ë¦¬ ë¶€ì¡± ê²½ê³ 
                for device, info in memory_info.items():
                    if info.usage_percent > 90:
                        self.logger.warning(f"âš ï¸ {device} ë©”ëª¨ë¦¬ ë¶€ì¡±: {info.usage_percent:.1f}%")
                
                time.sleep(self.monitor_interval)
                
            except Exception as e:
                self.logger.warning(f"ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜: {e}")
                time.sleep(self.monitor_interval)
    
    def get_status_report(self) -> Dict[str, Any]:
        """ìƒíƒœ ë³´ê³ ì„œ ìƒì„±"""
        memory_info = self.get_memory_info()
        
        return {
            "current_mode": self.current_mode.value,
            "gpu_available": self.gpu_available,
            "gpu_count": len(self.gpu_info_list),
            "selected_gpu_id": self.selected_gpu_id if self.gpu_available else None,
            "memory_threshold_mb": self.memory_threshold_mb,
            "cpu_fallback_enabled": self.cpu_fallback_enabled,
            "monitoring_active": self.monitoring_active,
            "memory_info": {k: {
                "total_mb": round(v.total_mb, 1),
                "available_mb": round(v.available_mb, 1),
                "used_mb": round(v.used_mb, 1),
                "usage_percent": round(v.usage_percent, 1),
                "device_name": v.device_name
            } for k, v in memory_info.items()},
            "gpu_details": [
                {
                    "device_id": gpu.device_id,
                    "name": gpu.name,
                    "memory_total_mb": round(gpu.memory_total_mb, 1),
                    "memory_free_mb": round(gpu.memory_free_mb, 1),
                    "utilization_percent": round(gpu.utilization_percent, 1),
                    "temperature_c": gpu.temperature_c,
                    "power_draw_w": gpu.power_draw_w
                } for gpu in self.gpu_info_list
            ]
        }
    
    def __del__(self):
        """ì†Œë©¸ì"""
        if hasattr(self, 'monitoring_active'):
            self.stop_monitoring()

# ì „ì—­ GPU ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤
global_gpu_manager = GPUMemoryManager()

def get_recommended_compute_mode(required_memory_mb: float = 1000) -> ComputeMode:
    """ê¶Œì¥ ì—°ì‚° ëª¨ë“œ ë°˜í™˜"""
    return global_gpu_manager.get_recommended_mode(required_memory_mb)

def switch_compute_mode(target_mode: ComputeMode, required_memory_mb: float = None) -> bool:
    """ì—°ì‚° ëª¨ë“œ ì „í™˜"""
    return global_gpu_manager.switch_mode(target_mode, required_memory_mb)

def get_memory_status() -> Dict[str, Any]:
    """ë©”ëª¨ë¦¬ ìƒíƒœ ì •ë³´ ë°˜í™˜"""
    return global_gpu_manager.get_status_report()

def cleanup_memory():
    """ë©”ëª¨ë¦¬ ì •ë¦¬"""
    global_gpu_manager._cleanup_memory()

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    manager = GPUMemoryManager()
    
    print("=== GPU ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € í…ŒìŠ¤íŠ¸ ===")
    
    # ìƒíƒœ ë³´ê³ 
    status = manager.get_status_report()
    print(f"í˜„ì¬ ëª¨ë“œ: {status['current_mode']}")
    print(f"GPU ì‚¬ìš© ê°€ëŠ¥: {status['gpu_available']}")
    
    # ë©”ëª¨ë¦¬ ì •ë³´
    for device, info in status['memory_info'].items():
        print(f"{device}: {info['used_mb']:.1f}MB / {info['total_mb']:.1f}MB ({info['usage_percent']:.1f}%)")
    
    # ëª¨ë‹ˆí„°ë§ í…ŒìŠ¤íŠ¸
    if input("ëª¨ë‹ˆí„°ë§ì„ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower() == 'y':
        manager.start_monitoring()
        time.sleep(10)
        manager.stop_monitoring()