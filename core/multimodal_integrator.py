"""
ì†”ë¡œëª¬ë“œ AI ì‹œìŠ¤í…œ - ë©€í‹°ëª¨ë‹¬ í†µí•© ë¶„ì„ ì—”ì§„ v3.0
ìŒì„±, ë¹„ë””ì˜¤, ì´ë¯¸ì§€, ë¬¸ì„œ, ì›¹ ë°ì´í„°ë¥¼ í†µí•©í•˜ì—¬ ì¢…í•© ë¶„ì„ ë° ê²°ë¡  ë„ì¶œ
í’ˆì§ˆ ë¶„ì„ + í•œêµ­ì–´ ìµœì¢… ìš”ì•½ í†µí•©
"""

import asyncio
import io
from typing import Dict, List, Optional, Union, Tuple, Any
from datetime import datetime
import json
import logging
from collections import defaultdict, Counter
import re
from pathlib import Path

# ê° ëª¨ë“ˆ import
from .analyzer import STTAnalyzer, get_stt_analyzer
from .video_processor import get_video_processor, extract_audio_from_video
from .image_processor import get_image_processor, process_image_file, process_document_file
from .web_crawler import get_web_crawler, crawl_url, crawl_jewelry_news
from .jewelry_ai_engine import JewelryAIEngine
from .cross_validation_visualizer import CrossValidationVisualizer
from .speaker_analyzer import SpeakerAnalyzer

# ìƒˆë¡œìš´ í’ˆì§ˆ ë¶„ì„ ë° í•œêµ­ì–´ ìš”ì•½ ëª¨ë“ˆ
from .quality_analyzer import get_quality_analyzer, analyze_audio_quality, analyze_image_quality
from .korean_summarizer import get_korean_summarizer, analyze_situation_in_korean

class MultimodalIntegrator:
    """ë©€í‹°ëª¨ë‹¬ ë°ì´í„° í†µí•© ë¶„ì„ í´ë˜ìŠ¤ v3.0"""
    
    def __init__(self):
        # ê° í”„ë¡œì„¸ì„œ ì¸ìŠ¤í„´ìŠ¤
        self.stt_analyzer = None
        self.video_processor = get_video_processor()
        self.image_processor = get_image_processor()
        self.web_crawler = get_web_crawler()
        self.ai_engine = JewelryAIEngine()
        self.cross_validator = CrossValidationVisualizer()
        self.speaker_analyzer = SpeakerAnalyzer()
        
        # ìƒˆë¡œìš´ í’ˆì§ˆ ë¶„ì„ ë° í•œêµ­ì–´ ìš”ì•½ ëª¨ë“ˆ
        self.quality_analyzer = get_quality_analyzer()
        self.korean_summarizer = get_korean_summarizer()
        
        # ë¶„ì„ ê°€ì¤‘ì¹˜ (ì†ŒìŠ¤ë³„ ì‹ ë¢°ë„)
        self.source_weights = {
            "audio": 1.0,      # ìŒì„± (ê¸°ë³¸)
            "video": 0.9,      # ë¹„ë””ì˜¤ (ìŒì„±ì—ì„œ ì¶”ì¶œ)
            "image": 0.8,      # ì´ë¯¸ì§€ OCR
            "document": 0.9,   # ë¬¸ì„œ (ë†’ì€ ì‹ ë¢°ë„)
            "web": 0.6         # ì›¹ í¬ë¡¤ë§ (ë‚®ì€ ì‹ ë¢°ë„)
        }
        
        # ì£¼ì–¼ë¦¬ ìš©ì–´ ì¹´í…Œê³ ë¦¬ ë§¤í•‘
        self.jewelry_categories = {
            "gems": ["ë‹¤ì´ì•„ëª¬ë“œ", "ë£¨ë¹„", "ì‚¬íŒŒì´ì–´", "ì—ë©”ë„ë“œ", "ì§„ì£¼"],
            "metals": ["ê¸ˆ", "ì€", "ë°±ê¸ˆ", "í”Œë˜í‹°ë„˜"],
            "grading": ["4C", "ìºëŸ¿", "ì»·", "ì»¬ëŸ¬", "í´ë˜ë¦¬í‹°"],
            "certification": ["GIA", "AGS", "ê°ì •ì„œ", "ì¸ì¦ì„œ"],
            "business": ["ê°€ê²©", "í• ì¸", "ë„ë§¤", "ì†Œë§¤", "ë¬´ì—­"],
            "techniques": ["ì„¸íŒ…", "ê°€ê³µ", "ì—°ë§ˆ", "ì¡°ê°"]
        }
        
        logging.info("ë©€í‹°ëª¨ë‹¬ í†µí•© ë¶„ì„ ì—”ì§„ v3.0 ì´ˆê¸°í™” ì™„ë£Œ (í’ˆì§ˆë¶„ì„+í•œêµ­ì–´ìš”ì•½ í†µí•©)")
    
    def _get_stt_analyzer(self):
        """STT ë¶„ì„ê¸° ì§€ì—° ì´ˆê¸°í™”"""
        if self.stt_analyzer is None:
            self.stt_analyzer = get_stt_analyzer()
        return self.stt_analyzer
    
    async def process_multimodal_session(self, 
                                       session_data: Dict,
                                       analysis_depth: str = "comprehensive",
                                       situation_type: str = "auto") -> Dict:
        """
        ë©€í‹°ëª¨ë‹¬ ì„¸ì…˜ í†µí•© ì²˜ë¦¬ v3.0
        
        Args:
            session_data: ì„¸ì…˜ ë°ì´í„° (ê° ì†ŒìŠ¤ë³„ íŒŒì¼/URL ì •ë³´)
            analysis_depth: ë¶„ì„ ê¹Šì´ ("quick", "standard", "comprehensive")
            situation_type: ìƒí™© íƒ€ì… ("auto", "seminar", "meeting", "lecture", "conference")
            
        Returns:
            í†µí•© ë¶„ì„ ê²°ê³¼ (í’ˆì§ˆë¶„ì„ + í•œêµ­ì–´ ìš”ì•½ í¬í•¨)
        """
        try:
            print("ğŸ”„ ë©€í‹°ëª¨ë‹¬ ì„¸ì…˜ ë¶„ì„ v3.0 ì‹œì‘")
            
            # ì„¸ì…˜ ì •ë³´ ì´ˆê¸°í™”
            session_id = session_data.get("session_id", f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
            session_title = session_data.get("title", "Untitled Session")
            
            # ê° ì†ŒìŠ¤ë³„ ì²˜ë¦¬ ê²°ê³¼ ì €ì¥
            source_results = {}
            
            # 1. ìŒì„± íŒŒì¼ ì²˜ë¦¬ (í’ˆì§ˆ ë¶„ì„ í¬í•¨)
            if "audio_files" in session_data:
                source_results["audio"] = await self._process_audio_sources_with_quality(session_data["audio_files"])
            
            # 2. ë¹„ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬ (í’ˆì§ˆ ë¶„ì„ í¬í•¨)
            if "video_files" in session_data:
                source_results["video"] = await self._process_video_sources_with_quality(session_data["video_files"])
            
            # 3. ì´ë¯¸ì§€/ë¬¸ì„œ íŒŒì¼ ì²˜ë¦¬ (í’ˆì§ˆ ë¶„ì„ í¬í•¨)
            if "document_files" in session_data:
                source_results["documents"] = await self._process_document_sources_with_quality(session_data["document_files"])
            
            # 4. ì›¹ ì†ŒìŠ¤ ì²˜ë¦¬
            if "web_urls" in session_data:
                source_results["web"] = await self._process_web_sources(session_data["web_urls"])
            
            # 5. í†µí•© ë¶„ì„ ìˆ˜í–‰
            integrated_analysis = await self._perform_integrated_analysis(
                source_results, 
                analysis_depth
            )
            
            # 6. í¬ë¡œìŠ¤ ê²€ì¦ ìˆ˜í–‰
            cross_validation = await self._perform_cross_validation(source_results)
            
            # 7. í•œêµ­ì–´ ì¢…í•© ë¶„ì„ (ìƒˆë¡œìš´ ê¸°ëŠ¥)
            korean_analysis = await self.korean_summarizer.analyze_situation_comprehensively(
                {"source_results": source_results},
                situation_type=situation_type,
                focus_areas=None
            )
            
            # 8. ìµœì¢… ì¸ì‚¬ì´íŠ¸ ìƒì„± (í’ˆì§ˆ ì •ë³´ í¬í•¨)
            final_insights = await self._generate_final_insights_with_quality(
                integrated_analysis,
                cross_validation,
                korean_analysis,
                session_data
            )
            
            # 9. ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„± v3.0
            comprehensive_report = self._generate_comprehensive_report_v3(
                session_id,
                session_title,
                source_results,
                integrated_analysis,
                cross_validation,
                korean_analysis,
                final_insights
            )
            
            print(f"âœ… ë©€í‹°ëª¨ë‹¬ ì„¸ì…˜ ë¶„ì„ v3.0 ì™„ë£Œ: {len(source_results)}ê°œ ì†ŒìŠ¤ í†µí•© + í’ˆì§ˆë¶„ì„ + í•œêµ­ì–´ìš”ì•½")
            return comprehensive_report
            
        except Exception as e:
            logging.error(f"ë©€í‹°ëª¨ë‹¬ ì„¸ì…˜ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {
                "success": False,
                "error": str(e),
                "session_id": session_data.get("session_id", "unknown")
            }
    
    async def _process_audio_sources_with_quality(self, audio_files: List[Dict]) -> Dict:
        """ìŒì„± íŒŒì¼ë“¤ ì²˜ë¦¬ (í’ˆì§ˆ ë¶„ì„ í¬í•¨)"""
        print("ğŸ¤ ìŒì„± ì†ŒìŠ¤ ì²˜ë¦¬ ì¤‘... (í’ˆì§ˆ ë¶„ì„ í¬í•¨)")
        
        results = []
        quality_results = []
        total_duration = 0
        combined_text = ""
        
        stt_analyzer = self._get_stt_analyzer()
        
        for audio_file in audio_files:
            try:
                # 1. í’ˆì§ˆ ë¶„ì„ ë¨¼ì € ìˆ˜í–‰
                quality_result = await self.quality_analyzer.analyze_audio_quality(
                    audio_file["content"],
                    audio_file.get("filename", "unknown.wav")
                )
                quality_results.append(quality_result)
                
                # 2. STT ë¶„ì„ ìˆ˜í–‰
                if "content" in audio_file:
                    result = await stt_analyzer.analyze_audio(
                        audio_file["content"],
                        audio_file.get("filename", "unknown.wav"),
                        language=audio_file.get("language", "ko"),
                        enable_jewelry_enhancement=True
                    )
                    
                    if result.get("success"):
                        # í’ˆì§ˆ ì •ë³´ í†µí•©
                        result["quality_analysis"] = quality_result
                        results.append(result)
                        total_duration += result.get("duration", 0)
                        combined_text += f" {result.get('enhanced_text', '')}"
                        
                        # í™”ì ë¶„ì„ ì¶”ê°€
                        if hasattr(self.speaker_analyzer, 'analyze_speakers'):
                            speaker_result = await self.speaker_analyzer.analyze_speakers(
                                audio_file["content"],
                                audio_file.get("filename", "unknown.wav")
                            )
                            result["speaker_analysis"] = speaker_result
                
            except Exception as e:
                logging.error(f"ìŒì„± íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue
        
        # ì „ì²´ í’ˆì§ˆ í‰ê°€
        overall_quality = self._calculate_overall_audio_quality(quality_results)
        
        return {
            "source_type": "audio",
            "files_processed": len(results),
            "files_successful": len([r for r in results if r.get("success")]),
            "total_duration": total_duration,
            "combined_text": combined_text.strip(),
            "individual_results": results,
            "quality_analysis": {
                "overall_quality": overall_quality,
                "individual_quality": quality_results,
                "quality_issues": [q.get("improvement_suggestions", []) for q in quality_results if not q.get("success", True) or q.get("quality_metrics", {}).get("overall_quality", 1.0) < 0.6]
            },
            "summary": {
                "total_words": len(combined_text.split()),
                "average_confidence": sum(r.get("confidence", 0) for r in results) / max(len(results), 1),
                "quality_score": overall_quality
            }
        }
    
    async def _process_video_sources_with_quality(self, video_files: List[Dict]) -> Dict:
        """ë¹„ë””ì˜¤ íŒŒì¼ë“¤ ì²˜ë¦¬ (í’ˆì§ˆ ë¶„ì„ í¬í•¨)"""
        print("ğŸ¥ ë¹„ë””ì˜¤ ì†ŒìŠ¤ ì²˜ë¦¬ ì¤‘... (í’ˆì§ˆ ë¶„ì„ í¬í•¨)")
        
        results = []
        total_duration = 0
        combined_text = ""
        
        for video_file in video_files:
            try:
                # ë¹„ë””ì˜¤ì—ì„œ ìŒì„± ì¶”ì¶œ
                extraction_result = await extract_audio_from_video(
                    video_file["content"],
                    video_file.get("filename", "unknown.mp4")
                )
                
                if extraction_result.get("success"):
                    # ì¶”ì¶œëœ ìŒì„±ì— ëŒ€í•œ í’ˆì§ˆ ë¶„ì„
                    audio_quality = await self.quality_analyzer.analyze_audio_quality(
                        extraction_result["audio_content"],
                        extraction_result["extracted_filename"]
                    )
                    
                    # ì¶”ì¶œëœ ìŒì„±ìœ¼ë¡œ STT ìˆ˜í–‰
                    stt_analyzer = self._get_stt_analyzer()
                    stt_result = await stt_analyzer.analyze_audio(
                        extraction_result["audio_content"],
                        extraction_result["extracted_filename"],
                        language=video_file.get("language", "ko"),
                        enable_jewelry_enhancement=True
                    )
                    
                    if stt_result.get("success"):
                        # ë¹„ë””ì˜¤ ì •ë³´ì™€ STT ê²°ê³¼ ê²°í•©
                        combined_result = {
                            **stt_result,
                            "video_info": extraction_result,
                            "source_file": video_file.get("filename"),
                            "audio_quality_analysis": audio_quality
                        }
                        
                        results.append(combined_result)
                        total_duration += stt_result.get("duration", 0)
                        combined_text += f" {stt_result.get('enhanced_text', '')}"
                
            except Exception as e:
                logging.error(f"ë¹„ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue
        
        return {
            "source_type": "video",
            "files_processed": len(results),
            "files_successful": len([r for r in results if r.get("success")]),
            "total_duration": total_duration,
            "combined_text": combined_text.strip(),
            "individual_results": results,
            "summary": {
                "total_words": len(combined_text.split()),
                "extraction_method": "ffmpeg",
                "quality_info": "ìŒì„± ì¶”ì¶œ í›„ í’ˆì§ˆ ë¶„ì„ ì™„ë£Œ"
            }
        }
    
    async def _process_document_sources_with_quality(self, document_files: List[Dict]) -> Dict:
        """ë¬¸ì„œ/ì´ë¯¸ì§€ íŒŒì¼ë“¤ ì²˜ë¦¬ (í’ˆì§ˆ ë¶„ì„ í¬í•¨)"""
        print("ğŸ“„ ë¬¸ì„œ ì†ŒìŠ¤ ì²˜ë¦¬ ì¤‘... (í’ˆì§ˆ ë¶„ì„ í¬í•¨)")
        
        results = []
        quality_results = []
        combined_text = ""
        total_pages = 0
        
        for doc_file in document_files:
            try:
                # íŒŒì¼ íƒ€ì…ì— ë”°ë¼ ì²˜ë¦¬
                file_type = self.image_processor.get_file_type(doc_file.get("filename", ""))
                
                # 1. í’ˆì§ˆ ë¶„ì„ ë¨¼ì € ìˆ˜í–‰ (ì´ë¯¸ì§€ì¸ ê²½ìš°)
                if file_type == "image":
                    # PPT í™”ë©´ ì—¬ë¶€ ìë™ ê°ì§€
                    filename = doc_file.get("filename", "")
                    is_ppt_hint = any(keyword in filename.lower() for keyword in ["ppt", "slide", "presentation", "screen"])
                    
                    image_quality = await self.quality_analyzer.analyze_image_quality(
                        doc_file["content"],
                        filename,
                        is_ppt_screen=is_ppt_hint
                    )
                    quality_results.append(image_quality)
                    
                    # 2. OCR ì²˜ë¦¬
                    result = await process_image_file(
                        doc_file["content"],
                        filename,
                        enhance_quality=True,
                        ocr_method="auto"
                    )
                    
                    if result.get("success"):
                        result["quality_analysis"] = image_quality
                        
                elif file_type == "document":
                    result = await process_document_file(
                        doc_file["content"],
                        doc_file.get("filename", "unknown.pdf")
                    )
                    
                    # ë¬¸ì„œì˜ ê²½ìš° ê°„ë‹¨í•œ í’ˆì§ˆ í‰ê°€
                    if result.get("success"):
                        doc_quality = {
                            "success": True,
                            "filename": doc_file.get("filename", "unknown.pdf"),
                            "quality_assessment": "ë¬¸ì„œ ì²˜ë¦¬ ì„±ê³µ",
                            "text_length": len(result.get("text", "")),
                            "overall_quality": 0.9 if len(result.get("text", "")) > 100 else 0.6
                        }
                        result["quality_analysis"] = doc_quality
                        quality_results.append(doc_quality)
                else:
                    continue
                
                if result.get("success"):
                    results.append(result)
                    
                    # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    if "text" in result:
                        combined_text += f" {result['text']}"
                    elif "ocr_results" in result and "text" in result["ocr_results"]:
                        combined_text += f" {result['ocr_results']['text']}"
                    
                    # í˜ì´ì§€ ìˆ˜ ê³„ì‚°
                    if "page_count" in result:
                        total_pages += result["page_count"]
                    else:
                        total_pages += 1
                
            except Exception as e:
                logging.error(f"ë¬¸ì„œ íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue
        
        # ì „ì²´ í’ˆì§ˆ í‰ê°€
        overall_quality = self._calculate_overall_document_quality(quality_results)
        
        return {
            "source_type": "documents",
            "files_processed": len(results),
            "files_successful": len([r for r in results if r.get("success")]),
            "total_pages": total_pages,
            "combined_text": combined_text.strip(),
            "individual_results": results,
            "quality_analysis": {
                "overall_quality": overall_quality,
                "individual_quality": quality_results,
                "ppt_screens_detected": len([q for q in quality_results if q.get("ppt_analysis", {}).get("is_ppt_screen", False)]),
                "ocr_quality_summary": self._summarize_ocr_quality(quality_results)
            },
            "summary": {
                "total_words": len(combined_text.split()),
                "document_types": list(set(r.get("file_type", "unknown") for r in results)),
                "quality_score": overall_quality
            }
        }
    
    async def _process_web_sources(self, web_urls: List[str]) -> Dict:
        """ì›¹ ì†ŒìŠ¤ë“¤ ì²˜ë¦¬"""
        print("ğŸŒ ì›¹ ì†ŒìŠ¤ ì²˜ë¦¬ ì¤‘...")
        
        results = []
        combined_text = ""
        
        async with self.web_crawler:
            for url in web_urls:
                try:
                    result = await self.web_crawler.process_url(
                        url,
                        content_type="auto",
                        extract_video=False
                    )
                    
                    if result.get("success"):
                        results.append(result)
                        
                        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                        if "content" in result:
                            combined_text += f" {result['content']}"
                        elif "text" in result:
                            combined_text += f" {result['text']}"
                
                except Exception as e:
                    logging.error(f"ì›¹ URL ì²˜ë¦¬ ì˜¤ë¥˜ ({url}): {e}")
                    continue
        
        return {
            "source_type": "web",
            "urls_processed": len(results),
            "urls_successful": len([r for r in results if r.get("success")]),
            "combined_text": combined_text.strip(),
            "individual_results": results,
            "summary": {
                "total_words": len(combined_text.split()),
                "content_types": list(set(r.get("content_type", "unknown") for r in results))
            }
        }
    
    def _calculate_overall_audio_quality(self, quality_results: List[Dict]) -> float:
        """ì „ì²´ ìŒì„± í’ˆì§ˆ ê³„ì‚°"""
        if not quality_results:
            return 0.5
        
        successful_results = [q for q in quality_results if q.get("success")]
        if not successful_results:
            return 0.3
        
        scores = []
        for result in successful_results:
            quality_metrics = result.get("quality_metrics", {})
            overall_quality = quality_metrics.get("overall_quality", 0.5)
            scores.append(overall_quality)
        
        return sum(scores) / len(scores) if scores else 0.5
    
    def _calculate_overall_document_quality(self, quality_results: List[Dict]) -> float:
        """ì „ì²´ ë¬¸ì„œ í’ˆì§ˆ ê³„ì‚°"""
        if not quality_results:
            return 0.5
        
        successful_results = [q for q in quality_results if q.get("success")]
        if not successful_results:
            return 0.3
        
        scores = []
        for result in successful_results:
            overall_quality = result.get("overall_quality", 0.5)
            scores.append(overall_quality)
        
        return sum(scores) / len(scores) if scores else 0.5
    
    def _summarize_ocr_quality(self, quality_results: List[Dict]) -> Dict:
        """OCR í’ˆì§ˆ ìš”ì•½"""
        ocr_summary = {
            "total_files": len(quality_results),
            "successful_ocr": 0,
            "average_confidence": 0.0,
            "ppt_optimized": 0,
            "issues_found": []
        }
        
        confidences = []
        
        for result in quality_results:
            if result.get("success"):
                ocr_quality = result.get("ocr_quality", {})
                if "average_confidence" in ocr_quality:
                    confidences.append(ocr_quality["average_confidence"])
                    ocr_summary["successful_ocr"] += 1
                
                # PPT ìµœì í™” í™•ì¸
                ppt_analysis = result.get("ppt_analysis", {})
                if ppt_analysis.get("is_ppt_screen"):
                    ocr_summary["ppt_optimized"] += 1
                
                # í’ˆì§ˆ ì´ìŠˆ ìˆ˜ì§‘
                improvement_suggestions = result.get("improvement_suggestions", [])
                if improvement_suggestions:
                    ocr_summary["issues_found"].extend(improvement_suggestions[:2])
        
        if confidences:
            ocr_summary["average_confidence"] = sum(confidences) / len(confidences)
        
        return ocr_summary
    
    async def _perform_integrated_analysis(self, 
                                         source_results: Dict,
                                         analysis_depth: str) -> Dict:
        """í†µí•© ë¶„ì„ ìˆ˜í–‰"""
        print("ğŸ§  í†µí•© AI ë¶„ì„ ìˆ˜í–‰ ì¤‘...")
        
        # ëª¨ë“  í…ìŠ¤íŠ¸ ê²°í•©
        all_texts = []
        source_weights_applied = {}
        
        for source_type, result in source_results.items():
            if result and "combined_text" in result:
                text = result["combined_text"]
                weight = self.source_weights.get(source_type, 0.5)
                
                all_texts.append({
                    "text": text,
                    "source": source_type,
                    "weight": weight,
                    "word_count": len(text.split())
                })
                source_weights_applied[source_type] = weight
        
        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ í†µí•© í…ìŠ¤íŠ¸ ìƒì„±
        combined_text = " ".join([item["text"] for item in all_texts])
        
        # ì£¼ì–¼ë¦¬ AI ì—”ì§„ìœ¼ë¡œ ê³ ê¸‰ ë¶„ì„
        ai_analysis = await self.ai_engine.analyze_jewelry_content(
            combined_text,
            analysis_type="comprehensive" if analysis_depth == "comprehensive" else "standard"
        )
        
        # ì†ŒìŠ¤ë³„ ì£¼ì–¼ë¦¬ ìš©ì–´ ë¶„ì„
        source_term_analysis = {}
        for source_type, result in source_results.items():
            if result and "combined_text" in result:
                terms = self._extract_jewelry_terms(result["combined_text"])
                source_term_analysis[source_type] = terms
        
        # ì£¼ì œ ì¼ê´€ì„± ë¶„ì„
        topic_consistency = self._analyze_topic_consistency(source_results)
        
        # ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚° (í’ˆì§ˆ ì •ë³´ ë°˜ì˜)
        confidence_score = self._calculate_confidence_score_with_quality(source_results, source_weights_applied)
        
        return {
            "ai_analysis": ai_analysis,
            "source_term_analysis": source_term_analysis,
            "topic_consistency": topic_consistency,
            "confidence_score": confidence_score,
            "total_word_count": len(combined_text.split()),
            "source_distribution": {
                source: len(result.get("combined_text", "").split())
                for source, result in source_results.items()
                if result
            },
            "analysis_depth": analysis_depth
        }
    
    def _calculate_confidence_score_with_quality(self, source_results: Dict, weights: Dict) -> float:
        """í’ˆì§ˆ ì •ë³´ë¥¼ ë°˜ì˜í•œ ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°"""
        total_weight = 0
        weighted_confidence = 0
        
        for source_type, result in source_results.items():
            if result:
                weight = weights.get(source_type, 0.5)
                
                # ê¸°ë³¸ ì‹ ë¢°ë„
                source_confidence = result.get("summary", {}).get("average_confidence", 0.5)
                
                # í’ˆì§ˆ ì ìˆ˜ ë°˜ì˜
                quality_score = result.get("summary", {}).get("quality_score", 0.7)
                
                # í’ˆì§ˆê³¼ ì‹ ë¢°ë„ ì¡°í•© (7:3 ë¹„ìœ¨)
                combined_confidence = source_confidence * 0.7 + quality_score * 0.3
                
                weighted_confidence += combined_confidence * weight
                total_weight += weight
        
        return round(weighted_confidence / max(total_weight, 1), 3)
    
    async def _perform_cross_validation(self, source_results: Dict) -> Dict:
        """í¬ë¡œìŠ¤ ê²€ì¦ ìˆ˜í–‰"""
        print("ğŸ” í¬ë¡œìŠ¤ ê²€ì¦ ìˆ˜í–‰ ì¤‘...")
        
        # ì†ŒìŠ¤ ê°„ ì¼ì¹˜ë„ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±
        if hasattr(self.cross_validator, 'calculate_consensus_matrix'):
            consensus_matrix = await self.cross_validator.calculate_consensus_matrix(source_results)
        else:
            consensus_matrix = self._simple_consensus_matrix(source_results)
        
        # ì£¼ìš” í‚¤ì›Œë“œ ì¼ì¹˜ë„ ë¶„ì„
        keyword_consistency = self._analyze_keyword_consistency(source_results)
        
        # íƒ€ì„ë¼ì¸ ì¼ì¹˜ì„± (ì‹œê°„ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°)
        timeline_consistency = self._analyze_timeline_consistency(source_results)
        
        return {
            "consensus_matrix": consensus_matrix,
            "keyword_consistency": keyword_consistency,
            "timeline_consistency": timeline_consistency,
            "overall_consistency": self._calculate_overall_consistency(consensus_matrix)
        }
    
    def _simple_consensus_matrix(self, source_results: Dict) -> Dict:
        """ê°„ë‹¨í•œ í•©ì˜ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°"""
        sources = list(source_results.keys())
        matrix = {}
        
        for i, source1 in enumerate(sources):
            matrix[source1] = {}
            for j, source2 in enumerate(sources):
                if i == j:
                    similarity = 1.0
                else:
                    # í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê°„ë‹¨ ê³„ì‚°
                    text1 = source_results[source1].get("combined_text", "")
                    text2 = source_results[source2].get("combined_text", "")
                    similarity = self._calculate_text_similarity(text1, text2)
                
                matrix[source1][source2] = round(similarity, 3)
        
        return matrix
    
    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚° (ê°„ë‹¨í•œ êµ¬í˜„)"""
        if not text1 or not text2:
            return 0.0
        
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = words1 & words2
        union = words1 | words2
        
        return len(intersection) / len(union) if union else 0.0
    
    def _extract_jewelry_terms(self, text: str) -> Dict:
        """í…ìŠ¤íŠ¸ì—ì„œ ì£¼ì–¼ë¦¬ ìš©ì–´ ì¶”ì¶œ"""
        terms_by_category = defaultdict(list)
        text_lower = text.lower()
        
        for category, terms in self.jewelry_categories.items():
            for term in terms:
                if term.lower() in text_lower:
                    terms_by_category[category].append(term)
        
        return dict(terms_by_category)
    
    def _analyze_topic_consistency(self, source_results: Dict) -> Dict:
        """ì£¼ì œ ì¼ê´€ì„± ë¶„ì„"""
        all_terms = defaultdict(int)
        source_terms = {}
        
        # ê° ì†ŒìŠ¤ë³„ ìš©ì–´ ë¹ˆë„ ê³„ì‚°
        for source_type, result in source_results.items():
            if result and "combined_text" in result:
                terms = self._extract_jewelry_terms(result["combined_text"])
                source_terms[source_type] = terms
                
                for category, category_terms in terms.items():
                    for term in category_terms:
                        all_terms[term] += 1
        
        # ê³µí†µ ì£¼ì œ ì‹ë³„
        common_terms = {term: count for term, count in all_terms.items() if count >= 2}
        
        return {
            "common_terms": common_terms,
            "source_terms": source_terms,
            "consistency_score": len(common_terms) / max(len(all_terms), 1)
        }
    
    def _analyze_keyword_consistency(self, source_results: Dict) -> Dict:
        """í‚¤ì›Œë“œ ì¼ì¹˜ë„ ë¶„ì„"""
        keyword_counts = defaultdict(lambda: defaultdict(int))
        
        for source_type, result in source_results.items():
            if result and "combined_text" in result:
                text = result["combined_text"].lower()
                words = text.split()
                
                # ì£¼ì–¼ë¦¬ í‚¤ì›Œë“œ ë¹ˆë„ ê³„ì‚°
                for category, terms in self.jewelry_categories.items():
                    for term in terms:
                        if term.lower() in text:
                            keyword_counts[category][source_type] += text.count(term.lower())
        
        return dict(keyword_counts)
    
    def _analyze_timeline_consistency(self, source_results: Dict) -> Dict:
        """íƒ€ì„ë¼ì¸ ì¼ì¹˜ì„± ë¶„ì„"""
        # ê°„ë‹¨í•œ êµ¬í˜„ - ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ì‹œê°„ ì •ë³´ ì¶”ì¶œ í•„ìš”
        timeline_info = {}
        
        for source_type, result in source_results.items():
            if result:
                timeline_info[source_type] = {
                    "processing_time": result.get("processing_time", ""),
                    "duration": result.get("total_duration", 0),
                    "timestamp": datetime.now().isoformat()
                }
        
        return timeline_info
    
    def _calculate_overall_consistency(self, consensus_matrix: Dict) -> float:
        """ì „ì²´ ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚°"""
        if not consensus_matrix:
            return 0.0
        
        total_similarity = 0
        count = 0
        
        for source1, similarities in consensus_matrix.items():
            for source2, similarity in similarities.items():
                if source1 != source2:  # ìê¸° ìì‹ ê³¼ì˜ ë¹„êµ ì œì™¸
                    total_similarity += similarity
                    count += 1
        
        return round(total_similarity / max(count, 1), 3)
    
    async def _generate_final_insights_with_quality(self, 
                                                  integrated_analysis: Dict,
                                                  cross_validation: Dict,
                                                  korean_analysis: Dict,
                                                  session_data: Dict) -> Dict:
        """ìµœì¢… ì¸ì‚¬ì´íŠ¸ ìƒì„± (í’ˆì§ˆ ì •ë³´ í¬í•¨)"""
        print("ğŸ’¡ ìµœì¢… ì¸ì‚¬ì´íŠ¸ ìƒì„± ì¤‘... (í’ˆì§ˆ ì •ë³´ í¬í•¨)")
        
        # ê¸°ì¡´ ì¸ì‚¬ì´íŠ¸
        basic_insights = await self._generate_basic_insights(integrated_analysis, cross_validation, session_data)
        
        # í’ˆì§ˆ ê´€ë ¨ ì¸ì‚¬ì´íŠ¸
        quality_insights = self._generate_quality_insights(korean_analysis)
        
        # í•œêµ­ì–´ ë¶„ì„ ì¸ì‚¬ì´íŠ¸
        korean_insights = self._extract_korean_insights(korean_analysis)
        
        return {
            **basic_insights,
            "quality_insights": quality_insights,
            "korean_analysis_insights": korean_insights,
            "generated_at": datetime.now().isoformat()
        }
    
    async def _generate_basic_insights(self, 
                                     integrated_analysis: Dict,
                                     cross_validation: Dict,
                                     session_data: Dict) -> Dict:
        """ê¸°ë³¸ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        # í•µì‹¬ ë°œê²¬ì‚¬í•­
        key_findings = []
        
        # 1. ì£¼ìš” ì£¼ì œ
        ai_analysis = integrated_analysis.get("ai_analysis", {})
        if "main_topics" in ai_analysis:
            key_findings.append(f"ì£¼ìš” ì£¼ì œ: {', '.join(ai_analysis['main_topics'])}")
        
        # 2. ì‹ ë¢°ë„ í‰ê°€
        confidence = integrated_analysis.get("confidence_score", 0)
        if confidence >= 0.8:
            key_findings.append("ë†’ì€ ì‹ ë¢°ë„ì˜ ë¶„ì„ ê²°ê³¼")
        elif confidence >= 0.6:
            key_findings.append("ë³´í†µ ìˆ˜ì¤€ì˜ ì‹ ë¢°ë„")
        else:
            key_findings.append("ì¶”ê°€ ê²€ì¦ì´ í•„ìš”í•œ ë¶„ì„ ê²°ê³¼")
        
        # 3. ì¼ê´€ì„± í‰ê°€
        consistency = cross_validation.get("overall_consistency", 0)
        if consistency >= 0.7:
            key_findings.append("ì†ŒìŠ¤ ê°„ ë†’ì€ ì¼ê´€ì„± í™•ì¸")
        elif consistency >= 0.5:
            key_findings.append("ì†ŒìŠ¤ ê°„ ì ë‹¹í•œ ì¼ê´€ì„±")
        else:
            key_findings.append("ì†ŒìŠ¤ ê°„ ì¼ê´€ì„± ë¶€ì¡± - ì¶”ê°€ ê²€í†  í•„ìš”")
        
        # 4. ë°ì´í„° í’ˆì§ˆ í‰ê°€
        total_words = integrated_analysis.get("total_word_count", 0)
        if total_words >= 1000:
            key_findings.append("ì¶©ë¶„í•œ ì–‘ì˜ ë°ì´í„°ë¡œ ì‹ ë¢°í•  ë§Œí•œ ë¶„ì„")
        elif total_words >= 500:
            key_findings.append("ì ë‹¹í•œ ì–‘ì˜ ë°ì´í„°")
        else:
            key_findings.append("ì œí•œì ì¸ ë°ì´í„° ì–‘ - ì¶”ê°€ ìë£Œ ìˆ˜ì§‘ ê¶Œì¥")
        
        # ì‹¤í–‰ ê°€ëŠ¥í•œ ê¶Œì¥ì‚¬í•­
        recommendations = []
        
        # ì£¼ì–¼ë¦¬ ë¹„ì¦ˆë‹ˆìŠ¤ ê´€ì  ê¶Œì¥ì‚¬í•­
        jewelry_analysis = ai_analysis.get("jewelry_insights", {})
        if jewelry_analysis.get("business_opportunities"):
            recommendations.extend(jewelry_analysis["business_opportunities"][:3])
        
        # ê¸°ìˆ ì  ê¶Œì¥ì‚¬í•­
        if confidence < 0.7:
            recommendations.append("ë” ë§ì€ ë°ì´í„° ì†ŒìŠ¤ë¥¼ ì¶”ê°€í•˜ì—¬ ë¶„ì„ ì •í™•ë„ í–¥ìƒ")
        
        if consistency < 0.6:
            recommendations.append("ì†ŒìŠ¤ ê°„ ë¶ˆì¼ì¹˜ ë¶€ë¶„ì— ëŒ€í•œ ì„¸ë¶€ ê²€ì¦ ìˆ˜í–‰")
        
        # ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ
        next_steps = [
            "í•µì‹¬ ë°œê²¬ì‚¬í•­ì— ëŒ€í•œ ìƒì„¸ ë¶„ì„ ìˆ˜í–‰",
            "ì´í•´ê´€ê³„ìë“¤ê³¼ ê²°ê³¼ ê³µìœ  ë° í”¼ë“œë°± ìˆ˜ì§‘",
            "ì‹¤í–‰ ê³„íš ìˆ˜ë¦½ ë° ìš°ì„ ìˆœìœ„ ì„¤ì •"
        ]
        
        return {
            "key_findings": key_findings,
            "recommendations": recommendations,
            "next_steps": next_steps,
            "quality_assessment": {
                "confidence_level": "ë†’ìŒ" if confidence >= 0.8 else "ë³´í†µ" if confidence >= 0.6 else "ë‚®ìŒ",
                "consistency_level": "ë†’ìŒ" if consistency >= 0.7 else "ë³´í†µ" if consistency >= 0.5 else "ë‚®ìŒ",
                "data_sufficiency": "ì¶©ë¶„" if total_words >= 1000 else "ë³´í†µ" if total_words >= 500 else "ë¶€ì¡±"
            }
        }
    
    def _generate_quality_insights(self, korean_analysis: Dict) -> Dict:
        """í’ˆì§ˆ ê´€ë ¨ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        quality_assessment = korean_analysis.get("quality_assessment", {})
        
        insights = {
            "overall_quality_score": quality_assessment.get("overall_score", 0.5),
            "quality_issues": quality_assessment.get("issues_found", []),
            "improvement_recommendations": quality_assessment.get("recommendations", []),
            "source_quality_breakdown": quality_assessment.get("source_qualities", {}),
            "recording_quality_tips": []
        }
        
        # í’ˆì§ˆ ê°œì„  íŒ ìƒì„±
        overall_score = quality_assessment.get("overall_score", 0.5)
        if overall_score < 0.7:
            insights["recording_quality_tips"] = [
                "ğŸ“± í˜„ì¥ ë…¹í™” ì‹œ ë§ˆì´í¬ë¥¼ í™”ìì—ê²Œ ë” ê°€ê¹Œì´ ë°°ì¹˜",
                "ğŸ’¡ PPT í™”ë©´ ì´¬ì˜ ì‹œ ì¡°ëª…ê³¼ ê°ë„ ìµœì í™”",
                "ğŸ”‡ ì£¼ë³€ ì†ŒìŒì´ ì ì€ í™˜ê²½ì—ì„œ ë…¹í™”",
                "ğŸ“· í”ë“¤ë¦¼ ë°©ì§€ë¥¼ ìœ„í•œ ì‚¼ê°ëŒ€ ì‚¬ìš© ê¶Œì¥"
            ]
        else:
            insights["recording_quality_tips"] = [
                "âœ… í˜„ì¬ ë…¹í™” í’ˆì§ˆì´ ìš°ìˆ˜í•©ë‹ˆë‹¤!",
                "ğŸ“Š ì§€ì†ì ì¸ í’ˆì§ˆ ìœ ì§€ë¥¼ ìœ„í•´ ë™ì¼í•œ í™˜ê²½ ì„¤ì • ê¶Œì¥"
            ]
        
        return insights
    
    def _extract_korean_insights(self, korean_analysis: Dict) -> Dict:
        """í•œêµ­ì–´ ë¶„ì„ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ"""
        if not korean_analysis.get("success"):
            return {"error": "í•œêµ­ì–´ ë¶„ì„ ì‹¤íŒ¨"}
        
        return {
            "situation_type": korean_analysis.get("analysis_info", {}).get("situation_name", "ì¼ë°˜ ìƒí™©"),
            "executive_summary": korean_analysis.get("final_summary", {}).get("executive_summary", ""),
            "key_business_findings": korean_analysis.get("business_analysis", {}).get("market_insights", [])[:3],
            "actionable_items": korean_analysis.get("actionable_insights", {}).get("immediate_actions", [])[:3],
            "strategic_recommendations": korean_analysis.get("business_analysis", {}).get("strategic_recommendations", [])[:3]
        }
    
    def _generate_comprehensive_report_v3(self, 
                                        session_id: str,
                                        session_title: str,
                                        source_results: Dict,
                                        integrated_analysis: Dict,
                                        cross_validation: Dict,
                                        korean_analysis: Dict,
                                        final_insights: Dict) -> Dict:
        """ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„± v3.0"""
        print("ğŸ“Š ì¢…í•© ë¦¬í¬íŠ¸ v3.0 ìƒì„± ì¤‘...")
        
        # ìš”ì•½ í†µê³„
        summary_stats = {
            "session_id": session_id,
            "session_title": session_title,
            "sources_processed": len([s for s in source_results.values() if s]),
            "total_files": sum(r.get("files_processed", r.get("urls_processed", 0)) for r in source_results.values() if r),
            "successful_files": sum(r.get("files_successful", r.get("urls_successful", 0)) for r in source_results.values() if r),
            "total_words": integrated_analysis.get("total_word_count", 0),
            "confidence_score": integrated_analysis.get("confidence_score", 0),
            "consistency_score": cross_validation.get("overall_consistency", 0),
            "korean_analysis_success": korean_analysis.get("success", False)
        }
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ v3.0
        performance_metrics = {
            "processing_time": "ì‹¤ì‹œê°„ ê³„ì‚°ë¨",
            "source_distribution": integrated_analysis.get("source_distribution", {}),
            "analysis_depth": integrated_analysis.get("analysis_depth", "standard"),
            "quality_indicators": {
                "data_quality": "ë†’ìŒ" if summary_stats["total_words"] >= 1000 else "ë³´í†µ",
                "source_reliability": "ë†’ìŒ" if summary_stats["confidence_score"] >= 0.8 else "ë³´í†µ",
                "cross_validation": "í†µê³¼" if summary_stats["consistency_score"] >= 0.6 else "ê²€í†  í•„ìš”",
                "korean_analysis": "ì„±ê³µ" if korean_analysis.get("success") else "ì‹¤íŒ¨"
            },
            "new_features": {
                "quality_analysis": "í™œì„±í™”ë¨",
                "korean_summarization": "í™œì„±í™”ë¨",
                "ppt_optimization": "í™œì„±í™”ë¨",
                "noise_analysis": "í™œì„±í™”ë¨"
            }
        }
        
        return {
            "success": True,
            "session_info": {
                "session_id": session_id,
                "title": session_title,
                "generated_at": datetime.now().isoformat(),
                "report_version": "3.0",
                "features": "í’ˆì§ˆë¶„ì„ + í•œêµ­ì–´ìš”ì•½ í†µí•©"
            },
            "summary_statistics": summary_stats,
            "source_results": source_results,
            "integrated_analysis": integrated_analysis,
            "cross_validation": cross_validation,
            "korean_analysis": korean_analysis,  # ìƒˆë¡œìš´ ì„¹ì…˜
            "final_insights": final_insights,
            "performance_metrics": performance_metrics,
            "report_type": "multimodal_comprehensive_v3"
        }


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
_multimodal_integrator_instance = None

def get_multimodal_integrator() -> MultimodalIntegrator:
    """ì „ì—­ ë©€í‹°ëª¨ë‹¬ í†µí•©ê¸° ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _multimodal_integrator_instance
    if _multimodal_integrator_instance is None:
        _multimodal_integrator_instance = MultimodalIntegrator()
    return _multimodal_integrator_instance

# í¸ì˜ í•¨ìˆ˜ë“¤
async def process_multimodal_session(session_data: Dict, **kwargs) -> Dict:
    """ë©€í‹°ëª¨ë‹¬ ì„¸ì…˜ ì²˜ë¦¬ í¸ì˜ í•¨ìˆ˜ v3.0"""
    integrator = get_multimodal_integrator()
    return await integrator.process_multimodal_session(session_data, **kwargs)

async def analyze_mixed_content_with_quality(files_data: List[Dict], 
                                           urls: List[str] = None,
                                           situation_type: str = "auto") -> Dict:
    """í˜¼í•© ì½˜í…ì¸  í’ˆì§ˆ ë¶„ì„ í¸ì˜ í•¨ìˆ˜"""
    session_data = {
        "session_id": f"mixed_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
        "title": "Mixed Content Analysis with Quality Check"
    }
    
    # íŒŒì¼ë“¤ì„ íƒ€ì…ë³„ë¡œ ë¶„ë¥˜
    audio_files = []
    video_files = []
    document_files = []
    
    for file_data in files_data:
        filename = file_data.get("filename", "")
        file_ext = Path(filename).suffix.lower()
        
        if file_ext in ['.mp3', '.wav', '.m4a', '.aac']:
            audio_files.append(file_data)
        elif file_ext in ['.mp4', '.avi', '.mov', '.mkv']:
            video_files.append(file_data)
        else:
            document_files.append(file_data)
    
    if audio_files:
        session_data["audio_files"] = audio_files
    if video_files:
        session_data["video_files"] = video_files
    if document_files:
        session_data["document_files"] = document_files
    if urls:
        session_data["web_urls"] = urls
    
    return await process_multimodal_session(
        session_data, 
        analysis_depth="comprehensive",
        situation_type=situation_type
    )

def get_integration_capabilities_v3() -> Dict:
    """ë©€í‹°ëª¨ë‹¬ í†µí•© ê¸°ëŠ¥ ì •ë³´ v3.0 ë°˜í™˜"""
    integrator = get_multimodal_integrator()
    return {
        "supported_sources": ["audio", "video", "images", "documents", "web"],
        "source_weights": integrator.source_weights,
        "jewelry_categories": list(integrator.jewelry_categories.keys()),
        "analysis_depths": ["quick", "standard", "comprehensive"],
        "situation_types": ["auto", "seminar", "meeting", "lecture", "conference"],
        "output_formats": ["comprehensive_report_v3", "korean_summary", "quality_analysis"],
        "new_features_v3": [
            "ì‹¤ì‹œê°„ í’ˆì§ˆ ë¶„ì„ (ìŒì„± ë…¸ì´ì¦ˆ, ì´ë¯¸ì§€ OCR)",
            "PPT í™”ë©´ íŠ¹í™” ë¶„ì„ ë° ìµœì í™”",
            "ë‹¤êµ­ì–´ â†’ í•œêµ­ì–´ í†µí•© ìš”ì•½",
            "ìƒí™©ë³„ ë§ì¶¤ ë¶„ì„ (ì„¸ë¯¸ë‚˜/íšŒì˜/ê°•ì˜/ì»¨í¼ëŸ°ìŠ¤)",
            "ë¹„ì¦ˆë‹ˆìŠ¤ ê´€ì  ì¸ì‚¬ì´íŠ¸ ìë™ ìƒì„±",
            "í˜„ì¥ ì´¬ì˜ í’ˆì§ˆ ê°œì„  ì œì•ˆ"
        ]
    }

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    async def test_integrator_v3():
        print("ë©€í‹°ëª¨ë‹¬ í†µí•© ë¶„ì„ ì—”ì§„ v3.0 í…ŒìŠ¤íŠ¸")
        capabilities = get_integration_capabilities_v3()
        print(f"í†µí•© ê¸°ëŠ¥ v3.0: {capabilities}")
    
    asyncio.run(test_integrator_v3())
