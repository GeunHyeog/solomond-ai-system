"""
ì†”ë¡œëª¬ë“œ AI ì‹œìŠ¤í…œ - ë©€í‹°ëª¨ë‹¬ í†µí•© ë¶„ì„ ì—”ì§„
ìŒì„±, ë¹„ë””ì˜¤, ì´ë¯¸ì§€, ë¬¸ì„œ, ì›¹ ë°ì´í„°ë¥¼ í†µí•©í•˜ì—¬ ì¢…í•© ë¶„ì„ ë° ê²°ë¡  ë„ì¶œ
"""

import asyncio
from typing import Dict, List, Optional, Union, Tuple, Any
from datetime import datetime
import json
import logging
from collections import defaultdict, Counter
import re
from pathlib import Path

# ê° ëª¨ë“ˆ import
from .analyzer import STTAnalyzer, get_stt_analyzer
from .video_processor import get_video_processor, extract_audio_from_video
from .image_processor import get_image_processor, process_image_file, process_document_file
from .web_crawler import get_web_crawler, crawl_url, crawl_jewelry_news
from .jewelry_ai_engine import JewelryAIEngine
from .cross_validation_visualizer import CrossValidationVisualizer
from .speaker_analyzer import SpeakerAnalyzer

class MultimodalIntegrator:
    """ë©€í‹°ëª¨ë‹¬ ë°ì´í„° í†µí•© ë¶„ì„ í´ë˜ìŠ¤"""
    
    def __init__(self):
        # ê° í”„ë¡œì„¸ì„œ ì¸ìŠ¤í„´ìŠ¤
        self.stt_analyzer = None
        self.video_processor = get_video_processor()
        self.image_processor = get_image_processor()
        self.web_crawler = get_web_crawler()
        self.ai_engine = JewelryAIEngine()
        self.cross_validator = CrossValidationVisualizer()
        self.speaker_analyzer = SpeakerAnalyzer()
        
        # ë¶„ì„ ê°€ì¤‘ì¹˜ (ì†ŒìŠ¤ë³„ ì‹ ë¢°ë„)
        self.source_weights = {
            "audio": 1.0,      # ìŒì„± (ê¸°ë³¸)
            "video": 0.9,      # ë¹„ë””ì˜¤ (ìŒì„±ì—ì„œ ì¶”ì¶œ)
            "image": 0.8,      # ì´ë¯¸ì§€ OCR
            "document": 0.9,   # ë¬¸ì„œ (ë†’ì€ ì‹ ë¢°ë„)
            "web": 0.6         # ì›¹ í¬ë¡¤ë§ (ë‚®ì€ ì‹ ë¢°ë„)
        }
        
        # ì£¼ì–¼ë¦¬ ìš©ì–´ ì¹´í…Œê³ ë¦¬ ë§¤í•‘
        self.jewelry_categories = {
            "gems": ["ë‹¤ì´ì•„ëª¬ë“œ", "ë£¨ë¹„", "ì‚¬íŒŒì´ì–´", "ì—ë©”ë„ë“œ", "ì§„ì£¼"],
            "metals": ["ê¸ˆ", "ì€", "ë°±ê¸ˆ", "í”Œë˜í‹°ë„˜"],
            "grading": ["4C", "ìºëŸ¿", "ì»·", "ì»¬ëŸ¬", "í´ë˜ë¦¬í‹°"],
            "certification": ["GIA", "AGS", "ê°ì •ì„œ", "ì¸ì¦ì„œ"],
            "business": ["ê°€ê²©", "í• ì¸", "ë„ë§¤", "ì†Œë§¤", "ë¬´ì—­"],
            "techniques": ["ì„¸íŒ…", "ê°€ê³µ", "ì—°ë§ˆ", "ì¡°ê°"]
        }
        
        logging.info("ë©€í‹°ëª¨ë‹¬ í†µí•© ë¶„ì„ ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _get_stt_analyzer(self):
        """STT ë¶„ì„ê¸° ì§€ì—° ì´ˆê¸°í™”"""
        if self.stt_analyzer is None:
            self.stt_analyzer = get_stt_analyzer()
        return self.stt_analyzer
    
    async def process_multimodal_session(self, 
                                       session_data: Dict,
                                       analysis_depth: str = "comprehensive") -> Dict:
        """
        ë©€í‹°ëª¨ë‹¬ ì„¸ì…˜ í†µí•© ì²˜ë¦¬
        
        Args:
            session_data: ì„¸ì…˜ ë°ì´í„° (ê° ì†ŒìŠ¤ë³„ íŒŒì¼/URL ì •ë³´)
            analysis_depth: ë¶„ì„ ê¹Šì´ ("quick", "standard", "comprehensive")
            
        Returns:
            í†µí•© ë¶„ì„ ê²°ê³¼
        """
        try:
            print("ğŸ”„ ë©€í‹°ëª¨ë‹¬ ì„¸ì…˜ ë¶„ì„ ì‹œì‘")
            
            # ì„¸ì…˜ ì •ë³´ ì´ˆê¸°í™”
            session_id = session_data.get("session_id", f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
            session_title = session_data.get("title", "Untitled Session")
            
            # ê° ì†ŒìŠ¤ë³„ ì²˜ë¦¬ ê²°ê³¼ ì €ì¥
            source_results = {}
            
            # 1. ìŒì„± íŒŒì¼ ì²˜ë¦¬
            if "audio_files" in session_data:
                source_results["audio"] = await self._process_audio_sources(session_data["audio_files"])
            
            # 2. ë¹„ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬
            if "video_files" in session_data:
                source_results["video"] = await self._process_video_sources(session_data["video_files"])
            
            # 3. ì´ë¯¸ì§€/ë¬¸ì„œ íŒŒì¼ ì²˜ë¦¬
            if "document_files" in session_data:
                source_results["documents"] = await self._process_document_sources(session_data["document_files"])
            
            # 4. ì›¹ ì†ŒìŠ¤ ì²˜ë¦¬
            if "web_urls" in session_data:
                source_results["web"] = await self._process_web_sources(session_data["web_urls"])
            
            # 5. í†µí•© ë¶„ì„ ìˆ˜í–‰
            integrated_analysis = await self._perform_integrated_analysis(
                source_results, 
                analysis_depth
            )
            
            # 6. í¬ë¡œìŠ¤ ê²€ì¦ ìˆ˜í–‰
            cross_validation = await self._perform_cross_validation(source_results)
            
            # 7. ìµœì¢… ì¸ì‚¬ì´íŠ¸ ìƒì„±
            final_insights = await self._generate_final_insights(
                integrated_analysis,
                cross_validation,
                session_data
            )
            
            # 8. ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±
            comprehensive_report = self._generate_comprehensive_report(
                session_id,
                session_title,
                source_results,
                integrated_analysis,
                cross_validation,
                final_insights
            )
            
            print(f"âœ… ë©€í‹°ëª¨ë‹¬ ì„¸ì…˜ ë¶„ì„ ì™„ë£Œ: {len(source_results)}ê°œ ì†ŒìŠ¤ í†µí•©")
            return comprehensive_report
            
        except Exception as e:
            logging.error(f"ë©€í‹°ëª¨ë‹¬ ì„¸ì…˜ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {
                "success": False,
                "error": str(e),
                "session_id": session_data.get("session_id", "unknown")
            }
    
    async def _process_audio_sources(self, audio_files: List[Dict]) -> Dict:
        """ìŒì„± íŒŒì¼ë“¤ ì²˜ë¦¬"""
        print("ğŸ¤ ìŒì„± ì†ŒìŠ¤ ì²˜ë¦¬ ì¤‘...")
        
        results = []
        total_duration = 0
        combined_text = ""
        
        stt_analyzer = self._get_stt_analyzer()
        
        for audio_file in audio_files:
            try:
                # STT ë¶„ì„ ìˆ˜í–‰
                if "content" in audio_file:
                    result = await stt_analyzer.analyze_audio(
                        audio_file["content"],
                        audio_file.get("filename", "unknown.wav"),
                        language=audio_file.get("language", "ko"),
                        enable_jewelry_enhancement=True
                    )
                    
                    if result.get("success"):
                        results.append(result)
                        total_duration += result.get("duration", 0)
                        combined_text += f" {result.get('enhanced_text', '')}"
                        
                        # í™”ì ë¶„ì„ ì¶”ê°€
                        if hasattr(self.speaker_analyzer, 'analyze_speakers'):
                            speaker_result = await self.speaker_analyzer.analyze_speakers(
                                audio_file["content"],
                                audio_file.get("filename", "unknown.wav")
                            )
                            result["speaker_analysis"] = speaker_result
                
            except Exception as e:
                logging.error(f"ìŒì„± íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue
        
        return {
            "source_type": "audio",
            "files_processed": len(results),
            "files_successful": len([r for r in results if r.get("success")]),
            "total_duration": total_duration,
            "combined_text": combined_text.strip(),
            "individual_results": results,
            "summary": {
                "total_words": len(combined_text.split()),
                "average_confidence": sum(r.get("confidence", 0) for r in results) / max(len(results), 1)
            }
        }
    
    async def _process_video_sources(self, video_files: List[Dict]) -> Dict:
        """ë¹„ë””ì˜¤ íŒŒì¼ë“¤ ì²˜ë¦¬"""
        print("ğŸ¥ ë¹„ë””ì˜¤ ì†ŒìŠ¤ ì²˜ë¦¬ ì¤‘...")
        
        results = []
        total_duration = 0
        combined_text = ""
        
        for video_file in video_files:
            try:
                # ë¹„ë””ì˜¤ì—ì„œ ìŒì„± ì¶”ì¶œ
                extraction_result = await extract_audio_from_video(
                    video_file["content"],
                    video_file.get("filename", "unknown.mp4")
                )
                
                if extraction_result.get("success"):
                    # ì¶”ì¶œëœ ìŒì„±ìœ¼ë¡œ STT ìˆ˜í–‰
                    stt_analyzer = self._get_stt_analyzer()
                    stt_result = await stt_analyzer.analyze_audio(
                        extraction_result["audio_content"],
                        extraction_result["extracted_filename"],
                        language=video_file.get("language", "ko"),
                        enable_jewelry_enhancement=True
                    )
                    
                    if stt_result.get("success"):
                        # ë¹„ë””ì˜¤ ì •ë³´ì™€ STT ê²°ê³¼ ê²°í•©
                        combined_result = {
                            **stt_result,
                            "video_info": extraction_result,
                            "source_file": video_file.get("filename")
                        }
                        
                        results.append(combined_result)
                        total_duration += stt_result.get("duration", 0)
                        combined_text += f" {stt_result.get('enhanced_text', '')}"
                
            except Exception as e:
                logging.error(f"ë¹„ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue
        
        return {
            "source_type": "video",
            "files_processed": len(results),
            "files_successful": len([r for r in results if r.get("success")]),
            "total_duration": total_duration,
            "combined_text": combined_text.strip(),
            "individual_results": results,
            "summary": {
                "total_words": len(combined_text.split()),
                "extraction_method": "ffmpeg"
            }
        }
    
    async def _process_document_sources(self, document_files: List[Dict]) -> Dict:
        """ë¬¸ì„œ/ì´ë¯¸ì§€ íŒŒì¼ë“¤ ì²˜ë¦¬"""
        print("ğŸ“„ ë¬¸ì„œ ì†ŒìŠ¤ ì²˜ë¦¬ ì¤‘...")
        
        results = []
        combined_text = ""
        total_pages = 0
        
        for doc_file in document_files:
            try:
                # íŒŒì¼ íƒ€ì…ì— ë”°ë¼ ì²˜ë¦¬
                file_type = self.image_processor.get_file_type(doc_file.get("filename", ""))
                
                if file_type == "image":
                    result = await process_image_file(
                        doc_file["content"],
                        doc_file.get("filename", "unknown.jpg"),
                        enhance_quality=True,
                        ocr_method="auto"
                    )
                elif file_type == "document":
                    result = await process_document_file(
                        doc_file["content"],
                        doc_file.get("filename", "unknown.pdf")
                    )
                else:
                    continue
                
                if result.get("success"):
                    results.append(result)
                    
                    # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    if "text" in result:
                        combined_text += f" {result['text']}"
                    elif "ocr_results" in result and "text" in result["ocr_results"]:
                        combined_text += f" {result['ocr_results']['text']}"
                    
                    # í˜ì´ì§€ ìˆ˜ ê³„ì‚°
                    if "page_count" in result:
                        total_pages += result["page_count"]
                    else:
                        total_pages += 1
                
            except Exception as e:
                logging.error(f"ë¬¸ì„œ íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue
        
        return {
            "source_type": "documents",
            "files_processed": len(results),
            "files_successful": len([r for r in results if r.get("success")]),
            "total_pages": total_pages,
            "combined_text": combined_text.strip(),
            "individual_results": results,
            "summary": {
                "total_words": len(combined_text.split()),
                "document_types": list(set(r.get("file_type", "unknown") for r in results))
            }
        }
    
    async def _process_web_sources(self, web_urls: List[str]) -> Dict:
        """ì›¹ ì†ŒìŠ¤ë“¤ ì²˜ë¦¬"""
        print("ğŸŒ ì›¹ ì†ŒìŠ¤ ì²˜ë¦¬ ì¤‘...")
        
        results = []
        combined_text = ""
        
        async with self.web_crawler:
            for url in web_urls:
                try:
                    result = await self.web_crawler.process_url(
                        url,
                        content_type="auto",
                        extract_video=False
                    )
                    
                    if result.get("success"):
                        results.append(result)
                        
                        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                        if "content" in result:
                            combined_text += f" {result['content']}"
                        elif "text" in result:
                            combined_text += f" {result['text']}"
                
                except Exception as e:
                    logging.error(f"ì›¹ URL ì²˜ë¦¬ ì˜¤ë¥˜ ({url}): {e}")
                    continue
        
        return {
            "source_type": "web",
            "urls_processed": len(results),
            "urls_successful": len([r for r in results if r.get("success")]),
            "combined_text": combined_text.strip(),
            "individual_results": results,
            "summary": {
                "total_words": len(combined_text.split()),
                "content_types": list(set(r.get("content_type", "unknown") for r in results))
            }
        }
    
    async def _perform_integrated_analysis(self, 
                                         source_results: Dict,
                                         analysis_depth: str) -> Dict:
        """í†µí•© ë¶„ì„ ìˆ˜í–‰"""
        print("ğŸ§  í†µí•© AI ë¶„ì„ ìˆ˜í–‰ ì¤‘...")
        
        # ëª¨ë“  í…ìŠ¤íŠ¸ ê²°í•©
        all_texts = []
        source_weights_applied = {}
        
        for source_type, result in source_results.items():
            if result and "combined_text" in result:
                text = result["combined_text"]
                weight = self.source_weights.get(source_type, 0.5)
                
                all_texts.append({
                    "text": text,
                    "source": source_type,
                    "weight": weight,
                    "word_count": len(text.split())
                })
                source_weights_applied[source_type] = weight
        
        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ í†µí•© í…ìŠ¤íŠ¸ ìƒì„±
        combined_text = " ".join([item["text"] for item in all_texts])
        
        # ì£¼ì–¼ë¦¬ AI ì—”ì§„ìœ¼ë¡œ ê³ ê¸‰ ë¶„ì„
        ai_analysis = await self.ai_engine.analyze_jewelry_content(
            combined_text,
            analysis_type="comprehensive" if analysis_depth == "comprehensive" else "standard"
        )
        
        # ì†ŒìŠ¤ë³„ ì£¼ì–¼ë¦¬ ìš©ì–´ ë¶„ì„
        source_term_analysis = {}
        for source_type, result in source_results.items():
            if result and "combined_text" in result:
                terms = self._extract_jewelry_terms(result["combined_text"])
                source_term_analysis[source_type] = terms
        
        # ì£¼ì œ ì¼ê´€ì„± ë¶„ì„
        topic_consistency = self._analyze_topic_consistency(source_results)
        
        # ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
        confidence_score = self._calculate_confidence_score(source_results, source_weights_applied)
        
        return {
            "ai_analysis": ai_analysis,
            "source_term_analysis": source_term_analysis,
            "topic_consistency": topic_consistency,
            "confidence_score": confidence_score,
            "total_word_count": len(combined_text.split()),
            "source_distribution": {
                source: len(result.get("combined_text", "").split())
                for source, result in source_results.items()
                if result
            },
            "analysis_depth": analysis_depth
        }
    
    async def _perform_cross_validation(self, source_results: Dict) -> Dict:
        """í¬ë¡œìŠ¤ ê²€ì¦ ìˆ˜í–‰"""
        print("ğŸ” í¬ë¡œìŠ¤ ê²€ì¦ ìˆ˜í–‰ ì¤‘...")
        
        # ì†ŒìŠ¤ ê°„ ì¼ì¹˜ë„ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±
        if hasattr(self.cross_validator, 'calculate_consensus_matrix'):
            consensus_matrix = await self.cross_validator.calculate_consensus_matrix(source_results)
        else:
            consensus_matrix = self._simple_consensus_matrix(source_results)
        
        # ì£¼ìš” í‚¤ì›Œë“œ ì¼ì¹˜ë„ ë¶„ì„
        keyword_consistency = self._analyze_keyword_consistency(source_results)
        
        # íƒ€ì„ë¼ì¸ ì¼ì¹˜ì„± (ì‹œê°„ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°)
        timeline_consistency = self._analyze_timeline_consistency(source_results)
        
        return {
            "consensus_matrix": consensus_matrix,
            "keyword_consistency": keyword_consistency,
            "timeline_consistency": timeline_consistency,
            "overall_consistency": self._calculate_overall_consistency(consensus_matrix)
        }
    
    def _simple_consensus_matrix(self, source_results: Dict) -> Dict:
        """ê°„ë‹¨í•œ í•©ì˜ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°"""
        sources = list(source_results.keys())
        matrix = {}
        
        for i, source1 in enumerate(sources):
            matrix[source1] = {}
            for j, source2 in enumerate(sources):
                if i == j:
                    similarity = 1.0
                else:
                    # í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê°„ë‹¨ ê³„ì‚°
                    text1 = source_results[source1].get("combined_text", "")
                    text2 = source_results[source2].get("combined_text", "")
                    similarity = self._calculate_text_similarity(text1, text2)
                
                matrix[source1][source2] = round(similarity, 3)
        
        return matrix
    
    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚° (ê°„ë‹¨í•œ êµ¬í˜„)"""
        if not text1 or not text2:
            return 0.0
        
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = words1 & words2
        union = words1 | words2
        
        return len(intersection) / len(union) if union else 0.0
    
    def _extract_jewelry_terms(self, text: str) -> Dict:
        """í…ìŠ¤íŠ¸ì—ì„œ ì£¼ì–¼ë¦¬ ìš©ì–´ ì¶”ì¶œ"""
        terms_by_category = defaultdict(list)
        text_lower = text.lower()
        
        for category, terms in self.jewelry_categories.items():
            for term in terms:
                if term.lower() in text_lower:
                    terms_by_category[category].append(term)
        
        return dict(terms_by_category)
    
    def _analyze_topic_consistency(self, source_results: Dict) -> Dict:
        """ì£¼ì œ ì¼ê´€ì„± ë¶„ì„"""
        all_terms = defaultdict(int)
        source_terms = {}
        
        # ê° ì†ŒìŠ¤ë³„ ìš©ì–´ ë¹ˆë„ ê³„ì‚°
        for source_type, result in source_results.items():
            if result and "combined_text" in result:
                terms = self._extract_jewelry_terms(result["combined_text"])
                source_terms[source_type] = terms
                
                for category, category_terms in terms.items():
                    for term in category_terms:
                        all_terms[term] += 1
        
        # ê³µí†µ ì£¼ì œ ì‹ë³„
        common_terms = {term: count for term, count in all_terms.items() if count >= 2}
        
        return {
            "common_terms": common_terms,
            "source_terms": source_terms,
            "consistency_score": len(common_terms) / max(len(all_terms), 1)
        }
    
    def _analyze_keyword_consistency(self, source_results: Dict) -> Dict:
        """í‚¤ì›Œë“œ ì¼ì¹˜ë„ ë¶„ì„"""
        keyword_counts = defaultdict(lambda: defaultdict(int))
        
        for source_type, result in source_results.items():
            if result and "combined_text" in result:
                text = result["combined_text"].lower()
                words = text.split()
                
                # ì£¼ì–¼ë¦¬ í‚¤ì›Œë“œ ë¹ˆë„ ê³„ì‚°
                for category, terms in self.jewelry_categories.items():
                    for term in terms:
                        if term.lower() in text:
                            keyword_counts[category][source_type] += text.count(term.lower())
        
        return dict(keyword_counts)
    
    def _analyze_timeline_consistency(self, source_results: Dict) -> Dict:
        """íƒ€ì„ë¼ì¸ ì¼ì¹˜ì„± ë¶„ì„"""
        # ê°„ë‹¨í•œ êµ¬í˜„ - ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ì‹œê°„ ì •ë³´ ì¶”ì¶œ í•„ìš”
        timeline_info = {}
        
        for source_type, result in source_results.items():
            if result:
                timeline_info[source_type] = {
                    "processing_time": result.get("processing_time", ""),
                    "duration": result.get("total_duration", 0),
                    "timestamp": datetime.now().isoformat()
                }
        
        return timeline_info
    
    def _calculate_confidence_score(self, source_results: Dict, weights: Dict) -> float:
        """ì „ì²´ ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°"""
        total_weight = 0
        weighted_confidence = 0
        
        for source_type, result in source_results.items():
            if result:
                weight = weights.get(source_type, 0.5)
                source_confidence = result.get("summary", {}).get("average_confidence", 0.5)
                
                weighted_confidence += source_confidence * weight
                total_weight += weight
        
        return round(weighted_confidence / max(total_weight, 1), 3)
    
    def _calculate_overall_consistency(self, consensus_matrix: Dict) -> float:
        """ì „ì²´ ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚°"""
        if not consensus_matrix:
            return 0.0
        
        total_similarity = 0
        count = 0
        
        for source1, similarities in consensus_matrix.items():
            for source2, similarity in similarities.items():
                if source1 != source2:  # ìê¸° ìì‹ ê³¼ì˜ ë¹„êµ ì œì™¸
                    total_similarity += similarity
                    count += 1
        
        return round(total_similarity / max(count, 1), 3)
    
    async def _generate_final_insights(self, 
                                     integrated_analysis: Dict,
                                     cross_validation: Dict,
                                     session_data: Dict) -> Dict:
        """ìµœì¢… ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        print("ğŸ’¡ ìµœì¢… ì¸ì‚¬ì´íŠ¸ ìƒì„± ì¤‘...")
        
        # í•µì‹¬ ë°œê²¬ì‚¬í•­
        key_findings = []
        
        # 1. ì£¼ìš” ì£¼ì œ
        ai_analysis = integrated_analysis.get("ai_analysis", {})
        if "main_topics" in ai_analysis:
            key_findings.append(f"ì£¼ìš” ì£¼ì œ: {', '.join(ai_analysis['main_topics'])}")
        
        # 2. ì‹ ë¢°ë„ í‰ê°€
        confidence = integrated_analysis.get("confidence_score", 0)
        if confidence >= 0.8:
            key_findings.append("ë†’ì€ ì‹ ë¢°ë„ì˜ ë¶„ì„ ê²°ê³¼")
        elif confidence >= 0.6:
            key_findings.append("ë³´í†µ ìˆ˜ì¤€ì˜ ì‹ ë¢°ë„")
        else:
            key_findings.append("ì¶”ê°€ ê²€ì¦ì´ í•„ìš”í•œ ë¶„ì„ ê²°ê³¼")
        
        # 3. ì¼ê´€ì„± í‰ê°€
        consistency = cross_validation.get("overall_consistency", 0)
        if consistency >= 0.7:
            key_findings.append("ì†ŒìŠ¤ ê°„ ë†’ì€ ì¼ê´€ì„± í™•ì¸")
        elif consistency >= 0.5:
            key_findings.append("ì†ŒìŠ¤ ê°„ ì ë‹¹í•œ ì¼ê´€ì„±")
        else:
            key_findings.append("ì†ŒìŠ¤ ê°„ ì¼ê´€ì„± ë¶€ì¡± - ì¶”ê°€ ê²€í†  í•„ìš”")
        
        # 4. ë°ì´í„° í’ˆì§ˆ í‰ê°€
        total_words = integrated_analysis.get("total_word_count", 0)
        if total_words >= 1000:
            key_findings.append("ì¶©ë¶„í•œ ì–‘ì˜ ë°ì´í„°ë¡œ ì‹ ë¢°í•  ë§Œí•œ ë¶„ì„")
        elif total_words >= 500:
            key_findings.append("ì ë‹¹í•œ ì–‘ì˜ ë°ì´í„°")
        else:
            key_findings.append("ì œí•œì ì¸ ë°ì´í„° ì–‘ - ì¶”ê°€ ìë£Œ ìˆ˜ì§‘ ê¶Œì¥")
        
        # ì‹¤í–‰ ê°€ëŠ¥í•œ ê¶Œì¥ì‚¬í•­
        recommendations = []
        
        # ì£¼ì–¼ë¦¬ ë¹„ì¦ˆë‹ˆìŠ¤ ê´€ì  ê¶Œì¥ì‚¬í•­
        jewelry_analysis = ai_analysis.get("jewelry_insights", {})
        if jewelry_analysis.get("business_opportunities"):
            recommendations.extend(jewelry_analysis["business_opportunities"][:3])
        
        # ê¸°ìˆ ì  ê¶Œì¥ì‚¬í•­
        if confidence < 0.7:
            recommendations.append("ë” ë§ì€ ë°ì´í„° ì†ŒìŠ¤ë¥¼ ì¶”ê°€í•˜ì—¬ ë¶„ì„ ì •í™•ë„ í–¥ìƒ")
        
        if consistency < 0.6:
            recommendations.append("ì†ŒìŠ¤ ê°„ ë¶ˆì¼ì¹˜ ë¶€ë¶„ì— ëŒ€í•œ ì„¸ë¶€ ê²€ì¦ ìˆ˜í–‰")
        
        # ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ
        next_steps = [
            "í•µì‹¬ ë°œê²¬ì‚¬í•­ì— ëŒ€í•œ ìƒì„¸ ë¶„ì„ ìˆ˜í–‰",
            "ì´í•´ê´€ê³„ìë“¤ê³¼ ê²°ê³¼ ê³µìœ  ë° í”¼ë“œë°± ìˆ˜ì§‘",
            "ì‹¤í–‰ ê³„íš ìˆ˜ë¦½ ë° ìš°ì„ ìˆœìœ„ ì„¤ì •"
        ]
        
        return {
            "key_findings": key_findings,
            "recommendations": recommendations,
            "next_steps": next_steps,
            "quality_assessment": {
                "confidence_level": "ë†’ìŒ" if confidence >= 0.8 else "ë³´í†µ" if confidence >= 0.6 else "ë‚®ìŒ",
                "consistency_level": "ë†’ìŒ" if consistency >= 0.7 else "ë³´í†µ" if consistency >= 0.5 else "ë‚®ìŒ",
                "data_sufficiency": "ì¶©ë¶„" if total_words >= 1000 else "ë³´í†µ" if total_words >= 500 else "ë¶€ì¡±"
            },
            "generated_at": datetime.now().isoformat()
        }
    
    def _generate_comprehensive_report(self, 
                                     session_id: str,
                                     session_title: str,
                                     source_results: Dict,
                                     integrated_analysis: Dict,
                                     cross_validation: Dict,
                                     final_insights: Dict) -> Dict:
        """ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±"""
        print("ğŸ“Š ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        
        # ìš”ì•½ í†µê³„
        summary_stats = {
            "session_id": session_id,
            "session_title": session_title,
            "sources_processed": len([s for s in source_results.values() if s]),
            "total_files": sum(r.get("files_processed", r.get("urls_processed", 0)) for r in source_results.values() if r),
            "successful_files": sum(r.get("files_successful", r.get("urls_successful", 0)) for r in source_results.values() if r),
            "total_words": integrated_analysis.get("total_word_count", 0),
            "confidence_score": integrated_analysis.get("confidence_score", 0),
            "consistency_score": cross_validation.get("overall_consistency", 0)
        }
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        performance_metrics = {
            "processing_time": "ì‹¤ì‹œê°„ ê³„ì‚°ë¨",
            "source_distribution": integrated_analysis.get("source_distribution", {}),
            "analysis_depth": integrated_analysis.get("analysis_depth", "standard"),
            "quality_indicators": {
                "data_quality": "ë†’ìŒ" if summary_stats["total_words"] >= 1000 else "ë³´í†µ",
                "source_reliability": "ë†’ìŒ" if summary_stats["confidence_score"] >= 0.8 else "ë³´í†µ",
                "cross_validation": "í†µê³¼" if summary_stats["consistency_score"] >= 0.6 else "ê²€í†  í•„ìš”"
            }
        }
        
        return {
            "success": True,
            "session_info": {
                "session_id": session_id,
                "title": session_title,
                "generated_at": datetime.now().isoformat(),
                "report_version": "1.0"
            },
            "summary_statistics": summary_stats,
            "source_results": source_results,
            "integrated_analysis": integrated_analysis,
            "cross_validation": cross_validation,
            "final_insights": final_insights,
            "performance_metrics": performance_metrics,
            "report_type": "multimodal_comprehensive"
        }

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
_multimodal_integrator_instance = None

def get_multimodal_integrator() -> MultimodalIntegrator:
    """ì „ì—­ ë©€í‹°ëª¨ë‹¬ í†µí•©ê¸° ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _multimodal_integrator_instance
    if _multimodal_integrator_instance is None:
        _multimodal_integrator_instance = MultimodalIntegrator()
    return _multimodal_integrator_instance

# í¸ì˜ í•¨ìˆ˜ë“¤
async def process_multimodal_session(session_data: Dict, **kwargs) -> Dict:
    """ë©€í‹°ëª¨ë‹¬ ì„¸ì…˜ ì²˜ë¦¬ í¸ì˜ í•¨ìˆ˜"""
    integrator = get_multimodal_integrator()
    return await integrator.process_multimodal_session(session_data, **kwargs)

async def analyze_mixed_content(files_data: List[Dict], urls: List[str] = None) -> Dict:
    """í˜¼í•© ì½˜í…ì¸  ë¶„ì„ í¸ì˜ í•¨ìˆ˜"""
    session_data = {
        "session_id": f"mixed_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
        "title": "Mixed Content Analysis"
    }
    
    # íŒŒì¼ë“¤ì„ íƒ€ì…ë³„ë¡œ ë¶„ë¥˜
    audio_files = []
    video_files = []
    document_files = []
    
    for file_data in files_data:
        filename = file_data.get("filename", "")
        file_ext = Path(filename).suffix.lower()
        
        if file_ext in ['.mp3', '.wav', '.m4a', '.aac']:
            audio_files.append(file_data)
        elif file_ext in ['.mp4', '.avi', '.mov', '.mkv']:
            video_files.append(file_data)
        else:
            document_files.append(file_data)
    
    if audio_files:
        session_data["audio_files"] = audio_files
    if video_files:
        session_data["video_files"] = video_files
    if document_files:
        session_data["document_files"] = document_files
    if urls:
        session_data["web_urls"] = urls
    
    return await process_multimodal_session(session_data)

def get_integration_capabilities() -> Dict:
    """ë©€í‹°ëª¨ë‹¬ í†µí•© ê¸°ëŠ¥ ì •ë³´ ë°˜í™˜"""
    integrator = get_multimodal_integrator()
    return {
        "supported_sources": ["audio", "video", "images", "documents", "web"],
        "source_weights": integrator.source_weights,
        "jewelry_categories": list(integrator.jewelry_categories.keys()),
        "analysis_depths": ["quick", "standard", "comprehensive"],
        "output_formats": ["comprehensive_report", "summary", "insights_only"]
    }

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    async def test_integrator():
        print("ë©€í‹°ëª¨ë‹¬ í†µí•© ë¶„ì„ ì—”ì§„ í…ŒìŠ¤íŠ¸")
        capabilities = get_integration_capabilities()
        print(f"í†µí•© ê¸°ëŠ¥: {capabilities}")
    
    asyncio.run(test_integrator())
