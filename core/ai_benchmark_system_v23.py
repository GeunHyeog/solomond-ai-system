"""
ğŸ“Š ì†”ë¡œëª¬ë“œ AI ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œìŠ¤í…œ v2.3
99.2% ì •í™•ë„ ë‹¬ì„±ì„ ìœ„í•œ ì¢…í•©ì  ì„±ëŠ¥ ì¸¡ì • ë° A/B í…ŒìŠ¤íŠ¸ ìë™í™” ì‹œìŠ¤í…œ

ğŸ“… ê°œë°œì¼: 2025.07.13
ğŸ¯ ëª©í‘œ: ì‹¤ì‹œê°„ ì„±ëŠ¥ ì¸¡ì •ìœ¼ë¡œ 99.2% ì •í™•ë„ ë‹¬ì„± ì¶”ì 
ğŸ”¥ ì£¼ìš” ê¸°ëŠ¥:
- A/B í…ŒìŠ¤íŠ¸ ìë™í™” ì‹œìŠ¤í…œ
- ë‹¤ì°¨ì› ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¶„ì„
- ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë° ë­í‚¹
- ì •í™•ë„ ë‹¬ì„±ë„ ì‹¤ì‹œê°„ ì¶”ì 
- ìë™ ìµœì í™” ê¶Œì¥ì‚¬í•­ ìƒì„±
- ì„±ëŠ¥ íŠ¸ë Œë“œ ë¶„ì„ ë° ì˜ˆì¸¡

ì—°ë™ ì‹œìŠ¤í…œ:
- hybrid_llm_manager_v23.py
- ai_quality_validator_v23.py  
- jewelry_specialized_prompts_v23.py
"""

import asyncio
import logging
import time
import json
import statistics
import random
from typing import Dict, List, Optional, Any, Union, Tuple, Callable
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime, timedelta
import numpy as np
from collections import defaultdict, deque
import concurrent.futures
from pathlib import Path
import csv

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger('AIBenchmark_v23')

class BenchmarkType(Enum):
    """ë²¤ì¹˜ë§ˆí¬ ìœ í˜•"""
    ACCURACY = "accuracy"
    SPEED = "speed"
    COST_EFFICIENCY = "cost_efficiency"
    JEWELRY_EXPERTISE = "jewelry_expertise"
    COMPREHENSIVE = "comprehensive"
    A_B_TEST = "a_b_test"
    STRESS_TEST = "stress_test"
    REAL_TIME_MONITORING = "real_time_monitoring"

class MetricType(Enum):
    """ì„±ëŠ¥ ì§€í‘œ ìœ í˜•"""
    ACCURACY_SCORE = "accuracy_score"
    RESPONSE_TIME = "response_time"
    THROUGHPUT = "throughput"
    COST_PER_REQUEST = "cost_per_request"
    JEWELRY_RELEVANCE = "jewelry_relevance"
    USER_SATISFACTION = "user_satisfaction"
    QUALITY_CONSISTENCY = "quality_consistency"
    ERROR_RATE = "error_rate"

class TestScenario(Enum):
    """í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤"""
    DIAMOND_4C_ANALYSIS = "diamond_4c_analysis"
    RUBY_GRADING = "ruby_grading"
    EMERALD_EVALUATION = "emerald_evaluation"
    SAPPHIRE_ASSESSMENT = "sapphire_assessment"
    MARKET_VALUATION = "market_valuation"
    INVESTMENT_ANALYSIS = "investment_analysis"
    INSURANCE_APPRAISAL = "insurance_appraisal"
    COMPLEX_MULTIMODAL = "complex_multimodal"

@dataclass
class BenchmarkMetric:
    """ë²¤ì¹˜ë§ˆí¬ ì§€í‘œ"""
    metric_type: MetricType
    value: float
    unit: str
    timestamp: datetime = field(default_factory=datetime.now)
    confidence: float = 1.0
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class TestCase:
    """í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤"""
    id: str
    scenario: TestScenario
    input_data: Dict[str, Any]
    expected_output: Optional[Dict[str, Any]] = None
    difficulty_level: float = 0.5  # 0.0 (ì‰¬ì›€) ~ 1.0 (ì–´ë ¤ì›€)
    priority: str = "normal"  # low, normal, high, critical
    tags: List[str] = field(default_factory=list)

@dataclass
class BenchmarkResult:
    """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼"""
    test_id: str
    model_name: str
    scenario: TestScenario
    metrics: Dict[MetricType, BenchmarkMetric]
    overall_score: float
    
    # ìƒì„¸ ì •ë³´
    execution_time: float
    success: bool
    error_message: Optional[str] = None
    
    # í’ˆì§ˆ ë¶„ì„
    quality_analysis: Dict[str, Any] = field(default_factory=dict)
    improvement_suggestions: List[str] = field(default_factory=list)
    
    timestamp: datetime = field(default_factory=datetime.now)

@dataclass
class ABTestResult:
    """A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼"""
    test_name: str
    model_a: str
    model_b: str
    
    # ì„±ëŠ¥ ë¹„êµ
    model_a_metrics: Dict[MetricType, float]
    model_b_metrics: Dict[MetricType, float]
    
    # í†µê³„ì  ìœ ì˜ì„±
    statistical_significance: bool
    confidence_level: float
    p_value: float
    
    # ê²°ë¡ 
    winner: Optional[str] = None
    improvement_percentage: float = 0.0
    recommendation: str = ""
    
    test_duration: timedelta = field(default_factory=lambda: timedelta(hours=1))
    sample_size: int = 100
    timestamp: datetime = field(default_factory=datetime.now)

class TestCaseGenerator:
    """í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìƒì„±ê¸°"""
    
    def __init__(self):
        self.scenario_templates = {
            TestScenario.DIAMOND_4C_ANALYSIS: {
                "templates": [
                    "2.50ìºëŸ¿ ë¼ìš´ë“œ ë¸Œë¦´ë¦¬ì–¸íŠ¸ ë‹¤ì´ì•„ëª¬ë“œì˜ 4C ë¶„ì„",
                    "1.75ìºëŸ¿ í”„ë¦°ì„¸ìŠ¤ ì»· ë‹¤ì´ì•„ëª¬ë“œ ê°ì •",
                    "3.25ìºëŸ¿ ì—ë©”ë„ë“œ ì»· ë‹¤ì´ì•„ëª¬ë“œ ë“±ê¸‰ í‰ê°€",
                    "5.10ìºëŸ¿ ì¿ ì…˜ ì»· ë‹¤ì´ì•„ëª¬ë“œ íˆ¬ì ê°€ì¹˜ ë¶„ì„"
                ],
                "variables": {
                    "carat": [0.50, 1.00, 1.50, 2.00, 2.50, 3.00, 5.00],
                    "cut_type": ["ë¼ìš´ë“œ", "í”„ë¦°ì„¸ìŠ¤", "ì—ë©”ë„ë“œ", "ì¿ ì…˜", "ì˜¤ë²Œ", "ë§ˆí‚¤ì¦ˆ"],
                    "color_grade": ["D", "E", "F", "G", "H", "I", "J"],
                    "clarity_grade": ["FL", "IF", "VVS1", "VVS2", "VS1", "VS2", "SI1"]
                }
            },
            
            TestScenario.RUBY_GRADING: {
                "templates": [
                    "ë²„ë§ˆì‚° ë£¨ë¹„ì˜ ì›ì‚°ì§€ ê°ì • ë° í’ˆì§ˆ í‰ê°€",
                    "íƒœêµ­ ë£¨ë¹„ì™€ ë¯¸ì–€ë§ˆ ë£¨ë¹„ì˜ ë¹„êµ ë¶„ì„",
                    "ë¬´ê°€ì—´ ë£¨ë¹„ì˜ ì‹œì¥ ê°€ì¹˜ í‰ê°€",
                    "Pigeon Blood ë£¨ë¹„ì˜ ì§„ìœ„ì„± ê²€ì¦"
                ],
                "variables": {
                    "origin": ["ë¯¸ì–€ë§ˆ", "íƒœêµ­", "ë§ˆë‹¤ê°€ìŠ¤ì¹´ë¥´", "ëª¨ì ë¹„í¬"],
                    "treatment": ["ë¬´ê°€ì—´", "ê°€ì—´", "ë¶ˆëª…"],
                    "color_quality": ["Pigeon Blood", "Red", "Purplish Red", "Pink Red"],
                    "size": ["1-3ìºëŸ¿", "3-5ìºëŸ¿", "5-10ìºëŸ¿", "10ìºëŸ¿ ì´ìƒ"]
                }
            },
            
            TestScenario.MARKET_VALUATION: {
                "templates": [
                    "êµ­ì œ ë‹¤ì´ì•„ëª¬ë“œ ì‹œì¥ ë™í–¥ ë¶„ì„",
                    "ìœ ìƒ‰ë³´ì„ íˆ¬ì ì‹œì¥ ì „ë§",
                    "ì£¼ì–¼ë¦¬ ê²½ë§¤ ì‹œì¥ ê°€ê²© ë¶„ì„",
                    "ì½”ë¡œë‚˜ ì´í›„ ë³´ì„ ì‹œì¥ ë³€í™”"
                ],
                "variables": {
                    "market_type": ["ë‹¤ì´ì•„ëª¬ë“œ", "ë£¨ë¹„", "ì‚¬íŒŒì´ì–´", "ì—ë©”ë„ë“œ"],
                    "time_period": ["ìµœê·¼ 1ë…„", "ìµœê·¼ 3ë…„", "ìµœê·¼ 5ë…„", "ì¥ê¸° ì „ë§"],
                    "region": ["ê¸€ë¡œë²Œ", "ì•„ì‹œì•„", "ë¶ë¯¸", "ìœ ëŸ½"],
                    "segment": ["íˆ¬ìë“±ê¸‰", "ìƒì—…ì í’ˆì§ˆ", "ìˆ˜ì§‘ê°€ê¸‰"]
                }
            }
        }
    
    def generate_test_cases(self, count: int = 10, 
                          scenarios: Optional[List[TestScenario]] = None) -> List[TestCase]:
        """í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìƒì„±"""
        
        if not scenarios:
            scenarios = list(TestScenario)
        
        test_cases = []
        
        for i in range(count):
            scenario = random.choice(scenarios)
            test_case = self._generate_single_test_case(f"TEST_{i+1:03d}", scenario)
            test_cases.append(test_case)
        
        return test_cases
    
    def _generate_single_test_case(self, test_id: str, scenario: TestScenario) -> TestCase:
        """ë‹¨ì¼ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìƒì„±"""
        
        template_data = self.scenario_templates.get(scenario, {})
        templates = template_data.get("templates", ["ê¸°ë³¸ ë¶„ì„ ìš”ì²­"])
        variables = template_data.get("variables", {})
        
        # ëœë¤ í…œí”Œë¦¿ ì„ íƒ
        base_template = random.choice(templates)
        
        # ë³€ìˆ˜ ì¹˜í™˜
        input_text = base_template
        for var_name, var_values in variables.items():
            if f"{{{var_name}}}" in input_text:
                input_text = input_text.replace(f"{{{var_name}}}", random.choice(var_values))
        
        # ë‚œì´ë„ ì„¤ì •
        difficulty_factors = {
            TestScenario.DIAMOND_4C_ANALYSIS: 0.6,
            TestScenario.RUBY_GRADING: 0.8,
            TestScenario.EMERALD_EVALUATION: 0.7,
            TestScenario.MARKET_VALUATION: 0.9,
            TestScenario.COMPLEX_MULTIMODAL: 1.0
        }
        
        difficulty = difficulty_factors.get(scenario, 0.5)
        difficulty += random.uniform(-0.1, 0.1)  # ì•½ê°„ì˜ ëœë¤ì„±
        difficulty = max(0.0, min(1.0, difficulty))
        
        input_data = {
            "text": input_text,
            "analysis_type": scenario.value,
            "gemstone_type": self._infer_gemstone_type(input_text),
            "priority": random.choice(["normal", "high", "normal", "normal"]),  # normal ìš°ì„ 
            "context": {
                "test_case": True,
                "difficulty": difficulty,
                "generated_at": datetime.now().isoformat()
            }
        }
        
        # íƒœê·¸ ì„¤ì •
        tags = [scenario.value]
        if difficulty > 0.8:
            tags.append("high_difficulty")
        if "íˆ¬ì" in input_text or "ê°€ì¹˜" in input_text:
            tags.append("investment_related")
        if "ê°ì •" in input_text or "ë“±ê¸‰" in input_text:
            tags.append("grading_related")
        
        return TestCase(
            id=test_id,
            scenario=scenario,
            input_data=input_data,
            difficulty_level=difficulty,
            priority="high" if difficulty > 0.8 else "normal",
            tags=tags
        )
    
    def _infer_gemstone_type(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ì—ì„œ ë³´ì„ íƒ€ì… ì¶”ë¡ """
        
        text_lower = text.lower()
        
        if "ë‹¤ì´ì•„ëª¬ë“œ" in text_lower or "diamond" in text_lower:
            return "diamond"
        elif "ë£¨ë¹„" in text_lower or "ruby" in text_lower:
            return "ruby"
        elif "ì‚¬íŒŒì´ì–´" in text_lower or "sapphire" in text_lower:
            return "sapphire"
        elif "ì—ë©”ë„ë“œ" in text_lower or "emerald" in text_lower:
            return "emerald"
        else:
            return "general"

class PerformanceAnalyzer:
    """ì„±ëŠ¥ ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.target_accuracy = 0.992  # 99.2%
        self.performance_thresholds = {
            MetricType.ACCURACY_SCORE: 0.992,
            MetricType.RESPONSE_TIME: 15.0,  # 15ì´ˆ ì´ë‚´
            MetricType.COST_PER_REQUEST: 0.05,  # 5ì„¼íŠ¸ ì´ë‚´
            MetricType.JEWELRY_RELEVANCE: 0.90,
            MetricType.QUALITY_CONSISTENCY: 0.95,
            MetricType.ERROR_RATE: 0.02  # 2% ì´í•˜
        }
    
    def analyze_performance(self, results: List[BenchmarkResult]) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë¶„ì„"""
        
        if not results:
            return {"error": "ë¶„ì„í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤"}
        
        analysis = {
            "summary": {},
            "detailed_metrics": {},
            "target_achievement": {},
            "trends": {},
            "recommendations": []
        }
        
        # 1. ê¸°ë³¸ í†µê³„
        success_rate = sum(1 for r in results if r.success) / len(results)
        avg_execution_time = statistics.mean(r.execution_time for r in results)
        
        analysis["summary"] = {
            "total_tests": len(results),
            "success_rate": success_rate,
            "average_execution_time": avg_execution_time,
            "test_period": f"{results[0].timestamp} ~ {results[-1].timestamp}"
        }
        
        # 2. ë©”íŠ¸ë¦­ë³„ ìƒì„¸ ë¶„ì„
        metric_aggregates = defaultdict(list)
        
        for result in results:
            for metric_type, metric in result.metrics.items():
                metric_aggregates[metric_type].append(metric.value)
        
        for metric_type, values in metric_aggregates.items():
            if values:
                analysis["detailed_metrics"][metric_type.value] = {
                    "average": statistics.mean(values),
                    "median": statistics.median(values),
                    "std_dev": statistics.stdev(values) if len(values) > 1 else 0,
                    "min": min(values),
                    "max": max(values),
                    "samples": len(values)
                }
        
        # 3. ëª©í‘œ ë‹¬ì„±ë„ ë¶„ì„
        for metric_type, threshold in self.performance_thresholds.items():
            if metric_type in metric_aggregates:
                values = metric_aggregates[metric_type]
                
                if metric_type in [MetricType.RESPONSE_TIME, MetricType.COST_PER_REQUEST, MetricType.ERROR_RATE]:
                    # ë‚®ì„ìˆ˜ë¡ ì¢‹ì€ ì§€í‘œ
                    achievement_rate = sum(1 for v in values if v <= threshold) / len(values)
                else:
                    # ë†’ì„ìˆ˜ë¡ ì¢‹ì€ ì§€í‘œ
                    achievement_rate = sum(1 for v in values if v >= threshold) / len(values)
                
                analysis["target_achievement"][metric_type.value] = {
                    "threshold": threshold,
                    "achievement_rate": achievement_rate,
                    "status": "ë‹¬ì„±" if achievement_rate >= 0.8 else "ë¯¸ë‹¬ì„±"
                }
        
        # 4. íŠ¸ë Œë“œ ë¶„ì„ (ì‹œê°„ìˆœ ì •ë ¬ëœ ìµœê·¼ ê²°ê³¼ë“¤)
        if len(results) >= 5:
            recent_results = sorted(results, key=lambda r: r.timestamp)[-10:]
            
            for metric_type in metric_aggregates.keys():
                recent_values = []
                for result in recent_results:
                    if metric_type in result.metrics:
                        recent_values.append(result.metrics[metric_type].value)
                
                if len(recent_values) >= 3:
                    # ì„ í˜• íšŒê·€ë¡œ íŠ¸ë Œë“œ ê³„ì‚°
                    x = list(range(len(recent_values)))
                    slope = self._calculate_trend_slope(x, recent_values)
                    
                    trend_direction = "ìƒìŠ¹" if slope > 0.01 else ("í•˜ë½" if slope < -0.01 else "ì•ˆì •")
                    
                    analysis["trends"][metric_type.value] = {
                        "direction": trend_direction,
                        "slope": slope,
                        "recent_average": statistics.mean(recent_values)
                    }
        
        # 5. ê¶Œì¥ì‚¬í•­ ìƒì„±
        recommendations = self._generate_recommendations(analysis)
        analysis["recommendations"] = recommendations
        
        return analysis
    
    def _calculate_trend_slope(self, x: List[float], y: List[float]) -> float:
        """íŠ¸ë Œë“œ ê¸°ìš¸ê¸° ê³„ì‚° (ë‹¨ìˆœ ì„ í˜• íšŒê·€)"""
        
        n = len(x)
        if n < 2:
            return 0.0
        
        x_mean = statistics.mean(x)
        y_mean = statistics.mean(y)
        
        numerator = sum((x[i] - x_mean) * (y[i] - y_mean) for i in range(n))
        denominator = sum((x[i] - x_mean) ** 2 for i in range(n))
        
        if denominator == 0:
            return 0.0
        
        return numerator / denominator
    
    def _generate_recommendations(self, analysis: Dict[str, Any]) -> List[str]:
        """ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        
        recommendations = []
        
        # ì„±ê³µë¥  ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        success_rate = analysis["summary"]["success_rate"]
        if success_rate < 0.95:
            recommendations.append(f"ì„±ê³µë¥  ê°œì„  í•„ìš” (í˜„ì¬: {success_rate:.1%}, ëª©í‘œ: 95% ì´ìƒ)")
        
        # ëª©í‘œ ë‹¬ì„±ë„ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        target_achievement = analysis.get("target_achievement", {})
        
        for metric_name, data in target_achievement.items():
            if data["status"] == "ë¯¸ë‹¬ì„±":
                recommendations.append(
                    f"{metric_name} ê°œì„  í•„ìš” (ë‹¬ì„±ë¥ : {data['achievement_rate']:.1%})"
                )
        
        # íŠ¸ë Œë“œ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        trends = analysis.get("trends", {})
        
        declining_metrics = [
            metric for metric, trend in trends.items()
            if trend["direction"] == "í•˜ë½"
        ]
        
        if declining_metrics:
            recommendations.append(
                f"ì„±ëŠ¥ í•˜ë½ ì¶”ì„¸ ëª¨ë‹ˆí„°ë§ í•„ìš”: {', '.join(declining_metrics)}"
            )
        
        # 99.2% ëª©í‘œ ê´€ë ¨ ê¶Œì¥ì‚¬í•­
        accuracy_data = analysis["detailed_metrics"].get("accuracy_score")
        if accuracy_data and accuracy_data["average"] < self.target_accuracy:
            gap = self.target_accuracy - accuracy_data["average"]
            recommendations.append(
                f"99.2% ì •í™•ë„ ëª©í‘œê¹Œì§€ {gap:.1%} ì¶”ê°€ ê°œì„  í•„ìš”"
            )
        
        # ì‘ë‹µ ì‹œê°„ ê´€ë ¨ ê¶Œì¥ì‚¬í•­
        response_time_data = analysis["detailed_metrics"].get("response_time")
        if response_time_data and response_time_data["average"] > 15.0:
            recommendations.append("ì‘ë‹µ ì‹œê°„ ìµœì í™” í•„ìš” (ëª©í‘œ: 15ì´ˆ ì´ë‚´)")
        
        # ì¼ë°˜ì ì¸ ê¶Œì¥ì‚¬í•­
        if not recommendations:
            recommendations.append("í˜„ì¬ ì„±ëŠ¥ì´ ëª©í‘œ ìˆ˜ì¤€ì„ ë§Œì¡±í•˜ê³  ìˆìŠµë‹ˆë‹¤.")
            recommendations.append("ì§€ì†ì ì¸ ëª¨ë‹ˆí„°ë§ì„ í†µí•œ ì„±ëŠ¥ ìœ ì§€ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.")
        
        return recommendations

class ABTestManager:
    """A/B í…ŒìŠ¤íŠ¸ ê´€ë¦¬ì"""
    
    def __init__(self):
        self.active_tests = {}
        self.completed_tests = []
        self.statistical_confidence = 0.95
    
    async def create_ab_test(self, test_name: str, model_a: str, model_b: str,
                           test_cases: List[TestCase], 
                           duration_hours: int = 24) -> str:
        """A/B í…ŒìŠ¤íŠ¸ ìƒì„±"""
        
        test_id = f"AB_{test_name}_{int(time.time())}"
        
        ab_test = {
            "test_id": test_id,
            "name": test_name,
            "model_a": model_a,
            "model_b": model_b,
            "test_cases": test_cases,
            "duration": timedelta(hours=duration_hours),
            "start_time": datetime.now(),
            "end_time": datetime.now() + timedelta(hours=duration_hours),
            "results_a": [],
            "results_b": [],
            "status": "running"
        }
        
        self.active_tests[test_id] = ab_test
        
        logger.info(f"A/B í…ŒìŠ¤íŠ¸ ì‹œì‘: {test_name} ({model_a} vs {model_b})")
        
        return test_id
    
    async def execute_ab_test(self, test_id: str, 
                            execution_func: Callable) -> ABTestResult:
        """A/B í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        
        if test_id not in self.active_tests:
            raise ValueError(f"ì¡´ì¬í•˜ì§€ ì•ŠëŠ” í…ŒìŠ¤íŠ¸: {test_id}")
        
        test_config = self.active_tests[test_id]
        
        logger.info(f"A/B í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘: {test_config['name']}")
        
        # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¥¼ ë‘ ê·¸ë£¹ìœ¼ë¡œ ë¶„í• 
        test_cases = test_config["test_cases"]
        random.shuffle(test_cases)
        
        mid_point = len(test_cases) // 2
        cases_a = test_cases[:mid_point]
        cases_b = test_cases[mid_point:]
        
        # ë™ì‹œ ì‹¤í–‰
        results_a = []
        results_b = []
        
        # ëª¨ë¸ A í…ŒìŠ¤íŠ¸
        for case in cases_a:
            try:
                result = await execution_func(test_config["model_a"], case)
                results_a.append(result)
            except Exception as e:
                logger.error(f"ëª¨ë¸ A í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        # ëª¨ë¸ B í…ŒìŠ¤íŠ¸
        for case in cases_b:
            try:
                result = await execution_func(test_config["model_b"], case)
                results_b.append(result)
            except Exception as e:
                logger.error(f"ëª¨ë¸ B í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        # ê²°ê³¼ ë¶„ì„
        ab_result = self._analyze_ab_test_results(
            test_config, results_a, results_b
        )
        
        # í…ŒìŠ¤íŠ¸ ì™„ë£Œ ì²˜ë¦¬
        test_config["status"] = "completed"
        test_config["results_a"] = results_a
        test_config["results_b"] = results_b
        
        self.completed_tests.append(ab_result)
        del self.active_tests[test_id]
        
        logger.info(f"A/B í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {test_config['name']}")
        
        return ab_result
    
    def _analyze_ab_test_results(self, test_config: Dict[str, Any],
                                results_a: List[BenchmarkResult],
                                results_b: List[BenchmarkResult]) -> ABTestResult:
        """A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„"""
        
        # ë©”íŠ¸ë¦­ë³„ í‰ê·  ê³„ì‚°
        metrics_a = self._calculate_average_metrics(results_a)
        metrics_b = self._calculate_average_metrics(results_b)
        
        # í†µê³„ì  ìœ ì˜ì„± ê²€ì • (ë‹¨ìˆœí™”ëœ ë²„ì „)
        significance_results = self._test_statistical_significance(
            results_a, results_b
        )
        
        # ìŠ¹ì ê²°ì •
        winner = None
        improvement = 0.0
        
        # ì£¼ìš” ì§€í‘œ (ì •í™•ë„) ê¸°ì¤€ìœ¼ë¡œ ìŠ¹ì ê²°ì •
        if MetricType.ACCURACY_SCORE in metrics_a and MetricType.ACCURACY_SCORE in metrics_b:
            accuracy_a = metrics_a[MetricType.ACCURACY_SCORE]
            accuracy_b = metrics_b[MetricType.ACCURACY_SCORE]
            
            if accuracy_a > accuracy_b:
                winner = test_config["model_a"]
                improvement = ((accuracy_a - accuracy_b) / accuracy_b) * 100
            else:
                winner = test_config["model_b"]
                improvement = ((accuracy_b - accuracy_a) / accuracy_a) * 100
        
        # ê¶Œì¥ì‚¬í•­ ìƒì„±
        recommendation = self._generate_ab_recommendation(
            winner, improvement, significance_results
        )
        
        return ABTestResult(
            test_name=test_config["name"],
            model_a=test_config["model_a"],
            model_b=test_config["model_b"],
            model_a_metrics=metrics_a,
            model_b_metrics=metrics_b,
            statistical_significance=significance_results["significant"],
            confidence_level=significance_results["confidence"],
            p_value=significance_results["p_value"],
            winner=winner,
            improvement_percentage=improvement,
            recommendation=recommendation,
            test_duration=test_config["duration"],
            sample_size=len(results_a) + len(results_b)
        )
    
    def _calculate_average_metrics(self, results: List[BenchmarkResult]) -> Dict[MetricType, float]:
        """í‰ê·  ë©”íŠ¸ë¦­ ê³„ì‚°"""
        
        metric_sums = defaultdict(list)
        
        for result in results:
            for metric_type, metric in result.metrics.items():
                metric_sums[metric_type].append(metric.value)
        
        averages = {}
        for metric_type, values in metric_sums.items():
            if values:
                averages[metric_type] = statistics.mean(values)
        
        return averages
    
    def _test_statistical_significance(self, results_a: List[BenchmarkResult],
                                     results_b: List[BenchmarkResult]) -> Dict[str, Any]:
        """í†µê³„ì  ìœ ì˜ì„± ê²€ì • (ë‹¨ìˆœí™”ëœ t-test)"""
        
        # ì •í™•ë„ ì ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê²€ì •
        scores_a = []
        scores_b = []
        
        for result in results_a:
            if MetricType.ACCURACY_SCORE in result.metrics:
                scores_a.append(result.metrics[MetricType.ACCURACY_SCORE].value)
        
        for result in results_b:
            if MetricType.ACCURACY_SCORE in result.metrics:
                scores_b.append(result.metrics[MetricType.ACCURACY_SCORE].value)
        
        if len(scores_a) < 2 or len(scores_b) < 2:
            return {
                "significant": False,
                "confidence": 0.0,
                "p_value": 1.0,
                "reason": "ìƒ˜í”Œ í¬ê¸° ë¶€ì¡±"
            }
        
        # ë‹¨ìˆœí™”ëœ t-test
        mean_a = statistics.mean(scores_a)
        mean_b = statistics.mean(scores_b)
        
        std_a = statistics.stdev(scores_a) if len(scores_a) > 1 else 0
        std_b = statistics.stdev(scores_b) if len(scores_b) > 1 else 0
        
        # íš¨ê³¼ í¬ê¸° ê³„ì‚°
        effect_size = abs(mean_a - mean_b) / max(std_a, std_b, 0.001)
        
        # ë‹¨ìˆœí™”ëœ ìœ ì˜ì„± íŒì •
        significant = effect_size > 0.5 and abs(mean_a - mean_b) > 0.02
        confidence = min(0.95, effect_size / 2.0)
        p_value = max(0.01, 1.0 - confidence)
        
        return {
            "significant": significant,
            "confidence": confidence,
            "p_value": p_value,
            "effect_size": effect_size
        }
    
    def _generate_ab_recommendation(self, winner: Optional[str], 
                                  improvement: float,
                                  significance: Dict[str, Any]) -> str:
        """A/B í…ŒìŠ¤íŠ¸ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        
        if not significance["significant"]:
            return "í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ì°¨ì´ê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì¶”ê°€ í…ŒìŠ¤íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤."
        
        if winner and improvement > 5.0:
            return f"{winner} ëª¨ë¸ ì±„íƒ ê¶Œì¥ (ì„±ëŠ¥ ê°œì„ : {improvement:.1f}%)"
        elif improvement > 0:
            return f"{winner} ëª¨ë¸ì´ ì•½ê°„ ìš°ìˆ˜í•˜ë‚˜, ì¶”ê°€ ê²€ì¦ ê¶Œì¥"
        else:
            return "ë‘ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ìœ ì‚¬í•©ë‹ˆë‹¤. ë‹¤ë¥¸ ìš”ì†Œë¥¼ ê³ ë ¤í•˜ì—¬ ì„ íƒí•˜ì„¸ìš”."

class AIBenchmarkSystemV23:
    """AI ë²¤ì¹˜ë§ˆí¬ ì‹œìŠ¤í…œ v2.3"""
    
    def __init__(self):
        self.version = "2.3.0"
        self.target_accuracy = 0.992  # 99.2%
        
        # í•µì‹¬ ì»´í¬ë„ŒíŠ¸
        self.test_generator = TestCaseGenerator()
        self.performance_analyzer = PerformanceAnalyzer()
        self.ab_test_manager = ABTestManager()
        
        # ê²°ê³¼ ì €ì¥ì†Œ
        self.benchmark_history = deque(maxlen=10000)
        self.performance_trends = defaultdict(deque)
        
        # ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
        self.monitoring_active = False
        self.monitoring_interval = 60  # ì´ˆ
        
        # ì„±ëŠ¥ í†µê³„
        self.system_stats = {
            "total_benchmarks": 0,
            "total_ab_tests": 0,
            "models_tested": set(),
            "scenarios_covered": set(),
            "achievement_history": []
        }
        
        logger.info(f"ğŸ“Š AI ë²¤ì¹˜ë§ˆí¬ ì‹œìŠ¤í…œ v{self.version} ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"ğŸ¯ ëª©í‘œ ì •í™•ë„: {self.target_accuracy * 100}%")
    
    async def run_comprehensive_benchmark(self, 
                                        models: List[str],
                                        test_count: int = 50,
                                        scenarios: Optional[List[TestScenario]] = None) -> Dict[str, Any]:
        """ì¢…í•© ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        
        start_time = time.time()
        
        logger.info(f"ğŸ“Š ì¢…í•© ë²¤ì¹˜ë§ˆí¬ ì‹œì‘: {len(models)}ê°œ ëª¨ë¸, {test_count}ê°œ í…ŒìŠ¤íŠ¸")
        
        # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìƒì„±
        test_cases = self.test_generator.generate_test_cases(test_count, scenarios)
        
        # ëª¨ë¸ë³„ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        all_results = []
        model_summaries = {}
        
        for model in models:
            logger.info(f"ğŸ” ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì¤‘: {model}")
            
            model_results = await self._execute_model_benchmark(model, test_cases)
            all_results.extend(model_results)
            
            # ëª¨ë¸ë³„ ìš”ì•½
            model_summary = self._summarize_model_performance(model_results)
            model_summaries[model] = model_summary
            
            # ì‹œìŠ¤í…œ í†µê³„ ì—…ë°ì´íŠ¸
            self.system_stats["models_tested"].add(model)
        
        # ì „ì²´ ì„±ëŠ¥ ë¶„ì„
        overall_analysis = self.performance_analyzer.analyze_performance(all_results)
        
        # ëª¨ë¸ ìˆœìœ„
        model_rankings = self._rank_models(model_summaries)
        
        # 99.2% ëª©í‘œ ë‹¬ì„±ë„ ë¶„ì„
        target_analysis = self._analyze_target_achievement(model_summaries)
        
        # ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì €ì¥
        benchmark_result = {
            "benchmark_id": f"COMP_{int(time.time())}",
            "timestamp": datetime.now().isoformat(),
            "execution_time": time.time() - start_time,
            "models_tested": models,
            "test_cases_count": test_count,
            "scenarios": [s.value for s in (scenarios or list(TestScenario))],
            
            "model_summaries": model_summaries,
            "model_rankings": model_rankings,
            "overall_analysis": overall_analysis,
            "target_achievement": target_analysis,
            
            "recommendations": self._generate_benchmark_recommendations(
                model_rankings, target_analysis
            )
        }
        
        # ê²°ê³¼ ì €ì¥
        self.benchmark_history.append(benchmark_result)
        self.system_stats["total_benchmarks"] += 1
        
        execution_time = time.time() - start_time
        logger.info(f"âœ… ì¢…í•© ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ ({execution_time:.2f}ì´ˆ)")
        
        return benchmark_result
    
    async def _execute_model_benchmark(self, model: str, 
                                     test_cases: List[TestCase]) -> List[BenchmarkResult]:
        """ê°œë³„ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        
        results = []
        
        for test_case in test_cases:
            try:
                # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ëª¨ë¸ ì‹¤í–‰ ë¡œì§ì´ ë“¤ì–´ê°
                # ì—¬ê¸°ì„œëŠ” ì‹œë®¬ë ˆì´ì…˜
                result = await self._simulate_model_execution(model, test_case)
                results.append(result)
                
            except Exception as e:
                logger.error(f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨ {model} - {test_case.id}: {e}")
                
                # ì‹¤íŒ¨ ê²°ê³¼ ìƒì„±
                error_result = BenchmarkResult(
                    test_id=test_case.id,
                    model_name=model,
                    scenario=test_case.scenario,
                    metrics={},
                    overall_score=0.0,
                    execution_time=0.0,
                    success=False,
                    error_message=str(e)
                )
                results.append(error_result)
        
        return results
    
    async def _simulate_model_execution(self, model: str, 
                                      test_case: TestCase) -> BenchmarkResult:
        """ëª¨ë¸ ì‹¤í–‰ ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì‹¤ì œ ëª¨ë¸ í˜¸ì¶œ)"""
        
        start_time = time.time()
        
        # ëª¨ë¸ë³„ ì„±ëŠ¥ íŠ¹ì„± ì‹œë®¬ë ˆì´ì…˜
        model_characteristics = {
            "jewelry_specialized_v22": {
                "accuracy_base": 0.95,
                "speed_factor": 1.2,
                "jewelry_expertise": 0.98,
                "cost_factor": 0.8
            },
            "gpt4v": {
                "accuracy_base": 0.92,
                "speed_factor": 0.8,
                "jewelry_expertise": 0.75,
                "cost_factor": 1.5
            },
            "claude_vision": {
                "accuracy_base": 0.90,
                "speed_factor": 1.0,
                "jewelry_expertise": 0.78,
                "cost_factor": 1.2
            },
            "gemini_2.0": {
                "accuracy_base": 0.88,
                "speed_factor": 1.8,
                "jewelry_expertise": 0.70,
                "cost_factor": 0.5
            }
        }
        
        char = model_characteristics.get(model, {
            "accuracy_base": 0.85,
            "speed_factor": 1.0,
            "jewelry_expertise": 0.65,
            "cost_factor": 1.0
        })
        
        # ì‹œë®¬ë ˆì´ì…˜ëœ ì²˜ë¦¬ ì‹œê°„
        base_time = 5.0 + test_case.difficulty_level * 10.0
        actual_time = base_time / char["speed_factor"]
        actual_time += random.uniform(-2.0, 2.0)  # ëœë¤ ë³€ë™
        actual_time = max(1.0, actual_time)
        
        await asyncio.sleep(actual_time / 100)  # ì‹œë®¬ë ˆì´ì…˜ìš© ë‹¨ì¶•
        
        # ì„±ëŠ¥ ì§€í‘œ ì‹œë®¬ë ˆì´ì…˜
        difficulty_penalty = test_case.difficulty_level * 0.1
        
        # ì •í™•ë„ ì ìˆ˜
        accuracy = char["accuracy_base"] - difficulty_penalty
        accuracy += random.uniform(-0.05, 0.05)  # ëœë¤ ë³€ë™
        accuracy = max(0.0, min(1.0, accuracy))
        
        # ì£¼ì–¼ë¦¬ ì „ë¬¸ì„±
        jewelry_relevance = char["jewelry_expertise"] - difficulty_penalty * 0.5
        jewelry_relevance += random.uniform(-0.03, 0.03)
        jewelry_relevance = max(0.0, min(1.0, jewelry_relevance))
        
        # ë¹„ìš©
        base_cost = 0.02
        cost = base_cost * char["cost_factor"] * (1 + test_case.difficulty_level)
        
        # í’ˆì§ˆ ì¼ê´€ì„±
        consistency = accuracy * 0.95 + random.uniform(-0.02, 0.02)
        consistency = max(0.0, min(1.0, consistency))
        
        # ì—ëŸ¬ìœ¨
        error_rate = (1 - accuracy) * 0.1 + random.uniform(0.0, 0.01)
        error_rate = max(0.0, min(0.1, error_rate))
        
        # ì „ì²´ ì ìˆ˜ ê³„ì‚°
        overall_score = (
            accuracy * 0.4 +
            jewelry_relevance * 0.3 +
            consistency * 0.2 +
            (1 - error_rate) * 0.1
        )
        
        # ë©”íŠ¸ë¦­ ìƒì„±
        metrics = {
            MetricType.ACCURACY_SCORE: BenchmarkMetric(
                MetricType.ACCURACY_SCORE, accuracy, "score"
            ),
            MetricType.RESPONSE_TIME: BenchmarkMetric(
                MetricType.RESPONSE_TIME, actual_time, "seconds"
            ),
            MetricType.COST_PER_REQUEST: BenchmarkMetric(
                MetricType.COST_PER_REQUEST, cost, "USD"
            ),
            MetricType.JEWELRY_RELEVANCE: BenchmarkMetric(
                MetricType.JEWELRY_RELEVANCE, jewelry_relevance, "score"
            ),
            MetricType.QUALITY_CONSISTENCY: BenchmarkMetric(
                MetricType.QUALITY_CONSISTENCY, consistency, "score"
            ),
            MetricType.ERROR_RATE: BenchmarkMetric(
                MetricType.ERROR_RATE, error_rate, "rate"
            )
        }
        
        execution_time = time.time() - start_time
        
        return BenchmarkResult(
            test_id=test_case.id,
            model_name=model,
            scenario=test_case.scenario,
            metrics=metrics,
            overall_score=overall_score,
            execution_time=execution_time,
            success=True,
            quality_analysis={
                "difficulty_level": test_case.difficulty_level,
                "scenario": test_case.scenario.value
            }
        )
    
    def _summarize_model_performance(self, results: List[BenchmarkResult]) -> Dict[str, Any]:
        """ëª¨ë¸ ì„±ëŠ¥ ìš”ì•½"""
        
        if not results:
            return {"error": "ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤"}
        
        # ê¸°ë³¸ í†µê³„
        success_results = [r for r in results if r.success]
        success_rate = len(success_results) / len(results)
        
        if not success_results:
            return {
                "success_rate": success_rate,
                "overall_score": 0.0,
                "error": "ì„±ê³µí•œ í…ŒìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤"
            }
        
        # ë©”íŠ¸ë¦­ë³„ í‰ê· 
        metric_averages = {}
        metric_aggregates = defaultdict(list)
        
        for result in success_results:
            for metric_type, metric in result.metrics.items():
                metric_aggregates[metric_type].append(metric.value)
        
        for metric_type, values in metric_aggregates.items():
            metric_averages[metric_type.value] = {
                "average": statistics.mean(values),
                "std_dev": statistics.stdev(values) if len(values) > 1 else 0,
                "min": min(values),
                "max": max(values)
            }
        
        # ì „ì²´ í‰ê·  ì ìˆ˜
        overall_scores = [r.overall_score for r in success_results]
        avg_overall_score = statistics.mean(overall_scores)
        
        # ì‹œë‚˜ë¦¬ì˜¤ë³„ ì„±ëŠ¥
        scenario_performance = defaultdict(list)
        for result in success_results:
            scenario_performance[result.scenario.value].append(result.overall_score)
        
        scenario_averages = {
            scenario: statistics.mean(scores)
            for scenario, scores in scenario_performance.items()
        }
        
        return {
            "success_rate": success_rate,
            "overall_score": avg_overall_score,
            "total_tests": len(results),
            "successful_tests": len(success_results),
            "metric_averages": metric_averages,
            "scenario_performance": scenario_averages,
            "score_distribution": {
                "mean": avg_overall_score,
                "std_dev": statistics.stdev(overall_scores) if len(overall_scores) > 1 else 0,
                "min": min(overall_scores),
                "max": max(overall_scores)
            }
        }
    
    def _rank_models(self, model_summaries: Dict[str, Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ëª¨ë¸ ìˆœìœ„ ê²°ì •"""
        
        rankings = []
        
        for model, summary in model_summaries.items():
            if "error" in summary:
                continue
            
            # ìˆœìœ„ ì ìˆ˜ ê³„ì‚° (ì •í™•ë„ ìš°ì„ )
            rank_score = summary["overall_score"]
            
            # 99.2% ëª©í‘œ ê·¼ì ‘ë„ ë³´ë„ˆìŠ¤
            accuracy_avg = summary["metric_averages"].get("accuracy_score", {}).get("average", 0)
            if accuracy_avg >= self.target_accuracy:
                rank_score += 0.1  # ëª©í‘œ ë‹¬ì„± ë³´ë„ˆìŠ¤
            
            # ì„±ê³µë¥  ë³´ì •
            rank_score *= summary["success_rate"]
            
            rankings.append({
                "model": model,
                "rank_score": rank_score,
                "overall_score": summary["overall_score"],
                "accuracy": accuracy_avg,
                "success_rate": summary["success_rate"]
            })
        
        # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
        rankings.sort(key=lambda x: x["rank_score"], reverse=True)
        
        # ìˆœìœ„ ë²ˆí˜¸ ì¶”ê°€
        for i, ranking in enumerate(rankings):
            ranking["rank"] = i + 1
        
        return rankings
    
    def _analyze_target_achievement(self, model_summaries: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
        """99.2% ëª©í‘œ ë‹¬ì„±ë„ ë¶„ì„"""
        
        analysis = {
            "target_accuracy": self.target_accuracy,
            "achievement_status": {},
            "best_performing_model": None,
            "gap_analysis": {},
            "recommendations": []
        }
        
        best_accuracy = 0.0
        best_model = None
        
        for model, summary in model_summaries.items():
            if "error" in summary:
                continue
            
            accuracy = summary["metric_averages"].get("accuracy_score", {}).get("average", 0)
            
            # ëª©í‘œ ë‹¬ì„± ì—¬ë¶€
            achieved = accuracy >= self.target_accuracy
            gap = self.target_accuracy - accuracy if not achieved else 0
            
            analysis["achievement_status"][model] = {
                "accuracy": accuracy,
                "target_achieved": achieved,
                "gap": gap,
                "gap_percentage": (gap / self.target_accuracy * 100) if gap > 0 else 0
            }
            
            # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì¶”ì 
            if accuracy > best_accuracy:
                best_accuracy = accuracy
                best_model = model
        
        analysis["best_performing_model"] = best_model
        analysis["best_accuracy"] = best_accuracy
        
        # ì „ì²´ ê¶Œì¥ì‚¬í•­
        if best_accuracy >= self.target_accuracy:
            analysis["recommendations"].append(f"âœ… ëª©í‘œ ë‹¬ì„±! {best_model}ì´ 99.2% ì •í™•ë„ ê¸°ì¤€ì„ ë§Œì¡±í•©ë‹ˆë‹¤.")
        else:
            remaining_gap = self.target_accuracy - best_accuracy
            analysis["recommendations"].append(
                f"ğŸ¯ ëª©í‘œê¹Œì§€ {remaining_gap:.1%} ì¶”ê°€ ê°œì„  í•„ìš” (ìµœê³ : {best_model})"
            )
        
        return analysis
    
    def _generate_benchmark_recommendations(self, rankings: List[Dict[str, Any]], 
                                          target_analysis: Dict[str, Any]) -> List[str]:
        """ë²¤ì¹˜ë§ˆí¬ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        
        recommendations = []
        
        if not rankings:
            return ["ëª¨ë¸ í…ŒìŠ¤íŠ¸ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."]
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ê¶Œì¥
        best_model = rankings[0]
        recommendations.append(
            f"ğŸ¥‡ ìµœê³  ì„±ëŠ¥: {best_model['model']} (ì „ì²´ ì ìˆ˜: {best_model['overall_score']:.3f})"
        )
        
        # 99.2% ëª©í‘œ ê´€ë ¨ ê¶Œì¥
        if target_analysis.get("best_accuracy", 0) >= self.target_accuracy:
            recommendations.append("âœ… 99.2% ì •í™•ë„ ëª©í‘œ ë‹¬ì„± ê°€ëŠ¥í•œ ëª¨ë¸ í™•ì¸ë¨")
        else:
            gap = self.target_accuracy - target_analysis.get("best_accuracy", 0)
            recommendations.append(f"âš¡ 99.2% ëª©í‘œê¹Œì§€ {gap:.1%} ì¶”ê°€ ìµœì í™” í•„ìš”")
        
        # ì„±ëŠ¥ ê²©ì°¨ ë¶„ì„
        if len(rankings) >= 2:
            score_gap = rankings[0]["rank_score"] - rankings[1]["rank_score"]
            if score_gap > 0.1:
                recommendations.append("ğŸ“Š ëª¨ë¸ ê°„ ì„±ëŠ¥ ê²©ì°¨ê°€ í¼ - ìµœê³  ëª¨ë¸ ìš°ì„  ì‚¬ìš© ê¶Œì¥")
            else:
                recommendations.append("âš–ï¸ ëª¨ë¸ ê°„ ì„±ëŠ¥ì´ ë¹„ìŠ·í•¨ - ë¹„ìš©/ì†ë„ ë“± ì¶”ê°€ ìš”ì†Œ ê³ ë ¤")
        
        # ê°œì„  ìš°ì„ ìˆœìœ„
        improvement_areas = []
        
        # ì •í™•ë„ê°€ ë‚®ì€ ëª¨ë¸ë“¤ ì‹ë³„
        low_accuracy_models = [
            r["model"] for r in rankings
            if r.get("accuracy", 0) < 0.90
        ]
        
        if low_accuracy_models:
            improvement_areas.append(f"ì •í™•ë„ ê°œì„  í•„ìš”: {', '.join(low_accuracy_models)}")
        
        if improvement_areas:
            recommendations.extend(improvement_areas)
        
        # ì¼ë°˜ ê¶Œì¥ì‚¬í•­
        recommendations.append("ğŸ”„ ì •ê¸°ì  ë²¤ì¹˜ë§ˆí¬ë¥¼ í†µí•œ ì§€ì†ì  ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ê¶Œì¥")
        
        return recommendations
    
    async def run_ab_test(self, test_name: str, model_a: str, model_b: str,
                         test_count: int = 30,
                         scenarios: Optional[List[TestScenario]] = None) -> ABTestResult:
        """A/B í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        
        logger.info(f"ğŸ§ª A/B í…ŒìŠ¤íŠ¸ ì‹œì‘: {model_a} vs {model_b}")
        
        # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìƒì„±
        test_cases = self.test_generator.generate_test_cases(test_count, scenarios)
        
        # A/B í…ŒìŠ¤íŠ¸ ìƒì„±
        test_id = await self.ab_test_manager.create_ab_test(
            test_name, model_a, model_b, test_cases
        )
        
        # ì‹¤í–‰ í•¨ìˆ˜ ì •ì˜
        async def execute_model(model: str, test_case: TestCase) -> BenchmarkResult:
            return await self._simulate_model_execution(model, test_case)
        
        # A/B í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        ab_result = await self.ab_test_manager.execute_ab_test(test_id, execute_model)
        
        # ì‹œìŠ¤í…œ í†µê³„ ì—…ë°ì´íŠ¸
        self.system_stats["total_ab_tests"] += 1
        self.system_stats["models_tested"].add(model_a)
        self.system_stats["models_tested"].add(model_b)
        
        logger.info(f"âœ… A/B í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {ab_result.winner or 'ë¬´ìŠ¹ë¶€'}")
        
        return ab_result
    
    async def start_real_time_monitoring(self, models: List[str],
                                       monitoring_interval: int = 300):
        """ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        
        self.monitoring_active = True
        self.monitoring_interval = monitoring_interval
        
        logger.info(f"ğŸ‘ï¸ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œì‘: {len(models)}ê°œ ëª¨ë¸, {monitoring_interval}ì´ˆ ê°„ê²©")
        
        while self.monitoring_active:
            try:
                # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¡œ ëª¨ë‹ˆí„°ë§
                test_cases = self.test_generator.generate_test_cases(5)
                
                for model in models:
                    results = await self._execute_model_benchmark(model, test_cases)
                    
                    # ì„±ëŠ¥ íŠ¸ë Œë“œ ì—…ë°ì´íŠ¸
                    if results:
                        avg_score = statistics.mean(r.overall_score for r in results if r.success)
                        self.performance_trends[model].append({
                            "timestamp": datetime.now(),
                            "score": avg_score
                        })
                        
                        # ìµœê·¼ ë°ì´í„°ë§Œ ìœ ì§€ (ìµœëŒ€ 100ê°œ)
                        if len(self.performance_trends[model]) > 100:
                            self.performance_trends[model].popleft()
                
                await asyncio.sleep(monitoring_interval)
                
            except Exception as e:
                logger.error(f"ëª¨ë‹ˆí„°ë§ ì¤‘ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(60)  # ì˜¤ë¥˜ ì‹œ 1ë¶„ ëŒ€ê¸°
    
    def stop_real_time_monitoring(self):
        """ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        
        self.monitoring_active = False
        logger.info("ğŸ‘ï¸ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€ë¨")
    
    def get_comprehensive_report(self) -> Dict[str, Any]:
        """ì¢…í•© ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±"""
        
        report = {
            "system_info": {
                "version": self.version,
                "target_accuracy": f"{self.target_accuracy * 100}%",
                "total_benchmarks": self.system_stats["total_benchmarks"],
                "total_ab_tests": self.system_stats["total_ab_tests"],
                "models_tested": len(self.system_stats["models_tested"]),
                "scenarios_covered": len(self.system_stats["scenarios_covered"])
            },
            
            "recent_performance": {},
            "trending_analysis": {},
            "achievement_summary": {},
            
            "recommendations": [
                "ì •ê¸°ì  ì¢…í•© ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰",
                "A/B í…ŒìŠ¤íŠ¸ë¥¼ í†µí•œ ëª¨ë¸ ë¹„êµ",
                "ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ìœ¼ë¡œ ì„±ëŠ¥ ì¶”ì "
            ]
        }
        
        # ìµœê·¼ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìš”ì•½
        if self.benchmark_history:
            latest_benchmark = self.benchmark_history[-1]
            report["recent_performance"] = {
                "timestamp": latest_benchmark["timestamp"],
                "best_model": latest_benchmark["model_rankings"][0]["model"] if latest_benchmark["model_rankings"] else "ì—†ìŒ",
                "target_achievement": latest_benchmark["target_achievement"].get("best_accuracy", 0) >= self.target_accuracy
            }
        
        # ì„±ëŠ¥ íŠ¸ë Œë“œ ë¶„ì„
        for model, trend_data in self.performance_trends.items():
            if len(trend_data) >= 3:
                recent_scores = [d["score"] for d in list(trend_data)[-10:]]
                trend_direction = "ìƒìŠ¹" if recent_scores[-1] > recent_scores[0] else "í•˜ë½"
                
                report["trending_analysis"][model] = {
                    "direction": trend_direction,
                    "current_score": recent_scores[-1],
                    "score_change": recent_scores[-1] - recent_scores[0]
                }
        
        return report
    
    async def export_benchmark_data(self, filepath: str, format: str = "json"):
        """ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ë‚´ë³´ë‚´ê¸°"""
        
        export_data = {
            "export_info": {
                "timestamp": datetime.now().isoformat(),
                "version": self.version,
                "format": format
            },
            "system_stats": {
                **self.system_stats,
                "models_tested": list(self.system_stats["models_tested"]),
                "scenarios_covered": list(self.system_stats["scenarios_covered"])
            },
            "benchmark_history": list(self.benchmark_history),
            "performance_trends": {
                model: list(trend) for model, trend in self.performance_trends.items()
            }
        }
        
        if format.lower() == "json":
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, ensure_ascii=False, indent=2, default=str)
        
        elif format.lower() == "csv":
            # CSV í˜•íƒœë¡œ ì£¼ìš” ë©”íŠ¸ë¦­ë§Œ ë‚´ë³´ë‚´ê¸°
            with open(filepath, 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                
                # í—¤ë”
                writer.writerow([
                    "timestamp", "benchmark_id", "model", "overall_score", 
                    "accuracy", "response_time", "success_rate"
                ])
                
                # ë°ì´í„°
                for benchmark in self.benchmark_history:
                    for model, summary in benchmark.get("model_summaries", {}).items():
                        if "error" not in summary:
                            writer.writerow([
                                benchmark["timestamp"],
                                benchmark["benchmark_id"],
                                model,
                                summary["overall_score"],
                                summary["metric_averages"].get("accuracy_score", {}).get("average", 0),
                                summary["metric_averages"].get("response_time", {}).get("average", 0),
                                summary["success_rate"]
                            ])
        
        logger.info(f"ğŸ“„ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {filepath}")

# í…ŒìŠ¤íŠ¸ ë° ë°ëª¨ í•¨ìˆ˜ë“¤

async def test_benchmark_system_v23():
    """AI ë²¤ì¹˜ë§ˆí¬ ì‹œìŠ¤í…œ v2.3 í…ŒìŠ¤íŠ¸"""
    
    print("ğŸ“Š ì†”ë¡œëª¬ë“œ AI ë²¤ì¹˜ë§ˆí¬ ì‹œìŠ¤í…œ v2.3 í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    benchmark_system = AIBenchmarkSystemV23()
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 1: ì¢…í•© ë²¤ì¹˜ë§ˆí¬
    print("\nğŸ”¹ í…ŒìŠ¤íŠ¸ 1: ì¢…í•© ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰")
    
    test_models = [
        "jewelry_specialized_v22",
        "gpt4v",
        "claude_vision",
        "gemini_2.0"
    ]
    
    benchmark_result = await benchmark_system.run_comprehensive_benchmark(
        models=test_models,
        test_count=20,
        scenarios=[TestScenario.DIAMOND_4C_ANALYSIS, TestScenario.RUBY_GRADING]
    )
    
    print(f"ì‹¤í–‰ ì‹œê°„: {benchmark_result['execution_time']:.2f}ì´ˆ")
    print(f"í…ŒìŠ¤íŠ¸ëœ ëª¨ë¸: {len(benchmark_result['models_tested'])}ê°œ")
    print(f"í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤: {benchmark_result['test_cases_count']}ê°œ")
    
    print(f"\nğŸ† ëª¨ë¸ ìˆœìœ„:")
    for ranking in benchmark_result['model_rankings'][:3]:
        print(f"  {ranking['rank']}. {ranking['model']} - ì ìˆ˜: {ranking['rank_score']:.3f}")
    
    print(f"\nğŸ¯ 99.2% ëª©í‘œ ë‹¬ì„±ë„:")
    target_analysis = benchmark_result['target_achievement']
    print(f"ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {target_analysis['best_performing_model']}")
    print(f"ìµœê³  ì •í™•ë„: {target_analysis['best_accuracy']:.1%}")
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 2: A/B í…ŒìŠ¤íŠ¸
    print("\nğŸ”¹ í…ŒìŠ¤íŠ¸ 2: A/B í…ŒìŠ¤íŠ¸ ì‹¤í–‰")
    
    ab_result = await benchmark_system.run_ab_test(
        test_name="ì „ë¬¸ì„±_vs_ë²”ìš©ì„±",
        model_a="jewelry_specialized_v22",
        model_b="gpt4v",
        test_count=15
    )
    
    print(f"í…ŒìŠ¤íŠ¸ ëª¨ë¸: {ab_result.model_a} vs {ab_result.model_b}")
    print(f"ìƒ˜í”Œ í¬ê¸°: {ab_result.sample_size}")
    print(f"í†µê³„ì  ìœ ì˜ì„±: {'ìœ ì˜í•¨' if ab_result.statistical_significance else 'ìœ ì˜í•˜ì§€ ì•ŠìŒ'}")
    print(f"ìŠ¹ì: {ab_result.winner or 'ë¬´ìŠ¹ë¶€'}")
    
    if ab_result.winner:
        print(f"ì„±ëŠ¥ ê°œì„ : {ab_result.improvement_percentage:.1f}%")
    
    print(f"ê¶Œì¥ì‚¬í•­: {ab_result.recommendation}")
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 3: ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ (ì§§ì€ ì‹œê°„)
    print("\nğŸ”¹ í…ŒìŠ¤íŠ¸ 3: ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ (5ì´ˆ ì‹œì—°)")
    
    # ëª¨ë‹ˆí„°ë§ ì‹œì‘ (ë°±ê·¸ë¼ìš´ë“œ)
    monitoring_task = asyncio.create_task(
        benchmark_system.start_real_time_monitoring(
            models=["jewelry_specialized_v22", "gpt4v"],
            monitoring_interval=2
        )
    )
    
    # 5ì´ˆ ëŒ€ê¸°
    await asyncio.sleep(5)
    
    # ëª¨ë‹ˆí„°ë§ ì¤‘ì§€
    benchmark_system.stop_real_time_monitoring()
    
    try:
        await asyncio.wait_for(monitoring_task, timeout=1.0)
    except asyncio.TimeoutError:
        monitoring_task.cancel()
    
    print("âœ… ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    
    # ì¢…í•© ë¦¬í¬íŠ¸
    print("\nğŸ“ˆ ì¢…í•© ì„±ëŠ¥ ë¦¬í¬íŠ¸:")
    comprehensive_report = benchmark_system.get_comprehensive_report()
    
    print(f"ì‹œìŠ¤í…œ ë²„ì „: {comprehensive_report['system_info']['version']}")
    print(f"ëª©í‘œ ì •í™•ë„: {comprehensive_report['system_info']['target_accuracy']}")
    print(f"ì´ ë²¤ì¹˜ë§ˆí¬: {comprehensive_report['system_info']['total_benchmarks']}íšŒ")
    print(f"ì´ A/B í…ŒìŠ¤íŠ¸: {comprehensive_report['system_info']['total_ab_tests']}íšŒ")
    print(f"í…ŒìŠ¤íŠ¸ëœ ëª¨ë¸: {comprehensive_report['system_info']['models_tested']}ê°œ")
    
    if comprehensive_report.get('recent_performance'):
        recent = comprehensive_report['recent_performance']
        print(f"\nìµœê·¼ ì„±ëŠ¥:")
        print(f"  ìµœê³  ëª¨ë¸: {recent['best_model']}")
        print(f"  ëª©í‘œ ë‹¬ì„±: {'ë‹¬ì„±' if recent['target_achievement'] else 'ë¯¸ë‹¬ì„±'}")
    
    print(f"\nğŸ“‹ ê¶Œì¥ì‚¬í•­:")
    for recommendation in comprehensive_report['recommendations']:
        print(f"  â€¢ {recommendation}")
    
    # ë°ì´í„° ë‚´ë³´ë‚´ê¸° í…ŒìŠ¤íŠ¸
    print(f"\nğŸ’¾ ë°ì´í„° ë‚´ë³´ë‚´ê¸° í…ŒìŠ¤íŠ¸:")
    
    export_path = f"benchmark_data_{int(time.time())}.json"
    await benchmark_system.export_benchmark_data(export_path, "json")
    print(f"JSON ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {export_path}")
    
    print("\n" + "=" * 60)
    print("âœ… AI ë²¤ì¹˜ë§ˆí¬ ì‹œìŠ¤í…œ v2.3 í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    
    return benchmark_system

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    asyncio.run(test_benchmark_system_v23())
