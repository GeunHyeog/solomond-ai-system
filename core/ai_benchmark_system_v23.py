"""
ğŸ“Š ì†”ë¡œëª¬ë“œ AI ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë° A/B í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ v2.3
3ê°œ AI ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ + ìë™ ìµœì í™” + ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§

ê°œë°œì: ì „ê·¼í˜ (ì†”ë¡œëª¬ë“œ ëŒ€í‘œ)
ëª©í‘œ: ë°ì´í„° ê¸°ë°˜ AI ëª¨ë¸ ìµœì í™” ë° 99.2% ì •í™•ë„ ë‹¬ì„±
"""

import asyncio
import time
import json
import statistics
import random
from typing import Dict, List, Optional, Any, Tuple, Union
from dataclasses import dataclass, asdict
from enum import Enum
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
import numpy as np
from datetime import datetime, timedelta
import csv
import os

# ë‚´ë¶€ ëª¨ë“ˆ imports
try:
    from core.hybrid_llm_manager_v23 import HybridLLMManager, AIResponse, AIModel, AnalysisRequest
    from core.jewelry_specialized_prompts_v23 import JewelrySpecializedPrompts, AnalysisType, AIModelType
    from core.ai_quality_validator_v23 import AIQualityValidator, QualityReport
except ImportError as e:
    logging.warning(f"ëª¨ë“ˆ import ê²½ê³ : {e}")

logger = logging.getLogger(__name__)

class BenchmarkMetric(Enum):
    """ë²¤ì¹˜ë§ˆí¬ ì¸¡ì • ë©”íŠ¸ë¦­"""
    ACCURACY = "accuracy"
    RESPONSE_TIME = "response_time"
    COST_EFFICIENCY = "cost_efficiency"
    JEWELRY_EXPERTISE = "jewelry_expertise"
    CONSISTENCY = "consistency"
    USER_SATISFACTION = "user_satisfaction"

class TestScenario(Enum):
    """í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ íƒ€ì…"""
    DIAMOND_APPRAISAL = "diamond_appraisal"
    COLORED_STONE_ANALYSIS = "colored_stone_analysis"
    JEWELRY_DESIGN_REVIEW = "jewelry_design_review"
    BUSINESS_CONSULTATION = "business_consultation"
    MIXED_ANALYSIS = "mixed_analysis"

@dataclass
class BenchmarkResult:
    """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    model: AIModel
    scenario: TestScenario
    accuracy_score: float
    response_time: float
    cost: float
    jewelry_expertise_score: float
    consistency_score: float
    user_satisfaction_score: float
    timestamp: float
    test_id: str
    metadata: Dict[str, Any]

@dataclass
class ABTestResult:
    """A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼"""
    test_name: str
    model_a: AIModel
    model_b: AIModel
    model_c: Optional[AIModel]
    winner: AIModel
    confidence_level: float
    sample_size: int
    performance_improvement: float
    statistical_significance: bool
    detailed_metrics: Dict[str, float]
    recommendations: List[str]

class TestDataGenerator:
    """í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±ê¸°"""
    
    def __init__(self):
        self.test_cases = self._initialize_test_cases()
        self.ground_truth = self._initialize_ground_truth()
    
    def _initialize_test_cases(self) -> Dict[TestScenario, List[Dict[str, Any]]]:
        """ì‹œë‚˜ë¦¬ì˜¤ë³„ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì´ˆê¸°í™”"""
        return {
            TestScenario.DIAMOND_APPRAISAL: [
                {
                    "input": "1.5ìºëŸ¿ ë¼ìš´ë“œ ë‹¤ì´ì•„ëª¬ë“œ, Dì»¬ëŸ¬, VVS1 í´ë˜ë¦¬í‹°, Excellent ì»·, GIA ì¸ì¦ì„œ í¬í•¨",
                    "expected_grade": "Premium",
                    "expected_price_range": (15000, 25000),
                    "key_points": ["Dì»¬ëŸ¬", "VVS1", "Excellent", "GIA"]
                },
                {
                    "input": "0.8ìºëŸ¿ í”„ë¦°ì„¸ìŠ¤ ì»·, Hì»¬ëŸ¬, SI1 í´ë˜ë¦¬í‹°, Very Good ì»·",
                    "expected_grade": "Good",
                    "expected_price_range": (2500, 4000),
                    "key_points": ["í”„ë¦°ì„¸ìŠ¤", "Hì»¬ëŸ¬", "SI1", "Very Good"]
                },
                {
                    "input": "2.2ìºëŸ¿ ì¿ ì…˜ ì»·, Jì»¬ëŸ¬, VS2 í´ë˜ë¦¬í‹°, Good ì»·, í˜•ê´‘ì„± Medium",
                    "expected_grade": "Fair",
                    "expected_price_range": (8000, 12000),
                    "key_points": ["ì¿ ì…˜", "Jì»¬ëŸ¬", "VS2", "í˜•ê´‘ì„±"]
                }
            ],
            TestScenario.COLORED_STONE_ANALYSIS: [
                {
                    "input": "3ìºëŸ¿ ë¯¸ì–€ë§ˆì‚° ë£¨ë¹„, í”¼ì ¼ ë¸”ëŸ¬ë“œ ì»¬ëŸ¬, ê°€ì—´ì²˜ë¦¬, SSEF ì¸ì¦ì„œ",
                    "expected_grade": "Exceptional",
                    "expected_price_range": (30000, 60000),
                    "key_points": ["ë¯¸ì–€ë§ˆ", "í”¼ì ¼ ë¸”ëŸ¬ë“œ", "ê°€ì—´ì²˜ë¦¬", "SSEF"]
                },
                {
                    "input": "2ìºëŸ¿ ìŠ¤ë¦¬ë‘ì¹´ì‚° ì‚¬íŒŒì´ì–´, ì½”ë¥¸í”Œë¼ì›Œ ë¸”ë£¨, ë¬´ì²˜ë¦¬, GÃ¼belin ì¸ì¦",
                    "expected_grade": "Premium", 
                    "expected_price_range": (15000, 30000),
                    "key_points": ["ìŠ¤ë¦¬ë‘ì¹´", "ì½”ë¥¸í”Œë¼ì›Œ", "ë¬´ì²˜ë¦¬", "GÃ¼belin"]
                },
                {
                    "input": "1.5ìºëŸ¿ ì½œë¡¬ë¹„ì•„ì‚° ì—ë©”ë„ë“œ, ë¹„ë¹„ë“œ ê·¸ë¦°, ì˜¤ì¼ë§ ì²˜ë¦¬",
                    "expected_grade": "Good",
                    "expected_price_range": (8000, 15000),
                    "key_points": ["ì½œë¡¬ë¹„ì•„", "ë¹„ë¹„ë“œ ê·¸ë¦°", "ì˜¤ì¼ë§"]
                }
            ],
            TestScenario.JEWELRY_DESIGN_REVIEW: [
                {
                    "input": "Art Deco ìŠ¤íƒ€ì¼ ì—ë©”ë„ë“œ ë¸Œë¡œì¹˜, í”Œë˜í‹°ë‚˜ ì„¸íŒ…, ë‹¤ì´ì•„ëª¬ë“œ ì•¡ì„¼íŠ¸",
                    "expected_grade": "Excellent",
                    "expected_style": "Art Deco",
                    "key_points": ["Art Deco", "ì—ë©”ë„ë“œ", "í”Œë˜í‹°ë‚˜", "ë¸Œë¡œì¹˜"]
                },
                {
                    "input": "ë¹…í† ë¦¬ì•ˆ ìŠ¤íƒ€ì¼ ì§„ì£¼ ëª©ê±¸ì´, 18K ê³¨ë“œ ì²´ì¸, í•¸ë“œë©”ì´ë“œ",
                    "expected_grade": "Premium",
                    "expected_style": "Victorian",
                    "key_points": ["ë¹…í† ë¦¬ì•ˆ", "ì§„ì£¼", "18K", "í•¸ë“œë©”ì´ë“œ"]
                }
            ],
            TestScenario.BUSINESS_CONSULTATION: [
                {
                    "input": "2024ë…„ í•œêµ­ ë¸Œë¼ì´ëœ ì£¼ì–¼ë¦¬ ì‹œì¥ íŠ¸ë Œë“œ ë¶„ì„ ë° íˆ¬ì ì „ëµ",
                    "expected_insights": ["ì‹œì¥ê·œëª¨", "íŠ¸ë Œë“œ", "íˆ¬ìì „ëµ"],
                    "key_points": ["ë¸Œë¼ì´ëœ", "ì‹œì¥ë¶„ì„", "íˆ¬ìì „ëµ"]
                },
                {
                    "input": "ë©ê·¸ë¡œìš´ ë‹¤ì´ì•„ëª¬ë“œ vs ì²œì—° ë‹¤ì´ì•„ëª¬ë“œ ì‹œì¥ ì „ë§ ë° í¬ì§€ì…”ë‹ ì „ëµ",
                    "expected_insights": ["ì‹œì¥ì „ë§", "í¬ì§€ì…”ë‹", "ê²½ìŸë¶„ì„"],
                    "key_points": ["ë©ê·¸ë¡œìš´", "ì²œì—°", "í¬ì§€ì…”ë‹"]
                }
            ]
        }
    
    def _initialize_ground_truth(self) -> Dict[str, Any]:
        """ì •ë‹µ ë°ì´í„° ì´ˆê¸°í™”"""
        return {
            "accuracy_thresholds": {
                "excellent": 0.95,
                "good": 0.85,
                "fair": 0.70,
                "poor": 0.50
            },
            "response_time_targets": {
                "fast": 15.0,      # 15ì´ˆ ì´í•˜
                "normal": 30.0,    # 30ì´ˆ ì´í•˜  
                "slow": 60.0       # 60ì´ˆ ì´í•˜
            },
            "cost_thresholds": {
                "low": 0.01,       # $0.01 ì´í•˜
                "medium": 0.05,    # $0.05 ì´í•˜
                "high": 0.10       # $0.10 ì´í•˜
            }
        }
    
    def generate_test_batch(self, scenario: TestScenario, batch_size: int = 10) -> List[Dict[str, Any]]:
        """í…ŒìŠ¤íŠ¸ ë°°ì¹˜ ìƒì„±"""
        if scenario not in self.test_cases:
            return []
        
        base_cases = self.test_cases[scenario]
        test_batch = []
        
        for i in range(batch_size):
            # ê¸°ë³¸ ì¼€ì´ìŠ¤ì—ì„œ ì„ íƒí•˜ê³  ë³€í˜• ì¶”ê°€
            base_case = random.choice(base_cases)
            test_case = base_case.copy()
            test_case['test_id'] = f"{scenario.value}_{i+1:03d}"
            test_case['timestamp'] = time.time()
            
            test_batch.append(test_case)
        
        return test_batch

class PerformanceBenchmark:
    """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self, hybrid_manager: HybridLLMManager, quality_validator: AIQualityValidator):
        self.hybrid_manager = hybrid_manager
        self.quality_validator = quality_validator
        self.test_generator = TestDataGenerator()
        self.benchmark_history = []
        self.current_benchmark_id = None
        
    async def run_comprehensive_benchmark(self, scenarios: List[TestScenario] = None) -> Dict[str, Any]:
        """ì¢…í•© ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        
        if scenarios is None:
            scenarios = list(TestScenario)
        
        self.current_benchmark_id = f"benchmark_{int(time.time())}"
        logger.info(f"ğŸš€ ì¢…í•© ë²¤ì¹˜ë§ˆí¬ ì‹œì‘: {self.current_benchmark_id}")
        
        all_results = []
        scenario_summaries = {}
        
        for scenario in scenarios:
            logger.info(f"ğŸ“Š {scenario.value} ì‹œë‚˜ë¦¬ì˜¤ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")
            
            scenario_results = await self._benchmark_scenario(scenario)
            all_results.extend(scenario_results)
            
            # ì‹œë‚˜ë¦¬ì˜¤ë³„ ìš”ì•½
            scenario_summaries[scenario.value] = self._summarize_scenario_results(scenario_results)
        
        # ì „ì²´ ìš”ì•½ ë° ë¶„ì„
        comprehensive_summary = self._create_comprehensive_summary(all_results, scenario_summaries)
        
        # ê²°ê³¼ ì €ì¥
        benchmark_record = {
            "benchmark_id": self.current_benchmark_id,
            "timestamp": time.time(),
            "scenarios": [s.value for s in scenarios],
            "total_tests": len(all_results),
            "results": all_results,
            "scenario_summaries": scenario_summaries,
            "comprehensive_summary": comprehensive_summary
        }
        
        self.benchmark_history.append(benchmark_record)
        
        logger.info(f"âœ… ì¢…í•© ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ: {len(all_results)}ê°œ í…ŒìŠ¤íŠ¸")
        
        return comprehensive_summary
    
    async def _benchmark_scenario(self, scenario: TestScenario) -> List[BenchmarkResult]:
        """ì‹œë‚˜ë¦¬ì˜¤ë³„ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        
        test_cases = self.test_generator.generate_test_batch(scenario, batch_size=5)
        results = []
        
        for test_case in test_cases:
            logger.info(f"ğŸ§ª í…ŒìŠ¤íŠ¸ ì‹¤í–‰: {test_case['test_id']}")
            
            # AI ë¶„ì„ ì‹¤í–‰
            analysis_request = AnalysisRequest(
                text_content=test_case['input'],
                analysis_type=self._map_scenario_to_analysis_type(scenario),
                require_jewelry_expertise=True
            )
            
            start_time = time.time()
            hybrid_result = await self.hybrid_manager.hybrid_analyze(analysis_request)
            end_time = time.time()
            
            if hybrid_result['status'] != 'success':
                logger.error(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {test_case['test_id']}")
                continue
            
            # í’ˆì§ˆ ê²€ì¦
            ai_response = AIResponse(
                model=AIModel.GPT4V,  # hybrid_resultì—ì„œ ì‹¤ì œ ëª¨ë¸ ì¶”ì¶œ
                content=hybrid_result['content'],
                confidence=hybrid_result['confidence'],
                processing_time=hybrid_result['processing_time'],
                cost_estimate=hybrid_result['cost_estimate'],
                jewelry_relevance=hybrid_result['jewelry_relevance'],
                metadata={}
            )
            
            quality_report = await self.quality_validator.validate_ai_response(
                ai_response, 
                self._map_scenario_to_analysis_type(scenario),
                test_case['input']
            )
            
            # ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìƒì„±
            benchmark_result = BenchmarkResult(
                model=AIModel(hybrid_result['best_model']),
                scenario=scenario,
                accuracy_score=quality_report.overall_score,
                response_time=end_time - start_time,
                cost=hybrid_result['cost_estimate'],
                jewelry_expertise_score=quality_report.jewelry_expertise_score,
                consistency_score=quality_report.consistency_score,
                user_satisfaction_score=self._calculate_user_satisfaction(test_case, hybrid_result),
                timestamp=time.time(),
                test_id=test_case['test_id'],
                metadata={
                    "test_case": test_case,
                    "quality_report": asdict(quality_report),
                    "hybrid_result": hybrid_result
                }
            )
            
            results.append(benchmark_result)
            
            # ì ì‹œ ëŒ€ê¸° (API ë ˆì´íŠ¸ ë¦¬ë°‹ ê³ ë ¤)
            await asyncio.sleep(1)
        
        return results
    
    def _map_scenario_to_analysis_type(self, scenario: TestScenario) -> AnalysisType:
        """ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ë¶„ì„ íƒ€ì…ìœ¼ë¡œ ë§¤í•‘"""
        mapping = {
            TestScenario.DIAMOND_APPRAISAL: AnalysisType.DIAMOND_4C,
            TestScenario.COLORED_STONE_ANALYSIS: AnalysisType.COLORED_STONE,
            TestScenario.JEWELRY_DESIGN_REVIEW: AnalysisType.JEWELRY_DESIGN,
            TestScenario.BUSINESS_CONSULTATION: AnalysisType.BUSINESS_INSIGHT,
            TestScenario.MIXED_ANALYSIS: AnalysisType.DIAMOND_4C
        }
        return mapping.get(scenario, AnalysisType.DIAMOND_4C)
    
    def _calculate_user_satisfaction(self, test_case: Dict[str, Any], result: Dict[str, Any]) -> float:
        """ì‚¬ìš©ì ë§Œì¡±ë„ ì ìˆ˜ ê³„ì‚° (ì‹œë®¬ë ˆì´ì…˜)"""
        
        # í‚¤ í¬ì¸íŠ¸ ë§¤ì¹­ ì ìˆ˜
        key_points = test_case.get('key_points', [])
        content = result['content'].lower()
        
        matched_points = sum(1 for point in key_points if point.lower() in content)
        key_point_score = matched_points / len(key_points) if key_points else 1.0
        
        # ì‘ë‹µ ê¸¸ì´ ì ì ˆì„± (ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸¸ë©´ ê°ì )
        content_length = len(result['content'])
        if 500 <= content_length <= 2000:
            length_score = 1.0
        elif content_length < 200:
            length_score = 0.5
        elif content_length > 3000:
            length_score = 0.7
        else:
            length_score = 0.8
        
        # ì‹ ë¢°ë„ ì ìˆ˜
        confidence_score = result['confidence']
        
        # ì¢…í•© ë§Œì¡±ë„ (ê°€ì¤‘í‰ê· )
        satisfaction = (key_point_score * 0.4 + length_score * 0.3 + confidence_score * 0.3)
        
        return min(satisfaction, 1.0)
    
    def _summarize_scenario_results(self, results: List[BenchmarkResult]) -> Dict[str, Any]:
        """ì‹œë‚˜ë¦¬ì˜¤ë³„ ê²°ê³¼ ìš”ì•½"""
        
        if not results:
            return {"error": "ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤"}
        
        # ë©”íŠ¸ë¦­ë³„ í‰ê·  ê³„ì‚°
        accuracy_scores = [r.accuracy_score for r in results]
        response_times = [r.response_time for r in results]
        costs = [r.cost for r in results]
        expertise_scores = [r.jewelry_expertise_score for r in results]
        consistency_scores = [r.consistency_score for r in results]
        satisfaction_scores = [r.user_satisfaction_score for r in results]
        
        # ëª¨ë¸ë³„ ì„±ëŠ¥
        model_performance = {}
        for result in results:
            model_name = result.model.value
            if model_name not in model_performance:
                model_performance[model_name] = {
                    "count": 0,
                    "avg_accuracy": 0,
                    "avg_response_time": 0,
                    "avg_cost": 0
                }
            
            perf = model_performance[model_name]
            perf["count"] += 1
            perf["avg_accuracy"] = ((perf["avg_accuracy"] * (perf["count"] - 1)) + result.accuracy_score) / perf["count"]
            perf["avg_response_time"] = ((perf["avg_response_time"] * (perf["count"] - 1)) + result.response_time) / perf["count"]
            perf["avg_cost"] = ((perf["avg_cost"] * (perf["count"] - 1)) + result.cost) / perf["count"]
        
        return {
            "test_count": len(results),
            "average_metrics": {
                "accuracy": statistics.mean(accuracy_scores),
                "response_time": statistics.mean(response_times),
                "cost": statistics.mean(costs),
                "jewelry_expertise": statistics.mean(expertise_scores),
                "consistency": statistics.mean(consistency_scores),
                "user_satisfaction": statistics.mean(satisfaction_scores)
            },
            "metric_ranges": {
                "accuracy": {"min": min(accuracy_scores), "max": max(accuracy_scores)},
                "response_time": {"min": min(response_times), "max": max(response_times)},
                "cost": {"min": min(costs), "max": max(costs)}
            },
            "model_performance": model_performance,
            "quality_distribution": self._calculate_quality_distribution(accuracy_scores)
        }
    
    def _calculate_quality_distribution(self, scores: List[float]) -> Dict[str, int]:
        """í’ˆì§ˆ ë¶„í¬ ê³„ì‚°"""
        distribution = {"excellent": 0, "good": 0, "fair": 0, "poor": 0}
        
        for score in scores:
            if score >= 0.95:
                distribution["excellent"] += 1
            elif score >= 0.85:
                distribution["good"] += 1
            elif score >= 0.70:
                distribution["fair"] += 1
            else:
                distribution["poor"] += 1
        
        return distribution
    
    def _create_comprehensive_summary(self, all_results: List[BenchmarkResult], 
                                    scenario_summaries: Dict[str, Any]) -> Dict[str, Any]:
        """ì¢…í•© ìš”ì•½ ìƒì„±"""
        
        if not all_results:
            return {"error": "ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤"}
        
        # ì „ì²´ í†µê³„
        overall_accuracy = statistics.mean([r.accuracy_score for r in all_results])
        overall_response_time = statistics.mean([r.response_time for r in all_results])
        overall_cost = sum([r.cost for r in all_results])
        
        # 99.2% ëª©í‘œ ë‹¬ì„±ë¥ 
        target_achievement = sum(1 for r in all_results if r.accuracy_score >= 0.992) / len(all_results)
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸
        model_scores = {}
        for result in all_results:
            model_name = result.model.value
            if model_name not in model_scores:
                model_scores[model_name] = []
            model_scores[model_name].append(result.accuracy_score)
        
        model_averages = {model: statistics.mean(scores) for model, scores in model_scores.items()}
        best_model = max(model_averages, key=model_averages.get) if model_averages else "N/A"
        
        # ê°œì„  ê¶Œì¥ì‚¬í•­
        recommendations = self._generate_optimization_recommendations(all_results, scenario_summaries)
        
        return {
            "benchmark_overview": {
                "total_tests": len(all_results),
                "scenarios_tested": len(scenario_summaries),
                "overall_accuracy": overall_accuracy,
                "target_achievement_rate": target_achievement,
                "average_response_time": overall_response_time,
                "total_cost": overall_cost
            },
            "performance_highlights": {
                "best_performing_model": best_model,
                "best_accuracy": max(model_averages.values()) if model_averages else 0,
                "fastest_average_response": min([r.response_time for r in all_results]),
                "most_cost_efficient": min([r.cost for r in all_results])
            },
            "quality_assessment": {
                "meets_99_2_target": target_achievement >= 0.992,
                "quality_consistency": statistics.stdev([r.accuracy_score for r in all_results]),
                "expertise_level": statistics.mean([r.jewelry_expertise_score for r in all_results])
            },
            "optimization_recommendations": recommendations,
            "detailed_scenario_results": scenario_summaries
        }
    
    def _generate_optimization_recommendations(self, results: List[BenchmarkResult], 
                                             scenario_summaries: Dict[str, Any]) -> List[str]:
        """ìµœì í™” ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        
        recommendations = []
        
        # ì •í™•ë„ ê´€ë ¨
        avg_accuracy = statistics.mean([r.accuracy_score for r in results])
        if avg_accuracy < 0.992:
            recommendations.append(f"ì •í™•ë„ ê°œì„  í•„ìš”: í˜„ì¬ {avg_accuracy:.1%} â†’ ëª©í‘œ 99.2%")
        
        # ì‘ë‹µ ì‹œê°„ ê´€ë ¨
        avg_response_time = statistics.mean([r.response_time for r in results])
        if avg_response_time > 25:
            recommendations.append(f"ì‘ë‹µ ì‹œê°„ ë‹¨ì¶• í•„ìš”: í˜„ì¬ {avg_response_time:.1f}ì´ˆ â†’ ëª©í‘œ 25ì´ˆ")
        
        # ë¹„ìš© ê´€ë ¨
        total_cost = sum([r.cost for r in results])
        if total_cost > 1.0:
            recommendations.append(f"ë¹„ìš© ìµœì í™” ê³ ë ¤: ì´ ë¹„ìš© ${total_cost:.2f}")
        
        # ëª¨ë¸ë³„ ì„±ëŠ¥ ì°¨ì´
        model_performances = {}
        for result in results:
            model = result.model.value
            if model not in model_performances:
                model_performances[model] = []
            model_performances[model].append(result.accuracy_score)
        
        if len(model_performances) > 1:
            model_avgs = {m: statistics.mean(scores) for m, scores in model_performances.items()}
            best_model = max(model_avgs, key=model_avgs.get)
            worst_model = min(model_avgs, key=model_avgs.get)
            
            if model_avgs[best_model] - model_avgs[worst_model] > 0.1:
                recommendations.append(f"ëª¨ë¸ ì„ íƒ ìµœì í™”: {best_model} ìš°ì„  ì‚¬ìš© ê¶Œì¥")
        
        # ì‹œë‚˜ë¦¬ì˜¤ë³„ ì•½ì 
        for scenario, summary in scenario_summaries.items():
            if "average_metrics" in summary and summary["average_metrics"]["accuracy"] < 0.85:
                recommendations.append(f"{scenario} ì‹œë‚˜ë¦¬ì˜¤ íŠ¹ë³„ ê°œì„  í•„ìš”")
        
        return recommendations[:5]  # ìƒìœ„ 5ê°œ ê¶Œì¥ì‚¬í•­

class ABTestManager:
    """A/B í…ŒìŠ¤íŠ¸ ê´€ë¦¬ì"""
    
    def __init__(self, benchmark_system: PerformanceBenchmark):
        self.benchmark_system = benchmark_system
        self.ab_test_history = []
        
    async def run_ab_test(self, 
                         test_name: str,
                         models_to_test: List[AIModel],
                         test_scenarios: List[TestScenario],
                         sample_size: int = 30) -> ABTestResult:
        """A/B í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        
        logger.info(f"ğŸ§ª A/B í…ŒìŠ¤íŠ¸ ì‹œì‘: {test_name}")
        logger.info(f"   í…ŒìŠ¤íŠ¸ ëª¨ë¸: {[m.value for m in models_to_test]}")
        logger.info(f"   ìƒ˜í”Œ í¬ê¸°: {sample_size}")
        
        # ê° ëª¨ë¸ë³„ ì„±ëŠ¥ ìˆ˜ì§‘
        model_results = {}
        
        for model in models_to_test:
            logger.info(f"ğŸ“Š {model.value} ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì¤‘...")
            
            # ì„ì‹œë¡œ íŠ¹ì • ëª¨ë¸ ê°•ì œ ì‚¬ìš© (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” hybrid_manager ìˆ˜ì • í•„ìš”)
            model_performance = await self._test_single_model(model, test_scenarios, sample_size)
            model_results[model] = model_performance
        
        # í†µê³„ì  ìœ ì˜ì„± ê²€ì¦
        statistical_analysis = self._perform_statistical_analysis(model_results)
        
        # ìŠ¹ì ê²°ì •
        winner = self._determine_winner(model_results, statistical_analysis)
        
        # A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìƒì„±
        ab_result = ABTestResult(
            test_name=test_name,
            model_a=models_to_test[0],
            model_b=models_to_test[1] if len(models_to_test) > 1 else models_to_test[0],
            model_c=models_to_test[2] if len(models_to_test) > 2 else None,
            winner=winner,
            confidence_level=statistical_analysis["confidence_level"],
            sample_size=sample_size * len(models_to_test),
            performance_improvement=statistical_analysis["improvement_percentage"],
            statistical_significance=statistical_analysis["is_significant"],
            detailed_metrics=statistical_analysis["detailed_metrics"],
            recommendations=self._generate_ab_recommendations(model_results, statistical_analysis)
        )
        
        self.ab_test_history.append(ab_result)
        
        logger.info(f"âœ… A/B í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {winner.value} ìŠ¹ë¦¬")
        
        return ab_result
    
    async def _test_single_model(self, model: AIModel, scenarios: List[TestScenario], 
                                sample_size: int) -> Dict[str, Any]:
        """ë‹¨ì¼ ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        
        all_results = []
        
        for scenario in scenarios:
            # ì‹œë‚˜ë¦¬ì˜¤ë³„ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìƒì„±
            test_cases = self.benchmark_system.test_generator.generate_test_batch(
                scenario, sample_size // len(scenarios)
            )
            
            for test_case in test_cases:
                # ê°œë³„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ì‹¤ì œë¡œëŠ” íŠ¹ì • ëª¨ë¸ë¡œ ê°•ì œ ì‹¤í–‰)
                analysis_request = AnalysisRequest(
                    text_content=test_case['input'],
                    analysis_type=self.benchmark_system._map_scenario_to_analysis_type(scenario)
                )
                
                start_time = time.time()
                # ì—¬ê¸°ì„œëŠ” ì‹œë®¬ë ˆì´ì…˜ëœ ê²°ê³¼ ì‚¬ìš©
                simulated_result = self._simulate_model_response(model, test_case)
                end_time = time.time()
                
                # ê²°ê³¼ ì €ì¥
                result = {
                    "accuracy": simulated_result["accuracy"],
                    "response_time": end_time - start_time,
                    "cost": simulated_result["cost"],
                    "jewelry_expertise": simulated_result["expertise"],
                    "user_satisfaction": simulated_result["satisfaction"]
                }
                
                all_results.append(result)
        
        # ì§‘ê³„ í†µê³„ ê³„ì‚°
        return {
            "accuracy": statistics.mean([r["accuracy"] for r in all_results]),
            "response_time": statistics.mean([r["response_time"] for r in all_results]),
            "cost": statistics.mean([r["cost"] for r in all_results]),
            "jewelry_expertise": statistics.mean([r["jewelry_expertise"] for r in all_results]),
            "user_satisfaction": statistics.mean([r["user_satisfaction"] for r in all_results]),
            "sample_count": len(all_results),
            "raw_results": all_results
        }
    
    def _simulate_model_response(self, model: AIModel, test_case: Dict[str, Any]) -> Dict[str, Any]:
        """ëª¨ë¸ ì‘ë‹µ ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì œê±°)"""
        
        # ëª¨ë¸ë³„ ì„±ëŠ¥ íŠ¹ì„± ì‹œë®¬ë ˆì´ì…˜
        base_performance = {
            AIModel.GPT4V: {"accuracy": 0.92, "cost": 0.03, "expertise": 0.88, "satisfaction": 0.90},
            AIModel.CLAUDE_VISION: {"accuracy": 0.94, "cost": 0.02, "expertise": 0.91, "satisfaction": 0.93},
            AIModel.GEMINI_2: {"accuracy": 0.89, "cost": 0.01, "expertise": 0.85, "satisfaction": 0.87}
        }
        
        performance = base_performance.get(model, base_performance[AIModel.GPT4V])
        
        # ëœë¤ ë³€ë™ ì¶”ê°€
        return {
            "accuracy": min(1.0, performance["accuracy"] + random.gauss(0, 0.05)),
            "cost": max(0.001, performance["cost"] + random.gauss(0, 0.005)),
            "expertise": min(1.0, performance["expertise"] + random.gauss(0, 0.04)),
            "satisfaction": min(1.0, performance["satisfaction"] + random.gauss(0, 0.03))
        }
    
    def _perform_statistical_analysis(self, model_results: Dict[AIModel, Dict[str, Any]]) -> Dict[str, Any]:
        """í†µê³„ì  ë¶„ì„ ìˆ˜í–‰"""
        
        # ê°„ë‹¨í•œ í†µê³„ì  ìœ ì˜ì„± ê²€ì¦ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ í†µê³„ ë¶„ì„ í•„ìš”)
        models = list(model_results.keys())
        
        if len(models) < 2:
            return {
                "confidence_level": 1.0,
                "is_significant": True,
                "improvement_percentage": 0.0,
                "detailed_metrics": {}
            }
        
        # ì£¼ìš” ë©”íŠ¸ë¦­ ë¹„êµ
        model_a = models[0]
        model_b = models[1]
        
        accuracy_diff = model_results[model_b]["accuracy"] - model_results[model_a]["accuracy"]
        cost_diff = model_results[model_a]["cost"] - model_results[model_b]["cost"]  # ë¹„ìš©ì€ ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
        
        # ì¢…í•© ì„±ëŠ¥ ì ìˆ˜ ê³„ì‚°
        def calculate_composite_score(results):
            return (results["accuracy"] * 0.4 + 
                   results["jewelry_expertise"] * 0.3 + 
                   results["user_satisfaction"] * 0.2 + 
                   (1 - results["cost"]/0.1) * 0.1)  # ë¹„ìš©ì€ ì—­ìˆ˜
        
        score_a = calculate_composite_score(model_results[model_a])
        score_b = calculate_composite_score(model_results[model_b])
        
        improvement = (score_b - score_a) / score_a * 100 if score_a > 0 else 0
        
        # ì‹ ë¢°ë„ ê³„ì‚° (ê°„ë‹¨í•œ ë°©ì‹)
        confidence = min(0.99, abs(improvement) / 10)  # ê°œì„ ë¥ ì´ í´ìˆ˜ë¡ ì‹ ë¢°ë„ ë†’ìŒ
        
        return {
            "confidence_level": confidence,
            "is_significant": abs(improvement) > 5,  # 5% ì´ìƒ ê°œì„  ì‹œ ìœ ì˜ë¯¸
            "improvement_percentage": improvement,
            "detailed_metrics": {
                "accuracy_difference": accuracy_diff,
                "cost_difference": cost_diff,
                "composite_score_a": score_a,
                "composite_score_b": score_b
            }
        }
    
    def _determine_winner(self, model_results: Dict[AIModel, Dict[str, Any]], 
                         statistical_analysis: Dict[str, Any]) -> AIModel:
        """ìŠ¹ì ê²°ì •"""
        
        # ì¢…í•© ì ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ìŠ¹ì ê²°ì •
        def calculate_score(results):
            return (results["accuracy"] * 0.4 + 
                   results["jewelry_expertise"] * 0.3 + 
                   results["user_satisfaction"] * 0.2 + 
                   (1 - min(results["cost"], 0.1)/0.1) * 0.1)
        
        best_model = max(model_results.keys(), 
                        key=lambda m: calculate_score(model_results[m]))
        
        return best_model
    
    def _generate_ab_recommendations(self, model_results: Dict[AIModel, Dict[str, Any]], 
                                   statistical_analysis: Dict[str, Any]) -> List[str]:
        """A/B í…ŒìŠ¤íŠ¸ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        
        recommendations = []
        
        # ìŠ¹ì ëª¨ë¸ ì‚¬ìš© ê¶Œì¥
        winner = self._determine_winner(model_results, statistical_analysis)
        if statistical_analysis["is_significant"]:
            recommendations.append(f"{winner.value} ëª¨ë¸ ìš°ì„  ì‚¬ìš© ê¶Œì¥ (í†µê³„ì  ìœ ì˜ë¯¸í•œ ì„±ëŠ¥ í–¥ìƒ)")
        
        # ë¹„ìš© íš¨ìœ¨ì„± ê¶Œì¥
        cost_efficient_model = min(model_results.keys(), 
                                 key=lambda m: model_results[m]["cost"])
        recommendations.append(f"ë¹„ìš© íš¨ìœ¨ì„±: {cost_efficient_model.value} ëª¨ë¸ì´ ê°€ì¥ ê²½ì œì ")
        
        # ì •í™•ë„ ê¸°ë°˜ ê¶Œì¥
        most_accurate_model = max(model_results.keys(), 
                                key=lambda m: model_results[m]["accuracy"])
        if model_results[most_accurate_model]["accuracy"] >= 0.95:
            recommendations.append(f"ìµœê³  ì •í™•ë„: {most_accurate_model.value} ëª¨ë¸ ({model_results[most_accurate_model]['accuracy']:.1%})")
        
        # ê°œì„  í•„ìš” ì˜ì—­
        if statistical_analysis["improvement_percentage"] < 5:
            recommendations.append("ëª¨ë¸ ê°„ ì„±ëŠ¥ ì°¨ì´ê°€ ë¯¸ë¯¸í•¨ - ì¶”ê°€ ìµœì í™” í•„ìš”")
        
        return recommendations

class PerformanceReportGenerator:
    """ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±ê¸°"""
    
    def __init__(self):
        self.report_templates = self._initialize_templates()
    
    def _initialize_templates(self) -> Dict[str, str]:
        """ë¦¬í¬íŠ¸ í…œí”Œë¦¿ ì´ˆê¸°í™”"""
        return {
            "executive_summary": """
# ğŸ§  ì†”ë¡œëª¬ë“œ AI ì„±ëŠ¥ ë¶„ì„ Executive Summary

## ğŸ“Š í•µì‹¬ ì„±ê³¼ ì§€í‘œ
- **ì „ì²´ ì •í™•ë„**: {overall_accuracy:.1%}
- **99.2% ëª©í‘œ ë‹¬ì„±ë¥ **: {target_achievement:.1%}
- **í‰ê·  ì‘ë‹µ ì‹œê°„**: {avg_response_time:.1f}ì´ˆ
- **ë¹„ìš© íš¨ìœ¨ì„±**: ${total_cost:.3f}

## ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸
**{best_model}** - ì •í™•ë„ {best_accuracy:.1%}

## ğŸ¯ ê¶Œì¥ì‚¬í•­
{recommendations}
""",
            
            "detailed_analysis": """
# ğŸ“ˆ ìƒì„¸ ì„±ëŠ¥ ë¶„ì„ ë³´ê³ ì„œ

## ğŸ” ì‹œë‚˜ë¦¬ì˜¤ë³„ ì„±ëŠ¥

{scenario_details}

## ğŸ“Š ëª¨ë¸ ë¹„êµ ë¶„ì„

{model_comparison}

## ğŸ“‰ ì„±ëŠ¥ íŠ¸ë Œë“œ

{performance_trends}
""",
            
            "optimization_guide": """
# ğŸš€ AI ì‹œìŠ¤í…œ ìµœì í™” ê°€ì´ë“œ

## ğŸ¯ ì¦‰ì‹œ ê°œì„  í•­ëª©
{immediate_improvements}

## ğŸ“ˆ ì¤‘ì¥ê¸° ìµœì í™” ì „ëµ
{longterm_strategy}

## ğŸ’¡ ê¸°ìˆ ì  ê¶Œì¥ì‚¬í•­
{technical_recommendations}
"""
        }
    
    def generate_comprehensive_report(self, benchmark_summary: Dict[str, Any], 
                                    ab_test_results: List[ABTestResult] = None) -> str:
        """ì¢…í•© ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±"""
        
        # Executive Summary
        exec_summary = self.report_templates["executive_summary"].format(
            overall_accuracy=benchmark_summary["benchmark_overview"]["overall_accuracy"],
            target_achievement=benchmark_summary["benchmark_overview"]["target_achievement_rate"],
            avg_response_time=benchmark_summary["benchmark_overview"]["average_response_time"],
            total_cost=benchmark_summary["benchmark_overview"]["total_cost"],
            best_model=benchmark_summary["performance_highlights"]["best_performing_model"],
            best_accuracy=benchmark_summary["performance_highlights"]["best_accuracy"],
            recommendations="\n".join([f"â€¢ {rec}" for rec in benchmark_summary["optimization_recommendations"][:5]])
        )
        
        # ì‹œë‚˜ë¦¬ì˜¤ë³„ ìƒì„¸ ë¶„ì„
        scenario_details = ""
        for scenario, details in benchmark_summary["detailed_scenario_results"].items():
            if "average_metrics" in details:
                scenario_details += f"""
### {scenario}
- **ì •í™•ë„**: {details["average_metrics"]["accuracy"]:.1%}
- **ì‘ë‹µì‹œê°„**: {details["average_metrics"]["response_time"]:.1f}ì´ˆ
- **ì „ë¬¸ì„±**: {details["average_metrics"]["jewelry_expertise"]:.1%}
- **í…ŒìŠ¤íŠ¸ ìˆ˜**: {details["test_count"]}ê°œ

"""
        
        # A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì¶”ê°€
        ab_test_section = ""
        if ab_test_results:
            ab_test_section = "\n## ğŸ§ª A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼\n"
            for ab_result in ab_test_results[-3:]:  # ìµœê·¼ 3ê°œ
                ab_test_section += f"""
### {ab_result.test_name}
- **ìŠ¹ì**: {ab_result.winner.value}
- **ì„±ëŠ¥ ê°œì„ **: {ab_result.performance_improvement:.1f}%
- **ì‹ ë¢°ë„**: {ab_result.confidence_level:.1%}
- **í†µê³„ì  ìœ ì˜ì„±**: {'ìœ ì˜ë¯¸' if ab_result.statistical_significance else 'ë¯¸ë¯¸'}

"""
        
        # ìµœì¢… ë¦¬í¬íŠ¸ ì¡°í•©
        full_report = exec_summary + "\n" + scenario_details + ab_test_section
        
        # ìµœì í™” ê°€ì´ë“œ ì¶”ê°€
        optimization_guide = self._generate_optimization_guide(benchmark_summary)
        full_report += "\n" + optimization_guide
        
        return full_report
    
    def _generate_optimization_guide(self, benchmark_summary: Dict[str, Any]) -> str:
        """ìµœì í™” ê°€ì´ë“œ ìƒì„±"""
        
        immediate_improvements = []
        longterm_strategy = []
        technical_recommendations = []
        
        # í˜„ì¬ ì„±ëŠ¥ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        overall_accuracy = benchmark_summary["benchmark_overview"]["overall_accuracy"]
        
        if overall_accuracy < 0.992:
            immediate_improvements.append("í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ê³ ë„í™”")
            immediate_improvements.append("í’ˆì§ˆ ê²€ì¦ ì„ê³„ê°’ ê°•í™”")
        
        if benchmark_summary["benchmark_overview"]["average_response_time"] > 25:
            immediate_improvements.append("ì‘ë‹µ ì‹œê°„ ìµœì í™” (ë³‘ë ¬ ì²˜ë¦¬ ê°œì„ )")
        
        # ì¥ê¸° ì „ëµ
        longterm_strategy.extend([
            "AI ëª¨ë¸ íŒŒì¸íŠœë‹ í”„ë¡œê·¸ë¨ ë„ì…",
            "ì£¼ì–¼ë¦¬ ì „ë¬¸ ë°ì´í„°ì…‹ í™•ì¥",
            "ì‚¬ìš©ì í”¼ë“œë°± ê¸°ë°˜ ì§€ì†ì  í•™ìŠµ ì‹œìŠ¤í…œ êµ¬ì¶•"
        ])
        
        # ê¸°ìˆ ì  ê¶Œì¥ì‚¬í•­
        technical_recommendations.extend([
            "ìºì‹± ì‹œìŠ¤í…œ ë„ì…ìœ¼ë¡œ ì‘ë‹µ ì†ë„ ê°œì„ ",
            "ë¹„ìš© ìµœì í™”ë¥¼ ìœ„í•œ ëª¨ë¸ ë¼ìš°íŒ… ì•Œê³ ë¦¬ì¦˜ ê°œì„ ",
            "ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ êµ¬ì¶•"
        ])
        
        return self.report_templates["optimization_guide"].format(
            immediate_improvements="\n".join([f"â€¢ {item}" for item in immediate_improvements]),
            longterm_strategy="\n".join([f"â€¢ {item}" for item in longterm_strategy]),
            technical_recommendations="\n".join([f"â€¢ {item}" for item in technical_recommendations])
        )

# ë°ëª¨ ë° í†µí•© í…ŒìŠ¤íŠ¸
async def demo_performance_system():
    """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œìŠ¤í…œ ë°ëª¨"""
    print("ğŸ“Š ì†”ë¡œëª¬ë“œ AI ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë° A/B í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ v2.3")
    print("=" * 70)
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” API í‚¤ í•„ìš”)
    hybrid_manager = HybridLLMManager()
    quality_validator = AIQualityValidator()
    benchmark_system = PerformanceBenchmark(hybrid_manager, quality_validator)
    ab_test_manager = ABTestManager(benchmark_system)
    report_generator = PerformanceReportGenerator()
    
    print("ğŸš€ 1. ì¢…í•© ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì¤‘...")
    
    # ì‹œë‚˜ë¦¬ì˜¤ ì„ íƒ (ë°ëª¨ìš©ìœ¼ë¡œ 2ê°œë§Œ)
    test_scenarios = [TestScenario.DIAMOND_APPRAISAL, TestScenario.COLORED_STONE_ANALYSIS]
    
    benchmark_results = await benchmark_system.run_comprehensive_benchmark(test_scenarios)
    
    print("âœ… ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")
    print(f"   ì „ì²´ ì •í™•ë„: {benchmark_results['benchmark_overview']['overall_accuracy']:.1%}")
    print(f"   99.2% ëª©í‘œ ë‹¬ì„±ë¥ : {benchmark_results['benchmark_overview']['target_achievement_rate']:.1%}")
    print(f"   í‰ê·  ì‘ë‹µ ì‹œê°„: {benchmark_results['benchmark_overview']['average_response_time']:.1f}ì´ˆ")
    print()
    
    print("ğŸ§ª 2. A/B í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
    
    # A/B í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    ab_result = await ab_test_manager.run_ab_test(
        test_name="GPT4V vs Claude vs Gemini ì„±ëŠ¥ ë¹„êµ",
        models_to_test=[AIModel.GPT4V, AIModel.CLAUDE_VISION, AIModel.GEMINI_2],
        test_scenarios=test_scenarios,
        sample_size=9  # ë°ëª¨ìš© ì‘ì€ ìƒ˜í”Œ
    )
    
    print("âœ… A/B í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print(f"   ìŠ¹ì: {ab_result.winner.value}")
    print(f"   ì„±ëŠ¥ ê°œì„ : {ab_result.performance_improvement:.1f}%")
    print(f"   í†µê³„ì  ìœ ì˜ì„±: {'ìœ ì˜ë¯¸' if ab_result.statistical_significance else 'ë¯¸ë¯¸'}")
    print()
    
    print("ğŸ“‹ 3. ì¢…í•© ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
    
    # ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±
    comprehensive_report = report_generator.generate_comprehensive_report(
        benchmark_results, [ab_result]
    )
    
    print("âœ… ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ!")
    print("\n" + "="*50)
    print("ğŸ“Š ì„±ëŠ¥ ë¶„ì„ ë¦¬í¬íŠ¸ (ì¼ë¶€)")
    print("="*50)
    print(comprehensive_report[:1000] + "...")
    print()
    
    print("ğŸ’¡ ì£¼ìš” ê¶Œì¥ì‚¬í•­:")
    for i, rec in enumerate(benchmark_results["optimization_recommendations"][:3], 1):
        print(f"   {i}. {rec}")

if __name__ == "__main__":
    asyncio.run(demo_performance_system())
