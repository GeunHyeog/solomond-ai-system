"""
AI ë²¤ì¹˜ë§ˆí¬ ì‹œìŠ¤í…œ v2.3 - ì†”ë¡œëª¬ë“œ AI ì—”ì§„ ê³ ë„í™” í”„ë¡œì íŠ¸
99.2% ì •í™•ë„ ë‹¬ì„±ì„ ìœ„í•œ ì‹¤ì‹œê°„ ì„±ëŠ¥ ì¸¡ì • ë° ìµœì í™” ì‹œìŠ¤í…œ

í†µí•© ëŒ€ìƒ:
- hybrid_llm_manager_v23.py (í•˜ì´ë¸Œë¦¬ë“œ LLM)
- jewelry_specialized_prompts_v23.py (ì£¼ì–¼ë¦¬ íŠ¹í™” í”„ë¡¬í”„íŠ¸)
- ai_quality_validator_v23.py (í’ˆì§ˆ ê²€ì¦)
"""

import asyncio
import time
import statistics
import logging
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, field
from enum import Enum
import json
import numpy as np
from pathlib import Path
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
import psutil
import gc
import sys

# ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì •ì˜
class BenchmarkMetric(Enum):
    """ë²¤ì¹˜ë§ˆí¬ ë©”íŠ¸ë¦­ íƒ€ì…"""
    ACCURACY = "accuracy"
    PROCESSING_SPEED = "processing_speed"
    MEMORY_EFFICIENCY = "memory_efficiency"
    JEWELRY_RELEVANCE = "jewelry_relevance"
    COST_EFFICIENCY = "cost_efficiency"
    USER_SATISFACTION = "user_satisfaction"

@dataclass
class ModelPerformance:
    """ê°œë³„ ëª¨ë¸ ì„±ëŠ¥ ë°ì´í„°"""
    model_name: str
    accuracy_score: float = 0.0
    avg_response_time: float = 0.0
    memory_usage_mb: float = 0.0
    jewelry_relevance: float = 0.0
    cost_per_request: float = 0.0
    total_requests: int = 0
    successful_requests: int = 0
    error_rate: float = 0.0
    confidence_scores: List[float] = field(default_factory=list)
    processing_times: List[float] = field(default_factory=list)

@dataclass
class BenchmarkTestCase:
    """ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤"""
    test_id: str
    category: str
    input_data: Dict[str, Any]
    expected_output: Optional[str] = None
    expected_accuracy: float = 0.9
    max_response_time: float = 25.0
    difficulty_level: str = "medium"
    jewelry_keywords: List[str] = field(default_factory=list)

@dataclass
class BenchmarkResult:
    """ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ê²°ê³¼"""
    test_id: str
    model_name: str
    actual_output: str
    accuracy_achieved: float
    response_time: float
    memory_used: float
    jewelry_relevance: float
    cost: float
    passed: bool
    error_message: Optional[str] = None
    confidence: float = 0.0

class AIBenchmarkSystemV23:
    """AI ë²¤ì¹˜ë§ˆí¬ ì‹œìŠ¤í…œ v2.3 - 99.2% ì •í™•ë„ ë‹¬ì„± ì‹œìŠ¤í…œ"""
    
    def __init__(self, target_accuracy: float = 99.2):
        self.target_accuracy = target_accuracy
        self.performance_data: Dict[str, ModelPerformance] = {}
        self.test_cases: List[BenchmarkTestCase] = []
        self.benchmark_results: List[BenchmarkResult] = []
        
        # ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
        self.monitoring_active = False
        self.monitoring_thread = None
        self.real_time_metrics = {
            "current_accuracy": 0.0,
            "avg_response_time": 0.0,
            "memory_usage": 0.0,
            "requests_per_minute": 0.0,
            "error_rate": 0.0
        }
        
        # ì„±ëŠ¥ ì„ê³„ê°’
        self.performance_thresholds = {
            "min_accuracy": 95.0,
            "max_response_time": 25.0,
            "max_memory_mb": 500.0,
            "max_error_rate": 5.0,
            "min_jewelry_relevance": 80.0
        }
        
        self._initialize_test_cases()
        self._setup_logging()
    
    def _setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('benchmark_v23.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def _initialize_test_cases(self):
        """í‘œì¤€ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì´ˆê¸°í™”"""
        
        # ë‹¤ì´ì•„ëª¬ë“œ 4C ë¶„ì„ í…ŒìŠ¤íŠ¸
        self.test_cases.extend([
            BenchmarkTestCase(
                test_id="diamond_4c_basic",
                category="diamond_analysis",
                input_data={
                    "text": "ì´ ë‹¤ì´ì•„ëª¬ë“œëŠ” 1.5ìºëŸ¿, Hì»¬ëŸ¬, VS1 í´ë˜ë¦¬í‹°, ì—‘ì…€ëŸ°íŠ¸ ì»·ì…ë‹ˆë‹¤.",
                    "context": "ë‹¤ì´ì•„ëª¬ë“œ ê°ì •"
                },
                expected_accuracy=0.98,
                max_response_time=15.0,
                difficulty_level="easy",
                jewelry_keywords=["ë‹¤ì´ì•„ëª¬ë“œ", "ìºëŸ¿", "ì»¬ëŸ¬", "í´ë˜ë¦¬í‹°", "ì»·"]
            ),
            BenchmarkTestCase(
                test_id="diamond_4c_complex",
                category="diamond_analysis",
                input_data={
                    "text": "GIA ê°ì •ì„œì— ë”°ë¥´ë©´ 2.3ct Round Brilliant, D/FL, Triple Excellent, í˜•ê´‘ì„± Noneì…ë‹ˆë‹¤.",
                    "context": "ê³ ê¸‰ ë‹¤ì´ì•„ëª¬ë“œ ê°ì •"
                },
                expected_accuracy=0.99,
                max_response_time=20.0,
                difficulty_level="hard",
                jewelry_keywords=["GIA", "Round Brilliant", "Triple Excellent", "í˜•ê´‘ì„±"]
            ),
            
            # ìœ ìƒ‰ë³´ì„ ë¶„ì„ í…ŒìŠ¤íŠ¸
            BenchmarkTestCase(
                test_id="colored_gemstone_ruby",
                category="colored_gemstone",
                input_data={
                    "text": "ë¯¸ì–€ë§ˆì‚° ë¹„ë‘˜ê¸°í”¼ ë£¨ë¹„ 3ìºëŸ¿, SSEF ê°ì •ì„œ, ë¬´ì²˜ë¦¬ ì²œì—°ì„ì…ë‹ˆë‹¤.",
                    "context": "ìœ ìƒ‰ë³´ì„ ê°ì •"
                },
                expected_accuracy=0.97,
                max_response_time=18.0,
                difficulty_level="medium",
                jewelry_keywords=["ë£¨ë¹„", "ë¯¸ì–€ë§ˆì‚°", "ë¹„ë‘˜ê¸°í”¼", "SSEF", "ë¬´ì²˜ë¦¬"]
            ),
            
            # ì£¼ì–¼ë¦¬ ë””ìì¸ ë¶„ì„ í…ŒìŠ¤íŠ¸
            BenchmarkTestCase(
                test_id="jewelry_design_cartier",
                category="jewelry_design",
                input_data={
                    "text": "Cartier Love ë¸Œë ˆì´ìŠ¬ë¦¿, 18K í™”ì´íŠ¸ê³¨ë“œ, ë‹¤ì´ì•„ëª¬ë“œ ì„¸íŒ…, ìŠ¤í¬ë¥˜ ë””ìì¸",
                    "context": "ì£¼ì–¼ë¦¬ ë””ìì¸ ë¶„ì„"
                },
                expected_accuracy=0.95,
                max_response_time=22.0,
                difficulty_level="medium",
                jewelry_keywords=["Cartier", "Love", "ë¸Œë ˆì´ìŠ¬ë¦¿", "í™”ì´íŠ¸ê³¨ë“œ", "ìŠ¤í¬ë¥˜"]
            ),
            
            # ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ í…ŒìŠ¤íŠ¸
            BenchmarkTestCase(
                test_id="business_market_trend",
                category="business_insight",
                input_data={
                    "text": "2024ë…„ ì•„ì‹œì•„ ì£¼ì–¼ë¦¬ ì‹œì¥ì€ í•©ì„±ë‹¤ì´ì•„ëª¬ë“œ ê¸‰ì„±ì¥, ì Šì€ì¸µ ì„ í˜¸ë„ ë³€í™”",
                    "context": "ì‹œì¥ ë¶„ì„"
                },
                expected_accuracy=0.93,
                max_response_time=25.0,
                difficulty_level="hard",
                jewelry_keywords=["ì•„ì‹œì•„", "í•©ì„±ë‹¤ì´ì•„ëª¬ë“œ", "ì‹œì¥", "íŠ¸ë Œë“œ"]
            )
        ])
    
    async def run_comprehensive_benchmark(self, models: List[str]) -> Dict[str, Any]:
        """ì¢…í•© ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        
        self.logger.info(f"ì¢…í•© ë²¤ì¹˜ë§ˆí¬ ì‹œì‘ - ëª©í‘œ ì •í™•ë„: {self.target_accuracy}%")
        
        # ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œì‘
        self.start_real_time_monitoring()
        
        benchmark_results = {}
        
        for model_name in models:
            self.logger.info(f"ëª¨ë¸ '{model_name}' ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")
            
            model_results = await self._benchmark_single_model(model_name)
            benchmark_results[model_name] = model_results
            
            # ì‹¤ì‹œê°„ ì„±ëŠ¥ ë¶„ì„
            real_time_analysis = self._analyze_real_time_performance(model_name)
            benchmark_results[model_name]["real_time_analysis"] = real_time_analysis
        
        # ëª¨ë¸ ê°„ ë¹„êµ ë¶„ì„
        comparison_analysis = self._compare_models(benchmark_results)
        
        # ìµœì í™” ê¶Œì¥ì‚¬í•­ ìƒì„±
        optimization_recommendations = self._generate_optimization_recommendations(benchmark_results)
        
        # ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€
        self.stop_real_time_monitoring()
        
        final_report = {
            "timestamp": time.time(),
            "target_accuracy": self.target_accuracy,
            "models_tested": len(models),
            "total_test_cases": len(self.test_cases),
            "model_results": benchmark_results,
            "comparison_analysis": comparison_analysis,
            "optimization_recommendations": optimization_recommendations,
            "achievement_status": self._calculate_achievement_status(benchmark_results)
        }
        
        # ë¦¬í¬íŠ¸ ì €ì¥
        self._save_benchmark_report(final_report)
        
        return final_report
    
    async def _benchmark_single_model(self, model_name: str) -> Dict[str, Any]:
        """ë‹¨ì¼ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬"""
        
        if model_name not in self.performance_data:
            self.performance_data[model_name] = ModelPerformance(model_name)
        
        model_perf = self.performance_data[model_name]
        test_results = []
        
        # ë³‘ë ¬ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        with ThreadPoolExecutor(max_workers=3) as executor:
            future_to_test = {
                executor.submit(self._execute_single_test, model_name, test_case): test_case
                for test_case in self.test_cases
            }
            
            for future in as_completed(future_to_test):
                test_case = future_to_test[future]
                try:
                    result = future.result()
                    test_results.append(result)
                    self._update_model_performance(model_perf, result)
                except Exception as e:
                    self.logger.error(f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì˜¤ë¥˜ {test_case.test_id}: {e}")
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
        performance_metrics = self._calculate_performance_metrics(model_perf, test_results)
        
        # 99.2% ì •í™•ë„ ë‹¬ì„± ì—¬ë¶€ í™•ì¸
        accuracy_achievement = performance_metrics["overall_accuracy"] >= self.target_accuracy
        
        return {
            "model_name": model_name,
            "test_results": test_results,
            "performance_metrics": performance_metrics,
            "accuracy_achievement": accuracy_achievement,
            "target_accuracy_gap": self.target_accuracy - performance_metrics["overall_accuracy"],
            "recommendations": self._generate_model_recommendations(model_perf, performance_metrics)
        }
    
    def _execute_single_test(self, model_name: str, test_case: BenchmarkTestCase) -> BenchmarkResult:
        """ë‹¨ì¼ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì‹¤í–‰"""
        
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        try:
            # ëª¨ë¸ë³„ ë¶„ì„ ì‹¤í–‰ (ì‹œë®¬ë ˆì´ì…˜)
            output, confidence = self._simulate_model_analysis(model_name, test_case)
            
            processing_time = time.time() - start_time
            memory_used = psutil.Process().memory_info().rss / 1024 / 1024 - start_memory
            
            # ì •í™•ë„ ê³„ì‚°
            accuracy = self._calculate_accuracy(output, test_case)
            
            # ì£¼ì–¼ë¦¬ ê´€ë ¨ì„± í‰ê°€
            jewelry_relevance = self._evaluate_jewelry_relevance(output, test_case.jewelry_keywords)
            
            # ë¹„ìš© ê³„ì‚°
            cost = self._calculate_cost(model_name, test_case.input_data, output)
            
            # ì„±ê³µ ì—¬ë¶€ íŒë‹¨
            passed = (
                accuracy >= test_case.expected_accuracy and
                processing_time <= test_case.max_response_time and
                jewelry_relevance >= 0.8
            )
            
            return BenchmarkResult(
                test_id=test_case.test_id,
                model_name=model_name,
                actual_output=output,
                accuracy_achieved=accuracy,
                response_time=processing_time,
                memory_used=memory_used,
                jewelry_relevance=jewelry_relevance,
                cost=cost,
                passed=passed,
                confidence=confidence
            )
            
        except Exception as e:
            return BenchmarkResult(
                test_id=test_case.test_id,
                model_name=model_name,
                actual_output="",
                accuracy_achieved=0.0,
                response_time=time.time() - start_time,
                memory_used=0.0,
                jewelry_relevance=0.0,
                cost=0.0,
                passed=False,
                error_message=str(e)
            )
    
    def _simulate_model_analysis(self, model_name: str, test_case: BenchmarkTestCase) -> Tuple[str, float]:
        """ëª¨ë¸ ë¶„ì„ ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œ êµ¬í˜„ì‹œ ëª¨ë¸ í˜¸ì¶œë¡œ ëŒ€ì²´)"""
        
        # ëª¨ë¸ë³„ ì„±ëŠ¥ íŠ¹ì„± ì‹œë®¬ë ˆì´ì…˜
        base_accuracy = {
            "gpt-4v": 0.95,
            "claude-vision": 0.93,
            "gemini-2.0": 0.91,
            "jewelry-specialized": 0.97,
            "hybrid-ensemble": 0.99
        }.get(model_name.lower(), 0.85)
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥ ë³´ì •
        category_bonus = {
            "diamond_analysis": 0.02,
            "colored_gemstone": 0.01,
            "jewelry_design": 0.015,
            "business_insight": 0.005
        }.get(test_case.category, 0.0)
        
        # ë‚œì´ë„ë³„ ì„±ëŠ¥ ë³´ì •
        difficulty_penalty = {
            "easy": 0.0,
            "medium": -0.01,
            "hard": -0.02
        }.get(test_case.difficulty_level, 0.0)
        
        # ìµœì¢… ì„±ëŠ¥ ê³„ì‚°
        performance = base_accuracy + category_bonus + difficulty_penalty
        performance = max(0.0, min(1.0, performance))
        
        # ë…¸ì´ì¦ˆ ì¶”ê°€ (ì‹¤ì œ ì„±ëŠ¥ ë³€ë™ì„± ì‹œë®¬ë ˆì´ì…˜)
        noise = np.random.normal(0, 0.01)
        performance = max(0.0, min(1.0, performance + noise))
        
        # ì²˜ë¦¬ ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜
        processing_delay = np.random.uniform(0.5, 2.0)
        time.sleep(processing_delay)
        
        # ì¶œë ¥ ìƒì„±
        output = self._generate_model_output(model_name, test_case, performance)
        confidence = performance
        
        return output, confidence
    
    def _generate_model_output(self, model_name: str, test_case: BenchmarkTestCase, performance: float) -> str:
        """ëª¨ë¸ ì¶œë ¥ ìƒì„±"""
        
        input_text = test_case.input_data.get("text", "")
        
        if test_case.category == "diamond_analysis":
            if performance > 0.95:
                return f"ì •í™•í•œ ë‹¤ì´ì•„ëª¬ë“œ 4C ë¶„ì„: {input_text}ì— ëŒ€í•œ ì „ë¬¸ì  ê°ì • ì™„ë£Œ. GIA ê¸°ì¤€ ì ìš©, ì‹œì¥ê°€ì¹˜ 95% ì‹ ë¢°ë„"
            else:
                return f"ë‹¤ì´ì•„ëª¬ë“œ ê¸°ë³¸ ë¶„ì„: {input_text[:50]}... ì¼ë°˜ì  íŠ¹ì„± íŒŒì•…"
        
        elif test_case.category == "colored_gemstone":
            if performance > 0.95:
                return f"ìœ ìƒ‰ë³´ì„ ì „ë¬¸ ë¶„ì„: {input_text}. ì›ì‚°ì§€, ì²˜ë¦¬ì—¬ë¶€, í’ˆì§ˆë“±ê¸‰ ì¢…í•©í‰ê°€ ì™„ë£Œ"
            else:
                return f"ìœ ìƒ‰ë³´ì„ ê¸°ë³¸ ë¶„ì„: {input_text[:50]}... ê¸°ë³¸ íŠ¹ì„± í™•ì¸"
        
        elif test_case.category == "business_insight":
            if performance > 0.95:
                return f"ì‹œì¥ ì¸ì‚¬ì´íŠ¸: {input_text}ì—ì„œ 3ê°€ì§€ í•µì‹¬ íŠ¸ë Œë“œ ë„ì¶œ, ë¹„ì¦ˆë‹ˆìŠ¤ ê¸°íšŒ 5ê°œ ì‹ë³„"
            else:
                return f"ì‹œì¥ ë¶„ì„: {input_text[:50]}... ì¼ë°˜ì  ë™í–¥ íŒŒì•…"
        
        else:
            return f"{model_name} ë¶„ì„ ê²°ê³¼: {input_text[:100]}..."
    
    def _calculate_accuracy(self, output: str, test_case: BenchmarkTestCase) -> float:
        """ì •í™•ë„ ê³„ì‚°"""
        
        # í‚¤ì›Œë“œ ë§¤ì¹­ ê¸°ë°˜ ì •í™•ë„
        keyword_matches = sum(1 for keyword in test_case.jewelry_keywords 
                            if keyword.lower() in output.lower())
        keyword_score = keyword_matches / max(1, len(test_case.jewelry_keywords))
        
        # ì¶œë ¥ í’ˆì§ˆ í‰ê°€
        quality_indicators = ["ì „ë¬¸ì ", "ì •í™•í•œ", "ì¢…í•©", "ë¶„ì„", "í‰ê°€", "ì™„ë£Œ", "ì‹ ë¢°ë„"]
        quality_matches = sum(1 for indicator in quality_indicators 
                            if indicator in output)
        quality_score = min(1.0, quality_matches / 3)
        
        # ê¸¸ì´ ê¸°ë°˜ ì™„ì„±ë„
        length_score = min(1.0, len(output) / 100)
        
        # ìµœì¢… ì •í™•ë„ (ê°€ì¤‘í‰ê· )
        accuracy = (keyword_score * 0.5 + quality_score * 0.3 + length_score * 0.2)
        
        # ì¹´í…Œê³ ë¦¬ë³„ ë³´ì •
        if test_case.category == "diamond_analysis" and "4C" in output:
            accuracy += 0.1
        elif test_case.category == "colored_gemstone" and any(gem in output for gem in ["ë£¨ë¹„", "ì‚¬íŒŒì´ì–´", "ì—ë©”ë„ë“œ"]):
            accuracy += 0.1
        
        return min(1.0, accuracy)
    
    def _evaluate_jewelry_relevance(self, output: str, keywords: List[str]) -> float:
        """ì£¼ì–¼ë¦¬ ê´€ë ¨ì„± í‰ê°€"""
        
        jewelry_terms = [
            "ë‹¤ì´ì•„ëª¬ë“œ", "ë£¨ë¹„", "ì‚¬íŒŒì´ì–´", "ì—ë©”ë„ë“œ", "ë³´ì„",
            "ìºëŸ¿", "GIA", "SSEF", "4C", "ì»¬ëŸ¬", "í´ë˜ë¦¬í‹°",
            "ì£¼ì–¼ë¦¬", "ë°˜ì§€", "ëª©ê±¸ì´", "ë¸Œë ˆì´ìŠ¬ë¦¿", "ê·€ê±¸ì´"
        ]
        
        all_terms = keywords + jewelry_terms
        matches = sum(1 for term in all_terms if term.lower() in output.lower())
        
        return min(1.0, matches / 5)
    
    def _calculate_cost(self, model_name: str, input_data: Dict[str, Any], output: str) -> float:
        """ë¹„ìš© ê³„ì‚°"""
        
        cost_per_token = {
            "gpt-4v": 0.00003,
            "claude-vision": 0.000025,
            "gemini-2.0": 0.00002,
            "jewelry-specialized": 0.0,
            "hybrid-ensemble": 0.00004
        }.get(model_name.lower(), 0.00001)
        
        input_tokens = len(str(input_data).split())
        output_tokens = len(output.split())
        total_tokens = input_tokens + output_tokens
        
        return cost_per_token * total_tokens
    
    def _update_model_performance(self, model_perf: ModelPerformance, result: BenchmarkResult):
        """ëª¨ë¸ ì„±ëŠ¥ ë°ì´í„° ì—…ë°ì´íŠ¸"""
        
        model_perf.total_requests += 1
        if result.passed:
            model_perf.successful_requests += 1
        
        model_perf.confidence_scores.append(result.confidence)
        model_perf.processing_times.append(result.response_time)
        
        # í‰ê· ê°’ ì—…ë°ì´íŠ¸
        model_perf.avg_response_time = statistics.mean(model_perf.processing_times)
        model_perf.accuracy_score = statistics.mean(model_perf.confidence_scores)
        model_perf.error_rate = ((model_perf.total_requests - model_perf.successful_requests) / 
                               max(1, model_perf.total_requests)) * 100
    
    def _calculate_performance_metrics(self, model_perf: ModelPerformance, 
                                     test_results: List[BenchmarkResult]) -> Dict[str, float]:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        
        if not test_results:
            return {"overall_accuracy": 0.0}
        
        accuracies = [r.accuracy_achieved for r in test_results]
        response_times = [r.response_time for r in test_results]
        jewelry_relevances = [r.jewelry_relevance for r in test_results]
        costs = [r.cost for r in test_results]
        
        return {
            "overall_accuracy": statistics.mean(accuracies) * 100,
            "accuracy_std": statistics.stdev(accuracies) if len(accuracies) > 1 else 0.0,
            "avg_response_time": statistics.mean(response_times),
            "response_time_p95": np.percentile(response_times, 95),
            "avg_jewelry_relevance": statistics.mean(jewelry_relevances) * 100,
            "total_cost": sum(costs),
            "cost_per_request": statistics.mean(costs),
            "success_rate": (sum(1 for r in test_results if r.passed) / len(test_results)) * 100,
            "target_achievement": statistics.mean(accuracies) * 100 >= self.target_accuracy
        }
    
    def _compare_models(self, benchmark_results: Dict[str, Any]) -> Dict[str, Any]:
        """ëª¨ë¸ ê°„ ë¹„êµ ë¶„ì„"""
        
        comparison = {
            "accuracy_ranking": [],
            "speed_ranking": [],
            "cost_efficiency_ranking": [],
            "overall_ranking": [],
            "target_achievement_models": []
        }
        
        models_data = []
        for model_name, results in benchmark_results.items():
            metrics = results["performance_metrics"]
            models_data.append({
                "model": model_name,
                "accuracy": metrics["overall_accuracy"],
                "speed": 1 / metrics["avg_response_time"],  # ì†ë„ ì ìˆ˜ (ì—­ìˆ˜)
                "cost_efficiency": 1 / (metrics["cost_per_request"] + 0.0001),  # ë¹„ìš© íš¨ìœ¨ì„±
                "overall_score": (metrics["overall_accuracy"] * 0.5 + 
                                (1 / metrics["avg_response_time"]) * 0.3 + 
                                (1 / (metrics["cost_per_request"] + 0.0001)) * 0.2)
            })
            
            if metrics["target_achievement"]:
                comparison["target_achievement_models"].append(model_name)
        
        # ë­í‚¹ ìƒì„±
        comparison["accuracy_ranking"] = sorted(models_data, key=lambda x: x["accuracy"], reverse=True)
        comparison["speed_ranking"] = sorted(models_data, key=lambda x: x["speed"], reverse=True)
        comparison["cost_efficiency_ranking"] = sorted(models_data, key=lambda x: x["cost_efficiency"], reverse=True)
        comparison["overall_ranking"] = sorted(models_data, key=lambda x: x["overall_score"], reverse=True)
        
        return comparison
    
    def _generate_optimization_recommendations(self, benchmark_results: Dict[str, Any]) -> List[Dict[str, str]]:
        """ìµœì í™” ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        
        recommendations = []
        
        for model_name, results in benchmark_results.items():
            metrics = results["performance_metrics"]
            
            # ì •í™•ë„ ê°œì„  ê¶Œì¥ì‚¬í•­
            if metrics["overall_accuracy"] < self.target_accuracy:
                gap = self.target_accuracy - metrics["overall_accuracy"]
                recommendations.append({
                    "model": model_name,
                    "category": "accuracy_improvement",
                    "priority": "high" if gap > 5.0 else "medium",
                    "recommendation": f"ì •í™•ë„ {gap:.1f}%p ê°œì„  í•„ìš”. ì£¼ì–¼ë¦¬ íŠ¹í™” í”„ë¡¬í”„íŠ¸ íŠœë‹ ë° ì•™ìƒë¸” ë°©ë²• ì ìš© ê¶Œì¥"
                })
            
            # ì†ë„ ê°œì„  ê¶Œì¥ì‚¬í•­
            if metrics["avg_response_time"] > self.performance_thresholds["max_response_time"]:
                recommendations.append({
                    "model": model_name,
                    "category": "speed_optimization",
                    "priority": "medium",
                    "recommendation": f"ì‘ë‹µì‹œê°„ {metrics['avg_response_time']:.1f}ì´ˆ â†’ 25ì´ˆ ì´í•˜ë¡œ ë‹¨ì¶• í•„ìš”. ìºì‹± ë° ë³‘ë ¬ì²˜ë¦¬ ë„ì… ê¶Œì¥"
                })
            
            # ë¹„ìš© ìµœì í™” ê¶Œì¥ì‚¬í•­
            if metrics["cost_per_request"] > 0.01:
                recommendations.append({
                    "model": model_name,
                    "category": "cost_optimization",
                    "priority": "low",
                    "recommendation": f"ìš”ì²­ë‹¹ ë¹„ìš© ${metrics['cost_per_request']:.4f} ìµœì í™” í•„ìš”. í† í° íš¨ìœ¨ì„± ê°œì„  ê¶Œì¥"
                })
        
        # ì „ì²´ ì‹œìŠ¤í…œ ê¶Œì¥ì‚¬í•­
        target_achieved_count = sum(1 for results in benchmark_results.values() 
                                  if results["performance_metrics"]["target_achievement"])
        
        if target_achieved_count == 0:
            recommendations.append({
                "model": "system_wide",
                "category": "architecture_improvement",
                "priority": "critical",
                "recommendation": "99.2% ì •í™•ë„ ë¯¸ë‹¬ì„±. í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸” ì‹œìŠ¤í…œ êµ¬ì¶• ë° ì£¼ì–¼ë¦¬ íŠ¹í™” ëª¨ë¸ ì¶”ê°€ í›ˆë ¨ í•„ìš”"
            })
        elif target_achieved_count < len(benchmark_results):
            recommendations.append({
                "model": "system_wide",
                "category": "model_optimization",
                "priority": "medium",
                "recommendation": f"{target_achieved_count}/{len(benchmark_results)} ëª¨ë¸ë§Œ ëª©í‘œ ë‹¬ì„±. ì„±ëŠ¥ ì €ì¡° ëª¨ë¸ êµì²´ ë˜ëŠ” ê°œì„  í•„ìš”"
            })
        
        return recommendations
    
    def _generate_model_recommendations(self, model_perf: ModelPerformance, 
                                      metrics: Dict[str, float]) -> List[str]:
        """ê°œë³„ ëª¨ë¸ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        
        recommendations = []
        
        if metrics["overall_accuracy"] < 95:
            recommendations.append("ì •í™•ë„ í–¥ìƒì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ í•„ìš”")
        
        if metrics["avg_response_time"] > 20:
            recommendations.append("ì‘ë‹µ ì†ë„ ê°œì„ ì„ ìœ„í•œ ëª¨ë¸ ìµœì í™” í•„ìš”")
        
        if metrics["avg_jewelry_relevance"] < 80:
            recommendations.append("ì£¼ì–¼ë¦¬ ë„ë©”ì¸ íŠ¹í™” í›ˆë ¨ ë°ì´í„° ì¶”ê°€ í•„ìš”")
        
        if model_perf.error_rate > 5:
            recommendations.append("ì—ëŸ¬ìœ¨ ê°ì†Œë¥¼ ìœ„í•œ ì•ˆì •ì„± ê°œì„  í•„ìš”")
        
        if not recommendations:
            recommendations.append("í˜„ì¬ ì„±ëŠ¥ ìˆ˜ì¤€ ìš°ìˆ˜, ì§€ì†ì  ëª¨ë‹ˆí„°ë§ ê¶Œì¥")
        
        return recommendations
    
    def _calculate_achievement_status(self, benchmark_results: Dict[str, Any]) -> Dict[str, Any]:
        """ëª©í‘œ ë‹¬ì„± ìƒíƒœ ê³„ì‚°"""
        
        total_models = len(benchmark_results)
        achieved_models = sum(1 for results in benchmark_results.values() 
                            if results["performance_metrics"]["target_achievement"])
        
        overall_accuracy = statistics.mean([
            results["performance_metrics"]["overall_accuracy"] 
            for results in benchmark_results.values()
        ])
        
        return {
            "target_accuracy": self.target_accuracy,
            "achieved_accuracy": overall_accuracy,
            "achievement_rate": (achieved_models / total_models) * 100,
            "models_achieving_target": achieved_models,
            "total_models": total_models,
            "status": "ì™„ë£Œ" if achieved_models == total_models else "ê°œì„  í•„ìš”"
        }
    
    def start_real_time_monitoring(self):
        """ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        
        self.monitoring_active = True
        self.monitoring_thread = threading.Thread(target=self._real_time_monitor_loop)
        self.monitoring_thread.daemon = True
        self.monitoring_thread.start()
        self.logger.info("ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œì‘")
    
    def stop_real_time_monitoring(self):
        """ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        
        self.monitoring_active = False
        if self.monitoring_thread:
            self.monitoring_thread.join(timeout=1.0)
        self.logger.info("ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€")
    
    def _real_time_monitor_loop(self):
        """ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ë£¨í”„"""
        
        while self.monitoring_active:
            try:
                # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§
                memory_usage = psutil.Process().memory_info().rss / 1024 / 1024
                cpu_usage = psutil.cpu_percent()
                
                self.real_time_metrics.update({
                    "memory_usage": memory_usage,
                    "cpu_usage": cpu_usage,
                    "timestamp": time.time()
                })
                
                # ì„±ëŠ¥ ê²½ê³  í™•ì¸
                self._check_performance_alerts()
                
                time.sleep(1.0)  # 1ì´ˆ ê°„ê²© ëª¨ë‹ˆí„°ë§
                
            except Exception as e:
                self.logger.error(f"ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜: {e}")
    
    def _check_performance_alerts(self):
        """ì„±ëŠ¥ ê²½ê³  í™•ì¸"""
        
        if self.real_time_metrics["memory_usage"] > self.performance_thresholds["max_memory_mb"]:
            self.logger.warning(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì´ˆê³¼: {self.real_time_metrics['memory_usage']:.1f}MB")
        
        # ì¶”ê°€ ê²½ê³  ë¡œì§...
    
    def _analyze_real_time_performance(self, model_name: str) -> Dict[str, Any]:
        """ì‹¤ì‹œê°„ ì„±ëŠ¥ ë¶„ì„"""
        
        return {
            "current_memory_usage": self.real_time_metrics.get("memory_usage", 0),
            "performance_trend": "stable",  # ì‹¤ì œë¡œëŠ” ì‹œê³„ì—´ ë¶„ì„ í•„ìš”
            "bottlenecks": [],
            "optimization_opportunities": []
        }
    
    def _save_benchmark_report(self, report: Dict[str, Any]):
        """ë²¤ì¹˜ë§ˆí¬ ë¦¬í¬íŠ¸ ì €ì¥"""
        
        timestamp = int(time.time())
        filename = f"benchmark_report_v23_{timestamp}.json"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"ë²¤ì¹˜ë§ˆí¬ ë¦¬í¬íŠ¸ ì €ì¥ ì™„ë£Œ: {filename}")
        except Exception as e:
            self.logger.error(f"ë¦¬í¬íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")

# ì‹¤í–‰ ì˜ˆì‹œ
async def main():
    """AI ë²¤ì¹˜ë§ˆí¬ ì‹œìŠ¤í…œ v2.3 ì‹¤í–‰ ì˜ˆì‹œ"""
    
    # ë²¤ì¹˜ë§ˆí¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    benchmark_system = AIBenchmarkSystemV23(target_accuracy=99.2)
    
    # í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ë“¤
    models_to_test = [
        "gpt-4v",
        "claude-vision", 
        "gemini-2.0",
        "jewelry-specialized",
        "hybrid-ensemble"
    ]
    
    print("ğŸš€ ì†”ë¡œëª¬ë“œ AI ì—”ì§„ ê³ ë„í™” í”„ë¡œì íŠ¸ v2.3")
    print("ğŸ“Š ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œìŠ¤í…œ ì‹œì‘...")
    print(f"ğŸ¯ ëª©í‘œ ì •í™•ë„: {benchmark_system.target_accuracy}%")
    print()
    
    # ì¢…í•© ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    results = await benchmark_system.run_comprehensive_benchmark(models_to_test)
    
    # ê²°ê³¼ ì¶œë ¥
    print("=" * 60)
    print("ğŸ“ˆ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìš”ì•½")
    print("=" * 60)
    
    achievement_status = results["achievement_status"]
    print(f"ğŸ¯ ëª©í‘œ ì •í™•ë„: {achievement_status['target_accuracy']}%")
    print(f"ğŸ“Š ë‹¬ì„± ì •í™•ë„: {achievement_status['achieved_accuracy']:.1f}%")
    print(f"âœ… ë‹¬ì„±ë¥ : {achievement_status['achievement_rate']:.1f}%")
    print(f"ğŸ† ëª©í‘œ ë‹¬ì„± ëª¨ë¸: {achievement_status['models_achieving_target']}/{achievement_status['total_models']}")
    print(f"ğŸ“ ìƒíƒœ: {achievement_status['status']}")
    print()
    
    # ëª¨ë¸ë³„ ì„±ëŠ¥ ìš”ì•½
    print("ğŸ” ëª¨ë¸ë³„ ì„±ëŠ¥ ìš”ì•½:")
    for model_name, model_results in results["model_results"].items():
        metrics = model_results["performance_metrics"]
        status = "âœ… ëª©í‘œë‹¬ì„±" if metrics["target_achievement"] else "âŒ ê°œì„ í•„ìš”"
        print(f"  {model_name}: {metrics['overall_accuracy']:.1f}% | {metrics['avg_response_time']:.1f}ì´ˆ | {status}")
    
    print()
    
    # ìµœì í™” ê¶Œì¥ì‚¬í•­
    recommendations = results["optimization_recommendations"]
    if recommendations:
        print("ğŸ’¡ ìµœì í™” ê¶Œì¥ì‚¬í•­:")
        for rec in recommendations[:3]:  # ìƒìœ„ 3ê°œë§Œ í‘œì‹œ
            priority_emoji = {"critical": "ğŸš¨", "high": "âš ï¸", "medium": "ğŸ“‹", "low": "ğŸ’­"}
            emoji = priority_emoji.get(rec["priority"], "ğŸ“‹")
            print(f"  {emoji} [{rec['priority'].upper()}] {rec['recommendation']}")
    
    print("\nğŸ‰ ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")
    return results

if __name__ == "__main__":
    asyncio.run(main())
