#!/usr/bin/env python3
"""
ğŸ”„ í¬ë¡œìŠ¤ëª¨ë‹¬ ìœµí•© ë ˆì´ì–´ - SOLOMOND AI ì§„ì •í•œ ë©€í‹°ëª¨ë‹¬ë¦¬í‹° êµ¬í˜„
Cross-Modal Fusion Layer: Inter-Modal Correlation Learning & Fused Representation

ğŸ¯ ì£¼ìš” ê¸°ëŠ¥:
1. ë©€í‹°í—¤ë“œ ì–´í…ì…˜ - ëª¨ë‹¬ê°„ ë³µì¡í•œ ê´€ê³„ í•™ìŠµ
2. ìƒê´€ê´€ê³„ ë¶„ì„ - ë†’ì€ ê´€ë ¨ì„± ìŒ íƒì§€
3. ìœµí•© í‘œí˜„ ìƒì„± - ëª¨ë“  ëª¨ë‹¬ë¦¬í‹° í†µí•© ë²¡í„°
4. ì‹œë§¨í‹± ì •ë ¬ - ì˜ë¯¸ì  ì¼ê´€ì„± ë³´ì¥
5. ì ì‘ì  ê°€ì¤‘ì¹˜ - ëª¨ë‹¬ ì¤‘ìš”ë„ ìë™ ì¡°ì •
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Any, Optional, Tuple, Union
from dataclasses import dataclass
import logging
from pathlib import Path
import json
import time
from sklearn.metrics.pairwise import cosine_similarity
from scipy.spatial.distance import pdist, squareform
from scipy.cluster.hierarchy import linkage, fcluster

# ë¡œì»¬ ì„í¬íŠ¸
from .multimodal_encoder import EncodedResult, MultimodalEncoder

# ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class FusionResult:
    """í¬ë¡œìŠ¤ëª¨ë‹¬ ìœµí•© ê²°ê³¼"""
    fused_embedding: np.ndarray  # ìµœì¢… ìœµí•©ëœ 768ì°¨ì› ë²¡í„°
    modal_weights: Dict[str, float]  # ëª¨ë‹¬ë³„ ê¸°ì—¬ë„
    cross_modal_correlations: List[Dict]  # ëª¨ë‹¬ê°„ ìƒê´€ê´€ê³„
    dominant_modality: str  # ê°€ì¥ ì˜í–¥ë ¥ ìˆëŠ” ëª¨ë‹¬
    fusion_confidence: float  # ìœµí•© ì‹ ë¢°ë„
    semantic_coherence: float  # ì˜ë¯¸ì  ì¼ê´€ì„± ì ìˆ˜
    metadata: Dict[str, Any]

class MultiHeadAttention(nn.Module):
    """ë©€í‹°í—¤ë“œ ì–´í…ì…˜ ëª¨ë“ˆ"""
    
    def __init__(self, d_model=768, num_heads=8):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        self.w_q = nn.Linear(d_model, d_model)
        self.w_k = nn.Linear(d_model, d_model)
        self.w_v = nn.Linear(d_model, d_model)
        self.w_o = nn.Linear(d_model, d_model)
        
        self.dropout = nn.Dropout(0.1)
        
    def forward(self, query, key, value, mask=None):
        batch_size = query.size(0)
        
        # Linear transformations and split into heads
        Q = self.w_q(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        K = self.w_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        V = self.w_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        
        # Attention
        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_k)
        
        if mask is not None:
            attention_scores = attention_scores.masked_fill(mask == 0, -1e9)
            
        attention_weights = F.softmax(attention_scores, dim=-1)
        attention_weights = self.dropout(attention_weights)
        
        # Apply attention to values
        attended = torch.matmul(attention_weights, V)
        
        # Concatenate heads and apply final linear layer
        attended = attended.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)
        output = self.w_o(attended)
        
        return output, attention_weights

class CrossModalFusionLayer:
    """í¬ë¡œìŠ¤ëª¨ë‹¬ ìœµí•© ë ˆì´ì–´ í•µì‹¬ ì—”ì§„"""
    
    def __init__(self, config: Optional[Dict] = None):
        self.config = config or self._get_default_config()
        
        # PyTorch ëª¨ë¸ë“¤
        self.attention_layer = MultiHeadAttention(
            d_model=self.config['embedding_dim'],
            num_heads=self.config['num_attention_heads']
        )
        self.fusion_network = self._build_fusion_network()
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        self.stats = {
            'fusion_operations': 0,
            'high_correlation_pairs': 0,
            'average_coherence': 0.0,
            'modality_contributions': {'image': [], 'audio': [], 'text': []}
        }
        
    def _get_default_config(self) -> Dict:
        """ê¸°ë³¸ ì„¤ì •"""
        return {
            'embedding_dim': 768,
            'num_attention_heads': 8,
            'correlation_threshold': 0.6,
            'fusion_layers': [768, 512, 768],
            'dropout_rate': 0.1,
            'temperature': 0.1,  # ì†Œí”„íŠ¸ë§¥ìŠ¤ ì˜¨ë„
            'min_modalities': 2,  # ìµœì†Œ ëª¨ë‹¬ ìˆ˜
            'coherence_weight': 0.3
        }
    
    def _build_fusion_network(self) -> nn.Module:
        """ìœµí•© ë„¤íŠ¸ì›Œí¬ êµ¬ì¶•"""
        layers = []
        layer_sizes = self.config['fusion_layers']
        
        for i in range(len(layer_sizes) - 1):
            layers.extend([
                nn.Linear(layer_sizes[i], layer_sizes[i + 1]),
                nn.ReLU(),
                nn.Dropout(self.config['dropout_rate'])
            ])
            
        # ë§ˆì§€ë§‰ ë ˆì´ì–´ëŠ” í™œì„±í™” í•¨ìˆ˜ ì—†ìŒ
        layers = layers[:-2]  # ë§ˆì§€ë§‰ ReLU, Dropout ì œê±°
        layers.append(nn.Tanh())  # ìµœì¢… ì •ê·œí™”ë¥¼ ìœ„í•œ Tanh
        
        return nn.Sequential(*layers)
    
    def fuse_multimodal_encodings(self, encoded_results: List[EncodedResult]) -> FusionResult:
        """ë©€í‹°ëª¨ë‹¬ ì¸ì½”ë”© ê²°ê³¼ ìœµí•©"""
        if len(encoded_results) < self.config['min_modalities']:
            logger.warning(f"ìœµí•©ì— í•„ìš”í•œ ìµœì†Œ ëª¨ë‹¬ ìˆ˜({self.config['min_modalities']}) ë¯¸ë‹¬")
            return self._create_single_modal_result(encoded_results[0] if encoded_results else None)
            
        logger.info(f"ğŸ”„ í¬ë¡œìŠ¤ëª¨ë‹¬ ìœµí•© ì‹œì‘: {len(encoded_results)}ê°œ ëª¨ë‹¬")
        start_time = time.time()
        
        # 1. ëª¨ë‹¬ê°„ ìƒê´€ê´€ê³„ ë¶„ì„
        correlations = self._analyze_cross_modal_correlations(encoded_results)
        
        # 2. ì–´í…ì…˜ ê¸°ë°˜ ëª¨ë‹¬ ê°€ì¤‘ì¹˜ ê³„ì‚°
        modal_weights = self._calculate_modal_weights(encoded_results, correlations)
        
        # 3. ìœµí•©ëœ í‘œí˜„ ìƒì„±
        fused_embedding = self._generate_fused_representation(encoded_results, modal_weights)
        
        # 4. ì˜ë¯¸ì  ì¼ê´€ì„± í‰ê°€
        coherence_score = self._evaluate_semantic_coherence(encoded_results, fused_embedding)
        
        # 5. ì§€ë°°ì  ëª¨ë‹¬ë¦¬í‹° ê²°ì •
        dominant_modality = max(modal_weights.items(), key=lambda x: x[1])[0]
        
        # 6. ìœµí•© ì‹ ë¢°ë„ ê³„ì‚°
        fusion_confidence = self._calculate_fusion_confidence(encoded_results, correlations, coherence_score)
        
        processing_time = time.time() - start_time
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        self._update_stats(modal_weights, correlations, coherence_score)
        
        result = FusionResult(
            fused_embedding=fused_embedding,
            modal_weights=modal_weights,
            cross_modal_correlations=correlations,
            dominant_modality=dominant_modality,
            fusion_confidence=fusion_confidence,
            semantic_coherence=coherence_score,
            metadata={
                'num_modalities': len(encoded_results),
                'processing_time': processing_time,
                'high_correlation_pairs': len([c for c in correlations if c['correlation'] > self.config['correlation_threshold']]),
                'modality_types': [r.modality for r in encoded_results]
            }
        )
        
        logger.info(f"âœ… í¬ë¡œìŠ¤ëª¨ë‹¬ ìœµí•© ì™„ë£Œ: ì‹ ë¢°ë„ {fusion_confidence:.3f}, ì¼ê´€ì„± {coherence_score:.3f}")
        return result
    
    def _analyze_cross_modal_correlations(self, encoded_results: List[EncodedResult]) -> List[Dict]:
        """ëª¨ë‹¬ê°„ ìƒê´€ê´€ê³„ ë¶„ì„"""
        correlations = []
        
        # ëª¨ë“  ìŒì— ëŒ€í•´ ìƒê´€ê´€ê³„ ê³„ì‚°
        for i, result_a in enumerate(encoded_results):
            for j, result_b in enumerate(encoded_results[i + 1:], i + 1):
                
                # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                similarity = cosine_similarity(
                    result_a.encoding.reshape(1, -1),
                    result_b.encoding.reshape(1, -1)
                )[0][0]
                
                # ìƒê´€ê´€ê³„ ì •ë³´ ì €ì¥
                correlation_info = {
                    'modality_pair': f"{result_a.modality}-{result_b.modality}",
                    'file_pair': (Path(result_a.file_path).name, Path(result_b.file_path).name),
                    'correlation': float(similarity),
                    'confidence_product': result_a.confidence * result_b.confidence,
                    'semantic_distance': 1.0 - similarity,
                    'is_high_correlation': similarity > self.config['correlation_threshold']
                }
                
                correlations.append(correlation_info)
                
        # ìƒê´€ê´€ê³„ ìˆœìœ¼ë¡œ ì •ë ¬
        correlations.sort(key=lambda x: x['correlation'], reverse=True)
        
        return correlations
    
    def _calculate_modal_weights(self, encoded_results: List[EncodedResult], correlations: List[Dict]) -> Dict[str, float]:
        """ì–´í…ì…˜ ê¸°ë°˜ ëª¨ë‹¬ ê°€ì¤‘ì¹˜ ê³„ì‚°"""
        
        # ê° ëª¨ë‹¬ì˜ ê¸°ë³¸ ê°€ì¤‘ì¹˜ (ì‹ ë¢°ë„ ê¸°ë°˜)
        base_weights = {}
        for result in encoded_results:
            base_weights[result.modality] = result.confidence
            
        # ìƒê´€ê´€ê³„ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì¡°ì •
        correlation_bonus = {modality: 0.0 for modality in base_weights.keys()}
        
        for corr in correlations:
            if corr['is_high_correlation']:
                modalities = corr['modality_pair'].split('-')
                bonus = corr['correlation'] * 0.2  # 20% ë³´ë„ˆìŠ¤
                
                for modality in modalities:
                    if modality in correlation_bonus:
                        correlation_bonus[modality] += bonus
                        
        # ìµœì¢… ê°€ì¤‘ì¹˜ ê³„ì‚° ë° ì •ê·œí™”
        final_weights = {}
        total_weight = 0.0
        
        for modality in base_weights.keys():
            weight = base_weights[modality] + correlation_bonus[modality]
            final_weights[modality] = weight
            total_weight += weight
            
        # ì •ê·œí™” (ì´í•© 1.0)
        if total_weight > 0:
            for modality in final_weights.keys():
                final_weights[modality] /= total_weight
        else:
            # ê· ë“± ê°€ì¤‘ì¹˜ë¡œ í´ë°±
            uniform_weight = 1.0 / len(final_weights)
            final_weights = {modality: uniform_weight for modality in final_weights.keys()}
            
        return final_weights
    
    def _generate_fused_representation(self, encoded_results: List[EncodedResult], modal_weights: Dict[str, float]) -> np.ndarray:
        """ìœµí•©ëœ í‘œí˜„ ìƒì„±"""
        
        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ê¸°ë³¸ ìœµí•©
        fused_embedding = np.zeros(self.config['embedding_dim'], dtype=np.float32)
        
        for result in encoded_results:
            weight = modal_weights.get(result.modality, 0.0)
            fused_embedding += weight * result.encoding
            
        # ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ ì ìš© (PyTorch)
        try:
            # ì„ë² ë”©ë“¤ì„ í…ì„œë¡œ ë³€í™˜
            embeddings_tensor = torch.FloatTensor([result.encoding for result in encoded_results])
            embeddings_tensor = embeddings_tensor.unsqueeze(0)  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
            
            # ì…€í”„ ì–´í…ì…˜ ì ìš©
            with torch.no_grad():
                attended_embeddings, attention_weights = self.attention_layer(
                    embeddings_tensor, embeddings_tensor, embeddings_tensor
                )
                
                # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ìµœì¢… ìœµí•©
                weights_tensor = torch.FloatTensor(list(modal_weights.values())).unsqueeze(0).unsqueeze(-1)
                final_fused = (attended_embeddings * weights_tensor).mean(dim=1).squeeze()
                
                fused_embedding = final_fused.numpy()
                
        except Exception as e:
            logger.warning(f"ì–´í…ì…˜ ì ìš© ì‹¤íŒ¨, ê¸°ë³¸ ìœµí•© ì‚¬ìš©: {e}")
            
        # L2 ì •ê·œí™”
        norm = np.linalg.norm(fused_embedding)
        if norm > 0:
            fused_embedding = fused_embedding / norm
            
        return fused_embedding
    
    def _evaluate_semantic_coherence(self, encoded_results: List[EncodedResult], fused_embedding: np.ndarray) -> float:
        """ì˜ë¯¸ì  ì¼ê´€ì„± í‰ê°€"""
        coherence_scores = []
        
        # ê° ëª¨ë‹¬ì´ ìœµí•© ê²°ê³¼ì™€ ì–¼ë§ˆë‚˜ ì¼ì¹˜í•˜ëŠ”ì§€ ì¸¡ì •
        for result in encoded_results:
            similarity = cosine_similarity(
                result.encoding.reshape(1, -1),
                fused_embedding.reshape(1, -1)
            )[0][0]
            coherence_scores.append(similarity * result.confidence)
            
        # ëª¨ë‹¬ê°„ ì¼ê´€ì„±ë„ ê³ ë ¤
        if len(encoded_results) > 1:
            embeddings = np.array([result.encoding for result in encoded_results])
            pairwise_similarities = cosine_similarity(embeddings)
            
            # ëŒ€ê°ì„  ì œì™¸í•œ í‰ê·  ìœ ì‚¬ë„
            mask = np.ones_like(pairwise_similarities, dtype=bool)
            np.fill_diagonal(mask, False)
            inter_modal_coherence = pairwise_similarities[mask].mean()
            coherence_scores.append(inter_modal_coherence)
            
        return float(np.mean(coherence_scores))
    
    def _calculate_fusion_confidence(self, encoded_results: List[EncodedResult], correlations: List[Dict], coherence: float) -> float:
        """ìœµí•© ì‹ ë¢°ë„ ê³„ì‚°"""
        
        # ê°œë³„ ëª¨ë‹¬ ì‹ ë¢°ë„ í‰ê· 
        individual_confidence = np.mean([result.confidence for result in encoded_results])
        
        # ìƒê´€ê´€ê³„ ì‹ ë¢°ë„
        if correlations:
            correlation_confidence = np.mean([corr['correlation'] for corr in correlations])
        else:
            correlation_confidence = 0.5
            
        # ëª¨ë‹¬ ìˆ˜ì— ë”°ë¥¸ ë³´ë„ˆìŠ¤ (ë” ë§ì€ ëª¨ë‹¬ = ë” ë†’ì€ ì‹ ë¢°ë„)
        modality_bonus = min(0.2, (len(encoded_results) - 1) * 0.05)
        
        # ìµœì¢… ì‹ ë¢°ë„ ê³„ì‚°
        fusion_confidence = (
            individual_confidence * 0.4 +
            correlation_confidence * 0.3 +
            coherence * 0.3 +
            modality_bonus
        )
        
        return min(1.0, fusion_confidence)
    
    def _create_single_modal_result(self, encoded_result: Optional[EncodedResult]) -> FusionResult:
        """ë‹¨ì¼ ëª¨ë‹¬ ê²°ê³¼ ìƒì„± (í´ë°±)"""
        if not encoded_result:
            # ë¹ˆ ê²°ê³¼
            return FusionResult(
                fused_embedding=np.zeros(self.config['embedding_dim'], dtype=np.float32),
                modal_weights={},
                cross_modal_correlations=[],
                dominant_modality="none",
                fusion_confidence=0.0,
                semantic_coherence=0.0,
                metadata={'single_modal_fallback': True}
            )
            
        return FusionResult(
            fused_embedding=encoded_result.encoding,
            modal_weights={encoded_result.modality: 1.0},
            cross_modal_correlations=[],
            dominant_modality=encoded_result.modality,
            fusion_confidence=encoded_result.confidence,
            semantic_coherence=1.0,  # ë‹¨ì¼ ëª¨ë‹¬ì´ë¯€ë¡œ ì™„ì „ ì¼ê´€ì„±
            metadata={
                'single_modal_fallback': True,
                'original_modality': encoded_result.modality
            }
        )
    
    def _update_stats(self, modal_weights: Dict[str, float], correlations: List[Dict], coherence: float):
        """í†µê³„ ì—…ë°ì´íŠ¸"""
        self.stats['fusion_operations'] += 1
        self.stats['high_correlation_pairs'] += sum(1 for c in correlations if c['is_high_correlation'])
        
        # ì´ë™ í‰ê· ìœ¼ë¡œ ì¼ê´€ì„± ì—…ë°ì´íŠ¸
        alpha = 0.1
        self.stats['average_coherence'] = (1 - alpha) * self.stats['average_coherence'] + alpha * coherence
        
        # ëª¨ë‹¬ë³„ ê¸°ì—¬ë„ ê¸°ë¡
        for modality, weight in modal_weights.items():
            if modality in self.stats['modality_contributions']:
                self.stats['modality_contributions'][modality].append(weight)
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
        avg_contributions = {}
        for modality, contributions in self.stats['modality_contributions'].items():
            avg_contributions[modality] = np.mean(contributions) if contributions else 0.0
            
        return {
            'total_fusions': self.stats['fusion_operations'],
            'high_correlation_pairs': self.stats['high_correlation_pairs'],
            'average_coherence': round(self.stats['average_coherence'], 3),
            'average_modal_contributions': avg_contributions,
            'correlation_rate': (
                self.stats['high_correlation_pairs'] / max(1, self.stats['fusion_operations'])
            )
        }
    
    def analyze_fusion_patterns(self, fusion_results: List[FusionResult]) -> Dict[str, Any]:
        """ìœµí•© íŒ¨í„´ ë¶„ì„"""
        if not fusion_results:
            return {'error': 'ë¶„ì„í•  ìœµí•© ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤'}
            
        # ì§€ë°°ì  ëª¨ë‹¬ë¦¬í‹° íŒ¨í„´
        dominant_modalities = [result.dominant_modality for result in fusion_results]
        modality_counts = {mod: dominant_modalities.count(mod) for mod in set(dominant_modalities)}
        
        # í‰ê·  ì‹ ë¢°ë„ ë° ì¼ê´€ì„±
        avg_confidence = np.mean([result.fusion_confidence for result in fusion_results])
        avg_coherence = np.mean([result.semantic_coherence for result in fusion_results])
        
        # ë†’ì€ ìƒê´€ê´€ê³„ íŒ¨í„´
        all_correlations = []
        for result in fusion_results:
            all_correlations.extend(result.cross_modal_correlations)
            
        high_corr_pairs = [corr for corr in all_correlations if corr['is_high_correlation']]
        
        return {
            'total_fusions': len(fusion_results),
            'dominant_modality_distribution': modality_counts,
            'average_fusion_confidence': round(avg_confidence, 3),
            'average_semantic_coherence': round(avg_coherence, 3),
            'high_correlation_patterns': len(high_corr_pairs),
            'most_correlated_pair': max(all_correlations, key=lambda x: x['correlation']) if all_correlations else None,
            'fusion_quality': 'excellent' if avg_confidence > 0.8 else 'good' if avg_confidence > 0.6 else 'needs_improvement'
        }

# ì‚¬ìš© ì˜ˆì œ
def main():
    """ì‚¬ìš© ì˜ˆì œ"""
    
    # ë©€í‹°ëª¨ë‹¬ ì¸ì½”ë” ì´ˆê¸°í™”
    encoder = MultimodalEncoder()
    fusion_layer = CrossModalFusionLayer()
    
    # í…ŒìŠ¤íŠ¸ íŒŒì¼ë“¤
    test_files = [
        Path("test_image.jpg"),
        Path("test_audio.wav"),
        Path("test_document.txt")
    ]
    
    existing_files = [f for f in test_files if f.exists()]
    
    if len(existing_files) >= 2:
        print("ğŸ”„ í¬ë¡œìŠ¤ëª¨ë‹¬ ìœµí•© ë ˆì´ì–´ í…ŒìŠ¤íŠ¸")
        print("=" * 50)
        
        # 1. ë©€í‹°ëª¨ë‹¬ ì¸ì½”ë”©
        encoded_results = encoder.encode_batch(existing_files)
        print(f"ğŸ“Š ì¸ì½”ë”© ì™„ë£Œ: {len(encoded_results)}ê°œ ëª¨ë‹¬")
        
        # 2. í¬ë¡œìŠ¤ëª¨ë‹¬ ìœµí•©
        fusion_result = fusion_layer.fuse_multimodal_encodings(encoded_results)
        
        print(f"\nğŸ¯ ìœµí•© ê²°ê³¼:")
        print(f"   ìœµí•© ì„ë² ë”© ì°¨ì›: {fusion_result.fused_embedding.shape}")
        print(f"   ì§€ë°°ì  ëª¨ë‹¬ë¦¬í‹°: {fusion_result.dominant_modality}")
        print(f"   ìœµí•© ì‹ ë¢°ë„: {fusion_result.fusion_confidence:.3f}")
        print(f"   ì˜ë¯¸ì  ì¼ê´€ì„±: {fusion_result.semantic_coherence:.3f}")
        
        print(f"\nâš–ï¸ ëª¨ë‹¬ ê°€ì¤‘ì¹˜:")
        for modality, weight in fusion_result.modal_weights.items():
            print(f"   {modality}: {weight:.3f}")
            
        print(f"\nğŸ”— í¬ë¡œìŠ¤ëª¨ë‹¬ ìƒê´€ê´€ê³„:")
        for corr in fusion_result.cross_modal_correlations[:3]:  # ìƒìœ„ 3ê°œë§Œ
            print(f"   {corr['modality_pair']}: {corr['correlation']:.3f} {'âœ“' if corr['is_high_correlation'] else ''}")
            
        # 3. ì„±ëŠ¥ í†µê³„
        stats = fusion_layer.get_performance_stats()
        print(f"\nğŸ“ˆ ì„±ëŠ¥ í†µê³„:")
        for key, value in stats.items():
            print(f"   {key}: {value}")
            
    else:
        print("âŒ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ìµœì†Œ 2ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ëª¨ë‹¬ë¦¬í‹° íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")

if __name__ == "__main__":
    main()