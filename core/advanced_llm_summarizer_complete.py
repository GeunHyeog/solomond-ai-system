"""
ì†”ë¡œëª¬ë“œ AI ì‹œìŠ¤í…œ - ì™„ì „í•œ ê³ ìš©ëŸ‰ ë‹¤ì¤‘ë¶„ì„ LLM ìš”ì•½ ì—”ì§„
5GB íŒŒì¼ 50ê°œ ë™ì‹œ ì²˜ë¦¬ + GEMMA í†µí•© ìš”ì•½ ì‹œìŠ¤í…œ

í•µì‹¬ ê¸°ëŠ¥:
1. ìŠ¤íŠ¸ë¦¬ë° ì²­í¬ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
2. ê³„ì¸µì  ìš”ì•½ ìƒì„± (ì²­í¬ â†’ ì†ŒìŠ¤ â†’ ìµœì¢…)
3. ì£¼ì–¼ë¦¬ ë„ë©”ì¸ íŠ¹í™” ë¶„ì„
4. í’ˆì§ˆ í‰ê°€ ë° ì‹ ë¢°ë„ ê²€ì¦
5. ëŒ€ìš©ëŸ‰ íŒŒì¼ ìµœì í™” ì²˜ë¦¬
"""

import asyncio
import json
import logging
from typing import Dict, List, Optional, Any
from datetime import datetime
from dataclasses import dataclass
import time
import gc
from pathlib import Path
import re
from collections import defaultdict

# ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    import torch
    from transformers import AutoModelForCausalLM, AutoTokenizer
    GEMMA_AVAILABLE = True
except ImportError:
    GEMMA_AVAILABLE = False

class EnhancedLLMSummarizer:
    """ê³ ìš©ëŸ‰ ë‹¤ì¤‘ë¶„ì„ ì „ìš© LLM ìš”ì•½ ì—”ì§„"""
    
    def __init__(self, model_name: str = "google/gemma-2b-it"):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # ì£¼ì–¼ë¦¬ íŠ¹í™” í”„ë¡¬í”„íŠ¸
        self.prompts = {
            "executive": """ë‹¤ìŒ ì£¼ì–¼ë¦¬ ì—…ê³„ ì •ë³´ë¥¼ ê²½ì˜ì§„ ê´€ì ì—ì„œ ìš”ì•½í•˜ì„¸ìš”:
- í•µì‹¬ ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ 
- ì‹œì¥ ê¸°íšŒ ë° ìœ„í—˜
- ì‹¤í–‰ ê°€ëŠ¥í•œ ì „ëµì  ê¶Œì¥ì‚¬í•­

ë‚´ìš©: {content}
ìš”ì•½ (í•œêµ­ì–´, 500ì):""",
            
            "technical": """ë‹¤ìŒ ì£¼ì–¼ë¦¬ ê¸°ìˆ  ì •ë³´ë¥¼ ì „ë¬¸ê°€ ê´€ì ì—ì„œ ìš”ì•½í•˜ì„¸ìš”:
- ë³´ì„í•™ì  íŠ¹ì§• ë° í’ˆì§ˆ ë¶„ì„
- ê°€ê³µ ê¸°ìˆ  ë° ì²˜ë¦¬ ë°©ë²•  
- ê°ì • ë° ì¸ì¦ ê´€ë ¨ ì‚¬í•­

ë‚´ìš©: {content}
ê¸°ìˆ  ìš”ì•½ (í•œêµ­ì–´, 600ì):""",
            
            "comprehensive": """ë‹¤ìŒ ì£¼ì–¼ë¦¬ ì—…ê³„ ì¢…í•© ì •ë³´ë¥¼ ì „ì²´ì ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”:
- ì‹œì¥ í˜„í™© ë° ì „ë§
- ê¸°ìˆ ì  íŠ¹ì§• ë° í’ˆì§ˆ ë¶„ì„
- ë¹„ì¦ˆë‹ˆìŠ¤ ê¸°íšŒ ë° ì „ëµ
- ì—…ê³„ íŠ¸ë Œë“œ ë° ë¯¸ë˜ ë°©í–¥

ë‚´ìš©: {content}
ì¢…í•© ìš”ì•½ (í•œêµ­ì–´, 1000ì):"""
        }
    
    async def initialize_model(self):
        """ëª¨ë¸ ì´ˆê¸°í™”"""
        if self.model is not None:
            return
            
        try:
            if GEMMA_AVAILABLE:
                print("ğŸ¤– GEMMA ëª¨ë¸ ë¡œë”© ì¤‘...")
                self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    torch_dtype=torch.float16,
                    device_map="auto"
                )
                print("âœ… GEMMA ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            else:
                print("âš ï¸ GEMMA ëª¨ì˜ ëª¨ë“œë¡œ ì‹¤í–‰")
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
    
    async def process_large_batch(self, files_data: List[Dict]) -> Dict:
        """ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜"""
        print(f"ğŸš€ ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: {len(files_data)}ê°œ íŒŒì¼")
        start_time = time.time()
        
        try:
            # 1. ëª¨ë¸ ì´ˆê¸°í™”
            await self.initialize_model()
            
            # 2. íŒŒì¼ ë¶„ë¥˜ ë° ê²€ì¦
            classified_files = self._classify_files(files_data)
            
            # 3. ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„í• 
            chunks = await self._create_chunks(classified_files)
            
            # 4. ë³‘ë ¬ ì²˜ë¦¬
            processed_chunks = await self._process_chunks_parallel(chunks)
            
            # 5. ê³„ì¸µì  ìš”ì•½ ìƒì„±
            hierarchical_summary = await self._generate_hierarchical_summary(processed_chunks)
            
            # 6. í’ˆì§ˆ í‰ê°€
            quality_assessment = await self._assess_summary_quality(hierarchical_summary, processed_chunks)
            
            # 7. ìµœì¢… ë³´ê³ ì„œ ìƒì„±
            final_report = {
                "success": True,
                "session_id": f"batch_{int(time.time())}",
                "processing_time": time.time() - start_time,
                "files_processed": len(files_data),
                "chunks_processed": len(processed_chunks),
                "hierarchical_summary": hierarchical_summary,
                "quality_assessment": quality_assessment,
                "recommendations": self._generate_recommendations(quality_assessment)
            }
            
            print(f"âœ… ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ ({final_report['processing_time']:.1f}ì´ˆ)")
            return final_report
            
        except Exception as e:
            logging.error(f"ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return {"success": False, "error": str(e)}
    
    def _classify_files(self, files_data: List[Dict]) -> Dict:
        """íŒŒì¼ ë¶„ë¥˜"""
        classified = {
            "audio": [],
            "video": [], 
            "documents": [],
            "images": []
        }
        
        for file_data in files_data:
            filename = file_data.get("filename", "")
            ext = Path(filename).suffix.lower()
            
            if ext in ['.mp3', '.wav', '.m4a']:
                classified["audio"].append(file_data)
            elif ext in ['.mp4', '.avi', '.mov']:
                classified["video"].append(file_data)
            elif ext in ['.pdf', '.docx', '.txt']:
                classified["documents"].append(file_data)
            elif ext in ['.jpg', '.jpeg', '.png']:
                classified["images"].append(file_data)
        
        return classified
    
    async def _create_chunks(self, classified_files: Dict) -> List[Dict]:
        """ì²­í¬ ìƒì„±"""
        chunks = []
        chunk_id = 0
        
        for file_type, files in classified_files.items():
            for file_data in files:
                text = file_data.get("processed_text", "")
                if not text:
                    continue
                
                # í…ìŠ¤íŠ¸ë¥¼ 1000ì ë‹¨ìœ„ë¡œ ì²­í¬ ë¶„í• 
                chunk_size = 1000
                for i in range(0, len(text), chunk_size):
                    chunk_text = text[i:i+chunk_size]
                    
                    chunks.append({
                        "chunk_id": f"chunk_{chunk_id:04d}",
                        "source_file": file_data.get("filename", ""),
                        "source_type": file_type,
                        "text": chunk_text,
                        "token_count": len(chunk_text.split()),
                        "jewelry_terms": self._extract_jewelry_terms(chunk_text)
                    })
                    chunk_id += 1
        
        return chunks
    
    async def _process_chunks_parallel(self, chunks: List[Dict]) -> List[Dict]:
        """ë³‘ë ¬ ì²­í¬ ì²˜ë¦¬"""
        print(f"âš¡ ë³‘ë ¬ ì²˜ë¦¬: {len(chunks)}ê°œ ì²­í¬")
        
        # ìµœëŒ€ 10ê°œì”© ë°°ì¹˜ ì²˜ë¦¬
        max_concurrent = 10
        processed_chunks = []
        
        for i in range(0, len(chunks), max_concurrent):
            batch = chunks[i:i+max_concurrent]
            
            # ë°°ì¹˜ ë‚´ ë³‘ë ¬ ì²˜ë¦¬
            tasks = [self._process_single_chunk(chunk) for chunk in batch]
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for result in batch_results:
                if not isinstance(result, Exception):
                    processed_chunks.append(result)
        
        return processed_chunks
    
    async def _process_single_chunk(self, chunk: Dict) -> Dict:
        """ê°œë³„ ì²­í¬ ì²˜ë¦¬"""
        try:
            # ì£¼ì–¼ë¦¬ ìš©ì–´ ê°•í™”
            enhanced_text = self._enhance_jewelry_content(chunk["text"])
            
            # ì²­í¬ ìš”ì•½ ìƒì„±
            summary = await self._generate_chunk_summary(enhanced_text)
            
            chunk["summary"] = summary
            chunk["enhanced_text"] = enhanced_text
            
            return chunk
            
        except Exception as e:
            logging.error(f"ì²­í¬ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return chunk
    
    async def _generate_chunk_summary(self, text: str) -> str:
        """ì²­í¬ ìš”ì•½ ìƒì„±"""
        if GEMMA_AVAILABLE and self.model is not None:
            return await self._generate_with_gemma(text, "comprehensive")
        else:
            return await self._generate_mock_summary(text)
    
    async def _generate_with_gemma(self, text: str, summary_type: str) -> str:
        """GEMMA ëª¨ë¸ë¡œ ìš”ì•½ ìƒì„±"""
        try:
            prompt = self.prompts[summary_type].format(content=text)
            
            inputs = self.tokenizer.encode(prompt, return_tensors="pt", max_length=2048, truncation=True)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    max_length=inputs.shape[1] + 200,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            summary = generated_text[len(prompt):].strip()
            
            return summary
            
        except Exception as e:
            logging.error(f"GEMMA ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {e}")
            return await self._generate_mock_summary(text)
    
    async def _generate_mock_summary(self, text: str) -> str:
        """ëª¨ì˜ ìš”ì•½ ìƒì„±"""
        await asyncio.sleep(0.1)
        
        sentences = text.split('. ')
        if len(sentences) <= 3:
            return text
        
        # ì£¼ì–¼ë¦¬ í‚¤ì›Œë“œ ìš°ì„  ì„ íƒ
        jewelry_keywords = ["ë‹¤ì´ì•„ëª¬ë“œ", "ë£¨ë¹„", "ì‚¬íŒŒì´ì–´", "ì—ë©”ë„ë“œ", "4C", "GIA", "ìºëŸ¿"]
        
        scored_sentences = []
        for sentence in sentences:
            score = sum(1 for keyword in jewelry_keywords if keyword in sentence)
            scored_sentences.append((sentence, score))
        
        top_sentences = sorted(scored_sentences, key=lambda x: x[1], reverse=True)[:3]
        return '. '.join([s[0] for s in top_sentences]) + '.'
    
    async def _generate_hierarchical_summary(self, chunks: List[Dict]) -> Dict:
        """ê³„ì¸µì  ìš”ì•½ ìƒì„±"""
        print("ğŸ“Š ê³„ì¸µì  ìš”ì•½ ìƒì„± ì¤‘...")
        
        # 1. ì†ŒìŠ¤ë³„ ê·¸ë£¹í™”
        source_groups = defaultdict(list)
        for chunk in chunks:
            source_groups[chunk["source_type"]].append(chunk)
        
        # 2. ì†ŒìŠ¤ë³„ ì¤‘ê°„ ìš”ì•½
        source_summaries = {}
        for source_type, source_chunks in source_groups.items():
            combined_text = " ".join([chunk.get("summary", chunk["text"]) for chunk in source_chunks])
            
            if GEMMA_AVAILABLE and self.model is not None:
                summary = await self._generate_with_gemma(combined_text, "comprehensive")
            else:
                summary = await self._generate_mock_summary(combined_text)
            
            source_summaries[source_type] = {
                "summary": summary,
                "chunk_count": len(source_chunks),
                "total_tokens": sum(chunk["token_count"] for chunk in source_chunks)
            }
        
        # 3. ìµœì¢… í†µí•© ìš”ì•½
        all_summaries = " ".join([s["summary"] for s in source_summaries.values()])
        
        if GEMMA_AVAILABLE and self.model is not None:
            final_summary = await self._generate_with_gemma(all_summaries, "comprehensive")
        else:
            final_summary = await self._generate_mock_summary(all_summaries)
        
        return {
            "final_summary": final_summary,
            "source_summaries": source_summaries,
            "total_chunks": len(chunks)
        }
    
    async def _assess_summary_quality(self, hierarchical_summary: Dict, chunks: List[Dict]) -> Dict:
        """ìš”ì•½ í’ˆì§ˆ í‰ê°€"""
        print("ğŸ” ìš”ì•½ í’ˆì§ˆ í‰ê°€ ì¤‘...")
        
        final_summary = hierarchical_summary["final_summary"]
        
        # í‚¤ì›Œë“œ ì»¤ë²„ë¦¬ì§€ ë¶„ì„
        all_jewelry_terms = set()
        for chunk in chunks:
            all_jewelry_terms.update(chunk["jewelry_terms"])
        
        summary_terms = set(self._extract_jewelry_terms(final_summary))
        coverage_ratio = len(summary_terms) / max(len(all_jewelry_terms), 1)
        
        # ì••ì¶•ë¥  ë¶„ì„
        original_length = sum(len(chunk["text"]) for chunk in chunks)
        summary_length = len(final_summary)
        compression_ratio = summary_length / max(original_length, 1)
        
        # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        quality_score = (coverage_ratio * 0.4 + (1 - compression_ratio) * 0.3 + 0.3) * 100
        
        return {
            "quality_score": min(100, max(0, quality_score)),
            "coverage_ratio": coverage_ratio,
            "compression_ratio": compression_ratio,
            "summary_length": summary_length,
            "original_length": original_length,
            "jewelry_terms_found": len(summary_terms),
            "jewelry_terms_total": len(all_jewelry_terms)
        }
    
    def _generate_recommendations(self, quality_assessment: Dict) -> List[str]:
        """í’ˆì§ˆ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        quality_score = quality_assessment["quality_score"]
        
        if quality_score >= 85:
            recommendations.append("âœ… ìš°ìˆ˜í•œ í’ˆì§ˆì˜ ìš”ì•½ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        elif quality_score >= 70:
            recommendations.append("âš ï¸ ì–‘í˜¸í•œ í’ˆì§ˆì´ë‚˜ ì¶”ê°€ ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        else:
            recommendations.append("âŒ í’ˆì§ˆ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤. ë” ë§ì€ ë°ì´í„°ë‚˜ ìˆ˜ë™ ê²€í† ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.")
        
        if quality_assessment["coverage_ratio"] < 0.6:
            recommendations.append("ğŸ’¡ ì£¼ì–¼ë¦¬ ì „ë¬¸ ìš©ì–´ ì»¤ë²„ë¦¬ì§€ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ë„ë©”ì¸ íŠ¹í™” ë³´ê°•ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        if quality_assessment["compression_ratio"] > 0.8:
            recommendations.append("ğŸ“ ì••ì¶•ë¥ ì´ ë‚®ìŠµë‹ˆë‹¤. ë” ê°„ê²°í•œ ìš”ì•½ì´ ê°€ëŠ¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        return recommendations
    
    def _extract_jewelry_terms(self, text: str) -> List[str]:
        """ì£¼ì–¼ë¦¬ ìš©ì–´ ì¶”ì¶œ"""
        jewelry_terms = [
            "ë‹¤ì´ì•„ëª¬ë“œ", "ë£¨ë¹„", "ì‚¬íŒŒì´ì–´", "ì—ë©”ë„ë“œ", "4C", "GIA", "ìºëŸ¿", 
            "ì»¬ëŸ¬", "í´ë˜ë¦¬í‹°", "ì»·", "ë„ë§¤ê°€", "ì†Œë§¤ê°€", "ì¸ì¦ì„œ", "ê°ì •ì„œ",
            "í”„ë¦°ì„¸ìŠ¤ ì»·", "ë¼ìš´ë“œ ë¸Œë¦´ë¦¬ì–¸íŠ¸", "VS", "VVS", "IF", "FL"
        ]
        
        found_terms = []
        text_lower = text.lower()
        
        for term in jewelry_terms:
            if term.lower() in text_lower:
                found_terms.append(term)
        
        return found_terms
    
    def _enhance_jewelry_content(self, text: str) -> str:
        """ì£¼ì–¼ë¦¬ ì½˜í…ì¸  í–¥ìƒ"""
        # ìš©ì–´ ì •ê·œí™”
        term_mapping = {
            "4ì”¨": "4C",
            "ì§€ì•„": "GIA",
            "ë¸Œë¦´ë¦¬ì–¸íŠ¸": "Brilliant",
            "í”„ë¦°ì„¸ìŠ¤": "Princess"
        }
        
        enhanced = text
        for korean, english in term_mapping.items():
            enhanced = enhanced.replace(korean, f"{korean}({english})")
        
        return enhanced

# ì‚¬ìš© ì˜ˆì‹œ ë° í…ŒìŠ¤íŠ¸
async def test_enhanced_summarizer():
    """í–¥ìƒëœ ìš”ì•½ê¸° í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª í–¥ìƒëœ LLM ìš”ì•½ê¸° í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    summarizer = EnhancedLLMSummarizer()
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    test_files = [
        {
            "filename": "diamond_market_2025.mp3",
            "processed_text": "2025ë…„ ë‹¤ì´ì•„ëª¬ë“œ ì‹œì¥ ì „ë§ì— ëŒ€í•´ ë§ì”€ë“œë¦¬ê² ìŠµë‹ˆë‹¤. 4C ë“±ê¸‰ ì¤‘ì—ì„œ íŠ¹íˆ ì»¬ëŸ¬ì™€ í´ë˜ë¦¬í‹° ë“±ê¸‰ì´ ê°€ê²©ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì´ í´ ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤. GIA ì¸ì¦ì„œì˜ ì¤‘ìš”ì„±ì´ ë”ìš± ê°•ì¡°ë˜ê³  ìˆìœ¼ë©°, í”„ë¦°ì„¸ìŠ¤ ì»·ê³¼ ë¼ìš´ë“œ ë¸Œë¦´ë¦¬ì–¸íŠ¸ ì»·ì˜ ìˆ˜ìš”ê°€ ì¦ê°€í•˜ê³  ìˆìŠµë‹ˆë‹¤."
        },
        {
            "filename": "ruby_pricing.pdf", 
            "processed_text": "ë²„ë§ˆì‚° ë£¨ë¹„ì˜ ê°€ê²© ë™í–¥ì„ ë¶„ì„í•´ë³´ê² ìŠµë‹ˆë‹¤. 1ìºëŸ¿ ê¸°ì¤€ìœ¼ë¡œ íˆíŠ¸ íŠ¸ë¦¬íŠ¸ë¨¼íŠ¸ê°€ ë˜ì§€ ì•Šì€ ì²œì—° ë£¨ë¹„ì˜ ê²½ìš° $4,000ì—ì„œ $6,000 ì‚¬ì´ì˜ ê°€ê²©ëŒ€ë¥¼ í˜•ì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤. ìƒ‰ìƒì˜ ìˆœë„ì™€ íˆ¬ëª…ë„ê°€ ê°€ê²© ê²°ì •ì— ê°€ì¥ ì¤‘ìš”í•œ ìš”ì†Œì…ë‹ˆë‹¤."
        },
        {
            "filename": "emerald_certification.jpg",
            "processed_text": "ì—ë©”ë„ë“œ ê°ì •ì„œì— ë‚˜íƒ€ë‚œ ì£¼ìš” ì •ë³´ë“¤ì…ë‹ˆë‹¤. ì½œë¡¬ë¹„ì•„ì‚° ì—ë©”ë„ë“œ 2.15ìºëŸ¿, ì»¬ëŸ¬ ë“±ê¸‰ Vivid Green, í´ë˜ë¦¬í‹° VS, ì˜¤ì¼ íŠ¸ë¦¬íŠ¸ë¨¼íŠ¸ Minor. GIA ì¸ì¦ì„œ ë²ˆí˜¸ 5141234567ì…ë‹ˆë‹¤."
        }
    ]
    
    # ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰
    result = await summarizer.process_large_batch(test_files)
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*60)
    print("ğŸ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ ê²°ê³¼")
    print("="*60)
    print(f"ì„±ê³µ ì—¬ë¶€: {result['success']}")
    print(f"ì²˜ë¦¬ ì‹œê°„: {result.get('processing_time', 0):.2f}ì´ˆ")
    print(f"ì²˜ë¦¬ëœ íŒŒì¼: {result.get('files_processed', 0)}ê°œ")
    print(f"ì²˜ë¦¬ëœ ì²­í¬: {result.get('chunks_processed', 0)}ê°œ")
    
    if result['success']:
        print(f"\nğŸ“Š ìµœì¢… ìš”ì•½:")
        print(result['hierarchical_summary']['final_summary'])
        
        print(f"\nğŸ¯ í’ˆì§ˆ í‰ê°€:")
        qa = result['quality_assessment']
        print(f"- í’ˆì§ˆ ì ìˆ˜: {qa['quality_score']:.1f}/100")
        print(f"- í‚¤ì›Œë“œ ì»¤ë²„ë¦¬ì§€: {qa['coverage_ratio']:.1%}")
        print(f"- ì••ì¶•ë¥ : {qa['compression_ratio']:.1%}")
        
        print(f"\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
        for rec in result['recommendations']:
            print(f"  {rec}")
    
    return result

# ë©”ì¸ ì‹¤í–‰
if __name__ == "__main__":
    asyncio.run(test_enhanced_summarizer())
