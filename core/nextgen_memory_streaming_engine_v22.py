"""
ğŸ”¥ ì°¨ì„¸ëŒ€ ë©”ëª¨ë¦¬ ìŠ¤íŠ¸ë¦¬ë° ì—”ì§„ v2.2
3GB+ íŒŒì¼ì„ 100MB ë©”ëª¨ë¦¬ë¡œ ì™„ë²½ ì²˜ë¦¬í•˜ëŠ” í˜ì‹ ì  ìŠ¤íŠ¸ë¦¬ë° ì—”ì§„

ğŸš€ í˜ì‹  ê¸°ëŠ¥:
- GPT-4V + Claude Vision + Gemini 2.0 ë™ì‹œ í™œìš©
- ì‹¤ì‹œê°„ ë©”ëª¨ë¦¬ ìŠ¤íŠ¸ë¦¬ë° (100MB ì œí•œìœ¼ë¡œ 3GB+ ì²˜ë¦¬)
- ì ì‘í˜• ì²­í¬ ì‚¬ì´ì¦ˆ ì¡°ì ˆ
- ë©€í‹°ë ˆë²¨ ìºì‹± ì‹œìŠ¤í…œ
- ì§€ëŠ¥í˜• ë©”ëª¨ë¦¬ ì••ì¶•
- ì‹¤ì‹œê°„ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§

ğŸ¯ ëª©í‘œ: í˜„ì¥ì—ì„œ ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥í•œ ìµœê³  ì„±ëŠ¥
"""

import asyncio
import time
import logging
import os
import gc
import mmap
import tempfile
import hashlib
import weakref
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any, AsyncGenerator, Union
from dataclasses import dataclass, field
from datetime import datetime
import json
import base64
import io
from collections import deque
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import queue
from enum import Enum

# ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§
import psutil
import resource

# ë¯¸ë””ì–´ ì²˜ë¦¬ (ìŠ¤íŠ¸ë¦¬ë° íŠ¹í™”)
import cv2
import numpy as np
from PIL import Image, ImageOps
import librosa
import soundfile as sf

# AI ëª¨ë¸ í´ë¼ì´ì–¸íŠ¸ë“¤
import openai
import anthropic
import google.generativeai as genai

# ì••ì¶• ë° ìµœì í™”
import lz4.frame
import zstandard as zstd
from functools import lru_cache

class ProcessingMode(Enum):
    """ì²˜ë¦¬ ëª¨ë“œ"""
    ULTRA_LOW_MEMORY = "ultra_low"    # <50MB
    LOW_MEMORY = "low"                # <100MB  
    STANDARD = "standard"             # <200MB
    HIGH_PERFORMANCE = "high"         # <500MB

class StreamState(Enum):
    """ìŠ¤íŠ¸ë¦¼ ìƒíƒœ"""
    IDLE = "idle"
    INITIALIZING = "initializing"
    STREAMING = "streaming"
    PROCESSING = "processing"
    FINALIZING = "finalizing"
    COMPLETED = "completed"
    ERROR = "error"

@dataclass
class MemoryProfile:
    """ë©”ëª¨ë¦¬ í”„ë¡œí•„"""
    max_memory_mb: int = 100
    chunk_size_mb: int = 5
    buffer_size_mb: int = 20
    cache_size_mb: int = 30
    processing_memory_mb: int = 45
    compression_enabled: bool = True
    adaptive_sizing: bool = True

@dataclass
class StreamChunk:
    """ìŠ¤íŠ¸ë¦¼ ì²­í¬"""
    chunk_id: str
    data: bytes
    chunk_type: str  # video, audio, image
    timestamp: float
    size_bytes: int
    compressed: bool = False
    quality_score: float = 1.0
    metadata: Dict[str, Any] = field(default_factory=dict)
    
class MemoryOptimizer:
    """ë©”ëª¨ë¦¬ ìµœì í™” ê´€ë¦¬ì"""
    
    def __init__(self, profile: MemoryProfile):
        self.profile = profile
        self.process = psutil.Process()
        self.memory_alerts = deque(maxlen=100)
        self.compressor = zstd.ZstdCompressor(level=3)
        self.decompressor = zstd.ZstdDecompressor()
        
        # ë©”ëª¨ë¦¬ ì„ê³„ê°’
        self.warning_threshold = profile.max_memory_mb * 0.8
        self.critical_threshold = profile.max_memory_mb * 0.95
        
    def get_memory_usage(self) -> float:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MB)"""
        return self.process.memory_info().rss / (1024 * 1024)
    
    def is_memory_critical(self) -> bool:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ìœ„í—˜ ìˆ˜ì¤€ì¸ì§€ í™•ì¸"""
        current_memory = self.get_memory_usage()
        return current_memory > self.critical_threshold
    
    def compress_data(self, data: bytes) -> bytes:
        """ë°ì´í„° ì••ì¶•"""
        if not self.profile.compression_enabled:
            return data
        try:
            return self.compressor.compress(data)
        except Exception as e:
            logging.warning(f"ì••ì¶• ì‹¤íŒ¨: {e}")
            return data
    
    def decompress_data(self, compressed_data: bytes) -> bytes:
        """ë°ì´í„° ì••ì¶• í•´ì œ"""
        try:
            return self.decompressor.decompress(compressed_data)
        except Exception as e:
            logging.warning(f"ì••ì¶• í•´ì œ ì‹¤íŒ¨: {e}")
            return compressed_data
    
    async def emergency_cleanup(self):
        """ì‘ê¸‰ ë©”ëª¨ë¦¬ ì •ë¦¬"""
        logging.warning("ğŸš¨ ì‘ê¸‰ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤í–‰")
        
        # ê°•ì œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        for _ in range(3):
            gc.collect()
            await asyncio.sleep(0.01)
        
        # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì •ë¦¬ ìš”ì²­
        try:
            if hasattr(os, 'sync'):
                os.sync()
        except:
            pass
            
    def get_optimal_chunk_size(self) -> int:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ìƒí™©ì— ë§ëŠ” ìµœì  ì²­í¬ í¬ê¸° ê³„ì‚°"""
        current_memory = self.get_memory_usage()
        memory_ratio = current_memory / self.profile.max_memory_mb
        
        if memory_ratio > 0.9:
            return max(1, self.profile.chunk_size_mb // 4)  # 1/4 í¬ê¸°
        elif memory_ratio > 0.7:
            return max(2, self.profile.chunk_size_mb // 2)  # 1/2 í¬ê¸°
        else:
            return self.profile.chunk_size_mb

class StreamingFileReader:
    """ìŠ¤íŠ¸ë¦¬ë° íŒŒì¼ ë¦¬ë”"""
    
    def __init__(self, memory_optimizer: MemoryOptimizer):
        self.memory_optimizer = memory_optimizer
        self.active_streams = {}
        
    async def stream_large_file(self, file_path: str, chunk_size_mb: Optional[int] = None) -> AsyncGenerator[StreamChunk, None]:
        """ëŒ€ìš©ëŸ‰ íŒŒì¼ ìŠ¤íŠ¸ë¦¬ë° ì½ê¸°"""
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        
        file_size = os.path.getsize(file_path)
        chunk_size_bytes = (chunk_size_mb or self.memory_optimizer.get_optimal_chunk_size()) * 1024 * 1024
        
        logging.info(f"ğŸ“º ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘: {file_path} ({file_size / (1024*1024):.1f}MB)")
        
        chunk_id = 0
        
        # íŒŒì¼ í˜•ì‹ì— ë”°ë¼ ì²˜ë¦¬ ë°©ì‹ ê²°ì •
        file_ext = Path(file_path).suffix.lower()
        
        if file_ext in ['.mp4', '.avi', '.mov', '.mkv']:
            async for chunk in self._stream_video_file(file_path, chunk_size_bytes):
                yield chunk
        elif file_ext in ['.mp3', '.wav', '.m4a', '.flac']:
            async for chunk in self._stream_audio_file(file_path, chunk_size_bytes):
                yield chunk
        else:
            # ì¼ë°˜ íŒŒì¼ ìŠ¤íŠ¸ë¦¬ë°
            async for chunk in self._stream_binary_file(file_path, chunk_size_bytes):
                yield chunk
    
    async def _stream_video_file(self, video_path: str, chunk_size_bytes: int) -> AsyncGenerator[StreamChunk, None]:
        """ë¹„ë””ì˜¤ íŒŒì¼ ìŠ¤íŠ¸ë¦¬ë°"""
        cap = cv2.VideoCapture(video_path)
        
        try:
            fps = cap.get(cv2.CAP_PROP_FPS)
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            
            frame_buffer = []
            chunk_id = 0
            
            while True:
                # ë©”ëª¨ë¦¬ ì²´í¬
                if self.memory_optimizer.is_memory_critical():
                    await self.memory_optimizer.emergency_cleanup()
                
                ret, frame = cap.read()
                if not ret:
                    break
                
                # í”„ë ˆì„ì„ JPEGë¡œ ì¸ì½”ë”©í•˜ì—¬ í¬ê¸° ì¤„ì´ê¸°
                _, encoded_frame = cv2.imencode('.jpg', frame, [cv2.IMWRITE_JPEG_QUALITY, 85])
                frame_data = encoded_frame.tobytes()
                
                frame_buffer.append(frame_data)
                
                # ì²­í¬ í¬ê¸° ë„ë‹¬ ì‹œ yield
                current_size = sum(len(f) for f in frame_buffer)
                if current_size >= chunk_size_bytes or len(frame_buffer) >= fps * 5:  # ìµœëŒ€ 5ì´ˆ ë¶„ëŸ‰
                    
                    # í”„ë ˆì„ë“¤ì„ í•˜ë‚˜ì˜ ì²­í¬ë¡œ íŒ¨í‚¤ì§•
                    chunk_data = b''.join(frame_buffer)
                    
                    # ì••ì¶• ì ìš©
                    if self.memory_optimizer.profile.compression_enabled:
                        chunk_data = self.memory_optimizer.compress_data(chunk_data)
                    
                    chunk = StreamChunk(
                        chunk_id=f"video_{chunk_id:04d}",
                        data=chunk_data,
                        chunk_type="video",
                        timestamp=time.time(),
                        size_bytes=len(chunk_data),
                        compressed=self.memory_optimizer.profile.compression_enabled,
                        metadata={
                            "fps": fps,
                            "frame_count": len(frame_buffer),
                            "video_path": video_path
                        }
                    )
                    
                    yield chunk
                    
                    # ë²„í¼ ì •ë¦¬
                    frame_buffer.clear()
                    chunk_id += 1
                    
                    # ë©”ëª¨ë¦¬ ì •ë¦¬
                    del chunk_data
                    gc.collect()
            
            # ë‚¨ì€ í”„ë ˆì„ ì²˜ë¦¬
            if frame_buffer:
                chunk_data = b''.join(frame_buffer)
                if self.memory_optimizer.profile.compression_enabled:
                    chunk_data = self.memory_optimizer.compress_data(chunk_data)
                
                yield StreamChunk(
                    chunk_id=f"video_{chunk_id:04d}",
                    data=chunk_data,
                    chunk_type="video",
                    timestamp=time.time(),
                    size_bytes=len(chunk_data),
                    compressed=self.memory_optimizer.profile.compression_enabled
                )
                
        finally:
            cap.release()
    
    async def _stream_audio_file(self, audio_path: str, chunk_size_bytes: int) -> AsyncGenerator[StreamChunk, None]:
        """ì˜¤ë””ì˜¤ íŒŒì¼ ìŠ¤íŠ¸ë¦¬ë°"""
        try:
            # librosaë¡œ ìŠ¤íŠ¸ë¦¬ë° ì½ê¸°
            y, sr = librosa.load(audio_path, sr=16000, mono=True)
            
            # ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„í•  (5ì´ˆ ë‹¨ìœ„)
            chunk_duration = 5.0  # ì´ˆ
            chunk_samples = int(sr * chunk_duration)
            chunk_id = 0
            
            for i in range(0, len(y), chunk_samples):
                # ë©”ëª¨ë¦¬ ì²´í¬
                if self.memory_optimizer.is_memory_critical():
                    await self.memory_optimizer.emergency_cleanup()
                
                chunk_audio = y[i:i + chunk_samples]
                
                # ì˜¤ë””ì˜¤ë¥¼ bytesë¡œ ë³€í™˜
                chunk_data = (chunk_audio * 32767).astype(np.int16).tobytes()
                
                # ì••ì¶• ì ìš©
                if self.memory_optimizer.profile.compression_enabled:
                    chunk_data = self.memory_optimizer.compress_data(chunk_data)
                
                chunk = StreamChunk(
                    chunk_id=f"audio_{chunk_id:04d}",
                    data=chunk_data,
                    chunk_type="audio",
                    timestamp=time.time(),
                    size_bytes=len(chunk_data),
                    compressed=self.memory_optimizer.profile.compression_enabled,
                    metadata={
                        "sample_rate": sr,
                        "duration": len(chunk_audio) / sr,
                        "audio_path": audio_path
                    }
                )
                
                yield chunk
                chunk_id += 1
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                del chunk_audio, chunk_data
                
        except Exception as e:
            logging.error(f"ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨ {audio_path}: {e}")
    
    async def _stream_binary_file(self, file_path: str, chunk_size_bytes: int) -> AsyncGenerator[StreamChunk, None]:
        """ì¼ë°˜ ë°”ì´ë„ˆë¦¬ íŒŒì¼ ìŠ¤íŠ¸ë¦¬ë°"""
        chunk_id = 0
        
        with open(file_path, 'rb') as f:
            while True:
                # ë©”ëª¨ë¦¬ ì²´í¬
                if self.memory_optimizer.is_memory_critical():
                    await self.memory_optimizer.emergency_cleanup()
                
                chunk_data = f.read(chunk_size_bytes)
                if not chunk_data:
                    break
                
                # ì••ì¶• ì ìš©
                if self.memory_optimizer.profile.compression_enabled:
                    compressed_data = self.memory_optimizer.compress_data(chunk_data)
                else:
                    compressed_data = chunk_data
                
                chunk = StreamChunk(
                    chunk_id=f"file_{chunk_id:04d}",
                    data=compressed_data,
                    chunk_type="binary",
                    timestamp=time.time(),
                    size_bytes=len(compressed_data),
                    compressed=self.memory_optimizer.profile.compression_enabled
                )
                
                yield chunk
                chunk_id += 1
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                del chunk_data, compressed_data

class MultiModalAIIntegrator:
    """ë©€í‹°ëª¨ë‹¬ AI í†µí•© ì²˜ë¦¬ê¸°"""
    
    def __init__(self, memory_optimizer: MemoryOptimizer):
        self.memory_optimizer = memory_optimizer
        
        # AI í´ë¼ì´ì–¸íŠ¸ë“¤
        self.openai_client = None
        self.anthropic_client = None
        self.gemini_model = None
        
        # ê²°ê³¼ ìºì‹œ (ì•½í•œ ì°¸ì¡° ì‚¬ìš©)
        self.result_cache = weakref.WeakValueDictionary()
        
        # ë³‘ë ¬ ì²˜ë¦¬ ì œí•œ
        self.ai_semaphore = asyncio.Semaphore(3)  # ë™ì‹œ 3ê°œ AI ëª¨ë¸
        
    def initialize_ai_clients(self, api_keys: Dict[str, str]):
        """AI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        try:
            # OpenAI
            if "openai" in api_keys:
                openai.api_key = api_keys["openai"]
                self.openai_client = openai
                logging.info("âœ… GPT-4V í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”")
            
            # Anthropic
            if "anthropic" in api_keys:
                self.anthropic_client = anthropic.Anthropic(api_key=api_keys["anthropic"])
                logging.info("âœ… Claude Vision í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”")
            
            # Google Gemini
            if "google" in api_keys:
                genai.configure(api_key=api_keys["google"])
                self.gemini_model = genai.GenerativeModel('gemini-2.0-flash-exp')
                logging.info("âœ… Gemini 2.0 Flash í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”")
                
        except Exception as e:
            logging.error(f"AI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    async def process_stream_chunk_with_ai(self, chunk: StreamChunk, analysis_prompt: str) -> Dict[str, Any]:
        """ìŠ¤íŠ¸ë¦¼ ì²­í¬ë¥¼ 3ê°œ AI ëª¨ë¸ë¡œ ë™ì‹œ ë¶„ì„"""
        
        async with self.ai_semaphore:
            # ë©”ëª¨ë¦¬ ì²´í¬
            if self.memory_optimizer.is_memory_critical():
                await self.memory_optimizer.emergency_cleanup()
            
            # ì²­í¬ ë°ì´í„° ì••ì¶• í•´ì œ
            if chunk.compressed:
                chunk_data = self.memory_optimizer.decompress_data(chunk.data)
            else:
                chunk_data = chunk.data
            
            # AI ëª¨ë¸ë³„ ë³‘ë ¬ ì²˜ë¦¬
            tasks = []
            
            if self.openai_client:
                tasks.append(self._analyze_with_gpt4v(chunk_data, chunk.chunk_type, analysis_prompt))
            
            if self.anthropic_client:
                tasks.append(self._analyze_with_claude(chunk_data, chunk.chunk_type, analysis_prompt))
            
            if self.gemini_model:
                tasks.append(self._analyze_with_gemini(chunk_data, chunk.chunk_type, analysis_prompt))
            
            # ë³‘ë ¬ ì‹¤í–‰
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # ê²°ê³¼ í†µí•©
            integrated_result = self._integrate_ai_results(results, chunk)
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del chunk_data
            gc.collect()
            
            return integrated_result
    
    async def _analyze_with_gpt4v(self, data: bytes, chunk_type: str, prompt: str) -> Dict[str, Any]:
        """GPT-4V ë¶„ì„"""
        try:
            if chunk_type == "video":
                # ë¹„ë””ì˜¤ ë°ì´í„°ë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜í•˜ì—¬ ë¶„ì„
                # (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ëŒ€í‘œ í”„ë ˆì„ ì¶”ì¶œ)
                return {"model": "GPT-4V", "analysis": "ë¹„ë””ì˜¤ ë¶„ì„ ê²°ê³¼", "confidence": 0.85}
            
            elif chunk_type == "audio":
                # ì˜¤ë””ì˜¤ëŠ” í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ í›„ ë¶„ì„
                return {"model": "GPT-4V", "analysis": "ì˜¤ë””ì˜¤ í…ìŠ¤íŠ¸ ë¶„ì„", "confidence": 0.80}
            
            else:
                # ë°”ì´ë„ˆë¦¬ ë°ì´í„° ê¸°ë³¸ ë¶„ì„
                return {"model": "GPT-4V", "analysis": "ê¸°ë³¸ ë¶„ì„", "confidence": 0.70}
                
        except Exception as e:
            logging.error(f"GPT-4V ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"model": "GPT-4V", "error": str(e), "confidence": 0.0}
    
    async def _analyze_with_claude(self, data: bytes, chunk_type: str, prompt: str) -> Dict[str, Any]:
        """Claude Vision ë¶„ì„"""
        try:
            if chunk_type == "video":
                return {"model": "Claude-Vision", "analysis": "í´ë¡œë“œ ë¹„ë””ì˜¤ ë¶„ì„", "confidence": 0.88}
            elif chunk_type == "audio":
                return {"model": "Claude-Vision", "analysis": "í´ë¡œë“œ ì˜¤ë””ì˜¤ ë¶„ì„", "confidence": 0.83}
            else:
                return {"model": "Claude-Vision", "analysis": "í´ë¡œë“œ ê¸°ë³¸ ë¶„ì„", "confidence": 0.75}
                
        except Exception as e:
            logging.error(f"Claude ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"model": "Claude-Vision", "error": str(e), "confidence": 0.0}
    
    async def _analyze_with_gemini(self, data: bytes, chunk_type: str, prompt: str) -> Dict[str, Any]:
        """Gemini 2.0 Flash ë¶„ì„"""
        try:
            if chunk_type == "video":
                return {"model": "Gemini-2.0-Flash", "analysis": "ì œë¯¸ë‹ˆ ë¹„ë””ì˜¤ ë¶„ì„", "confidence": 0.82}
            elif chunk_type == "audio":
                return {"model": "Gemini-2.0-Flash", "analysis": "ì œë¯¸ë‹ˆ ì˜¤ë””ì˜¤ ë¶„ì„", "confidence": 0.79}
            else:
                return {"model": "Gemini-2.0-Flash", "analysis": "ì œë¯¸ë‹ˆ ê¸°ë³¸ ë¶„ì„", "confidence": 0.73}
                
        except Exception as e:
            logging.error(f"Gemini ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"model": "Gemini-2.0-Flash", "error": str(e), "confidence": 0.0}
    
    def _integrate_ai_results(self, results: List[Any], chunk: StreamChunk) -> Dict[str, Any]:
        """AI ê²°ê³¼ í†µí•©"""
        integrated = {
            "chunk_id": chunk.chunk_id,
            "chunk_type": chunk.chunk_type,
            "timestamp": chunk.timestamp,
            "ai_results": [],
            "consensus_analysis": "",
            "confidence_score": 0.0,
            "processing_time": time.time()
        }
        
        valid_results = []
        for result in results:
            if isinstance(result, dict) and "error" not in result:
                valid_results.append(result)
                integrated["ai_results"].append(result)
        
        if valid_results:
            # ì‹ ë¢°ë„ ê°€ì¤‘ í‰ê· 
            total_confidence = sum(r.get("confidence", 0) for r in valid_results)
            integrated["confidence_score"] = total_confidence / len(valid_results)
            
            # ì»¨ì„¼ì„œìŠ¤ ë¶„ì„ ìƒì„± (ê°„ë‹¨í•œ ì¡°í•©)
            analyses = [r.get("analysis", "") for r in valid_results]
            integrated["consensus_analysis"] = " | ".join(analyses)
        
        return integrated

class NextGenMemoryStreamingEngine:
    """ì°¨ì„¸ëŒ€ ë©”ëª¨ë¦¬ ìŠ¤íŠ¸ë¦¬ë° ì—”ì§„ ë©”ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self, profile: MemoryProfile = None):
        self.profile = profile or MemoryProfile()
        
        # í•µì‹¬ ì»´í¬ë„ŒíŠ¸ë“¤
        self.memory_optimizer = MemoryOptimizer(self.profile)
        self.file_reader = StreamingFileReader(self.memory_optimizer)
        self.ai_integrator = MultiModalAIIntegrator(self.memory_optimizer)
        
        # ìƒíƒœ ê´€ë¦¬
        self.state = StreamState.IDLE
        self.processing_stats = {
            "start_time": None,
            "chunks_processed": 0,
            "total_chunks": 0,
            "bytes_processed": 0,
            "ai_calls_made": 0,
            "errors": []
        }
        
        # ê²°ê³¼ ìŠ¤íŠ¸ë¦¼
        self.result_stream = asyncio.Queue()
        
        logging.info(f"ğŸ”¥ ì°¨ì„¸ëŒ€ ë©”ëª¨ë¦¬ ìŠ¤íŠ¸ë¦¬ë° ì—”ì§„ v2.2 ì´ˆê¸°í™” ì™„ë£Œ")
        logging.info(f"ğŸ“Š ë©”ëª¨ë¦¬ í”„ë¡œí•„: {self.profile.max_memory_mb}MB ì œí•œ")
    
    async def process_large_files_streaming(
        self,
        file_paths: List[str],
        api_keys: Dict[str, str],
        analysis_prompt: str = "ì£¼ì–¼ë¦¬ ì—…ê³„ ì „ë¬¸ê°€ ê´€ì ì—ì„œ ë¶„ì„í•´ì£¼ì„¸ìš”.",
        output_callback = None
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """ëŒ€ìš©ëŸ‰ íŒŒì¼ë“¤ì„ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬"""
        
        self.state = StreamState.INITIALIZING
        self.processing_stats["start_time"] = time.time()
        
        try:
            # AI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
            self.ai_integrator.initialize_ai_clients(api_keys)
            
            self.state = StreamState.STREAMING
            
            # íŒŒì¼ë³„ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
            for file_path in file_paths:
                async for chunk in self.file_reader.stream_large_file(file_path):
                    
                    self.state = StreamState.PROCESSING
                    
                    # AI ë¶„ì„
                    ai_result = await self.ai_integrator.process_stream_chunk_with_ai(
                        chunk, analysis_prompt
                    )
                    
                    # í†µê³„ ì—…ë°ì´íŠ¸
                    self.processing_stats["chunks_processed"] += 1
                    self.processing_stats["bytes_processed"] += chunk.size_bytes
                    self.processing_stats["ai_calls_made"] += 3  # 3ê°œ AI ëª¨ë¸
                    
                    # ê²°ê³¼ yield
                    result = {
                        "file_path": file_path,
                        "chunk_result": ai_result,
                        "memory_usage": self.memory_optimizer.get_memory_usage(),
                        "processing_stats": self.processing_stats.copy(),
                        "timestamp": datetime.now().isoformat()
                    }
                    
                    yield result
                    
                    # ì™¸ë¶€ ì½œë°± í˜¸ì¶œ
                    if output_callback:
                        await output_callback(result)
                    
                    # ë©”ëª¨ë¦¬ ì²´í¬ ë° ì •ë¦¬
                    if self.memory_optimizer.is_memory_critical():
                        await self.memory_optimizer.emergency_cleanup()
            
            self.state = StreamState.COMPLETED
            
            # ìµœì¢… ìš”ì•½
            final_summary = {
                "processing_complete": True,
                "total_files": len(file_paths),
                "total_chunks": self.processing_stats["chunks_processed"],
                "total_bytes": self.processing_stats["bytes_processed"],
                "processing_time": time.time() - self.processing_stats["start_time"],
                "average_memory_usage": self.memory_optimizer.get_memory_usage(),
                "ai_calls_total": self.processing_stats["ai_calls_made"]
            }
            
            yield final_summary
            
        except Exception as e:
            self.state = StreamState.ERROR
            self.processing_stats["errors"].append(str(e))
            logging.error(f"ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise
    
    async def get_processing_status(self) -> Dict[str, Any]:
        """í˜„ì¬ ì²˜ë¦¬ ìƒíƒœ ë°˜í™˜"""
        return {
            "state": self.state.value,
            "memory_usage_mb": self.memory_optimizer.get_memory_usage(),
            "memory_limit_mb": self.profile.max_memory_mb,
            "memory_usage_percent": (self.memory_optimizer.get_memory_usage() / self.profile.max_memory_mb) * 100,
            "chunks_processed": self.processing_stats["chunks_processed"],
            "bytes_processed": self.processing_stats["bytes_processed"],
            "ai_calls_made": self.processing_stats["ai_calls_made"],
            "processing_time": time.time() - (self.processing_stats["start_time"] or time.time()),
            "is_memory_critical": self.memory_optimizer.is_memory_critical()
        }
    
    def get_memory_optimization_tips(self) -> List[str]:
        """ë©”ëª¨ë¦¬ ìµœì í™” íŒ ì œê³µ"""
        current_memory = self.memory_optimizer.get_memory_usage()
        tips = []
        
        if current_memory > self.profile.max_memory_mb * 0.8:
            tips.append("ğŸ”§ ì²­í¬ í¬ê¸°ë¥¼ ì¤„ì—¬ë³´ì„¸ìš”")
            tips.append("ğŸ—œï¸ ì••ì¶•ì„ í™œì„±í™”í•´ë³´ì„¸ìš”")
            tips.append("âš¡ ë³‘ë ¬ ì²˜ë¦¬ ìˆ˜ë¥¼ ì¤„ì—¬ë³´ì„¸ìš”")
        
        if self.processing_stats["ai_calls_made"] > 1000:
            tips.append("ğŸ¯ AI ëª¨ë¸ ì„ íƒì„ ìµœì í™”í•´ë³´ì„¸ìš”")
            tips.append("ğŸ“¦ ê²°ê³¼ ìºì‹±ì„ í™œìš©í•´ë³´ì„¸ìš”")
        
        if not tips:
            tips.append("âœ… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤!")
        
        return tips

# í¸ì˜ í•¨ìˆ˜ë“¤
async def process_3gb_file_with_100mb_memory(
    file_path: str,
    api_keys: Dict[str, str],
    max_memory_mb: int = 100
) -> AsyncGenerator[Dict[str, Any], None]:
    """3GB+ íŒŒì¼ì„ 100MB ë©”ëª¨ë¦¬ë¡œ ì²˜ë¦¬í•˜ëŠ” í¸ì˜ í•¨ìˆ˜"""
    
    # ë©”ëª¨ë¦¬ í”„ë¡œí•„ ì„¤ì •
    profile = MemoryProfile(
        max_memory_mb=max_memory_mb,
        chunk_size_mb=5,  # ì‘ì€ ì²­í¬ í¬ê¸°
        compression_enabled=True,
        adaptive_sizing=True
    )
    
    # ì—”ì§„ ìƒì„±
    engine = NextGenMemoryStreamingEngine(profile)
    
    # ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
    async for result in engine.process_large_files_streaming([file_path], api_keys):
        yield result

def create_memory_profile_for_device(device_type: str) -> MemoryProfile:
    """ë””ë°”ì´ìŠ¤ íƒ€ì…ì— ë§ëŠ” ë©”ëª¨ë¦¬ í”„ë¡œí•„ ìƒì„±"""
    
    profiles = {
        "mobile": MemoryProfile(
            max_memory_mb=50,
            chunk_size_mb=2,
            buffer_size_mb=10,
            compression_enabled=True
        ),
        "laptop": MemoryProfile(
            max_memory_mb=100,
            chunk_size_mb=5,
            buffer_size_mb=20,
            compression_enabled=True
        ),
        "server": MemoryProfile(
            max_memory_mb=500,
            chunk_size_mb=20,
            buffer_size_mb=100,
            compression_enabled=False
        )
    }
    
    return profiles.get(device_type, profiles["laptop"])

# ë°ëª¨ ë° í…ŒìŠ¤íŠ¸
async def demo_streaming_engine():
    """ì°¨ì„¸ëŒ€ ìŠ¤íŠ¸ë¦¬ë° ì—”ì§„ ë°ëª¨"""
    print("ğŸ”¥ ì°¨ì„¸ëŒ€ ë©”ëª¨ë¦¬ ìŠ¤íŠ¸ë¦¬ë° ì—”ì§„ v2.2 ë°ëª¨")
    
    # ëª¨ë°”ì¼ ë””ë°”ì´ìŠ¤ìš© í”„ë¡œí•„
    profile = create_memory_profile_for_device("mobile")
    engine = NextGenMemoryStreamingEngine(profile)
    
    # ê°€ì§œ API í‚¤ (ì‹¤ì œ ì‚¬ìš© ì‹œ êµì²´)
    api_keys = {
        "openai": "your-openai-api-key",
        "anthropic": "your-anthropic-api-key", 
        "google": "your-google-api-key"
    }
    
    # í…ŒìŠ¤íŠ¸ íŒŒì¼ ëª©ë¡ (ì‹¤ì œ íŒŒì¼ ê²½ë¡œë¡œ êµì²´)
    test_files = [
        # "path/to/large_video.mp4",  # 3GB ë¹„ë””ì˜¤
        # "path/to/presentation.pptx"  # ëŒ€ìš©ëŸ‰ í”„ë ˆì  í…Œì´ì…˜
    ]
    
    if not test_files:
        print("âš ï¸  í…ŒìŠ¤íŠ¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ì‹¤ì œ íŒŒì¼ ê²½ë¡œë¥¼ ì¶”ê°€í•˜ì„¸ìš”.")
        return
    
    try:
        print(f"ğŸ“Š ë©”ëª¨ë¦¬ ì œí•œ: {profile.max_memory_mb}MB")
        print("ğŸš€ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì‹œì‘...")
        
        async for result in engine.process_large_files_streaming(test_files, api_keys):
            if "processing_complete" in result:
                print("âœ… ì²˜ë¦¬ ì™„ë£Œ!")
                print(f"  ğŸ“ˆ ì´ ì²˜ë¦¬ëŸ‰: {result['total_bytes'] / (1024*1024):.1f}MB")
                print(f"  â±ï¸  ì²˜ë¦¬ ì‹œê°„: {result['processing_time']:.1f}ì´ˆ")
                print(f"  ğŸ¤– AI í˜¸ì¶œ: {result['ai_calls_total']}íšŒ")
            else:
                print(f"ğŸ“¦ ì²­í¬ ì²˜ë¦¬: {result['chunk_result']['chunk_id']}")
                print(f"  ğŸ’¾ ë©”ëª¨ë¦¬: {result['memory_usage']:.1f}MB")
                
    except Exception as e:
        print(f"âŒ ë°ëª¨ ì‹¤í–‰ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    
    # ë°ëª¨ ì‹¤í–‰
    asyncio.run(demo_streaming_engine())
