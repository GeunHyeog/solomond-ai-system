# Phase 2 Week 3 Day 4-5: í†µí•© í…ŒìŠ¤íŠ¸ ë° ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
# ìŠ¤íŠ¸ë¦¬ë° + ë³µêµ¬ + ê³ ê¸‰ê²€ì¦ ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸

import asyncio
import time
import json
import psutil
import matplotlib.pyplot as plt
import pandas as pd
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field, asdict
from pathlib import Path
import logging
import tempfile
import os
import numpy as np
from datetime import datetime
import threading
import traceback

# ê°œë°œí•œ ëª¨ë“ˆë“¤ ì„í¬íŠ¸
from core.streaming_processor import StreamingProcessor, StreamingConfig, MemoryMonitor
from core.recovery_manager import RecoveryManager, RecoveryConfig, RecoveryLevel
from core.advanced_validator import AdvancedCrossValidator, ValidationLevel

class BenchmarkLevel(Enum):
    """ë²¤ì¹˜ë§ˆí¬ ë ˆë²¨"""
    LIGHT = "light"       # ê°€ë²¼ìš´ í…ŒìŠ¤íŠ¸
    STANDARD = "standard" # í‘œì¤€ í…ŒìŠ¤íŠ¸
    HEAVY = "heavy"       # ë¬´ê±°ìš´ í…ŒìŠ¤íŠ¸
    EXTREME = "extreme"   # ê·¹í•œ í…ŒìŠ¤íŠ¸

@dataclass
class TestScenario:
    """í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤"""
    scenario_id: str
    name: str
    description: str
    file_count: int
    total_size_mb: float
    file_types: List[str]
    expected_duration: float
    stress_factors: Dict[str, Any] = field(default_factory=dict)

@dataclass
class PerformanceMetrics:
    """ì„±ëŠ¥ ì§€í‘œ"""
    scenario_id: str
    
    # ì²˜ë¦¬ ì„±ëŠ¥
    total_processing_time: float
    average_file_time: float
    throughput_mbps: float
    
    # ë©”ëª¨ë¦¬ ì„±ëŠ¥
    peak_memory_mb: float
    average_memory_mb: float
    memory_efficiency: float  # ì²˜ë¦¬ëŸ‰ ëŒ€ë¹„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
    
    # ê²€ì¦ ì„±ëŠ¥
    validation_accuracy: float
    anomaly_detection_rate: float
    false_positive_rate: float
    
    # ë³µêµ¬ ì„±ëŠ¥
    error_recovery_rate: float
    recovery_time: float
    checkpoint_overhead: float
    
    # ì•ˆì •ì„± ì§€í‘œ
    success_rate: float
    error_count: int
    critical_errors: int
    
    # ì‚¬ìš©ì ê²½í—˜
    responsiveness_score: float  # UI ë°˜ì‘ì„±
    progress_accuracy: float     # ì§„í–‰ë¥  ì •í™•ë„

@dataclass
class BenchmarkResult:
    """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼"""
    benchmark_id: str
    timestamp: datetime
    system_info: Dict[str, Any]
    test_scenarios: List[TestScenario]
    performance_metrics: List[PerformanceMetrics]
    overall_score: float
    recommendations: List[str]
    detailed_logs: Dict[str, Any]

class SystemResourceMonitor:
    """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°"""
    
    def __init__(self, sampling_interval: float = 1.0):
        self.sampling_interval = sampling_interval
        self.monitoring = False
        self.resource_history = []
        self.monitor_thread = None
    
    def start_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        self.monitoring = True
        self.resource_history.clear()
        self.monitor_thread = threading.Thread(target=self._monitor_loop)
        self.monitor_thread.start()
    
    def stop_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join()
    
    def _monitor_loop(self):
        """ëª¨ë‹ˆí„°ë§ ë£¨í”„"""
        while self.monitoring:
            try:
                # CPU ì‚¬ìš©ë¥ 
                cpu_percent = psutil.cpu_percent(interval=None)
                
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
                memory = psutil.virtual_memory()
                
                # ë””ìŠ¤í¬ I/O
                disk_io = psutil.disk_io_counters()
                
                # ë„¤íŠ¸ì›Œí¬ I/O (ê°€ëŠ¥í•œ ê²½ìš°)
                try:
                    net_io = psutil.net_io_counters()
                except:
                    net_io = None
                
                resource_data = {
                    "timestamp": time.time(),
                    "cpu_percent": cpu_percent,
                    "memory_used_mb": memory.used / 1024 / 1024,
                    "memory_percent": memory.percent,
                    "disk_read_mb": disk_io.read_bytes / 1024 / 1024 if disk_io else 0,
                    "disk_write_mb": disk_io.write_bytes / 1024 / 1024 if disk_io else 0,
                    "net_sent_mb": net_io.bytes_sent / 1024 / 1024 if net_io else 0,
                    "net_recv_mb": net_io.bytes_recv / 1024 / 1024 if net_io else 0
                }
                
                self.resource_history.append(resource_data)
                
                # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ì˜¤ë˜ëœ ë°ì´í„° ì œê±° (ìµœê·¼ 1000ê°œ ìœ ì§€)
                if len(self.resource_history) > 1000:
                    self.resource_history = self.resource_history[-1000:]
                
                time.sleep(self.sampling_interval)
                
            except Exception as e:
                logging.error(f"ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜: {e}")
                time.sleep(self.sampling_interval)
    
    def get_summary(self) -> Dict[str, Any]:
        """ë¦¬ì†ŒìŠ¤ ì‚¬ìš© ìš”ì•½"""
        if not self.resource_history:
            return {}
        
        cpu_values = [r["cpu_percent"] for r in self.resource_history]
        memory_values = [r["memory_used_mb"] for r in self.resource_history]
        
        return {
            "cpu": {
                "average": np.mean(cpu_values),
                "peak": np.max(cpu_values),
                "min": np.min(cpu_values)
            },
            "memory": {
                "average_mb": np.mean(memory_values),
                "peak_mb": np.max(memory_values),
                "min_mb": np.min(memory_values)
            },
            "samples_count": len(self.resource_history),
            "duration_seconds": (self.resource_history[-1]["timestamp"] - self.resource_history[0]["timestamp"]) if len(self.resource_history) > 1 else 0
        }

class TestDataGenerator:
    """í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±ê¸°"""
    
    def __init__(self, temp_dir: Path):
        self.temp_dir = Path(temp_dir)
        self.temp_dir.mkdir(exist_ok=True)
    
    def generate_test_files(self, scenario: TestScenario) -> List[str]:
        """ì‹œë‚˜ë¦¬ì˜¤ì— ë§ëŠ” í…ŒìŠ¤íŠ¸ íŒŒì¼ ìƒì„±"""
        test_files = []
        
        # íŒŒì¼ í¬ê¸° ë¶„ë°°
        file_sizes = self._calculate_file_sizes(scenario.total_size_mb, scenario.file_count)
        
        for i in range(scenario.file_count):
            file_type = scenario.file_types[i % len(scenario.file_types)]
            file_size_mb = file_sizes[i]
            
            file_path = self._generate_test_file(
                file_type=file_type,
                size_mb=file_size_mb,
                file_index=i,
                scenario_id=scenario.scenario_id
            )
            
            test_files.append(file_path)
        
        return test_files
    
    def _calculate_file_sizes(self, total_mb: float, file_count: int) -> List[float]:
        """íŒŒì¼ í¬ê¸° ë¶„ë°° ê³„ì‚°"""
        if file_count == 1:
            return [total_mb]
        
        # ë¡œê·¸ ì •ê·œë¶„í¬ë¥¼ ì‚¬ìš©í•˜ì—¬ í˜„ì‹¤ì ì¸ íŒŒì¼ í¬ê¸° ë¶„í¬ ìƒì„±
        base_size = total_mb / file_count
        
        # ì¼ë¶€ëŠ” í¬ê³ , ì¼ë¶€ëŠ” ì‘ê²Œ
        sizes = []
        remaining_mb = total_mb
        
        for i in range(file_count - 1):
            # ëœë¤í•œ í¬ê¸° (í‰ê·  ì£¼ë³€ìœ¼ë¡œ ë³€ë™)
            variation = np.random.uniform(0.5, 2.0)
            size = min(base_size * variation, remaining_mb * 0.8)
            size = max(size, 1.0)  # ìµœì†Œ 1MB
            
            sizes.append(size)
            remaining_mb -= size
        
        # ë§ˆì§€ë§‰ íŒŒì¼ì€ ë‚¨ì€ í¬ê¸°
        sizes.append(max(1.0, remaining_mb))
        
        return sizes
    
    def _generate_test_file(self, file_type: str, size_mb: float, file_index: int, scenario_id: str) -> str:
        """ë‹¨ì¼ í…ŒìŠ¤íŠ¸ íŒŒì¼ ìƒì„±"""
        
        # íŒŒì¼ í™•ì¥ì ê²°ì •
        extensions = {
            "audio": ".wav",
            "video": ".mp4", 
            "document": ".txt",
            "image": ".jpg"
        }
        
        ext = extensions.get(file_type, ".bin")
        filename = f"{scenario_id}_file_{file_index:03d}{ext}"
        file_path = self.temp_dir / filename
        
        # íŒŒì¼ í¬ê¸°ë¥¼ ë°”ì´íŠ¸ë¡œ ë³€í™˜
        size_bytes = int(size_mb * 1024 * 1024)
        
        # íŒŒì¼ íƒ€ì…ë³„ ë‚´ìš© ìƒì„±
        if file_type == "document":
            content = self._generate_jewelry_text_content(size_bytes)
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(content)
        else:
            # ë°”ì´ë„ˆë¦¬ íŒŒì¼ (ì‹¤ì œ ì˜¤ë””ì˜¤/ë¹„ë””ì˜¤/ì´ë¯¸ì§€ ëŒ€ì‹  ëœë¤ ë°ì´í„°)
            with open(file_path, 'wb') as f:
                # ì²­í¬ ë‹¨ìœ„ë¡œ ì‘ì„±í•˜ì—¬ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í™•ë³´
                chunk_size = 1024 * 1024  # 1MB ì²­í¬
                written = 0
                
                while written < size_bytes:
                    chunk_size_actual = min(chunk_size, size_bytes - written)
                    chunk_data = os.urandom(chunk_size_actual)
                    f.write(chunk_data)
                    written += chunk_size_actual
        
        return str(file_path)
    
    def _generate_jewelry_text_content(self, target_bytes: int) -> str:
        """ì£¼ì–¼ë¦¬ ê´€ë ¨ í…ìŠ¤íŠ¸ ë‚´ìš© ìƒì„±"""
        
        # ì£¼ì–¼ë¦¬ ê´€ë ¨ í…œí”Œë¦¿ í…ìŠ¤íŠ¸ë“¤
        jewelry_texts = [
            "ì´ ë‹¤ì´ì•„ëª¬ë“œëŠ” 1.2ìºëŸ¿ Dì»¬ëŸ¬ VVS1 ë“±ê¸‰ìœ¼ë¡œ GIA ê°ì •ì„œê°€ í•¨ê»˜ ì œê³µë©ë‹ˆë‹¤. ë¼ìš´ë“œ ë¸Œë¦´ë¦¬ì–¸íŠ¸ ì»·ìœ¼ë¡œ ê°€ê³µë˜ì–´ ë›°ì–´ë‚œ ê´‘ì±„ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.",
            "ë£¨ë¹„ì˜ í’ˆì§ˆì€ ìƒ‰ìƒ, íˆ¬ëª…ë„, ì»·, ìºëŸ¿ìœ¼ë¡œ í‰ê°€ë©ë‹ˆë‹¤. ì´ ë¯¸ì–€ë§ˆì‚° ë£¨ë¹„ëŠ” ë¹„ë‘˜ê¸°í”¼ ìƒ‰ìƒì„ ë³´ì´ë©° 2.5ìºëŸ¿ì˜ ë¬´ê²Œë¥¼ ê°€ì§‘ë‹ˆë‹¤.",
            "ì‚¬íŒŒì´ì–´ëŠ” ì½”ëŸ°ë¤ ê³„ì—´ì˜ ë³´ì„ìœ¼ë¡œ, ë¸”ë£¨ ì‚¬íŒŒì´ì–´ê°€ ê°€ì¥ ìœ ëª…í•©ë‹ˆë‹¤. ìŠ¤ë¦¬ë‘ì¹´ì‚° ì‚¬íŒŒì´ì–´ëŠ” ë¡œì–„ ë¸”ë£¨ ìƒ‰ìƒìœ¼ë¡œ ìœ ëª…í•©ë‹ˆë‹¤.",
            "ì—ë©”ë„ë“œëŠ” ë² ë¦´ ê³„ì—´ì˜ ë³´ì„ìœ¼ë¡œ ì½œë¡¬ë¹„ì•„ì‚°ì´ ìµœê³ ê¸‰ìœ¼ë¡œ ì¸ì •ë°›ìŠµë‹ˆë‹¤. ìë¥´ë”˜ì´ë¼ ë¶ˆë¦¬ëŠ” ë‚´í¬ë¬¼ì´ ì²œì—° ì—ë©”ë„ë“œì˜ íŠ¹ì§•ì…ë‹ˆë‹¤.",
            "í”„ë¦°ì„¸ìŠ¤ ì»· ë‹¤ì´ì•„ëª¬ë“œëŠ” ì •ì‚¬ê°í˜• ëª¨ì–‘ì˜ ë¸Œë¦´ë¦¬ì–¸íŠ¸ ì»·ìœ¼ë¡œ, ë¼ìš´ë“œ ì»· ë‹¤ìŒìœ¼ë¡œ ì¸ê¸°ê°€ ë†’ìŠµë‹ˆë‹¤.",
            "í”Œë˜í‹°ë„˜ì€ ë°±ê¸ˆì¡± ê¸ˆì†ìœ¼ë¡œ ë³€ìƒ‰ë˜ì§€ ì•Šì•„ ê³ ê¸‰ ì£¼ì–¼ë¦¬ ì œì‘ì— ì‚¬ìš©ë©ë‹ˆë‹¤. PT950, PT900 ë“±ì˜ ìˆœë„ë¡œ êµ¬ë¶„ë©ë‹ˆë‹¤.",
            "ë„ë§¤ê°€ê²©ê³¼ ì†Œë§¤ê°€ê²©ì˜ ì°¨ì´ëŠ” ì¼ë°˜ì ìœ¼ë¡œ 50-100% ì •ë„ì…ë‹ˆë‹¤. ë¸Œëœë“œ í”„ë¦¬ë¯¸ì—„ì— ë”°ë¼ ë” í° ì°¨ì´ë¥¼ ë³´ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
            "AGS 0ë“±ê¸‰ì€ ìµœê³  ë“±ê¸‰ì˜ ì»·ì„ ì˜ë¯¸í•˜ë©°, íŠ¸ë¦¬í”Œ ì œë¡œ(000)ë¼ê³ ë„ ë¶ˆë¦½ë‹ˆë‹¤.",
            "íˆíŠ¸ íŠ¸ë¦¬íŠ¸ë¨¼íŠ¸ëŠ” ë³´ì„ì˜ ìƒ‰ìƒê³¼ íˆ¬ëª…ë„ë¥¼ ê°œì„ í•˜ëŠ” ê°€ì—´ ì²˜ë¦¬ ë°©ë²•ì…ë‹ˆë‹¤.",
            "ì¸í´ë£¨ì „ì€ ë³´ì„ ë‚´ë¶€ì˜ ë‚´í¬ë¬¼ì„ ì˜ë¯¸í•˜ë©°, ì²œì—° ë³´ì„ì„ì„ ì¦ëª…í•˜ëŠ” ì¤‘ìš”í•œ íŠ¹ì§•ì…ë‹ˆë‹¤."
        ]
        
        content = ""
        current_bytes = 0
        
        while current_bytes < target_bytes:
            # ëœë¤í•˜ê²Œ í…ìŠ¤íŠ¸ ì„ íƒ ë° ë³€í˜•
            base_text = np.random.choice(jewelry_texts)
            
            # í…ìŠ¤íŠ¸ ë³€í˜• (ë¬¸ì¥ ë°˜ë³µ, ë‹¨ì–´ ì¶”ê°€ ë“±)
            variations = [
                base_text,
                base_text + " ì „ë¬¸ê°€ì˜ ê°ì •ì„ í†µí•´ í™•ì¸ëœ ì •ë³´ì…ë‹ˆë‹¤.",
                f"ì„¸ë¶€ ì‚¬í•­: {base_text}",
                f"ì¶”ê°€ ì •ë³´ë¡œëŠ” {base_text.lower()}",
                base_text + " ì´ëŠ” ì—…ê³„ í‘œì¤€ì— ë”°ë¥¸ í‰ê°€ì…ë‹ˆë‹¤."
            ]
            
            selected_text = np.random.choice(variations)
            content += selected_text + "\n\n"
            current_bytes = len(content.encode('utf-8'))
        
        return content[:target_bytes]  # ì •í™•í•œ í¬ê¸°ë¡œ ìë¥´ê¸°
    
    def cleanup_test_files(self, file_paths: List[str]):
        """í…ŒìŠ¤íŠ¸ íŒŒì¼ ì •ë¦¬"""
        for file_path in file_paths:
            try:
                if os.path.exists(file_path):
                    os.remove(file_path)
            except Exception as e:
                logging.warning(f"íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {file_path}, {e}")

class IntegratedBenchmarkSuite:
    """í†µí•© ë²¤ì¹˜ë§ˆí¬ ìŠ¤ìœ„íŠ¸"""
    
    def __init__(self, benchmark_level: BenchmarkLevel = BenchmarkLevel.STANDARD):
        self.benchmark_level = benchmark_level
        self.test_data_generator = TestDataGenerator(Path("benchmark_temp"))
        self.resource_monitor = SystemResourceMonitor(sampling_interval=0.5)
        self.logger = logging.getLogger(__name__)
        
        # ë²¤ì¹˜ë§ˆí¬ ID ìƒì„±
        self.benchmark_id = f"benchmark_{int(time.time())}"
        
        # í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ ì •ì˜
        self.test_scenarios = self._define_test_scenarios()
        
        # ê²°ê³¼ ì €ì¥
        self.results: List[PerformanceMetrics] = []
        self.detailed_logs = {}
    
    def _define_test_scenarios(self) -> List[TestScenario]:
        """ë²¤ì¹˜ë§ˆí¬ ë ˆë²¨ì— ë”°ë¥¸ í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ ì •ì˜"""
        
        if self.benchmark_level == BenchmarkLevel.LIGHT:
            return [
                TestScenario(
                    scenario_id="light_mixed",
                    name="ê°€ë²¼ìš´ í˜¼í•© í…ŒìŠ¤íŠ¸",
                    description="ì†Œê·œëª¨ ë‹¤ì¤‘ íŒŒì¼ ì²˜ë¦¬",
                    file_count=3,
                    total_size_mb=50.0,
                    file_types=["audio", "document", "image"],
                    expected_duration=30.0
                )
            ]
        
        elif self.benchmark_level == BenchmarkLevel.STANDARD:
            return [
                TestScenario(
                    scenario_id="standard_audio",
                    name="í‘œì¤€ ì˜¤ë””ì˜¤ í…ŒìŠ¤íŠ¸", 
                    description="ì¤‘ê°„ ê·œëª¨ ì˜¤ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬",
                    file_count=5,
                    total_size_mb=150.0,
                    file_types=["audio"],
                    expected_duration=60.0
                ),
                TestScenario(
                    scenario_id="standard_mixed",
                    name="í‘œì¤€ í˜¼í•© í…ŒìŠ¤íŠ¸",
                    description="ë‹¤ì–‘í•œ íŒŒì¼ íƒ€ì… í˜¼í•© ì²˜ë¦¬",
                    file_count=8,
                    total_size_mb=200.0,
                    file_types=["audio", "video", "document", "image"],
                    expected_duration=90.0
                )
            ]
        
        elif self.benchmark_level == BenchmarkLevel.HEAVY:
            return [
                TestScenario(
                    scenario_id="heavy_batch",
                    name="ëŒ€ìš©ëŸ‰ ë°°ì¹˜ í…ŒìŠ¤íŠ¸",
                    description="ëŒ€ìš©ëŸ‰ íŒŒì¼ ë°°ì¹˜ ì²˜ë¦¬",
                    file_count=15,
                    total_size_mb=500.0,
                    file_types=["audio", "video"],
                    expected_duration=180.0
                ),
                TestScenario(
                    scenario_id="heavy_stress",
                    name="ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸",
                    description="ë©”ëª¨ë¦¬ ë° CPU ë¶€í•˜ í…ŒìŠ¤íŠ¸", 
                    file_count=20,
                    total_size_mb=800.0,
                    file_types=["audio", "video", "document"],
                    expected_duration=300.0,
                    stress_factors={"memory_pressure": True, "concurrent_load": True}
                )
            ]
        
        else:  # EXTREME
            return [
                TestScenario(
                    scenario_id="extreme_volume",
                    name="ê·¹í•œ ë³¼ë¥¨ í…ŒìŠ¤íŠ¸",
                    description="ìµœëŒ€ ìš©ëŸ‰ íŒŒì¼ ì²˜ë¦¬",
                    file_count=25,
                    total_size_mb=1500.0,
                    file_types=["audio", "video"],
                    expected_duration=600.0,
                    stress_factors={"memory_pressure": True, "disk_io_pressure": True}
                ),
                TestScenario(
                    scenario_id="extreme_reliability",
                    name="ê·¹í•œ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸",
                    description="ì˜¤ë¥˜ ì£¼ì… ë° ë³µêµ¬ í…ŒìŠ¤íŠ¸",
                    file_count=20,
                    total_size_mb=1000.0,
                    file_types=["audio", "video", "document", "image"],
                    expected_duration=400.0,
                    stress_factors={"inject_errors": True, "network_instability": True}
                )
            ]
    
    async def run_comprehensive_benchmark(self) -> BenchmarkResult:
        """ì¢…í•© ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        
        print(f"ğŸš€ í†µí•© ë²¤ì¹˜ë§ˆí¬ ì‹œì‘ (ë ˆë²¨: {self.benchmark_level.value})")
        print("=" * 80)
        
        # ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘
        system_info = self._collect_system_info()
        
        # ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œì‘
        self.resource_monitor.start_monitoring()
        
        overall_start_time = time.time()
        
        try:
            # ê° ì‹œë‚˜ë¦¬ì˜¤ ì‹¤í–‰
            for i, scenario in enumerate(self.test_scenarios, 1):
                print(f"\nğŸ“‹ ì‹œë‚˜ë¦¬ì˜¤ {i}/{len(self.test_scenarios)}: {scenario.name}")
                print("-" * 60)
                
                metrics = await self._run_scenario_benchmark(scenario)
                self.results.append(metrics)
                
                # ì¤‘ê°„ ê²°ê³¼ ì¶œë ¥
                self._print_scenario_results(metrics)
                
                # ì‹œë‚˜ë¦¬ì˜¤ ê°„ ì¿¨ë‹¤ìš´ (ë©”ëª¨ë¦¬ ì •ë¦¬)
                if i < len(self.test_scenarios):
                    print("ğŸ§¹ ì‹œìŠ¤í…œ ì •ë¦¬ ì¤‘...")
                    await asyncio.sleep(5)
                    import gc
                    gc.collect()
        
        finally:
            # ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€
            self.resource_monitor.stop_monitoring()
        
        # ì „ì²´ ì‹¤í–‰ ì‹œê°„
        total_benchmark_time = time.time() - overall_start_time
        
        # ì „ì²´ ì ìˆ˜ ê³„ì‚°
        overall_score = self._calculate_overall_score()
        
        # ì¶”ì²œì‚¬í•­ ìƒì„±
        recommendations = self._generate_recommendations()
        
        # ê²°ê³¼ ìƒì„±
        benchmark_result = BenchmarkResult(
            benchmark_id=self.benchmark_id,
            timestamp=datetime.now(),
            system_info=system_info,
            test_scenarios=self.test_scenarios,
            performance_metrics=self.results,
            overall_score=overall_score,
            recommendations=recommendations,
            detailed_logs=self.detailed_logs
        )
        
        # ìµœì¢… ë¦¬í¬íŠ¸ ì¶œë ¥
        self._print_final_report(benchmark_result, total_benchmark_time)
        
        return benchmark_result
    
    async def _run_scenario_benchmark(self, scenario: TestScenario) -> PerformanceMetrics:
        """ë‹¨ì¼ ì‹œë‚˜ë¦¬ì˜¤ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        
        scenario_start_time = time.time()
        
        # í…ŒìŠ¤íŠ¸ íŒŒì¼ ìƒì„±
        print(f"ğŸ“ í…ŒìŠ¤íŠ¸ íŒŒì¼ ìƒì„± ì¤‘... ({scenario.file_count}ê°œ, {scenario.total_size_mb:.1f}MB)")
        test_files = self.test_data_generator.generate_test_files(scenario)
        
        try:
            # í†µí•© ì‹œìŠ¤í…œ ì„¤ì •
            streaming_config = StreamingConfig(
                chunk_size_mb=25.0,
                max_memory_mb=512.0,
                compression_enabled=True,
                max_concurrent_chunks=3
            )
            
            recovery_config = RecoveryConfig(
                max_retry_attempts=3,
                auto_recovery_enabled=True,
                recovery_level=RecoveryLevel.FULL
            )
            
            # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
            streaming_processor = StreamingProcessor(streaming_config)
            recovery_manager = RecoveryManager(recovery_config, Path("benchmark_recovery"))
            validator = AdvancedCrossValidator(ValidationLevel.COMPREHENSIVE)
            
            await streaming_processor.start_monitoring()
            
            # ì²˜ë¦¬ ì‹¤í–‰
            processing_results = []
            error_count = 0
            critical_errors = 0
            
            print(f"âš¡ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘...")
            
            for i, file_path in enumerate(test_files):
                try:
                    print(f"   ì²˜ë¦¬ ì¤‘: {i+1}/{len(test_files)} ({os.path.basename(file_path)})")
                    
                    # ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ (ë³µêµ¬ ê¸°ëŠ¥ í¬í•¨)
                    async def process_with_recovery():
                        return await streaming_processor.process_file_streaming(
                            file_path=file_path,
                            file_id=f"test_file_{i}",
                            processor_func=self._mock_jewelry_processor
                        )
                    
                    result = await recovery_manager.execute_with_recovery(
                        operation_id=f"process_file_{i}",
                        operation_func=process_with_recovery,
                        session_id=scenario.scenario_id
                    )
                    
                    processing_results.append(result)
                    
                except Exception as e:
                    error_count += 1
                    if "critical" in str(e).lower() or "memory" in str(e).lower():
                        critical_errors += 1
                    self.logger.error(f"íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            
            # í¬ë¡œìŠ¤ ê²€ì¦ ì‹¤í–‰
            print(f"ğŸ” í¬ë¡œìŠ¤ ê²€ì¦ ì‹¤í–‰...")
            validation_items = [
                {
                    "id": f"item_{i}",
                    "content": result.get("result", {}).get("merged_content", ""),
                    "quality": result.get("result", {}).get("average_confidence", 0.8),
                    "reliability": 0.9 if result.get("success", False) else 0.5
                }
                for i, result in enumerate(processing_results)
                if result.get("success", False)
            ]
            
            validation_result = await validator.validate_cross_consistency(validation_items)
            
            # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
            metrics = self._calculate_performance_metrics(
                scenario, processing_results, validation_result, 
                error_count, critical_errors, scenario_start_time
            )
            
            return metrics
            
        finally:
            # ì •ë¦¬
            await streaming_processor.stop_monitoring()
            self.test_data_generator.cleanup_test_files(test_files)
    
    async def _mock_jewelry_processor(self, chunk_data: bytes, chunk) -> Dict[str, Any]:
        """ëª¨ì˜ ì£¼ì–¼ë¦¬ ì²˜ë¦¬ í•¨ìˆ˜"""
        
        # ì²˜ë¦¬ ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜ (íŒŒì¼ í¬ê¸°ì— ë¹„ë¡€)
        processing_time = len(chunk_data) / (1024 * 1024) * 0.1  # MBë‹¹ 0.1ì´ˆ
        await asyncio.sleep(min(processing_time, 2.0))  # ìµœëŒ€ 2ì´ˆ
        
        # ì£¼ì–¼ë¦¬ ê´€ë ¨ í‚¤ì›Œë“œ ìƒì„± (ëœë¤)
        jewelry_keywords = [
            "ë‹¤ì´ì•„ëª¬ë“œ", "ë£¨ë¹„", "ì‚¬íŒŒì´ì–´", "ì—ë©”ë„ë“œ", "4C", "GIA", 
            "ìºëŸ¿", "ì»¬ëŸ¬", "í´ë˜ë¦¬í‹°", "ì»·", "ë¸Œë¦´ë¦¬ì–¸íŠ¸", "í”„ë¦°ì„¸ìŠ¤"
        ]
        
        selected_keywords = np.random.choice(
            jewelry_keywords, 
            size=np.random.randint(2, 6), 
            replace=False
        ).tolist()
        
        # ì‹ ë¢°ë„ ê³„ì‚° (íŒŒì¼ í¬ê¸°ì™€ ì²˜ë¦¬ í’ˆì§ˆì— ê¸°ë°˜)
        base_confidence = 0.85
        size_factor = min(1.0, len(chunk_data) / (10 * 1024 * 1024))  # 10MB ê¸°ì¤€
        confidence = base_confidence + (size_factor * 0.1)
        
        return {
            "content": f"ì£¼ì–¼ë¦¬ ê´€ë ¨ ë‚´ìš© ë¶„ì„ ê²°ê³¼ (ì²­í¬: {chunk.chunk_id}): {', '.join(selected_keywords)}",
            "confidence": confidence,
            "keywords": selected_keywords,
            "chunk_info": {
                "chunk_id": chunk.chunk_id,
                "size_mb": chunk.size_mb,
                "processing_time": processing_time
            }
        }
    
    def _calculate_performance_metrics(
        self, 
        scenario: TestScenario,
        processing_results: List[Dict],
        validation_result,
        error_count: int,
        critical_errors: int,
        start_time: float
    ) -> PerformanceMetrics:
        """ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°"""
        
        total_time = time.time() - start_time
        successful_results = [r for r in processing_results if r.get("success", False)]
        
        # ì²˜ë¦¬ ì„±ëŠ¥
        total_processing_time = sum(r.get("processing_time", 0) for r in successful_results)
        average_file_time = total_processing_time / max(len(successful_results), 1)
        throughput_mbps = scenario.total_size_mb / max(total_time, 0.1)
        
        # ë©”ëª¨ë¦¬ ì„±ëŠ¥
        resource_summary = self.resource_monitor.get_summary()
        peak_memory = resource_summary.get("memory", {}).get("peak_mb", 0)
        avg_memory = resource_summary.get("memory", {}).get("average_mb", 0)
        memory_efficiency = throughput_mbps / max(avg_memory, 1)
        
        # ê²€ì¦ ì„±ëŠ¥
        validation_accuracy = validation_result.overall_score if validation_result else 0.0
        anomaly_count = len(validation_result.anomalies) if validation_result else 0
        anomaly_detection_rate = anomaly_count / max(len(successful_results), 1)
        
        # ë³µêµ¬ ì„±ëŠ¥ (ê°„ë‹¨í™”)
        error_recovery_rate = (scenario.file_count - error_count) / scenario.file_count
        
        # ì•ˆì •ì„± ì§€í‘œ
        success_rate = len(successful_results) / scenario.file_count
        
        # ì‚¬ìš©ì ê²½í—˜ ì ìˆ˜ (ì²˜ë¦¬ ì‹œê°„ ê¸°ë°˜)
        expected_time = scenario.expected_duration
        time_ratio = total_time / expected_time
        responsiveness_score = max(0.0, min(1.0, 2.0 - time_ratio))
        
        return PerformanceMetrics(
            scenario_id=scenario.scenario_id,
            total_processing_time=total_processing_time,
            average_file_time=average_file_time,
            throughput_mbps=throughput_mbps,
            peak_memory_mb=peak_memory,
            average_memory_mb=avg_memory,
            memory_efficiency=memory_efficiency,
            validation_accuracy=validation_accuracy,
            anomaly_detection_rate=anomaly_detection_rate,
            false_positive_rate=0.05,  # ê°€ì •ê°’
            error_recovery_rate=error_recovery_rate,
            recovery_time=1.5,  # ê°€ì •ê°’
            checkpoint_overhead=0.1,  # ê°€ì •ê°’
            success_rate=success_rate,
            error_count=error_count,
            critical_errors=critical_errors,
            responsiveness_score=responsiveness_score,
            progress_accuracy=0.95  # ê°€ì •ê°’
        )
    
    def _calculate_overall_score(self) -> float:
        """ì „ì²´ ì ìˆ˜ ê³„ì‚°"""
        if not self.results:
            return 0.0
        
        # ì£¼ìš” ì§€í‘œë“¤ì˜ ê°€ì¤‘ í‰ê· 
        weights = {
            "throughput": 0.25,
            "memory_efficiency": 0.20,
            "validation_accuracy": 0.20,
            "success_rate": 0.15,
            "responsiveness": 0.10,
            "error_recovery": 0.10
        }
        
        scores = []
        for metrics in self.results:
            # ê° ì§€í‘œë¥¼ 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
            normalized_throughput = min(1.0, metrics.throughput_mbps / 10.0)  # 10MB/së¥¼ ìµœëŒ€ë¡œ
            normalized_memory_eff = min(1.0, metrics.memory_efficiency / 0.1)  # 0.1ì„ ìµœëŒ€ë¡œ
            
            scenario_score = (
                weights["throughput"] * normalized_throughput +
                weights["memory_efficiency"] * normalized_memory_eff +
                weights["validation_accuracy"] * metrics.validation_accuracy +
                weights["success_rate"] * metrics.success_rate +
                weights["responsiveness"] * metrics.responsiveness_score +
                weights["error_recovery"] * metrics.error_recovery_rate
            )
            
            scores.append(scenario_score)
        
        return np.mean(scores)
    
    def _generate_recommendations(self) -> List[str]:
        """ì¶”ì²œì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        if not self.results:
            return ["ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."]
        
        # ì„±ëŠ¥ ë¶„ì„
        avg_throughput = np.mean([m.throughput_mbps for m in self.results])
        avg_memory = np.mean([m.peak_memory_mb for m in self.results])
        avg_success_rate = np.mean([m.success_rate for m in self.results])
        
        # ì²˜ë¦¬ëŸ‰ ê¸°ë°˜ ì¶”ì²œ
        if avg_throughput < 2.0:
            recommendations.append("ì²˜ë¦¬ ì†ë„ê°€ ë‚®ìŠµë‹ˆë‹¤. ì²­í¬ í¬ê¸° ëŠ˜ë¦¬ê¸° ë˜ëŠ” ë™ì‹œ ì²˜ë¦¬ ìˆ˜ ì¦ê°€ë¥¼ ê²€í† í•˜ì„¸ìš”.")
        elif avg_throughput > 8.0:
            recommendations.append("ìš°ìˆ˜í•œ ì²˜ë¦¬ ì†ë„ì…ë‹ˆë‹¤. í˜„ì¬ ì„¤ì •ì„ ìœ ì§€í•˜ì„¸ìš”.")
        
        # ë©”ëª¨ë¦¬ ê¸°ë°˜ ì¶”ì²œ
        if avg_memory > 1000:
            recommendations.append("ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ìŠµë‹ˆë‹¤. ì••ì¶• ê¸°ëŠ¥ í™œì„±í™” ë˜ëŠ” ì²­í¬ í¬ê¸° ì¤„ì´ê¸°ë¥¼ ê²€í† í•˜ì„¸ìš”.")
        
        # ì„±ê³µë¥  ê¸°ë°˜ ì¶”ì²œ
        if avg_success_rate < 0.9:
            recommendations.append("ì²˜ë¦¬ ì„±ê³µë¥ ì´ ë‚®ìŠµë‹ˆë‹¤. ì—ëŸ¬ ë³µêµ¬ ì‹œìŠ¤í…œ ê°•í™”ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        elif avg_success_rate > 0.95:
            recommendations.append("ë†’ì€ ì„±ê³µë¥ ì„ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤. ì•ˆì •ì„±ì´ ìš°ìˆ˜í•©ë‹ˆë‹¤.")
        
        # ê²€ì¦ ì •í™•ë„ ê¸°ë°˜ ì¶”ì²œ
        avg_validation = np.mean([m.validation_accuracy for m in self.results])
        if avg_validation < 0.8:
            recommendations.append("ê²€ì¦ ì •í™•ë„ê°€ ë‚®ìŠµë‹ˆë‹¤. í¬ë¡œìŠ¤ ê²€ì¦ ì•Œê³ ë¦¬ì¦˜ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        if not recommendations:
            recommendations.append("ì „ë°˜ì ìœ¼ë¡œ ì–‘í˜¸í•œ ì„±ëŠ¥ì„ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤.")
        
        return recommendations
    
    def _collect_system_info(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘"""
        return {
            "cpu_count": psutil.cpu_count(),
            "memory_total_gb": psutil.virtual_memory().total / (1024**3),
            "python_version": f"{sys.version_info.major}.{sys.version_info.minor}",
            "platform": platform.platform(),
            "benchmark_level": self.benchmark_level.value,
            "timestamp": datetime.now().isoformat()
        }
    
    def _print_scenario_results(self, metrics: PerformanceMetrics):
        """ì‹œë‚˜ë¦¬ì˜¤ ê²°ê³¼ ì¶œë ¥"""
        print(f"âœ… ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"   ğŸ“Š ì²˜ë¦¬ëŸ‰: {metrics.throughput_mbps:.2f} MB/s")
        print(f"   ğŸ§  í”¼í¬ ë©”ëª¨ë¦¬: {metrics.peak_memory_mb:.1f} MB")
        print(f"   âœ… ì„±ê³µë¥ : {metrics.success_rate:.1%}")
        print(f"   ğŸ” ê²€ì¦ ì •í™•ë„: {metrics.validation_accuracy:.1%}")
        print(f"   âš¡ ì‘ë‹µì„±: {metrics.responsiveness_score:.1%}")
    
    def _print_final_report(self, result: BenchmarkResult, total_time: float):
        """ìµœì¢… ë¦¬í¬íŠ¸ ì¶œë ¥"""
        print("\n" + "=" * 80)
        print("ğŸ¯ í†µí•© ë²¤ì¹˜ë§ˆí¬ ìµœì¢… ê²°ê³¼")
        print("=" * 80)
        
        print(f"ğŸ†” ë²¤ì¹˜ë§ˆí¬ ID: {result.benchmark_id}")
        print(f"â±ï¸ ì´ ì‹¤í–‰ ì‹œê°„: {total_time:.1f}ì´ˆ")
        print(f"ğŸ¯ ì „ì²´ ì ìˆ˜: {result.overall_score:.3f}/1.000")
        
        # ì‹œìŠ¤í…œ ì •ë³´
        print(f"\nğŸ’» ì‹œìŠ¤í…œ ì •ë³´:")
        print(f"   CPU ì½”ì–´: {result.system_info['cpu_count']}ê°œ")
        print(f"   ë©”ëª¨ë¦¬: {result.system_info['memory_total_gb']:.1f}GB")
        print(f"   ë²¤ì¹˜ë§ˆí¬ ë ˆë²¨: {result.system_info['benchmark_level']}")
        
        # ì‹œë‚˜ë¦¬ì˜¤ë³„ ìš”ì•½
        print(f"\nğŸ“‹ ì‹œë‚˜ë¦¬ì˜¤ë³„ ì„±ê³¼:")
        for i, metrics in enumerate(result.performance_metrics):
            scenario = result.test_scenarios[i]
            print(f"   {i+1}. {scenario.name}")
            print(f"      ì²˜ë¦¬ëŸ‰: {metrics.throughput_mbps:.2f} MB/s")
            print(f"      ì„±ê³µë¥ : {metrics.success_rate:.1%}")
            print(f"      ë©”ëª¨ë¦¬: {metrics.peak_memory_mb:.1f}MB")
        
        # ì¶”ì²œì‚¬í•­
        print(f"\nğŸ’¡ ì¶”ì²œì‚¬í•­:")
        for i, rec in enumerate(result.recommendations, 1):
            print(f"   {i}. {rec}")
        
        print("\n" + "=" * 80)
        print("âœ… í†µí•© ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")

# ì‚¬ìš© ì˜ˆì‹œ
async def demo_integrated_benchmark():
    """í†µí•© ë²¤ì¹˜ë§ˆí¬ ë°ëª¨"""
    
    # ë‹¤ì–‘í•œ ë ˆë²¨ë¡œ í…ŒìŠ¤íŠ¸
    benchmark_levels = [BenchmarkLevel.LIGHT, BenchmarkLevel.STANDARD]
    
    for level in benchmark_levels:
        print(f"\nğŸš€ {level.value.upper()} ë ˆë²¨ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")
        
        benchmark_suite = IntegratedBenchmarkSuite(level)
        result = await benchmark_suite.run_comprehensive_benchmark()
        
        # ê²°ê³¼ ì €ì¥ (ì˜µì…˜)
        result_file = f"benchmark_result_{level.value}_{int(time.time())}.json"
        with open(result_file, 'w') as f:
            # dataclassë¥¼ JSONìœ¼ë¡œ ë³€í™˜ (ê°„ë‹¨í™”)
            json.dump({
                "benchmark_id": result.benchmark_id,
                "overall_score": result.overall_score,
                "recommendations": result.recommendations
            }, f, indent=2)
        
        print(f"ğŸ“„ ê²°ê³¼ ì €ì¥: {result_file}")

if __name__ == "__main__":
    import sys
    import platform
    
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(level=logging.INFO)
    
    # ë°ëª¨ ì‹¤í–‰
    asyncio.run(demo_integrated_benchmark())
