"""
ì†”ë¡œëª¬ë“œ AI ì‹œìŠ¤í…œ - ê³ ìš©ëŸ‰ ë‹¤ì¤‘ë¶„ì„ ì „ìš© LLM ìš”ì•½ ì—”ì§„
GEMMA ê¸°ë°˜ìœ¼ë¡œ 5GB íŒŒì¼ 50ê°œ ë™ì‹œ ì²˜ë¦¬ ìµœì í™”

íŠ¹ì§•:
- ì²­í¬ ë‹¨ìœ„ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
- ë‹¤ì¤‘ ì†ŒìŠ¤ í†µí•© ìš”ì•½ (í¬ë¡œìŠ¤ ê²€ì¦)
- ì£¼ì–¼ë¦¬ ë„ë©”ì¸ íŠ¹í™” ë¶„ì„
- ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”
"""

import asyncio
import json
import logging
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime
from dataclasses import dataclass, field
from enum import Enum
import hashlib
import time
import gc
from pathlib import Path
import numpy as np
from collections import defaultdict, Counter
import re

# ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ (ì‹¤ì œ êµ¬í˜„ ì‹œ í•„ìš”)
try:
    import torch
    import transformers
    from transformers import AutoModelForCausalLM, AutoTokenizer
    GEMMA_AVAILABLE = True
except ImportError:
    GEMMA_AVAILABLE = False
    logging.warning("GEMMA ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ. ëª¨ì˜ ëª¨ë“œë¡œ ì‹¤í–‰")

class ProcessingMode(Enum):
    STREAMING = "streaming"  # ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ (ëŒ€ìš©ëŸ‰)
    BATCH = "batch"         # ë°°ì¹˜ ì²˜ë¦¬ (ì¤‘ê°„ ìš©ëŸ‰)
    MEMORY = "memory"       # ë©”ëª¨ë¦¬ ì²˜ë¦¬ (ì†Œìš©ëŸ‰)

class SummaryType(Enum):
    EXECUTIVE = "executive"     # ê²½ì˜ì§„ ìš”ì•½
    TECHNICAL = "technical"     # ê¸°ìˆ ì  ìš”ì•½
    BUSINESS = "business"       # ë¹„ì¦ˆë‹ˆìŠ¤ ìš”ì•½
    COMPREHENSIVE = "comprehensive"  # ì¢…í•© ìš”ì•½

@dataclass
class ChunkInfo:
    """í…ìŠ¤íŠ¸ ì²­í¬ ì •ë³´"""
    chunk_id: str
    source_file: str
    source_type: str  # audio, video, document, image
    text: str
    token_count: int
    jewelry_terms: List[str] = field(default_factory=list)
    confidence: float = 0.0
    processing_time: float = 0.0

@dataclass
class SummaryRequest:
    """ìš”ì•½ ìš”ì²­ ì •ë³´"""
    session_id: str
    title: str
    chunks: List[ChunkInfo]
    summary_type: SummaryType
    max_length: int = 2000
    focus_keywords: List[str] = field(default_factory=list)
    language: str = "ko"
    priority_sources: List[str] = field(default_factory=list)

class AdvancedLLMSummarizer:
    """ê³ ìš©ëŸ‰ ë‹¤ì¤‘ë¶„ì„ ì „ìš© LLM ìš”ì•½ ì—”ì§„"""
    
    def __init__(self, model_name: str = "google/gemma-2b-it"):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.max_context_length = 4096
        self.chunk_size = 1024
        self.overlap_size = 128
        
        # ì£¼ì–¼ë¦¬ íŠ¹í™” í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        self.jewelry_prompts = {
            "executive": """ë‹¤ìŒ ì£¼ì–¼ë¦¬ ì—…ê³„ ì •ë³´ë¥¼ ê²½ì˜ì§„ ê´€ì ì—ì„œ ìš”ì•½í•´ì£¼ì„¸ìš”:
- í•µì‹¬ ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸
- ì‹œì¥ ê¸°íšŒ ë° ìœ„í—˜
- ì‹¤í–‰ ê°€ëŠ¥í•œ ì „ëµì  ê¶Œì¥ì‚¬í•­
- ì¬ë¬´ì  ì˜í–¥ ë¶„ì„

ë‚´ìš©: {content}

ìš”ì•½ (í•œêµ­ì–´, 500ì ì´ë‚´):""",
            
            "technical": """ë‹¤ìŒ ì£¼ì–¼ë¦¬ ê¸°ìˆ  ì •ë³´ë¥¼ ì „ë¬¸ê°€ ê´€ì ì—ì„œ ìš”ì•½í•´ì£¼ì„¸ìš”:
- ë³´ì„í•™ì  íŠ¹ì§• ë° í’ˆì§ˆ ë¶„ì„
- ê°€ê³µ ê¸°ìˆ  ë° ì²˜ë¦¬ ë°©ë²•
- ê°ì • ë° ì¸ì¦ ê´€ë ¨ ì‚¬í•­
- ê¸°ìˆ ì  íŠ¸ë Œë“œ ë° í˜ì‹ 

ë‚´ìš©: {content}

ê¸°ìˆ  ìš”ì•½ (í•œêµ­ì–´, 800ì ì´ë‚´):""",
            
            "business": """ë‹¤ìŒ ì£¼ì–¼ë¦¬ ë¹„ì¦ˆë‹ˆìŠ¤ ì •ë³´ë¥¼ ì‹¤ë¬´ì§„ ê´€ì ì—ì„œ ìš”ì•½í•´ì£¼ì„¸ìš”:
- ê°€ê²© ë™í–¥ ë° ì‹œì¥ ë¶„ì„
- ê±°ë˜ ì¡°ê±´ ë° ìƒê±°ë˜ ì •ë³´
- ê³ ê° ë™í–¥ ë° ì„ í˜¸ë„ ë³€í™”
- ì˜ì—… ì „ëµ ë° ë§ˆì¼€íŒ… ì¸ì‚¬ì´íŠ¸

ë‚´ìš©: {content}

ë¹„ì¦ˆë‹ˆìŠ¤ ìš”ì•½ (í•œêµ­ì–´, 600ì ì´ë‚´):""",
            
            "comprehensive": """ë‹¤ìŒ ì£¼ì–¼ë¦¬ ì—…ê³„ ì¢…í•© ì •ë³´ë¥¼ ì „ì²´ì ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:
- ì‹œì¥ í˜„í™© ë° ì „ë§
- ê¸°ìˆ ì  íŠ¹ì§• ë° í’ˆì§ˆ ë¶„ì„
- ë¹„ì¦ˆë‹ˆìŠ¤ ê¸°íšŒ ë° ì „ëµ
- ì—…ê³„ íŠ¸ë Œë“œ ë° ë¯¸ë˜ ë°©í–¥

ë‚´ìš©: {content}

ì¢…í•© ìš”ì•½ (í•œêµ­ì–´, 1200ì ì´ë‚´):"""
        }
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì 
        self.memory_usage = {
            "peak_memory": 0,
            "current_memory": 0,
            "chunks_processed": 0,
            "gc_collections": 0
        }
        
        logging.info(f"ê³ ê¸‰ LLM ìš”ì•½ ì—”ì§„ ì´ˆê¸°í™” (ëª¨ë¸: {model_name})")
    
    async def initialize_model(self):
        """ëª¨ë¸ ì´ˆê¸°í™” (ì§€ì—° ë¡œë”©)"""
        if self.model is not None:
            return
            
        try:
            if GEMMA_AVAILABLE:
                print("ğŸ¤– GEMMA ëª¨ë¸ ë¡œë”© ì¤‘...")
                self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    torch_dtype=torch.float16,
                    device_map="auto" if self.device == "cuda" else None
                )
                print("âœ… GEMMA ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            else:
                print("âš ï¸ GEMMA ëª¨ì˜ ëª¨ë“œë¡œ ì‹¤í–‰")
                
        except Exception as e:
            logging.error(f"ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            print("âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨, ëª¨ì˜ ëª¨ë“œë¡œ ì „í™˜")
    
    async def process_large_batch(self, 
                                session_data: Dict,
                                processing_mode: ProcessingMode = ProcessingMode.STREAMING) -> Dict:
        """ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜"""
        print(f"ğŸš€ ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘ (ëª¨ë“œ: {processing_mode.value})")
        
        session_id = session_data.get("session_id", f"batch_{int(time.time())}")
        start_time = time.time()
        
        try:
            # 1. ëª¨ë¸ ì´ˆê¸°í™”
            await self.initialize_model()
            
            # 2. ì…ë ¥ ë°ì´í„° ê²€ì¦ ë° ë¶„ë¥˜
            validated_data = await self._validate_and_classify_inputs(session_data)
            
            # 3. ì²­í¬ ë‹¨ìœ„ ë¶„í•  (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
            chunks = await self._create_optimized_chunks(validated_data, processing_mode)
            
            # 4. ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™”
            processed_chunks = await self._process_chunks_parallel(chunks, processing_mode)
            
            # 5. ê³„ì¸µì  ìš”ì•½ ìƒì„±
            hierarchical_summary = await self._generate_hierarchical_summary(processed_chunks)
            
            # 6. í¬ë¡œìŠ¤ ê²€ì¦ ë° í’ˆì§ˆ í‰ê°€
            quality_assessment = await self._assess_summary_quality(hierarchical_summary, processed_chunks)
            
            # 7. ìµœì¢… í†µí•© ë³´ê³ ì„œ ìƒì„±
            final_report = await self._generate_final_report(
                session_id, 
                hierarchical_summary, 
                quality_assessment,
                processing_mode,
                start_time
            )
            
            # 8. ë©”ëª¨ë¦¬ ì •ë¦¬
            await self._cleanup_memory()
            
            print(f"âœ… ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ ({time.time() - start_time:.1f}ì´ˆ)")
            return final_report
            
        except Exception as e:
            logging.error(f"ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return {
                "success": False,
                "error": str(e),
                "session_id": session_id,
                "processing_mode": processing_mode.value
            }
    
    async def _validate_and_classify_inputs(self, session_data: Dict) -> Dict:
        """ì…ë ¥ ë°ì´í„° ê²€ì¦ ë° ë¶„ë¥˜"""
        print("ğŸ“‹ ì…ë ¥ ë°ì´í„° ê²€ì¦ ì¤‘...")
        
        files = session_data.get("files", [])
        total_size = sum(f.get("size_mb", 0) for f in files)
        
        # ìš©ëŸ‰ë³„ ì²˜ë¦¬ ëª¨ë“œ ìë™ ê²°ì •
        if total_size > 2000:  # 2GB ì´ìƒ
            recommended_mode = ProcessingMode.STREAMING
        elif total_size > 500:  # 500MB ì´ìƒ
            recommended_mode = ProcessingMode.BATCH
        else:
            recommended_mode = ProcessingMode.MEMORY
        
        # íŒŒì¼ íƒ€ì…ë³„ ë¶„ë¥˜
        classified_files = {
            "audio": [],
            "video": [],
            "documents": [],
            "images": []
        }
        
        for file_info in files:
            file_type = self._detect_file_type(file_info.get("filename", ""))
            if file_type in classified_files:
                classified_files[file_type].append(file_info)
        
        return {
            "total_files": len(files),
            "total_size_mb": total_size,
            "recommended_mode": recommended_mode,
            "classified_files": classified_files,
            "processing_complexity": self._calculate_complexity(files)
        }
    
    async def _create_optimized_chunks(self, validated_data: Dict, mode: ProcessingMode) -> List[ChunkInfo]:
        """ìµœì í™”ëœ ì²­í¬ ìƒì„±"""
        print(f"ğŸ”„ ì²­í¬ ìƒì„± ì¤‘... (ëª¨ë“œ: {mode.value})")
        
        chunks = []
        chunk_id_counter = 0
        
        for file_type, files in validated_data["classified_files"].items():
            for file_info in files:
                # ê° íŒŒì¼ì˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì´ë¯¸ ì²˜ë¦¬ëœ ìƒíƒœë¼ê³  ê°€ì •)
                text_content = file_info.get("processed_text", "")
                
                if not text_content:
                    continue
                
                # ì²­í¬ í¬ê¸° ê²°ì • (ëª¨ë“œì— ë”°ë¼ ë‹¤ë¦„)
                chunk_size = self._get_optimal_chunk_size(mode, len(text_content))
                
                # í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
                text_chunks = self._split_text_into_chunks(text_content, chunk_size)
                
                for i, chunk_text in enumerate(text_chunks):
                    chunk_info = ChunkInfo(
                        chunk_id=f"chunk_{chunk_id_counter:04d}",
                        source_file=file_info.get("filename", "unknown"),
                        source_type=file_type,
                        text=chunk_text,
                        token_count=len(chunk_text.split()),
                        jewelry_terms=self._extract_jewelry_terms(chunk_text),
                        confidence=file_info.get("confidence", 0.8)
                    )
                    chunks.append(chunk_info)
                    chunk_id_counter += 1
        
        print(f"âœ… ì²­í¬ ìƒì„± ì™„ë£Œ: {len(chunks)}ê°œ")
        return chunks
    
    async def _process_chunks_parallel(self, chunks: List[ChunkInfo], mode: ProcessingMode) -> List[ChunkInfo]:
        """ë³‘ë ¬ ì²­í¬ ì²˜ë¦¬"""
        print(f"âš¡ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘: {len(chunks)}ê°œ ì²­í¬")
        
        # ë™ì‹œ ì²˜ë¦¬ ìˆ˜ ê²°ì • (ëª¨ë“œì™€ ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ì— ë”°ë¼)
        max_concurrent = self._get_max_concurrent_tasks(mode)
        
        processed_chunks = []
        
        # ì²­í¬ë¥¼ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
        for i in range(0, len(chunks), max_concurrent):
            batch = chunks[i:i + max_concurrent]
            
            # ë°°ì¹˜ ë‚´ ë³‘ë ¬ ì²˜ë¦¬
            tasks = [self._process_single_chunk(chunk) for chunk in batch]
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # ê²°ê³¼ ìˆ˜ì§‘
            for result in batch_results:
                if isinstance(result, Exception):
                    logging.error(f"ì²­í¬ ì²˜ë¦¬ ì˜¤ë¥˜: {result}")
                    continue
                processed_chunks.append(result)
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬ (ì¤‘ê°„ ë‹¨ê³„)
            if i % (max_concurrent * 3) == 0:
                await self._intermediate_memory_cleanup()
        
        print(f"âœ… ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ: {len(processed_chunks)}ê°œ")
        return processed_chunks
    
    async def _process_single_chunk(self, chunk: ChunkInfo) -> ChunkInfo:
        """ê°œë³„ ì²­í¬ ì²˜ë¦¬"""
        start_time = time.time()
        
        try:
            # ì£¼ì–¼ë¦¬ ìš©ì–´ ê°•í™”
            enhanced_text = await self._enhance_jewelry_content(chunk.text)
            
            # í•µì‹¬ ì •ë³´ ì¶”ì¶œ
            key_info = await self._extract_key_information(enhanced_text)
            
            # ìš”ì•½ ìƒì„± (ì²­í¬ ìˆ˜ì¤€)
            chunk_summary = await self._generate_chunk_summary(enhanced_text, chunk.source_type)
            
            # ê²°ê³¼ ì—…ë°ì´íŠ¸
            chunk.text = chunk_summary
            chunk.jewelry_terms = self._extract_jewelry_terms(chunk_summary)
            chunk.processing_time = time.time() - start_time
            
            return chunk
            
        except Exception as e:
            logging.error(f"ì²­í¬ ì²˜ë¦¬ ì˜¤ë¥˜ ({chunk.chunk_id}): {e}")
            chunk.confidence = 0.0
            chunk.processing_time = time.time() - start_time
            return chunk
    
    async def _generate_chunk_summary(self, text: str, source_type: str) -> str:
        """ì²­í¬ ìˆ˜ì¤€ ìš”ì•½ ìƒì„±"""
        if GEMMA_AVAILABLE and self.model is not None:
            return await self._generate_with_gemma(text, "business")
        else:
            return await self._generate_mock_summary(text, source_type)
    
    async def _generate_with_gemma(self, text: str, summary_type: str) -> str:
        """GEMMA ëª¨ë¸ë¡œ ìš”ì•½ ìƒì„±"""
        try:
            prompt = self.jewelry_prompts[summary_type].format(content=text)
            
            inputs = self.tokenizer.encode(prompt, return_tensors="pt", max_length=self.max_context_length, truncation=True)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    max_length=inputs.shape[1] + 300,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            summary = generated_text[len(prompt):].strip()
            
            return summary
            
        