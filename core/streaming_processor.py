# Phase 2 Week 3: ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì—”ì§„
# ë©”ëª¨ë¦¬ ìµœì í™” - ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²­í‚¹ ë° ìˆœì°¨ ì²˜ë¦¬

import asyncio
import os
import psutil
import gc
import tempfile
import gzip
import pickle
from typing import AsyncGenerator, Dict, List, Any, Optional, Callable
from dataclasses import dataclass, field
from pathlib import Path
import time
import logging
from enum import Enum
import hashlib
import json

# ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§
@dataclass
class MemoryStats:
    """ë©”ëª¨ë¦¬ ì‚¬ìš© í†µê³„"""
    used_mb: float
    available_mb: float
    percent: float
    peak_mb: float
    timestamp: float

class ChunkStatus(Enum):
    PENDING = "pending"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"
    CACHED = "cached"

@dataclass
class FileChunk:
    """íŒŒì¼ ì²­í¬ ì •ë³´"""
    chunk_id: str
    file_id: str
    start_byte: int
    end_byte: int
    size_mb: float
    status: ChunkStatus = ChunkStatus.PENDING
    processing_time: float = 0.0
    content_hash: str = ""
    compressed_path: Optional[str] = None
    result_data: Optional[Dict] = None

@dataclass
class StreamingConfig:
    """ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì„¤ì •"""
    chunk_size_mb: float = 50.0  # ì²­í¬ í¬ê¸° (MB)
    max_memory_mb: float = 512.0  # ìµœëŒ€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MB)
    compression_enabled: bool = True  # ì••ì¶• ì‚¬ìš© ì—¬ë¶€
    cache_intermediate: bool = True  # ì¤‘ê°„ ê²°ê³¼ ìºì‹±
    max_concurrent_chunks: int = 3  # ë™ì‹œ ì²˜ë¦¬ ì²­í¬ ìˆ˜
    memory_threshold: float = 0.8  # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ì„ê³„ê°’
    cleanup_interval: int = 30  # ì •ë¦¬ ì£¼ê¸° (ì´ˆ)

class MemoryMonitor:
    """ì‹¤ì‹œê°„ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§"""
    
    def __init__(self):
        self.stats_history: List[MemoryStats] = []
        self.peak_memory = 0.0
        self.alerts: List[str] = []
        self.monitoring = False
    
    def start_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        self.monitoring = True
        asyncio.create_task(self._monitor_loop())
    
    def stop_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        self.monitoring = False
    
    async def _monitor_loop(self):
        """ëª¨ë‹ˆí„°ë§ ë£¨í”„"""
        while self.monitoring:
            stats = self.get_current_stats()
            self.stats_history.append(stats)
            
            # í”¼í¬ ë©”ëª¨ë¦¬ ì—…ë°ì´íŠ¸
            if stats.used_mb > self.peak_memory:
                self.peak_memory = stats.used_mb
            
            # ë©”ëª¨ë¦¬ ë¶€ì¡± ê²½ê³ 
            if stats.percent > 85:
                self.alerts.append(f"ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : {stats.percent:.1f}%")
            
            # íˆìŠ¤í† ë¦¬ í¬ê¸° ì œí•œ (ìµœê·¼ 100ê°œ)
            if len(self.stats_history) > 100:
                self.stats_history = self.stats_history[-100:]
            
            await asyncio.sleep(5)  # 5ì´ˆë§ˆë‹¤ ì²´í¬
    
    def get_current_stats(self) -> MemoryStats:
        """í˜„ì¬ ë©”ëª¨ë¦¬ í†µê³„ ë°˜í™˜"""
        memory = psutil.virtual_memory()
        return MemoryStats(
            used_mb=memory.used / 1024 / 1024,
            available_mb=memory.available / 1024 / 1024,
            percent=memory.percent,
            peak_mb=self.peak_memory,
            timestamp=time.time()
        )
    
    def get_memory_report(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš© ë¦¬í¬íŠ¸"""
        if not self.stats_history:
            return {"error": "ëª¨ë‹ˆí„°ë§ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"}
        
        current = self.stats_history[-1]
        avg_usage = sum(s.used_mb for s in self.stats_history[-10:]) / min(10, len(self.stats_history))
        
        return {
            "current_mb": current.used_mb,
            "peak_mb": self.peak_memory,
            "average_mb": avg_usage,
            "current_percent": current.percent,
            "available_mb": current.available_mb,
            "alerts": self.alerts[-5:],  # ìµœê·¼ 5ê°œ ì•Œë¦¼
            "trend": "increasing" if len(self.stats_history) > 5 and 
                    self.stats_history[-1].used_mb > self.stats_history[-5].used_mb else "stable"
        }

class FileChunker:
    """íŒŒì¼ ì²­í‚¹ ê´€ë¦¬ì"""
    
    def __init__(self, config: StreamingConfig):
        self.config = config
        self.temp_dir = Path(tempfile.mkdtemp(prefix="solomond_chunks_"))
        self.temp_dir.mkdir(exist_ok=True)
    
    def create_chunks(self, file_path: str, file_id: str) -> List[FileChunk]:
        """íŒŒì¼ì„ ì²­í¬ë¡œ ë¶„í• """
        file_size = os.path.getsize(file_path)
        chunk_size_bytes = int(self.config.chunk_size_mb * 1024 * 1024)
        
        chunks = []
        start = 0
        chunk_index = 0
        
        while start < file_size:
            end = min(start + chunk_size_bytes, file_size)
            chunk_size_mb = (end - start) / 1024 / 1024
            
            chunk = FileChunk(
                chunk_id=f"{file_id}_chunk_{chunk_index}",
                file_id=file_id,
                start_byte=start,
                end_byte=end,
                size_mb=chunk_size_mb
            )
            
            chunks.append(chunk)
            start = end
            chunk_index += 1
        
        return chunks
    
    async def read_chunk(self, file_path: str, chunk: FileChunk) -> bytes:
        """ì²­í¬ ë°ì´í„° ì½ê¸°"""
        try:
            with open(file_path, 'rb') as f:
                f.seek(chunk.start_byte)
                data = f.read(chunk.end_byte - chunk.start_byte)
                
                # í•´ì‹œ ìƒì„±
                chunk.content_hash = hashlib.md5(data).hexdigest()
                
                return data
        except Exception as e:
            raise Exception(f"ì²­í¬ ì½ê¸° ì‹¤íŒ¨: {e}")
    
    async def compress_chunk(self, data: bytes, chunk: FileChunk) -> str:
        """ì²­í¬ ì••ì¶• ì €ì¥"""
        if not self.config.compression_enabled:
            return None
        
        compressed_path = self.temp_dir / f"{chunk.chunk_id}.gz"
        
        try:
            with gzip.open(compressed_path, 'wb') as f:
                f.write(data)
            
            # ì••ì¶•ë¥  í™•ì¸
            original_size = len(data)
            compressed_size = os.path.getsize(compressed_path)
            compression_ratio = compressed_size / original_size
            
            chunk.compressed_path = str(compressed_path)
            
            return str(compressed_path)
        except Exception as e:
            raise Exception(f"ì²­í¬ ì••ì¶• ì‹¤íŒ¨: {e}")
    
    async def decompress_chunk(self, chunk: FileChunk) -> bytes:
        """ì²­í¬ ì••ì¶• í•´ì œ"""
        if not chunk.compressed_path:
            raise Exception("ì••ì¶•ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
        
        try:
            with gzip.open(chunk.compressed_path, 'rb') as f:
                return f.read()
        except Exception as e:
            raise Exception(f"ì••ì¶• í•´ì œ ì‹¤íŒ¨: {e}")
    
    def cleanup_chunks(self, chunks: List[FileChunk]):
        """ì²­í¬ íŒŒì¼ ì •ë¦¬"""
        for chunk in chunks:
            if chunk.compressed_path and os.path.exists(chunk.compressed_path):
                try:
                    os.remove(chunk.compressed_path)
                except:
                    pass
    
    def __del__(self):
        """ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬"""
        try:
            if hasattr(self, 'temp_dir') and self.temp_dir.exists():
                import shutil
                shutil.rmtree(self.temp_dir, ignore_errors=True)
        except:
            pass

class StreamingProcessor:
    """ìŠ¤íŠ¸ë¦¬ë° ê¸°ë°˜ íŒŒì¼ ì²˜ë¦¬ê¸°"""
    
    def __init__(self, config: StreamingConfig = None):
        self.config = config or StreamingConfig()
        self.memory_monitor = MemoryMonitor()
        self.chunker = FileChunker(self.config)
        self.processing_queue = asyncio.Queue()
        self.active_chunks: Dict[str, FileChunk] = {}
        self.completed_chunks: Dict[str, FileChunk] = {}
        self.logger = logging.getLogger(__name__)
        
        # í†µê³„
        self.total_processed_mb = 0.0
        self.total_processing_time = 0.0
        self.error_count = 0
        
    async def start_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        self.memory_monitor.start_monitoring()
        asyncio.create_task(self._cleanup_loop())
    
    async def stop_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        self.memory_monitor.stop_monitoring()
    
    async def _cleanup_loop(self):
        """ì£¼ê¸°ì  ë©”ëª¨ë¦¬ ì •ë¦¬"""
        while True:
            await asyncio.sleep(self.config.cleanup_interval)
            await self._force_cleanup()
    
    async def _force_cleanup(self):
        """ê°•ì œ ë©”ëª¨ë¦¬ ì •ë¦¬"""
        # ì™„ë£Œëœ ì²­í¬ ë©”ëª¨ë¦¬ í•´ì œ
        cleanup_count = 0
        for chunk_id in list(self.completed_chunks.keys()):
            chunk = self.completed_chunks[chunk_id]
            if chunk.result_data:
                # ì••ì¶• ì €ì¥ í›„ ë©”ëª¨ë¦¬ í•´ì œ
                if self.config.cache_intermediate:
                    await self._cache_chunk_result(chunk)
                chunk.result_data = None
                cleanup_count += 1
        
        # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
        gc.collect()
        
        if cleanup_count > 0:
            self.logger.info(f"ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ: {cleanup_count}ê°œ ì²­í¬")
    
    async def _cache_chunk_result(self, chunk: FileChunk):
        """ì²­í¬ ê²°ê³¼ ìºì‹±"""
        if not chunk.result_data:
            return
        
        cache_path = self.chunker.temp_dir / f"{chunk.chunk_id}_result.pkl.gz"
        
        try:
            with gzip.open(cache_path, 'wb') as f:
                pickle.dump(chunk.result_data, f)
            
            # ë©”ëª¨ë¦¬ì—ì„œ ì œê±°
            chunk.result_data = None
            chunk.status = ChunkStatus.CACHED
            
        except Exception as e:
            self.logger.warning(f"ìºì‹± ì‹¤íŒ¨: {e}")
    
    async def _load_cached_result(self, chunk: FileChunk) -> Optional[Dict]:
        """ìºì‹œëœ ê²°ê³¼ ë¡œë“œ"""
        cache_path = self.chunker.temp_dir / f"{chunk.chunk_id}_result.pkl.gz"
        
        if not os.path.exists(cache_path):
            return None
        
        try:
            with gzip.open(cache_path, 'rb') as f:
                return pickle.load(f)
        except Exception as e:
            self.logger.warning(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    async def process_file_streaming(
        self, 
        file_path: str, 
        file_id: str, 
        processor_func: Callable,
        progress_callback: Optional[Callable] = None
    ) -> Dict[str, Any]:
        """ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ íŒŒì¼ ì²˜ë¦¬"""
        
        start_time = time.time()
        
        try:
            # íŒŒì¼ ì²­í‚¹
            chunks = self.chunker.create_chunks(file_path, file_id)
            self.logger.info(f"íŒŒì¼ ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬")
            
            # ì²­í¬ë³„ ì²˜ë¦¬
            processed_chunks = []
            total_chunks = len(chunks)
            
            # ì„¸ë§ˆí¬ì–´ë¥¼ ì‚¬ìš©í•œ ë™ì‹œ ì²˜ë¦¬ ì œí•œ
            semaphore = asyncio.Semaphore(self.config.max_concurrent_chunks)
            
            async def process_chunk_with_semaphore(chunk):
                async with semaphore:
                    return await self._process_single_chunk(
                        file_path, chunk, processor_func
                    )
            
            # ì²­í¬ë“¤ì„ ë°°ì¹˜ë¡œ ì²˜ë¦¬
            tasks = [process_chunk_with_semaphore(chunk) for chunk in chunks]
            
            for i, task in enumerate(asyncio.as_completed(tasks)):
                try:
                    result_chunk = await task
                    processed_chunks.append(result_chunk)
                    
                    # ì§„í–‰ë¥  ì½œë°±
                    if progress_callback:
                        progress = (i + 1) / total_chunks * 100
                        await progress_callback(progress, f"ì²­í¬ {i+1}/{total_chunks} ì™„ë£Œ")
                    
                    # ë©”ëª¨ë¦¬ í™•ì¸ ë° ì •ë¦¬
                    await self._check_memory_and_cleanup()
                    
                except Exception as e:
                    self.error_count += 1
                    self.logger.error(f"ì²­í¬ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            
            # ê²°ê³¼ í†µí•©
            final_result = await self._merge_chunk_results(processed_chunks)
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            processing_time = time.time() - start_time
            self.total_processing_time += processing_time
            self.total_processed_mb += sum(c.size_mb for c in chunks)
            
            # ì •ë¦¬
            self.chunker.cleanup_chunks(chunks)
            
            return {
                "success": True,
                "file_id": file_id,
                "chunks_processed": len(processed_chunks),
                "total_chunks": total_chunks,
                "processing_time": processing_time,
                "total_size_mb": sum(c.size_mb for c in chunks),
                "average_chunk_time": processing_time / total_chunks,
                "memory_peak_mb": self.memory_monitor.peak_memory,
                "result": final_result
            }
            
        except Exception as e:
            self.logger.error(f"ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "file_id": file_id
            }
    
    async def _process_single_chunk(
        self, 
        file_path: str, 
        chunk: FileChunk, 
        processor_func: Callable
    ) -> FileChunk:
        """ë‹¨ì¼ ì²­í¬ ì²˜ë¦¬"""
        
        start_time = time.time()
        chunk.status = ChunkStatus.PROCESSING
        self.active_chunks[chunk.chunk_id] = chunk
        
        try:
            # ì²­í¬ ë°ì´í„° ì½ê¸°
            chunk_data = await self.chunker.read_chunk(file_path, chunk)
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬
            current_memory = self.memory_monitor.get_current_stats()
            if current_memory.percent > self.config.memory_threshold * 100:
                # ë©”ëª¨ë¦¬ ë¶€ì¡±ì‹œ ì••ì¶• ì €ì¥
                await self.chunker.compress_chunk(chunk_data, chunk)
                chunk_data = None  # ë©”ëª¨ë¦¬ í•´ì œ
                gc.collect()
            
            # ì²˜ë¦¬ í•¨ìˆ˜ ì‹¤í–‰
            if chunk_data:
                result = await processor_func(chunk_data, chunk)
            else:
                # ì••ì¶•ëœ ë°ì´í„°ë¡œë¶€í„° ì²˜ë¦¬
                chunk_data = await self.chunker.decompress_chunk(chunk)
                result = await processor_func(chunk_data, chunk)
                chunk_data = None  # ì¦‰ì‹œ ë©”ëª¨ë¦¬ í•´ì œ
            
            # ê²°ê³¼ ì €ì¥
            chunk.result_data = result
            chunk.status = ChunkStatus.COMPLETED
            chunk.processing_time = time.time() - start_time
            
            # í™œì„± ì²­í¬ì—ì„œ ì™„ë£Œ ì²­í¬ë¡œ ì´ë™
            del self.active_chunks[chunk.chunk_id]
            self.completed_chunks[chunk.chunk_id] = chunk
            
            return chunk
            
        except Exception as e:
            chunk.status = ChunkStatus.FAILED
            chunk.processing_time = time.time() - start_time
            self.logger.error(f"ì²­í¬ {chunk.chunk_id} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            
            if chunk.chunk_id in self.active_chunks:
                del self.active_chunks[chunk.chunk_id]
            
            raise e
    
    async def _check_memory_and_cleanup(self):
        """ë©”ëª¨ë¦¬ í™•ì¸ ë° í•„ìš”ì‹œ ì •ë¦¬"""
        current_stats = self.memory_monitor.get_current_stats()
        
        if current_stats.percent > self.config.memory_threshold * 100:
            self.logger.warning(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ë†’ìŒ: {current_stats.percent:.1f}%")
            await self._force_cleanup()
    
    async def _merge_chunk_results(self, chunks: List[FileChunk]) -> Dict[str, Any]:
        """ì²­í¬ ê²°ê³¼ í†µí•©"""
        
        merged_content = ""
        total_confidence = 0.0
        chunk_count = 0
        all_keywords = []
        processing_times = []
        
        for chunk in chunks:
            if chunk.status != ChunkStatus.COMPLETED:
                continue
            
            # ìºì‹œëœ ê²°ê³¼ ë¡œë“œ
            result_data = chunk.result_data
            if not result_data and chunk.status == ChunkStatus.CACHED:
                result_data = await self._load_cached_result(chunk)
            
            if result_data:
                merged_content += result_data.get("content", "")
                total_confidence += result_data.get("confidence", 0.0)
                all_keywords.extend(result_data.get("keywords", []))
                processing_times.append(chunk.processing_time)
                chunk_count += 1
        
        # í†µí•© ì§€í‘œ ê³„ì‚°
        avg_confidence = total_confidence / max(chunk_count, 1)
        unique_keywords = list(set(all_keywords))
        total_processing_time = sum(processing_times)
        
        return {
            "merged_content": merged_content,
            "average_confidence": avg_confidence,
            "unique_keywords": unique_keywords,
            "total_keywords": len(all_keywords),
            "chunks_merged": chunk_count,
            "total_processing_time": total_processing_time,
            "average_chunk_time": total_processing_time / max(chunk_count, 1)
        }
    
    def get_processing_stats(self) -> Dict[str, Any]:
        """ì²˜ë¦¬ í†µê³„ ë°˜í™˜"""
        memory_report = self.memory_monitor.get_memory_report()
        
        return {
            "memory": memory_report,
            "processing": {
                "total_processed_mb": self.total_processed_mb,
                "total_processing_time": self.total_processing_time,
                "average_speed_mbps": self.total_processed_mb / max(self.total_processing_time, 1),
                "error_count": self.error_count,
                "active_chunks": len(self.active_chunks),
                "completed_chunks": len(self.completed_chunks)
            },
            "configuration": {
                "chunk_size_mb": self.config.chunk_size_mb,
                "max_memory_mb": self.config.max_memory_mb,
                "compression_enabled": self.config.compression_enabled,
                "max_concurrent_chunks": self.config.max_concurrent_chunks
            }
        }

# ì‚¬ìš© ì˜ˆì‹œ: ì£¼ì–¼ë¦¬ STT ì²˜ë¦¬ë¥¼ ìœ„í•œ ìŠ¤íŠ¸ë¦¬ë° í”„ë¡œì„¸ì„œ
async def jewelry_stt_chunk_processor(chunk_data: bytes, chunk: FileChunk) -> Dict[str, Any]:
    """ì£¼ì–¼ë¦¬ STTìš© ì²­í¬ ì²˜ë¦¬ í•¨ìˆ˜"""
    
    # ì„ì‹œ íŒŒì¼ ìƒì„±
    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.wav')
    temp_file.write(chunk_data)
    temp_file.close()
    
    try:
        # ì‹¤ì œë¡œëŠ” Whisper STT í˜¸ì¶œ
        # ì—¬ê¸°ì„œëŠ” ëª¨ì˜ ì²˜ë¦¬
        await asyncio.sleep(0.1)  # STT ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜
        
        # ëª¨ì˜ STT ê²°ê³¼
        content = f"ë‹¤ì´ì•„ëª¬ë“œ 4C ë“±ê¸‰ì— ëŒ€í•´ ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤. (ì²­í¬ {chunk.chunk_id})"
        
        # ì£¼ì–¼ë¦¬ í‚¤ì›Œë“œ ì¶”ì¶œ
        jewelry_keywords = ["ë‹¤ì´ì•„ëª¬ë“œ", "4C", "ë“±ê¸‰", "ìºëŸ¿", "ì»¬ëŸ¬"]
        
        return {
            "content": content,
            "confidence": 0.95,
            "keywords": jewelry_keywords,
            "chunk_info": {
                "chunk_id": chunk.chunk_id,
                "size_mb": chunk.size_mb,
                "hash": chunk.content_hash
            }
        }
        
    finally:
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        os.unlink(temp_file.name)

# ë°ëª¨ í•¨ìˆ˜
async def demo_streaming_processing():
    """ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ë°ëª¨"""
    
    # ì„¤ì •
    config = StreamingConfig(
        chunk_size_mb=10.0,  # ì‘ì€ ì²­í¬ë¡œ í…ŒìŠ¤íŠ¸
        max_memory_mb=256.0,
        compression_enabled=True,
        max_concurrent_chunks=2
    )
    
    # ìŠ¤íŠ¸ë¦¬ë° í”„ë¡œì„¸ì„œ ìƒì„±
    processor = StreamingProcessor(config)
    await processor.start_monitoring()
    
    print("ğŸš€ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì—”ì§„ ë°ëª¨ ì‹œì‘")
    print("=" * 60)
    
    # ì§„í–‰ë¥  ì½œë°±
    async def progress_callback(progress: float, message: str):
        print(f"ğŸ“Š ì§„í–‰ë¥ : {progress:.1f}% - {message}")
    
    try:
        # ëŒ€í˜• íŒŒì¼ ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜
        # ì‹¤ì œë¡œëŠ” ì‚¬ìš©ì ì—…ë¡œë“œ íŒŒì¼ ê²½ë¡œ
        test_file = "test_large_audio.wav"
        
        # ëª¨ì˜ íŒŒì¼ ìƒì„± (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì‹¤ì œ íŒŒì¼ ì‚¬ìš©)
        if not os.path.exists(test_file):
            print("ğŸ“ í…ŒìŠ¤íŠ¸ íŒŒì¼ ìƒì„± ì¤‘...")
            with open(test_file, 'wb') as f:
                # 100MB ëª¨ì˜ ì˜¤ë””ì˜¤ ë°ì´í„°
                f.write(b'0' * (100 * 1024 * 1024))
        
        # ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì‹¤í–‰
        result = await processor.process_file_streaming(
            file_path=test_file,
            file_id="test_file_001",
            processor_func=jewelry_stt_chunk_processor,
            progress_callback=progress_callback
        )
        
        # ê²°ê³¼ ì¶œë ¥
        print("\n" + "=" * 60)
        print("ğŸ‰ ì²˜ë¦¬ ì™„ë£Œ!")
        print("=" * 60)
        
        if result["success"]:
            print(f"âœ… íŒŒì¼ ID: {result['file_id']}")
            print(f"ğŸ“Š ì²˜ë¦¬ëœ ì²­í¬: {result['chunks_processed']}/{result['total_chunks']}")
            print(f"â±ï¸ ì²˜ë¦¬ ì‹œê°„: {result['processing_time']:.2f}ì´ˆ")
            print(f"ğŸ“¦ íŒŒì¼ í¬ê¸°: {result['total_size_mb']:.1f}MB")
            print(f"âš¡ í‰ê·  ì†ë„: {result['total_size_mb']/result['processing_time']:.1f}MB/s")
            print(f"ğŸ§  ë©”ëª¨ë¦¬ í”¼í¬: {result['memory_peak_mb']:.1f}MB")
            
            # ê²°ê³¼ ìƒì„¸
            final_result = result['result']
            print(f"\nğŸ“ í†µí•© ê²°ê³¼:")
            print(f"   ì‹ ë¢°ë„: {final_result['average_confidence']:.1%}")
            print(f"   í‚¤ì›Œë“œ ìˆ˜: {final_result['total_keywords']}ê°œ")
            print(f"   ê³ ìœ  í‚¤ì›Œë“œ: {len(final_result['unique_keywords'])}ê°œ")
            
        else:
            print(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {result['error']}")
        
        # ì²˜ë¦¬ í†µê³„
        stats = processor.get_processing_stats()
        print(f"\nğŸ“ˆ ì„±ëŠ¥ í†µê³„:")
        print(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : {stats['memory']['current_percent']:.1f}%")
        print(f"   ë©”ëª¨ë¦¬ í”¼í¬: {stats['memory']['peak_mb']:.1f}MB")
        print(f"   í‰ê·  ì²˜ë¦¬ ì†ë„: {stats['processing']['average_speed_mbps']:.1f}MB/s")
        print(f"   ì˜¤ë¥˜ íšŸìˆ˜: {stats['processing']['error_count']}")
        
        # ë©”ëª¨ë¦¬ íŠ¸ë Œë“œ
        if stats['memory']['trend'] == 'increasing':
            print("âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€ ì¶”ì„¸")
        else:
            print("âœ… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì•ˆì •")
    
    finally:
        await processor.stop_monitoring()
        
        # í…ŒìŠ¤íŠ¸ íŒŒì¼ ì •ë¦¬
        if os.path.exists(test_file):
            os.remove(test_file)
        
        print("\nğŸ§¹ ì •ë¦¬ ì™„ë£Œ")

if __name__ == "__main__":
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(level=logging.INFO)
    
    # ë°ëª¨ ì‹¤í–‰
    asyncio.run(demo_streaming_processing())
