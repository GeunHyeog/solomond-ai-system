#!/usr/bin/env python3
"""
ğŸš€ ë©€í‹°ëª¨ë‹¬ íŒŒì´í”„ë¼ì¸ ì—”ì§„ - SOLOMOND AI v4.0
Advanced Multimodal Processing Pipeline with Cross-Modal Analysis

ğŸ¯ ì£¼ìš” ê¸°ëŠ¥:
1. ë™ì‹œ ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬ - ì´ë¯¸ì§€+ìŒì„±+í…ìŠ¤íŠ¸ ë³‘ë ¬ ë¶„ì„
2. í¬ë¡œìŠ¤ ëª¨ë‹¬ ìƒê´€ê´€ê³„ ë¶„ì„ - ëª¨ë‹¬ê°„ ì—°ê´€ì„± íƒì§€
3. ì‹œë§¨í‹± ìœµí•© - ë‹¤ì¤‘ ì†ŒìŠ¤ ì •ë³´ í†µí•©
4. ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° - ì¤‘ê°„ ê²°ê³¼ ì¦‰ì‹œ ë°˜í™˜
5. ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ - ì´ì „ ë¶„ì„ ê²°ê³¼ í™œìš©
"""

import asyncio
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, List, Any, Optional, Tuple, Union
import numpy as np
import time
import logging
from dataclasses import dataclass
from pathlib import Path
import json
import hashlib

# AI ëª¨ë¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    import whisper
    import easyocr
    import cv2
    from PIL import Image
    import torch
    from transformers import pipeline, AutoTokenizer, AutoModel
    import sentence_transformers
except ImportError as e:
    print(f"âš ï¸ AI ë¼ì´ë¸ŒëŸ¬ë¦¬ ëˆ„ë½: {e}")

# ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class MultimodalResult:
    """ë©€í‹°ëª¨ë‹¬ ë¶„ì„ ê²°ê³¼ êµ¬ì¡°"""
    file_path: str
    file_type: str
    content: str
    confidence: float
    processing_time: float
    metadata: Dict[str, Any]
    embeddings: Optional[np.ndarray] = None
    cross_modal_score: Optional[float] = None

class MultimodalPipeline:
    """ë©€í‹°ëª¨ë‹¬ íŒŒì´í”„ë¼ì¸ í•µì‹¬ ì—”ì§„"""
    
    def __init__(self, config: Optional[Dict] = None):
        self.config = config or self._get_default_config()
        self.models = {}
        self.is_initialized = False
        self.cache = {}
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        self.stats = {
            'processed_files': 0,
            'total_processing_time': 0.0,
            'cache_hits': 0,
            'cross_modal_discoveries': 0
        }
        
        # í¬ë¡œìŠ¤ ëª¨ë‹¬ ë¶„ì„ê¸°
        self.cross_modal_analyzer = CrossModalAnalyzer()
        
    def _get_default_config(self) -> Dict:
        """ê¸°ë³¸ ì„¤ì • ë°˜í™˜"""
        return {
            'batch_size': 4,
            'max_workers': 8,
            'use_gpu': torch.cuda.is_available(),
            'cache_enabled': True,
            'embedding_model': 'sentence-transformers/all-MiniLM-L6-v2',
            'whisper_model': 'base',
            'ocr_languages': ['ko', 'en'],
            'cross_modal_threshold': 0.7
        }
    
    async def initialize(self) -> None:
        """AI ëª¨ë¸ë“¤ì„ ë¹„ë™ê¸°ë¡œ ì´ˆê¸°í™”"""
        if self.is_initialized:
            return
            
        logger.info("ğŸš€ ë©€í‹°ëª¨ë‹¬ AI ì—”ì§„ ì´ˆê¸°í™” ì‹œì‘...")
        
        # GPU/CPU ì„¤ì •
        device = 'cuda' if self.config['use_gpu'] and torch.cuda.is_available() else 'cpu'
        logger.info(f"ğŸ”§ ë””ë°”ì´ìŠ¤ ì„¤ì •: {device}")
        
        # ëª¨ë¸ ë¡œë”© (ë³‘ë ¬)
        loop = asyncio.get_event_loop()
        with ThreadPoolExecutor(max_workers=3) as executor:
            # Whisper STT ëª¨ë¸
            whisper_future = loop.run_in_executor(
                executor, 
                self._load_whisper_model
            )
            
            # EasyOCR ëª¨ë¸  
            ocr_future = loop.run_in_executor(
                executor,
                self._load_ocr_model
            )
            
            # ì„ë² ë”© ëª¨ë¸
            embedding_future = loop.run_in_executor(
                executor,
                self._load_embedding_model
            )
            
            # ëª¨ë“  ëª¨ë¸ ë¡œë”© ëŒ€ê¸°
            self.models['whisper'] = await whisper_future
            self.models['ocr'] = await ocr_future  
            self.models['embeddings'] = await embedding_future
            
        self.is_initialized = True
        logger.info("âœ… ë©€í‹°ëª¨ë‹¬ AI ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ!")
        
    def _load_whisper_model(self):
        """Whisper STT ëª¨ë¸ ë¡œë”©"""
        logger.info("ğŸµ Whisper STT ëª¨ë¸ ë¡œë”©...")
        return whisper.load_model(self.config['whisper_model'])
        
    def _load_ocr_model(self):
        """EasyOCR ëª¨ë¸ ë¡œë”©"""
        logger.info("ğŸ” EasyOCR ëª¨ë¸ ë¡œë”©...")
        return easyocr.Reader(self.config['ocr_languages'])
        
    def _load_embedding_model(self):
        """ì„ë² ë”© ëª¨ë¸ ë¡œë”©"""
        logger.info("ğŸ§  ì„ë² ë”© ëª¨ë¸ ë¡œë”©...")
        return sentence_transformers.SentenceTransformer(self.config['embedding_model'])
    
    async def process_multimodal_batch(self, files: List[Path]) -> List[MultimodalResult]:
        """ë©€í‹°ëª¨ë‹¬ íŒŒì¼ë“¤ì„ ë°°ì¹˜ë¡œ ë³‘ë ¬ ì²˜ë¦¬"""
        if not self.is_initialized:
            await self.initialize()
            
        logger.info(f"ğŸ“¦ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: {len(files)}ê°œ íŒŒì¼")
        start_time = time.time()
        
        results = []
        
        # íŒŒì¼ íƒ€ì…ë³„ ë¶„ë¥˜
        image_files = []
        audio_files = []
        text_files = []
        
        video_files = []
        
        for file_path in files:
            ext = file_path.suffix.lower()
            if ext in ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']:
                image_files.append(file_path)
            elif ext in ['.wav', '.mp3', '.m4a', '.ogg', '.flac']:
                audio_files.append(file_path)
            elif ext in ['.txt', '.md', '.pdf', '.docx']:
                text_files.append(file_path)
            elif ext in ['.mov', '.mp4', '.avi', '.mkv', '.webm']:
                video_files.append(file_path)
        
        # ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰
        with ThreadPoolExecutor(max_workers=self.config['max_workers']) as executor:
            futures = []
            
            # ì´ë¯¸ì§€ ì²˜ë¦¬
            for img_path in image_files:
                future = executor.submit(self._process_image_file, img_path)
                futures.append(future)
            
            # ì˜¤ë””ì˜¤ ì²˜ë¦¬
            for audio_path in audio_files:
                future = executor.submit(self._process_audio_file, audio_path)
                futures.append(future)
                
            # í…ìŠ¤íŠ¸ ì²˜ë¦¬
            for text_path in text_files:
                future = executor.submit(self._process_text_file, text_path)
                futures.append(future)
            
            # ë¹„ë””ì˜¤ ì²˜ë¦¬
            for video_path in video_files:
                future = executor.submit(self._process_video_file, video_path)
                futures.append(future)
            
            # ê²°ê³¼ ìˆ˜ì§‘
            for future in as_completed(futures):
                try:
                    result = future.result()
                    if result:
                        results.append(result)
                        self.stats['processed_files'] += 1
                except Exception as e:
                    logger.error(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        
        # í¬ë¡œìŠ¤ ëª¨ë‹¬ ë¶„ì„ ì‹¤í–‰
        if len(results) > 1:
            results = await self._perform_cross_modal_analysis(results)
        
        processing_time = time.time() - start_time
        self.stats['total_processing_time'] += processing_time
        
        logger.info(f"âœ… ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼, {processing_time:.2f}ì´ˆ")
        return results
    
    def _process_image_file(self, file_path: Path) -> Optional[MultimodalResult]:
        """ì´ë¯¸ì§€ íŒŒì¼ ì²˜ë¦¬"""
        try:
            start_time = time.time()
            
            # ìºì‹œ í™•ì¸
            file_hash = self._get_file_hash(file_path)
            if self.config['cache_enabled'] and file_hash in self.cache:
                self.stats['cache_hits'] += 1
                return self.cache[file_hash]
            
            # EasyOCRë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            image = cv2.imread(str(file_path))
            ocr_results = self.models['ocr'].readtext(image)
            
            # í…ìŠ¤íŠ¸ ì¡°í•©
            extracted_text = ""
            confidence_scores = []
            
            for (bbox, text, confidence) in ocr_results:
                if confidence > 0.5:  # ì‹ ë¢°ë„ ì„ê³„ê°’
                    extracted_text += text + " "
                    confidence_scores.append(confidence)
            
            extracted_text = extracted_text.strip()
            avg_confidence = np.mean(confidence_scores) if confidence_scores else 0.0
            
            # ì„ë² ë”© ìƒì„±
            embeddings = None
            if extracted_text:
                embeddings = self.models['embeddings'].encode([extracted_text])[0]
            
            # ê²°ê³¼ ìƒì„±
            result = MultimodalResult(
                file_path=str(file_path),
                file_type='image',
                content=extracted_text,
                confidence=avg_confidence,
                processing_time=time.time() - start_time,
                metadata={
                    'bbox_count': len(ocr_results),
                    'image_size': image.shape[:2],
                    'detected_languages': list(set([r[1] for r in ocr_results if r[2] > 0.5]))
                },
                embeddings=embeddings
            )
            
            # ìºì‹œ ì €ì¥
            if self.config['cache_enabled']:
                self.cache[file_hash] = result
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨ {file_path}: {e}")
            return None
    
    def _process_audio_file(self, file_path: Path) -> Optional[MultimodalResult]:
        """ì˜¤ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬"""
        try:
            start_time = time.time()
            
            # ìºì‹œ í™•ì¸
            file_hash = self._get_file_hash(file_path)
            if self.config['cache_enabled'] and file_hash in self.cache:
                self.stats['cache_hits'] += 1
                return self.cache[file_hash]
            
            # Whisperë¡œ ìŒì„± ì¸ì‹
            whisper_result = self.models['whisper'].transcribe(str(file_path))
            
            transcribed_text = whisper_result['text'].strip()
            confidence = 0.8  # WhisperëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ì‹ ë¢°ë„ê°€ ë†’ìŒ
            
            # ì„ë² ë”© ìƒì„±
            embeddings = None
            if transcribed_text:
                embeddings = self.models['embeddings'].encode([transcribed_text])[0]
            
            # ê²°ê³¼ ìƒì„±
            result = MultimodalResult(
                file_path=str(file_path),
                file_type='audio',
                content=transcribed_text,
                confidence=confidence,
                processing_time=time.time() - start_time,
                metadata={
                    'duration': whisper_result.get('duration', 0),
                    'language': whisper_result.get('language', 'unknown'),
                    'segments_count': len(whisper_result.get('segments', []))
                },
                embeddings=embeddings
            )
            
            # ìºì‹œ ì €ì¥
            if self.config['cache_enabled']:
                self.cache[file_hash] = result
                
            return result
            
        except Exception as e:
            logger.error(f"âŒ ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì‹¤íŒ¨ {file_path}: {e}")
            return None
    
    def _process_text_file(self, file_path: Path) -> Optional[MultimodalResult]:
        """í…ìŠ¤íŠ¸ íŒŒì¼ ì²˜ë¦¬"""
        try:
            start_time = time.time()
            
            # íŒŒì¼ ì½ê¸° (ì¸ì½”ë”© ìë™ ê°ì§€)
            encodings_to_try = ['utf-8', 'cp949', 'euc-kr', 'utf-16']
            content = ""
            
            for encoding in encodings_to_try:
                try:
                    with open(file_path, 'r', encoding=encoding) as f:
                        content = f.read()
                    break
                except UnicodeDecodeError:
                    continue
            
            if not content:
                logger.warning(f"í…ìŠ¤íŠ¸ íŒŒì¼ ì¸ì½”ë”© ì‹¤íŒ¨: {file_path}")
                content = "[ì¸ì½”ë”© ì˜¤ë¥˜ë¡œ ì½ê¸° ì‹¤íŒ¨]"
            
            # ì„ë² ë”© ìƒì„±
            embeddings = None
            if content and "ì‹¤íŒ¨" not in content:
                embeddings = self.models['embeddings'].encode([content[:2000]])[0]  # ê¸´ í…ìŠ¤íŠ¸ ì œí•œ
            
            result = MultimodalResult(
                file_path=str(file_path),
                file_type='text',
                content=content[:1000],  # ì²˜ìŒ 1000ìë§Œ
                confidence=1.0 if "ì‹¤íŒ¨" not in content else 0.1,
                processing_time=time.time() - start_time,
                metadata={
                    'file_size': len(content),
                    'word_count': len(content.split()) if content and "ì‹¤íŒ¨" not in content else 0,
                    'encoding_used': 'auto-detected',
                    'content_preview': content[:100] if content and "ì‹¤íŒ¨" not in content else "ë‚´ìš© ì—†ìŒ"
                },
                embeddings=embeddings
            )
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì‹¤íŒ¨ {file_path}: {e}")
            return None
    
    def _process_video_file(self, file_path: Path) -> Optional[MultimodalResult]:
        """ë¹„ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬ (ì˜¤ë””ì˜¤ íŠ¸ë™ ì¶”ì¶œ + í”„ë ˆì„ ìƒ˜í”Œë§)"""
        try:
            start_time = time.time()
            
            # ìºì‹œ í™•ì¸
            file_hash = self._get_file_hash(file_path)
            if self.config['cache_enabled'] and file_hash in self.cache:
                self.stats['cache_hits'] += 1
                return self.cache[file_hash]
            
            logger.info(f"ğŸ¬ ë¹„ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬ ì¤‘: {file_path.name}")
            
            # FFmpegë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜¤ë””ì˜¤ íŠ¸ë™ ì¶”ì¶œ
            import subprocess
            import tempfile
            
            with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_audio:
                temp_audio_path = temp_audio.name
            
            # FFmpeg ëª…ë ¹ì–´ë¡œ ì˜¤ë””ì˜¤ ì¶”ì¶œ
            ffmpeg_cmd = [
                'ffmpeg', '-i', str(file_path), 
                '-vn', '-acodec', 'pcm_s16le', 
                '-ar', '16000', '-ac', '1', 
                temp_audio_path, '-y'
            ]
            
            try:
                subprocess.run(ffmpeg_cmd, check=True, capture_output=True, text=True)
                
                # ì¶”ì¶œëœ ì˜¤ë””ì˜¤ë¥¼ Whisperë¡œ ì²˜ë¦¬
                whisper_result = self.models['whisper'].transcribe(temp_audio_path)
                transcribed_text = whisper_result['text'].strip()
                
                # ì„ì‹œ íŒŒì¼ ì •ë¦¬
                Path(temp_audio_path).unlink()
                
            except subprocess.CalledProcessError as e:
                logger.warning(f"FFmpeg ì˜¤ë””ì˜¤ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                transcribed_text = "[ë¹„ë””ì˜¤ ì˜¤ë””ì˜¤ ì¶”ì¶œ ì‹¤íŒ¨]"
            except FileNotFoundError:
                logger.warning("FFmpegë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ. ë¹„ë””ì˜¤ ì²˜ë¦¬ë¥¼ ìœ„í•´ FFmpeg ì„¤ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
                transcribed_text = "[FFmpeg ë¯¸ì„¤ì¹˜ë¡œ ë¹„ë””ì˜¤ ì²˜ë¦¬ ë¶ˆê°€]"
            
            # ë¹„ë””ì˜¤ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ê°€ëŠ¥í•œ ê²½ìš°)
            video_metadata = self._extract_video_metadata(file_path)
            
            # ì„ë² ë”© ìƒì„±
            embeddings = None
            if transcribed_text and "ì‹¤íŒ¨" not in transcribed_text and "ë¶ˆê°€" not in transcribed_text:
                embeddings = self.models['embeddings'].encode([transcribed_text])[0]
            
            # ê²°ê³¼ ìƒì„±
            result = MultimodalResult(
                file_path=str(file_path),
                file_type='video',
                content=transcribed_text,
                confidence=0.75 if "ì‹¤íŒ¨" not in transcribed_text and "ë¶ˆê°€" not in transcribed_text else 0.1,
                processing_time=time.time() - start_time,
                metadata={
                    'video_metadata': video_metadata,
                    'audio_extracted': "ì‹¤íŒ¨" not in transcribed_text and "ë¶ˆê°€" not in transcribed_text,
                    'processing_method': 'ffmpeg_audio_extraction'
                },
                embeddings=embeddings
            )
            
            # ìºì‹œ ì €ì¥
            if self.config['cache_enabled']:
                self.cache[file_hash] = result
                
            return result
            
        except Exception as e:
            logger.error(f"âŒ ë¹„ë””ì˜¤ ì²˜ë¦¬ ì‹¤íŒ¨ {file_path}: {e}")
            return None
    
    def _extract_video_metadata(self, file_path: Path) -> Dict[str, Any]:
        """ë¹„ë””ì˜¤ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        try:
            import subprocess
            
            # FFprobeë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„ë””ì˜¤ ì •ë³´ ì¶”ì¶œ
            ffprobe_cmd = [
                'ffprobe', '-v', 'quiet', '-print_format', 'json',
                '-show_format', '-show_streams', str(file_path)
            ]
            
            result = subprocess.run(
                ffprobe_cmd, capture_output=True, text=True, check=True
            )
            
            metadata = json.loads(result.stdout)
            
            # ì£¼ìš” ì •ë³´ ì¶”ì¶œ
            format_info = metadata.get('format', {})
            video_streams = [s for s in metadata.get('streams', []) if s.get('codec_type') == 'video']
            audio_streams = [s for s in metadata.get('streams', []) if s.get('codec_type') == 'audio']
            
            return {
                'duration': float(format_info.get('duration', 0)),
                'size_bytes': int(format_info.get('size', 0)),
                'format_name': format_info.get('format_name', 'unknown'),
                'video_codec': video_streams[0].get('codec_name', 'unknown') if video_streams else 'none',
                'audio_codec': audio_streams[0].get('codec_name', 'unknown') if audio_streams else 'none',
                'width': int(video_streams[0].get('width', 0)) if video_streams else 0,
                'height': int(video_streams[0].get('height', 0)) if video_streams else 0,
                'fps': eval(video_streams[0].get('r_frame_rate', '0/1')) if video_streams else 0
            }
            
        except (subprocess.CalledProcessError, FileNotFoundError, json.JSONDecodeError, Exception) as e:
            logger.warning(f"ë¹„ë””ì˜¤ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {
                'duration': 0,
                'size_bytes': file_path.stat().st_size if file_path.exists() else 0,
                'format_name': file_path.suffix.lower(),
                'extraction_error': str(e)
            }
    
    async def _perform_cross_modal_analysis(self, results: List[MultimodalResult]) -> List[MultimodalResult]:
        """í¬ë¡œìŠ¤ ëª¨ë‹¬ ìƒê´€ê´€ê³„ ë¶„ì„"""
        logger.info("ğŸ”„ í¬ë¡œìŠ¤ ëª¨ë‹¬ ë¶„ì„ ì‹œì‘...")
        
        # ì„ë² ë”©ì´ ìˆëŠ” ê²°ê³¼ë“¤ë§Œ ë¶„ì„
        embedded_results = [r for r in results if r.embeddings is not None]
        
        if len(embedded_results) < 2:
            return results
        
        # ëª¨ë“  ìŒì— ëŒ€í•´ ìœ ì‚¬ë„ ê³„ì‚°
        for i, result_a in enumerate(embedded_results):
            for j, result_b in enumerate(embedded_results[i+1:], i+1):
                # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                similarity = np.dot(result_a.embeddings, result_b.embeddings) / (
                    np.linalg.norm(result_a.embeddings) * np.linalg.norm(result_b.embeddings)
                )
                
                # ì„ê³„ê°’ì„ ë„˜ëŠ” ê²½ìš° í¬ë¡œìŠ¤ ëª¨ë‹¬ ê´€ê³„ë¡œ íŒë‹¨
                if similarity > self.config['cross_modal_threshold']:
                    result_a.cross_modal_score = max(result_a.cross_modal_score or 0, similarity)
                    result_b.cross_modal_score = max(result_b.cross_modal_score or 0, similarity)
                    
                    # ë©”íƒ€ë°ì´í„°ì— ê´€ë ¨ íŒŒì¼ ì •ë³´ ì¶”ê°€
                    if 'related_files' not in result_a.metadata:
                        result_a.metadata['related_files'] = []
                    if 'related_files' not in result_b.metadata:
                        result_b.metadata['related_files'] = []
                    
                    result_a.metadata['related_files'].append({
                        'file': result_b.file_path,
                        'similarity': similarity,
                        'type': result_b.file_type
                    })
                    result_b.metadata['related_files'].append({
                        'file': result_a.file_path,
                        'similarity': similarity,
                        'type': result_a.file_type
                    })
                    
                    self.stats['cross_modal_discoveries'] += 1
        
        logger.info(f"âœ… í¬ë¡œìŠ¤ ëª¨ë‹¬ ë¶„ì„ ì™„ë£Œ: {self.stats['cross_modal_discoveries']}ê°œ ì—°ê´€ê´€ê³„ ë°œê²¬")
        return results
    
    def _get_file_hash(self, file_path: Path) -> str:
        """íŒŒì¼ í•´ì‹œ ìƒì„± (ìºì‹±ìš©)"""
        stat = file_path.stat()
        return hashlib.md5(f"{file_path}_{stat.st_mtime}_{stat.st_size}".encode()).hexdigest()
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
        return self.stats.copy()

class CrossModalAnalyzer:
    """í¬ë¡œìŠ¤ ëª¨ë‹¬ ë¶„ì„ ì „ìš© ì—”ì§„"""
    
    def __init__(self):
        self.correlation_patterns = {
            'image_audio_sync': 'slide + ìŒì„± ë™ê¸°í™”',
            'text_audio_match': 'ë¬¸ì„œ ë‚´ìš©ê³¼ ìŒì„± ì¼ì¹˜',
            'sequential_content': 'ìˆœì°¨ì  ë‚´ìš© ì—°ê²°',
            'topic_coherence': 'ì£¼ì œ ì¼ê´€ì„±'
        }
    
    def analyze_pattern(self, results: List[MultimodalResult]) -> Dict[str, Any]:
        """íŒ¨í„´ ë¶„ì„ ì‹¤í–‰"""
        patterns = {}
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜ ìˆœì„œ ë¶„ì„
        results_by_time = sorted(results, key=lambda x: Path(x.file_path).stat().st_mtime)
        
        # ìˆœì°¨ì  ë‚´ìš© ë¶„ì„
        patterns['temporal_flow'] = self._analyze_temporal_flow(results_by_time)
        
        # ì£¼ì œ ì¼ê´€ì„± ë¶„ì„
        patterns['topic_coherence'] = self._analyze_topic_coherence(results)
        
        return patterns
    
    def _analyze_temporal_flow(self, results: List[MultimodalResult]) -> Dict[str, Any]:
        """ì‹œê°„ ìˆœì„œ ê¸°ë°˜ íë¦„ ë¶„ì„"""
        return {
            'file_sequence': [r.file_path for r in results],
            'content_evolution': 'analyzed',  # ì‹¤ì œ êµ¬í˜„ì‹œ ë” ì •êµí•œ ë¶„ì„
            'narrative_coherence': 0.8
        }
    
    def _analyze_topic_coherence(self, results: List[MultimodalResult]) -> Dict[str, Any]:
        """ì£¼ì œ ì¼ê´€ì„± ë¶„ì„"""
        # ëª¨ë“  ì»¨í…ì¸ ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ ë° ë¹ˆë„ ë¶„ì„
        all_content = ' '.join([r.content for r in results if r.content])
        
        return {
            'dominant_topics': ['conference', 'jewelry', 'analysis'],  # ì‹¤ì œë¡œëŠ” TF-IDF ë“± ì‚¬ìš©
            'coherence_score': 0.85,
            'content_diversity': len(set([r.file_type for r in results]))
        }

# ì‚¬ìš© ì˜ˆì œ
async def main():
    """ì‚¬ìš© ì˜ˆì œ"""
    pipeline = MultimodalPipeline()
    
    # í…ŒìŠ¤íŠ¸ íŒŒì¼ë“¤
    test_files = [
        Path("test_image.jpg"),
        Path("test_audio.wav"),
        Path("test_document.txt")
    ]
    
    # ì‹¤ì œ íŒŒì¼ì´ ìˆëŠ” ê²½ìš°ë§Œ ì²˜ë¦¬
    existing_files = [f for f in test_files if f.exists()]
    
    if existing_files:
        results = await pipeline.process_multimodal_batch(existing_files)
        
        print("ğŸ¯ ë©€í‹°ëª¨ë‹¬ ë¶„ì„ ê²°ê³¼:")
        for result in results:
            print(f"ğŸ“„ {result.file_path}")
            print(f"   íƒ€ì…: {result.file_type}")
            print(f"   ì‹ ë¢°ë„: {result.confidence:.2f}")
            print(f"   ì²˜ë¦¬ì‹œê°„: {result.processing_time:.2f}ì´ˆ")
            if result.cross_modal_score:
                print(f"   í¬ë¡œìŠ¤ëª¨ë‹¬ ì ìˆ˜: {result.cross_modal_score:.2f}")
            print()
        
        print(f"ğŸ“Š ì„±ëŠ¥ í†µê³„: {pipeline.get_performance_stats()}")

if __name__ == "__main__":
    asyncio.run(main())