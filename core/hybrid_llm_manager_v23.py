"""
ğŸ§  ì†”ë¡œëª¬ë“œ AI í•˜ì´ë¸Œë¦¬ë“œ LLM ë§¤ë‹ˆì € v2.3
GPT-4V + Claude Vision + Gemini 2.0 ë™ì‹œ í™œìš© ì‹œìŠ¤í…œ

ê°œë°œì: ì „ê·¼í˜ (ì†”ë¡œëª¬ë“œ ëŒ€í‘œ)
ëª©í‘œ: 99.2% ì •í™•ë„ ë‹¬ì„±
"""

import asyncio
import json
import time
from typing import Dict, List, Optional, Any, Union
from dataclasses import dataclass, asdict
from enum import Enum
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
import statistics

# AI ëª¨ë¸ í´ë¼ì´ì–¸íŠ¸ imports
try:
    import openai  # GPT-4V
    import anthropic  # Claude Vision
    import google.generativeai as genai  # Gemini 2.0
except ImportError as e:
    print(f"âš ï¸ AI ëª¨ë¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ import ì˜¤ë¥˜: {e}")
    print("ğŸ’¡ ì„¤ì¹˜ ëª…ë ¹ì–´: pip install openai anthropic google-generativeai")

# ì„¤ì • ë° ìœ í‹¸ë¦¬í‹°
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class AIModel(Enum):
    """ì§€ì›í•˜ëŠ” AI ëª¨ë¸ ì—´ê±°í˜•"""
    GPT4V = "gpt-4-vision-preview"
    CLAUDE_VISION = "claude-3-5-sonnet-20241022"
    GEMINI_2 = "gemini-2.0-flash-exp"

@dataclass
class AIResponse:
    """AI ëª¨ë¸ ì‘ë‹µ ë°ì´í„° í´ë˜ìŠ¤"""
    model: AIModel
    content: str
    confidence: float
    processing_time: float
    cost_estimate: float
    jewelry_relevance: float
    metadata: Dict[str, Any]

@dataclass
class AnalysisRequest:
    """ë¶„ì„ ìš”ì²­ ë°ì´í„° í´ë˜ìŠ¤"""
    text_content: Optional[str] = None
    image_data: Optional[bytes] = None
    image_url: Optional[str] = None
    analysis_type: str = "general"
    priority: int = 1
    require_jewelry_expertise: bool = True

class JewelryPromptOptimizer:
    """ì£¼ì–¼ë¦¬ íŠ¹í™” í”„ë¡¬í”„íŠ¸ ìµœì í™” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.jewelry_contexts = {
            "diamond_analysis": """
ë‹¹ì‹ ì€ GIA ê³µì¸ ë‹¤ì´ì•„ëª¬ë“œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ë‹¤ì´ì•„ëª¬ë“œë¥¼ 4C (Carat, Cut, Color, Clarity) ê¸°ì¤€ìœ¼ë¡œ ì •í™•íˆ ë¶„ì„í•´ì£¼ì„¸ìš”.
- Carat: ì •í™•í•œ ì¤‘ëŸ‰ ë˜ëŠ” ì˜ˆìƒ ì¤‘ëŸ‰
- Cut: Excellent, Very Good, Good, Fair, Poor ì¤‘ í•˜ë‚˜
- Color: D-Z ë“±ê¸‰ (D=ë¬´ìƒ‰, Z=ì—°í•œ ë…¸ë€ìƒ‰)
- Clarity: FL, IF, VVS1, VVS2, VS1, VS2, SI1, SI2, I1, I2, I3 ì¤‘ í•˜ë‚˜
ë¶„ì„ í›„ í•œêµ­ì–´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.
""",
            "colored_stone_analysis": """
ë‹¹ì‹ ì€ ìœ ìƒ‰ë³´ì„ ì „ë¬¸ ê°ì •ì‚¬ì…ë‹ˆë‹¤. ë‹¤ìŒ ë³´ì„ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.
- ë³´ì„ ì¢…ë¥˜: ë£¨ë¹„, ì‚¬íŒŒì´ì–´, ì—ë©”ë„ë“œ, ê¸°íƒ€
- ì‚°ì§€ ì¶”ì •: ë¯¸ì–€ë§ˆ, ìŠ¤ë¦¬ë‘ì¹´, ë§ˆë‹¤ê°€ìŠ¤ì¹´ë¥´, ì½œë¡¬ë¹„ì•„ ë“±
- ì²˜ë¦¬ ì—¬ë¶€: ê°€ì—´, ì˜¤ì¼ë§, ê¸°íƒ€ ì²˜ë¦¬
- í’ˆì§ˆ ë“±ê¸‰: AAA, AA, A, B, C
- ì˜ˆìƒ ê°€ì¹˜: ìºëŸ¿ë‹¹ ê°€ê²©ëŒ€
í•œêµ­ì–´ë¡œ ì „ë¬¸ì ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”.
""",
            "jewelry_design_analysis": """
ë‹¹ì‹ ì€ ì£¼ì–¼ë¦¬ ë””ìì¸ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì£¼ì–¼ë¦¬ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.
- ì£¼ì–¼ë¦¬ ì¢…ë¥˜: ë°˜ì§€, ëª©ê±¸ì´, ê·€ê±¸ì´, ë¸Œë¡œì¹˜ ë“±
- ë””ìì¸ ìŠ¤íƒ€ì¼: í´ë˜ì‹, ëª¨ë˜, ë¹ˆí‹°ì§€, ì•„ë¥´ë°ì½” ë“±
- ì„¸íŒ… ë°©ì‹: í”„ë¡±, ë² ì ¤, íŒŒë² , ì±„ë„ ë“±
- ê¸ˆì† ì¬ì§ˆ: í”Œë˜í‹°ë‚˜, 18K ê³¨ë“œ, 14K ê³¨ë“œ, ì‹¤ë²„ ë“±
- ì œì‘ ê¸°ë²•: í•¸ë“œë©”ì´ë“œ, ìºìŠ¤íŒ…, CNC ë“±
í•œêµ­ì–´ë¡œ ì „ë¬¸ì ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”.
""",
            "business_analysis": """
ë‹¹ì‹ ì€ ì£¼ì–¼ë¦¬ ë¹„ì¦ˆë‹ˆìŠ¤ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ë‚´ìš©ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.
- ì‹œì¥ íŠ¸ë Œë“œ ë¶„ì„
- ê°€ê²© ë™í–¥ ë° ì˜ˆì¸¡
- íˆ¬ì ê°€ì¹˜ í‰ê°€
- ìˆ˜ì§‘ ê°€ì¹˜ ë° í¬ì†Œì„±
- ì¬íŒë§¤ ì‹œì¥ ì „ë§
í•œêµ­ì–´ë¡œ ë¹„ì¦ˆë‹ˆìŠ¤ ê´€ì ì—ì„œ ë¶„ì„í•´ì£¼ì„¸ìš”.
"""
        }
    
    def optimize_prompt(self, analysis_type: str, base_content: str) -> str:
        """ë¶„ì„ íƒ€ì…ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ìµœì í™”"""
        context = self.jewelry_contexts.get(analysis_type, "")
        
        optimized_prompt = f"""
{context}

ë¶„ì„í•  ë‚´ìš©:
{base_content}

ë°˜ë“œì‹œ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:
1. ì „ë¬¸ ë¶„ì„ ê²°ê³¼
2. í•µì‹¬ íŠ¹ì§• ìš”ì•½
3. ì£¼ì˜ì‚¬í•­ ë˜ëŠ” ì¶”ê°€ ê²€í†  í•„ìš” ì‚¬í•­
4. í•œêµ­ì–´ ìµœì¢… ìš”ì•½ (ì£¼ì–¼ë¦¬ ì „ë¬¸ê°€ìš©)
"""
        return optimized_prompt

class HybridLLMManager:
    """í•˜ì´ë¸Œë¦¬ë“œ LLM ë§¤ë‹ˆì € v2.3 í•µì‹¬ í´ë˜ìŠ¤"""
    
    def __init__(self, config: Optional[Dict[str, str]] = None):
        """
        í•˜ì´ë¸Œë¦¬ë“œ LLM ë§¤ë‹ˆì € ì´ˆê¸°í™”
        
        Args:
            config: API í‚¤ ì„¤ì • ë”•ì…”ë„ˆë¦¬
                   {"openai_key": "...", "anthropic_key": "...", "google_key": "..."}
        """
        self.config = config or {}
        self.prompt_optimizer = JewelryPromptOptimizer()
        self.performance_metrics = {}
        self.cost_tracker = {"total": 0.0, "by_model": {}}
        
        # AI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self._initialize_clients()
        
        # ì„±ëŠ¥ ì¶”ì 
        self.response_history = []
        
    def _initialize_clients(self):
        """AI ëª¨ë¸ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        try:
            # OpenAI GPT-4V í´ë¼ì´ì–¸íŠ¸
            if "openai_key" in self.config:
                openai.api_key = self.config["openai_key"]
                self.openai_client = openai.OpenAI(api_key=self.config["openai_key"])
            else:
                self.openai_client = None
                logger.warning("âš ï¸ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            # Anthropic Claude í´ë¼ì´ì–¸íŠ¸
            if "anthropic_key" in self.config:
                self.anthropic_client = anthropic.Anthropic(api_key=self.config["anthropic_key"])
            else:
                self.anthropic_client = None
                logger.warning("âš ï¸ Anthropic API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            # Google Gemini í´ë¼ì´ì–¸íŠ¸
            if "google_key" in self.config:
                genai.configure(api_key=self.config["google_key"])
                self.gemini_client = genai.GenerativeModel('gemini-2.0-flash-exp')
            else:
                self.gemini_client = None
                logger.warning("âš ï¸ Google API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                
        except Exception as e:
            logger.error(f"âŒ AI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    async def analyze_with_gpt4v(self, request: AnalysisRequest) -> AIResponse:
        """GPT-4Vë¡œ ë¶„ì„ ìˆ˜í–‰"""
        start_time = time.time()
        
        try:
            if not self.openai_client:
                raise ValueError("OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            # í”„ë¡¬í”„íŠ¸ ìµœì í™”
            optimized_prompt = self.prompt_optimizer.optimize_prompt(
                request.analysis_type, 
                request.text_content or "ì´ë¯¸ì§€ ë¶„ì„ ìš”ì²­"
            )
            
            messages = [{"role": "user", "content": optimized_prompt}]
            
            # ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš° ì¶”ê°€
            if request.image_url or request.image_data:
                if request.image_url:
                    messages[0]["content"] = [
                        {"type": "text", "text": optimized_prompt},
                        {"type": "image_url", "image_url": {"url": request.image_url}}
                    ]
            
            response = await asyncio.to_thread(
                self.openai_client.chat.completions.create,
                model="gpt-4-vision-preview",
                messages=messages,
                max_tokens=1500,
                temperature=0.1
            )
            
            processing_time = time.time() - start_time
            content = response.choices[0].message.content
            
            # ì£¼ì–¼ë¦¬ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
            jewelry_relevance = self._calculate_jewelry_relevance(content)
            
            return AIResponse(
                model=AIModel.GPT4V,
                content=content,
                confidence=0.9,  # GPT-4V ê¸°ë³¸ ì‹ ë¢°ë„
                processing_time=processing_time,
                cost_estimate=self._estimate_cost("gpt4v", len(optimized_prompt), len(content)),
                jewelry_relevance=jewelry_relevance,
                metadata={"tokens_used": response.usage.total_tokens}
            )
            
        except Exception as e:
            logger.error(f"âŒ GPT-4V ë¶„ì„ ì‹¤íŒ¨: {e}")
            return self._create_error_response(AIModel.GPT4V, str(e), time.time() - start_time)
    
    async def analyze_with_claude(self, request: AnalysisRequest) -> AIResponse:
        """Claude Visionìœ¼ë¡œ ë¶„ì„ ìˆ˜í–‰"""
        start_time = time.time()
        
        try:
            if not self.anthropic_client:
                raise ValueError("Anthropic í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            # í”„ë¡¬í”„íŠ¸ ìµœì í™”
            optimized_prompt = self.prompt_optimizer.optimize_prompt(
                request.analysis_type, 
                request.text_content or "ì´ë¯¸ì§€ ë¶„ì„ ìš”ì²­"
            )
            
            message_content = [{"type": "text", "text": optimized_prompt}]
            
            # ì´ë¯¸ì§€ ì²˜ë¦¬ (Claudeì˜ ê²½ìš° base64 ì¸ì½”ë”© í•„ìš”)
            if request.image_data:
                import base64
                image_base64 = base64.b64encode(request.image_data).decode()
                message_content.append({
                    "type": "image",
                    "source": {
                        "type": "base64",
                        "media_type": "image/jpeg",
                        "data": image_base64
                    }
                })
            
            response = await asyncio.to_thread(
                self.anthropic_client.messages.create,
                model="claude-3-5-sonnet-20241022",
                max_tokens=1500,
                temperature=0.1,
                messages=[{"role": "user", "content": message_content}]
            )
            
            processing_time = time.time() - start_time
            content = response.content[0].text
            
            # ì£¼ì–¼ë¦¬ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
            jewelry_relevance = self._calculate_jewelry_relevance(content)
            
            return AIResponse(
                model=AIModel.CLAUDE_VISION,
                content=content,
                confidence=0.92,  # Claudeì˜ ë†’ì€ ì‹ ë¢°ë„
                processing_time=processing_time,
                cost_estimate=self._estimate_cost("claude", len(optimized_prompt), len(content)),
                jewelry_relevance=jewelry_relevance,
                metadata={"input_tokens": response.usage.input_tokens, "output_tokens": response.usage.output_tokens}
            )
            
        except Exception as e:
            logger.error(f"âŒ Claude Vision ë¶„ì„ ì‹¤íŒ¨: {e}")
            return self._create_error_response(AIModel.CLAUDE_VISION, str(e), time.time() - start_time)
    
    async def analyze_with_gemini(self, request: AnalysisRequest) -> AIResponse:
        """Gemini 2.0ìœ¼ë¡œ ë¶„ì„ ìˆ˜í–‰"""
        start_time = time.time()
        
        try:
            if not self.gemini_client:
                raise ValueError("Gemini í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            # í”„ë¡¬í”„íŠ¸ ìµœì í™”
            optimized_prompt = self.prompt_optimizer.optimize_prompt(
                request.analysis_type, 
                request.text_content or "ì´ë¯¸ì§€ ë¶„ì„ ìš”ì²­"
            )
            
            # ì½˜í…ì¸  ì¤€ë¹„
            content_parts = [optimized_prompt]
            
            # ì´ë¯¸ì§€ ì²˜ë¦¬
            if request.image_data:
                import PIL.Image
                import io
                image = PIL.Image.open(io.BytesIO(request.image_data))
                content_parts.append(image)
            
            response = await asyncio.to_thread(
                self.gemini_client.generate_content,
                content_parts,
                generation_config=genai.types.GenerationConfig(
                    temperature=0.1,
                    max_output_tokens=1500
                )
            )
            
            processing_time = time.time() - start_time
            content = response.text
            
            # ì£¼ì–¼ë¦¬ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
            jewelry_relevance = self._calculate_jewelry_relevance(content)
            
            return AIResponse(
                model=AIModel.GEMINI_2,
                content=content,
                confidence=0.88,  # Gemini ê¸°ë³¸ ì‹ ë¢°ë„
                processing_time=processing_time,
                cost_estimate=self._estimate_cost("gemini", len(optimized_prompt), len(content)),
                jewelry_relevance=jewelry_relevance,
                metadata={"candidate_count": len(response.candidates)}
            )
            
        except Exception as e:
            logger.error(f"âŒ Gemini 2.0 ë¶„ì„ ì‹¤íŒ¨: {e}")
            return self._create_error_response(AIModel.GEMINI_2, str(e), time.time() - start_time)
    
    def _calculate_jewelry_relevance(self, content: str) -> float:
        """ì£¼ì–¼ë¦¬ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
        jewelry_keywords = [
            "ë‹¤ì´ì•„ëª¬ë“œ", "diamond", "ë£¨ë¹„", "ruby", "ì‚¬íŒŒì´ì–´", "sapphire", "ì—ë©”ë„ë“œ", "emerald",
            "ìºëŸ¿", "carat", "ì»·", "cut", "ì»¬ëŸ¬", "color", "í´ë˜ë¦¬í‹°", "clarity",
            "ë°˜ì§€", "ring", "ëª©ê±¸ì´", "necklace", "ê·€ê±¸ì´", "earring", "ë¸Œë¡œì¹˜", "brooch",
            "í”Œë˜í‹°ë‚˜", "platinum", "ê³¨ë“œ", "gold", "ì‹¤ë²„", "silver",
            "GIA", "AGS", "SSEF", "GÃ¼belin", "ê°ì •ì„œ", "certificate"
        ]
        
        content_lower = content.lower()
        found_keywords = sum(1 for keyword in jewelry_keywords if keyword.lower() in content_lower)
        relevance_score = min(found_keywords / len(jewelry_keywords) * 2, 1.0)  # ìµœëŒ€ 1.0
        
        return relevance_score
    
    def _estimate_cost(self, model: str, input_length: int, output_length: int) -> float:
        """API ì‚¬ìš© ë¹„ìš© ì¶”ì •"""
        cost_per_1k_tokens = {
            "gpt4v": 0.03,    # GPT-4V ëŒ€ëµì  ë¹„ìš©
            "claude": 0.008,  # Claude Sonnet ë¹„ìš©
            "gemini": 0.002   # Gemini Pro ë¹„ìš©
        }
        
        total_tokens = (input_length + output_length) / 4  # ëŒ€ëµì  í† í° ìˆ˜
        cost = (total_tokens / 1000) * cost_per_1k_tokens.get(model, 0.01)
        
        return round(cost, 4)
    
    def _create_error_response(self, model: AIModel, error_msg: str, processing_time: float) -> AIResponse:
        """ì—ëŸ¬ ì‘ë‹µ ìƒì„±"""
        return AIResponse(
            model=model,
            content=f"âŒ ë¶„ì„ ì‹¤íŒ¨: {error_msg}",
            confidence=0.0,
            processing_time=processing_time,
            cost_estimate=0.0,
            jewelry_relevance=0.0,
            metadata={"error": True, "error_message": error_msg}
        )
    
    async def hybrid_analyze(self, request: AnalysisRequest) -> Dict[str, Any]:
        """
        í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ - 3ê°œ ëª¨ë¸ ë™ì‹œ ì‹¤í–‰ í›„ ìµœì  ê²°ê³¼ ì„ íƒ
        
        Args:
            request: ë¶„ì„ ìš”ì²­ ê°ì²´
            
        Returns:
            Dict: í†µí•© ë¶„ì„ ê²°ê³¼
        """
        logger.info(f"ğŸ§  í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ ì‹œì‘: {request.analysis_type}")
        
        # 3ê°œ ëª¨ë¸ ë™ì‹œ ì‹¤í–‰
        tasks = []
        if self.openai_client:
            tasks.append(self.analyze_with_gpt4v(request))
        if self.anthropic_client:
            tasks.append(self.analyze_with_claude(request))
        if self.gemini_client:
            tasks.append(self.analyze_with_gemini(request))
        
        if not tasks:
            return {
                "status": "error",
                "message": "ì‚¬ìš© ê°€ëŠ¥í•œ AI ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.",
                "timestamp": time.time()
            }
        
        # ëª¨ë“  ëª¨ë¸ì˜ ì‘ë‹µ ëŒ€ê¸°
        responses = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ìœ íš¨í•œ ì‘ë‹µë§Œ í•„í„°ë§
        valid_responses = [r for r in responses if isinstance(r, AIResponse) and r.confidence > 0]
        
        if not valid_responses:
            return {
                "status": "error",
                "message": "ëª¨ë“  AI ëª¨ë¸ì—ì„œ ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
                "errors": [str(r) for r in responses if not isinstance(r, AIResponse)],
                "timestamp": time.time()
            }
        
        # ìµœì  ëª¨ë¸ ì„ íƒ
        best_response = self._select_best_response(valid_responses)
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        self._update_performance_metrics(valid_responses)
        
        # ë¹„ìš© ì¶”ì  ì—…ë°ì´íŠ¸
        total_cost = sum(r.cost_estimate for r in valid_responses)
        self.cost_tracker["total"] += total_cost
        
        # í†µí•© ê²°ê³¼ ë°˜í™˜
        result = {
            "status": "success",
            "best_model": best_response.model.value,
            "content": best_response.content,
            "confidence": best_response.confidence,
            "jewelry_relevance": best_response.jewelry_relevance,
            "processing_time": best_response.processing_time,
            "cost_estimate": total_cost,
            "all_responses": [asdict(r) for r in valid_responses],
            "performance_summary": self._get_performance_summary(),
            "timestamp": time.time()
        }
        
        # ì‘ë‹µ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
        self.response_history.append(result)
        
        logger.info(f"âœ… í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ ì™„ë£Œ: {best_response.model.value} ì„ íƒë¨")
        
        return result
    
    def _select_best_response(self, responses: List[AIResponse]) -> AIResponse:
        """ìµœì  ì‘ë‹µ ì„ íƒ ì•Œê³ ë¦¬ì¦˜"""
        if not responses:
            raise ValueError("ì„ íƒí•  ì‘ë‹µì´ ì—†ìŠµë‹ˆë‹¤.")
        
        if len(responses) == 1:
            return responses[0]
        
        # ë³µí•© ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘ì¹˜ ì ìš©)
        weights = {
            "confidence": 0.3,
            "jewelry_relevance": 0.4,
            "speed": 0.2,
            "cost_efficiency": 0.1
        }
        
        scored_responses = []
        max_time = max(r.processing_time for r in responses)
        max_cost = max(r.cost_estimate for r in responses)
        
        for response in responses:
            # ì •ê·œí™”ëœ ì ìˆ˜ ê³„ì‚°
            speed_score = 1.0 - (response.processing_time / max_time) if max_time > 0 else 1.0
            cost_score = 1.0 - (response.cost_estimate / max_cost) if max_cost > 0 else 1.0
            
            composite_score = (
                weights["confidence"] * response.confidence +
                weights["jewelry_relevance"] * response.jewelry_relevance +
                weights["speed"] * speed_score +
                weights["cost_efficiency"] * cost_score
            )
            
            scored_responses.append((response, composite_score))
        
        # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ì‘ë‹µ ì„ íƒ
        best_response = max(scored_responses, key=lambda x: x[1])[0]
        
        return best_response
    
    def _update_performance_metrics(self, responses: List[AIResponse]):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        for response in responses:
            model_name = response.model.value
            if model_name not in self.performance_metrics:
                self.performance_metrics[model_name] = {
                    "total_requests": 0,
                    "avg_confidence": 0.0,
                    "avg_processing_time": 0.0,
                    "avg_jewelry_relevance": 0.0,
                    "total_cost": 0.0
                }
            
            metrics = self.performance_metrics[model_name]
            metrics["total_requests"] += 1
            
            # ì´ë™ í‰ê·  ì—…ë°ì´íŠ¸
            n = metrics["total_requests"]
            metrics["avg_confidence"] = ((n-1) * metrics["avg_confidence"] + response.confidence) / n
            metrics["avg_processing_time"] = ((n-1) * metrics["avg_processing_time"] + response.processing_time) / n
            metrics["avg_jewelry_relevance"] = ((n-1) * metrics["avg_jewelry_relevance"] + response.jewelry_relevance) / n
            metrics["total_cost"] += response.cost_estimate
    
    def _get_performance_summary(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ìš”ì•½ ì •ë³´ ë°˜í™˜"""
        if not self.performance_metrics:
            return {"message": "ì„±ëŠ¥ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}
        
        summary = {}
        for model, metrics in self.performance_metrics.items():
            summary[model] = {
                "requests": metrics["total_requests"],
                "avg_confidence": round(metrics["avg_confidence"], 3),
                "avg_speed": round(metrics["avg_processing_time"], 2),
                "avg_jewelry_relevance": round(metrics["avg_jewelry_relevance"], 3),
                "total_cost": round(metrics["total_cost"], 4)
            }
        
        return summary
    
    def get_cost_report(self) -> Dict[str, Any]:
        """ë¹„ìš© ë¦¬í¬íŠ¸ ìƒì„±"""
        model_costs = {}
        for model, metrics in self.performance_metrics.items():
            model_costs[model] = metrics["total_cost"]
        
        return {
            "total_cost": round(self.cost_tracker["total"], 4),
            "cost_by_model": model_costs,
            "average_cost_per_request": round(
                self.cost_tracker["total"] / len(self.response_history) if self.response_history else 0, 4
            ),
            "total_requests": len(self.response_history)
        }

# ë°ëª¨ ë° í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
async def demo_hybrid_analysis():
    """í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ ë°ëª¨"""
    print("ğŸ§  ì†”ë¡œëª¬ë“œ AI í•˜ì´ë¸Œë¦¬ë“œ LLM ë§¤ë‹ˆì € v2.3 ë°ëª¨")
    print("=" * 60)
    
    # ì„¤ì • (ì‹¤ì œ ì‚¬ìš© ì‹œ API í‚¤ í•„ìš”)
    config = {
        # "openai_key": "your-openai-api-key",
        # "anthropic_key": "your-anthropic-api-key", 
        # "google_key": "your-google-api-key"
    }
    
    manager = HybridLLMManager(config)
    
    # í…ŒìŠ¤íŠ¸ ìš”ì²­
    request = AnalysisRequest(
        text_content="1ìºëŸ¿ ë¼ìš´ë“œ ë‹¤ì´ì•„ëª¬ë“œì˜ ë“±ê¸‰ì„ ë¶„ì„í•´ì£¼ì„¸ìš”. ì»¬ëŸ¬ëŠ” H, í´ë˜ë¦¬í‹°ëŠ” VS1 ë“±ê¸‰ì…ë‹ˆë‹¤.",
        analysis_type="diamond_analysis",
        require_jewelry_expertise=True
    )
    
    print("ğŸ“ ë¶„ì„ ìš”ì²­:")
    print(f"   ë‚´ìš©: {request.text_content}")
    print(f"   íƒ€ì…: {request.analysis_type}")
    print()
    
    # í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ ì‹¤í–‰
    result = await manager.hybrid_analyze(request)
    
    print("ğŸ“Š ë¶„ì„ ê²°ê³¼:")
    print(f"   ìƒíƒœ: {result['status']}")
    if result['status'] == 'success':
        print(f"   ìµœì  ëª¨ë¸: {result['best_model']}")
        print(f"   ì‹ ë¢°ë„: {result['confidence']:.1%}")
        print(f"   ì£¼ì–¼ë¦¬ ê´€ë ¨ì„±: {result['jewelry_relevance']:.1%}")
        print(f"   ì²˜ë¦¬ ì‹œê°„: {result['processing_time']:.2f}ì´ˆ")
        print(f"   ì˜ˆìƒ ë¹„ìš©: ${result['cost_estimate']:.4f}")
        print(f"   ì‘ë‹µ ìˆ˜: {len(result['all_responses'])}ê°œ")
        print()
        print("ğŸ’ ë¶„ì„ ë‚´ìš©:")
        print(result['content'][:200] + "..." if len(result['content']) > 200 else result['content'])
    else:
        print(f"   ì˜¤ë¥˜: {result['message']}")
    
    print()
    print("ğŸ“ˆ ì„±ëŠ¥ ìš”ì•½:")
    print(json.dumps(result.get('performance_summary', {}), indent=2, ensure_ascii=False))
    
    print()
    print("ğŸ’° ë¹„ìš© ë¦¬í¬íŠ¸:")
    cost_report = manager.get_cost_report()
    print(json.dumps(cost_report, indent=2, ensure_ascii=False))

if __name__ == "__main__":
    # ë°ëª¨ ì‹¤í–‰
    asyncio.run(demo_hybrid_analysis())
