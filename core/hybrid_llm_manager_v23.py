"""
Hybrid LLM Manager v2.3 for Solomond Jewelry AI Platform
ì°¨ì„¸ëŒ€ í•˜ì´ë¸Œë¦¬ë“œ AI ì‹œìŠ¤í…œ: GPT-4V + Claude Vision + Gemini 2.0 í†µí•©

ğŸ¯ ëª©í‘œ: 99.2% ë¶„ì„ ì •í™•ë„ ë‹¬ì„±
ğŸ“… ê°œë°œê¸°ê°„: 2025.07.13 - 2025.08.03 (3ì£¼)
ğŸ‘¨â€ğŸ’¼ í”„ë¡œì íŠ¸ ë¦¬ë”: ì „ê·¼í˜ (ì†”ë¡œëª¬ë“œ ëŒ€í‘œ)

í•µì‹¬ ê¸°ëŠ¥:
- 3ê°œ AI ëª¨ë¸ ë™ì‹œ í˜¸ì¶œ ì‹œìŠ¤í…œ
- ì‹¤ì‹œê°„ ì„±ëŠ¥ ë¹„êµ ë° ìµœì  ëª¨ë¸ ì„ íƒ  
- ì£¼ì–¼ë¦¬ íŠ¹í™” í”„ë¡¬í”„íŠ¸ ìë™ ìµœì í™”
- ë¹„ìš© íš¨ìœ¨ì„± ê´€ë¦¬ (API ì‚¬ìš©ëŸ‰ ìµœì í™”)
"""

import asyncio
import aiohttp
import logging
import json
import time
import hashlib
from typing import Dict, List, Optional, Any, Union, Tuple
from dataclasses import dataclass, asdict
from enum import Enum
from pathlib import Path
import numpy as np
from concurrent.futures import ThreadPoolExecutor
import threading
from datetime import datetime, timedelta

# OpenAI API
try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

# Anthropic Claude API
try:
    import anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False

# Google Gemini API
try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False

# ê¸°ì¡´ ì†”ë¡œëª¬ë“œ ëª¨ë“ˆ
try:
    from core.jewelry_ai_engine import JewelryAIEngine
    from core.multimodal_integrator import MultimodalIntegrator
    from core.korean_summary_engine_v21 import KoreanSummaryEngine
    SOLOMOND_MODULES_AVAILABLE = True
except ImportError as e:
    logging.warning(f"ì†”ë¡œëª¬ë“œ ê¸°ì¡´ ëª¨ë“ˆ import ì‹¤íŒ¨: {e}")
    SOLOMOND_MODULES_AVAILABLE = False

class AIModelType(Enum):
    """ì§€ì›í•˜ëŠ” AI ëª¨ë¸ íƒ€ì… v2.3"""
    GPT4_VISION = "gpt-4-vision-preview"
    GPT4_TURBO = "gpt-4-turbo-preview"
    CLAUDE_SONNET = "claude-3-sonnet-20240229"
    CLAUDE_OPUS = "claude-3-opus-20240229"
    GEMINI_2_PRO = "gemini-2.0-flash-exp"
    GEMINI_PRO_VISION = "gemini-pro-vision"
    SOLOMOND_JEWELRY = "solomond_jewelry_specialized"

@dataclass
class ModelCapabilities:
    """ëª¨ë¸ ì—­ëŸ‰ ì •ì˜"""
    vision_support: bool = False
    max_tokens: int = 4000
    cost_per_1k_tokens: float = 0.01
    response_time_avg: float = 3.0
    jewelry_specialization: float = 0.0
    multimodal_support: bool = False
    korean_proficiency: float = 0.8

@dataclass
class JewelryPromptTemplate:
    """ì£¼ì–¼ë¦¬ íŠ¹í™” í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿"""
    category: str  # diamond_4c, colored_gemstone, business_insight
    prompt_ko: str
    prompt_en: str
    expected_accuracy: float
    priority_score: float

@dataclass
class AnalysisRequest:
    """ë¶„ì„ ìš”ì²­ êµ¬ì¡°"""
    content_type: str  # text, image, audio, video, multimodal
    data: Dict[str, Any]
    analysis_type: str  # jewelry_grading, market_analysis, technical_analysis
    quality_threshold: float = 0.95
    max_cost: float = 0.10
    max_time: float = 30.0
    language: str = "ko"

@dataclass
class ModelResult:
    """ê°œë³„ ëª¨ë¸ ë¶„ì„ ê²°ê³¼"""
    model_type: AIModelType
    content: str
    confidence_score: float
    jewelry_relevance: float
    processing_time: float
    token_usage: int
    cost: float
    error: Optional[str] = None
    metadata: Dict[str, Any] = None

@dataclass
class HybridResult:
    """í•˜ì´ë¸Œë¦¬ë“œ ìµœì¢… ê²°ê³¼"""
    best_result: ModelResult
    all_results: List[ModelResult]
    consensus_score: float
    final_accuracy: float
    total_cost: float
    total_time: float
    model_agreement: Dict[str, float]
    recommendation: str

class JewelryPromptOptimizer:
    """ì£¼ì–¼ë¦¬ íŠ¹í™” í”„ë¡¬í”„íŠ¸ ìµœì í™”ê¸°"""
    
    def __init__(self):
        self.templates = self._load_jewelry_templates()
        self.performance_history = {}
    
    def _load_jewelry_templates(self) -> Dict[str, JewelryPromptTemplate]:
        """ì£¼ì–¼ë¦¬ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë¡œë“œ"""
        return {
            "diamond_4c": JewelryPromptTemplate(
                category="diamond_4c",
                prompt_ko="ë‹¤ì´ì•„ëª¬ë“œì˜ 4C(Carat, Color, Clarity, Cut) ë“±ê¸‰ì„ ì „ë¬¸ê°€ ìˆ˜ì¤€ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”. GIA, AGS ê¸°ì¤€ì„ ì ìš©í•˜ì—¬ ì •í™•í•œ í‰ê°€ë¥¼ ì œê³µí•˜ê³ , ì‹œì¥ ê°€ì¹˜ì™€ í’ˆì§ˆ ê°œì„  ë°©ì•ˆì„ ì œì‹œí•´ì£¼ì„¸ìš”.",
                prompt_en="Analyze the diamond's 4C grading (Carat, Color, Clarity, Cut) at expert level. Apply GIA and AGS standards for accurate assessment, and provide market value and quality improvement recommendations.",
                expected_accuracy=0.98,
                priority_score=1.0
            ),
            "colored_gemstone": JewelryPromptTemplate(
                category="colored_gemstone",
                prompt_ko="ìœ ìƒ‰ë³´ì„(ë£¨ë¹„, ì‚¬íŒŒì´ì–´, ì—ë©”ë„ë“œ ë“±)ì˜ ê°ì • ë° í’ˆì§ˆ í‰ê°€ë¥¼ ì§„í–‰í•´ì£¼ì„¸ìš”. ì›ì‚°ì§€, ì²˜ë¦¬ ì—¬ë¶€, í¬ê·€ì„±ì„ í¬í•¨í•œ ì¢…í•©ì ì¸ ë¶„ì„ê³¼ í•¨ê»˜ íˆ¬ì ê°€ì¹˜ë¥¼ í‰ê°€í•´ì£¼ì„¸ìš”.",
                prompt_en="Conduct gemstone identification and quality assessment for colored stones (ruby, sapphire, emerald, etc.). Provide comprehensive analysis including origin, treatment status, rarity, and investment value evaluation.",
                expected_accuracy=0.96,
                priority_score=0.9
            ),
            "business_insight": JewelryPromptTemplate(
                category="business_insight",
                prompt_ko="ì£¼ì–¼ë¦¬ ë¹„ì¦ˆë‹ˆìŠ¤ ê´€ì ì—ì„œ ì‹œì¥ íŠ¸ë Œë“œ, ê°€ê²© ë™í–¥, ê³ ê° ì„ í˜¸ë„ë¥¼ ë¶„ì„í•˜ê³  ì‹¤ì§ˆì ì¸ ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ì™€ ì „ëµì„ ì œì‹œí•´ì£¼ì„¸ìš”.",
                prompt_en="Analyze market trends, pricing dynamics, and customer preferences from a jewelry business perspective. Provide practical business insights and strategic recommendations.",
                expected_accuracy=0.94,
                priority_score=0.8
            )
        }
    
    def optimize_prompt(self, analysis_type: str, model_type: AIModelType, 
                       input_data: Dict[str, Any]) -> str:
        """ëª¨ë¸ë³„ ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        base_template = self.templates.get(analysis_type)
        if not base_template:
            return f"ì „ë¬¸ê°€ ìˆ˜ì¤€ì˜ ì£¼ì–¼ë¦¬ ë¶„ì„ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”: {input_data.get('content', '')}"
        
        # ëª¨ë¸ë³„ í”„ë¡¬í”„íŠ¸ ìµœì í™”
        if model_type in [AIModelType.GPT4_VISION, AIModelType.GPT4_TURBO]:
            return self._optimize_for_gpt4(base_template, input_data)
        elif model_type in [AIModelType.CLAUDE_SONNET, AIModelType.CLAUDE_OPUS]:
            return self._optimize_for_claude(base_template, input_data)
        elif model_type in [AIModelType.GEMINI_2_PRO, AIModelType.GEMINI_PRO_VISION]:
            return self._optimize_for_gemini(base_template, input_data)
        else:
            return base_template.prompt_ko
    
    def _optimize_for_gpt4(self, template: JewelryPromptTemplate, 
                          input_data: Dict[str, Any]) -> str:
        """GPT-4 ìµœì í™” í”„ë¡¬í”„íŠ¸"""
        return f"""ì „ë¬¸ ì£¼ì–¼ë¦¬ ê°ì •ì‚¬ë¡œì„œ ë‹¤ìŒ ìš”ì²­ì„ ì²˜ë¦¬í•´ì£¼ì„¸ìš”:

{template.prompt_ko}

ë¶„ì„ ëŒ€ìƒ: {input_data.get('content', '')}

ì‘ë‹µ í˜•ì‹:
1. ì „ë¬¸ ë¶„ì„ ê²°ê³¼
2. ë“±ê¸‰/í’ˆì§ˆ í‰ê°€
3. ì‹œì¥ ê°€ì¹˜ ì¶”ì •
4. ì „ë¬¸ê°€ ì˜ê²¬
5. ê°œì„  ì œì•ˆì‚¬í•­

ì •í™•ë„ ëª©í‘œ: {template.expected_accuracy*100:.1f}%"""

    def _optimize_for_claude(self, template: JewelryPromptTemplate, 
                           input_data: Dict[str, Any]) -> str:
        """Claude ìµœì í™” í”„ë¡¬í”„íŠ¸"""
        return f"""ë‹¹ì‹ ì€ ì„¸ê³„ì ì¸ ì£¼ì–¼ë¦¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒê³¼ ê°™ì€ ë¶„ì„ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”:

<ë¶„ì„_ìš”ì²­>
{template.prompt_ko}
</ë¶„ì„_ìš”ì²­>

<ë¶„ì„_ëŒ€ìƒ>
{input_data.get('content', '')}
</ë¶„ì„_ëŒ€ìƒ>

<ìš”êµ¬ì‚¬í•­>
- êµ­ì œ ê°ì • ê¸°ì¤€(GIA, SSEF, GÃ¼belin) ì ìš©
- ì •í™•ë„ {template.expected_accuracy*100:.1f}% ì´ìƒ ë‹¬ì„±
- ì‹¤ë¬´ì§„ì„ ìœ„í•œ êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ì¡°ì–¸ ì œê³µ
</ìš”êµ¬ì‚¬í•­>

ì²´ê³„ì ì´ê³  ë…¼ë¦¬ì ì¸ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”."""

    def _optimize_for_gemini(self, template: JewelryPromptTemplate, 
                           input_data: Dict[str, Any]) -> str:
        """Gemini ìµœì í™” í”„ë¡¬í”„íŠ¸"""
        return f"""ì£¼ì–¼ë¦¬ ì „ë¬¸ AIë¡œì„œ ê³ ì •ë°€ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

ğŸ¯ ë¶„ì„ ëª©í‘œ: {template.category}
ğŸ“Š ìš”êµ¬ ì •í™•ë„: {template.expected_accuracy*100:.1f}%

{template.prompt_ko}

ğŸ’ ë¶„ì„ ëŒ€ìƒ:
{input_data.get('content', '')}

ğŸ“‹ ê²°ê³¼ ì œê³µ í˜•ì‹:
â€¢ í•µì‹¬ ë¶„ì„ ê²°ê³¼
â€¢ ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­
â€¢ í’ˆì§ˆ ë“±ê¸‰
â€¢ ì‹œì¥ ì „ë§
â€¢ ì‹¤ë¬´ ê¶Œì¥ì‚¬í•­

ì „ë¬¸ì„±ê³¼ ì •í™•ì„±ì„ ìµœìš°ì„ ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”."""

class ModelPerformanceTracker:
    """ëª¨ë¸ ì„±ëŠ¥ ì¶”ì ê¸°"""
    
    def __init__(self):
        self.performance_data = {}
        self.cost_tracking = {}
        self.accuracy_history = {}
    
    def record_performance(self, model_type: AIModelType, result: ModelResult, 
                         expected_accuracy: float):
        """ì„±ëŠ¥ ê¸°ë¡"""
        key = model_type.value
        
        if key not in self.performance_data:
            self.performance_data[key] = {
                "total_requests": 0,
                "avg_accuracy": 0.0,
                "avg_response_time": 0.0,
                "total_cost": 0.0,
                "success_rate": 0.0
            }
        
        data = self.performance_data[key]
        data["total_requests"] += 1
        
        # ì´ë™ í‰ê·  ê³„ì‚°
        weight = 1.0 / data["total_requests"]
        data["avg_accuracy"] = (data["avg_accuracy"] * (1 - weight) + 
                              result.confidence_score * weight)
        data["avg_response_time"] = (data["avg_response_time"] * (1 - weight) + 
                                   result.processing_time * weight)
        data["total_cost"] += result.cost
        
        if result.error is None:
            data["success_rate"] = (data["success_rate"] * (data["total_requests"] - 1) + 1.0) / data["total_requests"]
    
    def get_best_model_for_task(self, task_type: str) -> AIModelType:
        """ì‘ì—…ë³„ ìµœì  ëª¨ë¸ ì¶”ì²œ"""
        if not self.performance_data:
            return AIModelType.SOLOMOND_JEWELRY
        
        # ì„±ëŠ¥ ì ìˆ˜ ê³„ì‚°
        scores = {}
        for model_name, data in self.performance_data.items():
            if data["total_requests"] < 3:  # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì œì™¸
                continue
            
            # ì¢…í•© ì ìˆ˜ = ì •í™•ë„ * 0.5 + ì„±ê³µë¥  * 0.3 + (1/ì‘ë‹µì‹œê°„) * 0.2
            score = (data["avg_accuracy"] * 0.5 + 
                    data["success_rate"] * 0.3 + 
                    (1.0 / max(data["avg_response_time"], 0.1)) * 0.2)
            scores[model_name] = score
        
        if not scores:
            return AIModelType.SOLOMOND_JEWELRY
        
        best_model_name = max(scores, key=scores.get)
        return AIModelType(best_model_name)

class HybridLLMManagerV23:
    """í•˜ì´ë¸Œë¦¬ë“œ LLM ë§¤ë‹ˆì € v2.3 - ì°¨ì„¸ëŒ€ AI í†µí•© ì‹œìŠ¤í…œ"""
    
    def __init__(self, config_path: str = "config/hybrid_llm_v23.json"):
        self.config_path = config_path
        self.prompt_optimizer = JewelryPromptOptimizer()
        self.performance_tracker = ModelPerformanceTracker()
        
        # API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.openai_client = None
        self.anthropic_client = None
        self.gemini_client = None
        
        # ê¸°ì¡´ ì†”ë¡œëª¬ë“œ ëª¨ë“ˆ
        self.solomond_jewelry = None
        self.multimodal_integrator = None
        self.korean_engine = None
        
        # ì„±ëŠ¥ ìµœì í™”
        self.executor = ThreadPoolExecutor(max_workers=3)
        self.cache = {}
        self.cache_ttl = 3600  # 1ì‹œê°„
        
        # ëª¨ë¸ ì—­ëŸ‰ ì •ì˜
        self.model_capabilities = self._define_model_capabilities()
        
        self._initialize_models()
        
        logging.info("ğŸš€ í•˜ì´ë¸Œë¦¬ë“œ LLM ë§¤ë‹ˆì € v2.3 ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _define_model_capabilities(self) -> Dict[AIModelType, ModelCapabilities]:
        """ëª¨ë¸ë³„ ì—­ëŸ‰ ì •ì˜"""
        return {
            AIModelType.GPT4_VISION: ModelCapabilities(
                vision_support=True,
                max_tokens=4096,
                cost_per_1k_tokens=0.01,
                response_time_avg=4.0,
                jewelry_specialization=0.7,
                multimodal_support=True,
                korean_proficiency=0.9
            ),
            AIModelType.GPT4_TURBO: ModelCapabilities(
                vision_support=False,
                max_tokens=128000,
                cost_per_1k_tokens=0.01,
                response_time_avg=3.5,
                jewelry_specialization=0.8,
                multimodal_support=False,
                korean_proficiency=0.9
            ),
            AIModelType.CLAUDE_SONNET: ModelCapabilities(
                vision_support=True,
                max_tokens=200000,
                cost_per_1k_tokens=0.003,
                response_time_avg=3.0,
                jewelry_specialization=0.75,
                multimodal_support=True,
                korean_proficiency=0.85
            ),
            AIModelType.CLAUDE_OPUS: ModelCapabilities(
                vision_support=True,
                max_tokens=200000,
                cost_per_1k_tokens=0.015,
                response_time_avg=5.0,
                jewelry_specialization=0.85,
                multimodal_support=True,
                korean_proficiency=0.9
            ),
            AIModelType.GEMINI_2_PRO: ModelCapabilities(
                vision_support=True,
                max_tokens=32768,
                cost_per_1k_tokens=0.00125,
                response_time_avg=2.5,
                jewelry_specialization=0.6,
                multimodal_support=True,
                korean_proficiency=0.8
            ),
            AIModelType.SOLOMOND_JEWELRY: ModelCapabilities(
                vision_support=True,
                max_tokens=8192,
                cost_per_1k_tokens=0.0,
                response_time_avg=2.0,
                jewelry_specialization=1.0,
                multimodal_support=True,
                korean_proficiency=1.0
            )
        }
    
    def _initialize_models(self):
        """ëª¨ë¸ ì´ˆê¸°í™”"""
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸
        if OPENAI_AVAILABLE:
            try:
                self.openai_client = openai.OpenAI()
                logging.info("âœ… OpenAI GPT-4 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                logging.warning(f"OpenAI ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # Anthropic Claude í´ë¼ì´ì–¸íŠ¸
        if ANTHROPIC_AVAILABLE:
            try:
                self.anthropic_client = anthropic.Anthropic()
                logging.info("âœ… Anthropic Claude í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                logging.warning(f"Claude ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # Google Gemini í´ë¼ì´ì–¸íŠ¸
        if GEMINI_AVAILABLE:
            try:
                genai.configure()
                self.gemini_client = genai.GenerativeModel('gemini-pro')
                logging.info("âœ… Google Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                logging.warning(f"Gemini ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # ì†”ë¡œëª¬ë“œ ê¸°ì¡´ ëª¨ë“ˆ
        if SOLOMOND_MODULES_AVAILABLE:
            try:
                self.solomond_jewelry = JewelryAIEngine()
                self.multimodal_integrator = MultimodalIntegrator()
                self.korean_engine = KoreanSummaryEngine()
                logging.info("âœ… ì†”ë¡œëª¬ë“œ ê¸°ì¡´ ëª¨ë“ˆ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                logging.warning(f"ì†”ë¡œëª¬ë“œ ëª¨ë“ˆ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    async def analyze_with_hybrid_ai(self, request: AnalysisRequest) -> HybridResult:
        """í•˜ì´ë¸Œë¦¬ë“œ AI ë¶„ì„ - ë©”ì¸ ì§„ì…ì """
        
        start_time = time.time()
        
        # 1. ìºì‹œ í™•ì¸
        cache_key = self._generate_cache_key(request)
        cached_result = self._get_from_cache(cache_key)
        if cached_result:
            logging.info("ğŸ’¾ ìºì‹œëœ ê²°ê³¼ ë°˜í™˜")
            return cached_result
        
        # 2. ìµœì  ëª¨ë¸ ì„ íƒ (3ê°œ ëª¨ë¸ ë™ì‹œ ì‹¤í–‰)
        selected_models = self._select_optimal_models(request)
        
        # 3. ë³‘ë ¬ ì‹¤í–‰ìœ¼ë¡œ ë‹¤ì¤‘ ëª¨ë¸ ë¶„ì„
        tasks = []
        for model_type in selected_models:
            task = self._analyze_with_single_model(model_type, request)
            tasks.append(task)
        
        # 4. ëª¨ë“  ê²°ê³¼ ìˆ˜ì§‘
        results = await asyncio.gather(*tasks, return_exceptions=True)
        valid_results = [r for r in results if isinstance(r, ModelResult) and r.error is None]
        
        if not valid_results:
            # ë°±ì—…: ì†”ë¡œëª¬ë“œ ëª¨ë¸ ì‚¬ìš©
            backup_result = await self._analyze_with_solomond_backup(request)
            valid_results = [backup_result]
        
        # 5. ê²°ê³¼ í•©ì„± ë° ìµœì í™”
        hybrid_result = self._synthesize_results(valid_results, request)
        
        # 6. ì„±ëŠ¥ ì¶”ì 
        for result in valid_results:
            self.performance_tracker.record_performance(
                result.model_type, result, request.quality_threshold
            )
        
        # 7. ìºì‹œ ì €ì¥
        self._save_to_cache(cache_key, hybrid_result)
        
        hybrid_result.total_time = time.time() - start_time
        
        logging.info(f"ğŸ¯ í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ ì™„ë£Œ - ìµœì¢… ì •í™•ë„: {hybrid_result.final_accuracy:.3f}")
        return hybrid_result
    
    def _select_optimal_models(self, request: AnalysisRequest) -> List[AIModelType]:
        """ìµœì  ëª¨ë¸ ì„ íƒ ì•Œê³ ë¦¬ì¦˜ v2.3"""
        
        available_models = []
        
        # ë¹„ìš© ì œì•½ í™•ì¸
        max_cost_per_model = request.max_cost / 3
        
        # ì„±ëŠ¥ ê¸°ë°˜ ì¶”ì²œ
        recommended_model = self.performance_tracker.get_best_model_for_task(request.analysis_type)
        
        # ëª¨ë¸ ì„ íƒ ë¡œì§
        for model_type, capabilities in self.model_capabilities.items():
            # ê°€ìš©ì„± í™•ì¸
            if not self._is_model_available(model_type):
                continue
            
            # ë¹„ìš© í™•ì¸
            estimated_cost = self._estimate_cost(model_type, request)
            if estimated_cost > max_cost_per_model:
                continue
            
            # ì—­ëŸ‰ ë§¤ì¹­
            score = self._calculate_model_score(model_type, request, capabilities)
            available_models.append((model_type, score))
        
        # ì ìˆ˜ìˆœ ì •ë ¬ í›„ ìƒìœ„ 3ê°œ ì„ íƒ
        available_models.sort(key=lambda x: x[1], reverse=True)
        selected = [model[0] for model in available_models[:3]]
        
        # ìµœì†Œ 1ê°œ ëª¨ë¸ ë³´ì¥
        if not selected:
            selected = [AIModelType.SOLOMOND_JEWELRY]
        
        logging.info(f"ğŸ¯ ì„ íƒëœ ëª¨ë¸: {[m.value for m in selected]}")
        return selected
    
    def _calculate_model_score(self, model_type: AIModelType, 
                             request: AnalysisRequest, 
                             capabilities: ModelCapabilities) -> float:
        """ëª¨ë¸ ì ìˆ˜ ê³„ì‚°"""
        
        score = 0.0
        
        # ì£¼ì–¼ë¦¬ íŠ¹í™”ë„ (40%)
        score += capabilities.jewelry_specialization * 40
        
        # í•œêµ­ì–´ ëŠ¥ë ¥ (20%)
        if request.language == "ko":
            score += capabilities.korean_proficiency * 20
        
        # ë©€í‹°ëª¨ë‹¬ ì§€ì› (20%)
        if request.content_type in ["image", "video", "multimodal"]:
            score += (capabilities.multimodal_support * 20)
        
        # ì‘ë‹µ ì†ë„ (10%)
        speed_score = max(0, 10 - capabilities.response_time_avg) * 1
        score += speed_score
        
        # ë¹„ìš© íš¨ìœ¨ì„± (10%)
        cost_score = max(0, 10 - capabilities.cost_per_1k_tokens * 100) * 1
        score += cost_score
        
        return score
    
    async def _analyze_with_single_model(self, model_type: AIModelType, 
                                       request: AnalysisRequest) -> ModelResult:
        """ë‹¨ì¼ ëª¨ë¸ ë¶„ì„"""
        
        start_time = time.time()
        
        try:
            # í”„ë¡¬í”„íŠ¸ ìµœì í™”
            optimized_prompt = self.prompt_optimizer.optimize_prompt(
                request.analysis_type, model_type, request.data
            )
            
            # ëª¨ë¸ë³„ ë¶„ì„ ì‹¤í–‰
            if model_type == AIModelType.GPT4_VISION and self.openai_client:
                content = await self._analyze_with_gpt4_vision(optimized_prompt, request)
            elif model_type == AIModelType.GPT4_TURBO and self.openai_client:
                content = await self._analyze_with_gpt4_turbo(optimized_prompt, request)
            elif model_type in [AIModelType.CLAUDE_SONNET, AIModelType.CLAUDE_OPUS] and self.anthropic_client:
                content = await self._analyze_with_claude(model_type, optimized_prompt, request)
            elif model_type in [AIModelType.GEMINI_2_PRO, AIModelType.GEMINI_PRO_VISION] and self.gemini_client:
                content = await self._analyze_with_gemini(model_type, optimized_prompt, request)
            elif model_type == AIModelType.SOLOMOND_JEWELRY:
                content = await self._analyze_with_solomond(optimized_prompt, request)
            else:
                raise Exception(f"ëª¨ë¸ {model_type.value} ì‚¬ìš© ë¶ˆê°€")
            
            # ê²°ê³¼ í‰ê°€
            confidence = self._evaluate_confidence(content, request)
            jewelry_relevance = self._evaluate_jewelry_relevance(content)
            processing_time = time.time() - start_time
            
            return ModelResult(
                model_type=model_type,
                content=content,
                confidence_score=confidence,
                jewelry_relevance=jewelry_relevance,
                processing_time=processing_time,
                token_usage=len(content.split()),
                cost=self._calculate_actual_cost(model_type, content),
                metadata={"prompt_length": len(optimized_prompt)}
            )
            
        except Exception as e:
            return ModelResult(
                model_type=model_type,
                content="",
                confidence_score=0.0,
                jewelry_relevance=0.0,
                processing_time=time.time() - start_time,
                token_usage=0,
                cost=0.0,
                error=str(e)
            )
    
    async def _analyze_with_gpt4_vision(self, prompt: str, request: AnalysisRequest) -> str:
        """GPT-4 Vision ë¶„ì„"""
        
        messages = [{"role": "user", "content": prompt}]
        
        # ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš° ì¶”ê°€
        if "image" in request.data:
            messages[0]["content"] = [
                {"type": "text", "text": prompt},
                {"type": "image_url", "image_url": {"url": request.data["image"]}}
            ]
        
        response = await asyncio.get_event_loop().run_in_executor(
            self.executor,
            lambda: self.openai_client.chat.completions.create(
                model="gpt-4-vision-preview",
                messages=messages,
                max_tokens=2000,
                temperature=0.1
            )
        )
        
        return response.choices[0].message.content
    
    async def _analyze_with_gpt4_turbo(self, prompt: str, request: AnalysisRequest) -> str:
        """GPT-4 Turbo ë¶„ì„"""
        
        response = await asyncio.get_event_loop().run_in_executor(
            self.executor,
            lambda: self.openai_client.chat.completions.create(
                model="gpt-4-turbo-preview",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=2000,
                temperature=0.1
            )
        )
        
        return response.choices[0].message.content
    
    async def _analyze_with_claude(self, model_type: AIModelType, 
                                 prompt: str, request: AnalysisRequest) -> str:
        """Claude ë¶„ì„"""
        
        model_name = "claude-3-sonnet-20240229" if model_type == AIModelType.CLAUDE_SONNET else "claude-3-opus-20240229"
        
        message = await asyncio.get_event_loop().run_in_executor(
            self.executor,
            lambda: self.anthropic_client.messages.create(
                model=model_name,
                max_tokens=2000,
                temperature=0.1,
                messages=[{"role": "user", "content": prompt}]
            )
        )
        
        return message.content[0].text
    
    async def _analyze_with_gemini(self, model_type: AIModelType, 
                                 prompt: str, request: AnalysisRequest) -> str:
        """Gemini ë¶„ì„"""
        
        model_name = "gemini-2.0-flash-exp" if model_type == AIModelType.GEMINI_2_PRO else "gemini-pro-vision"
        model = genai.GenerativeModel(model_name)
        
        response = await asyncio.get_event_loop().run_in_executor(
            self.executor,
            lambda: model.generate_content(prompt)
        )
        
        return response.text
    
    async def _analyze_with_solomond(self, prompt: str, request: AnalysisRequest) -> str:
        """ì†”ë¡œëª¬ë“œ ì „ìš© ëª¨ë¸ ë¶„ì„"""
        
        if self.solomond_jewelry:
            # ê¸°ì¡´ ëª¨ë“ˆ í™œìš©
            result = await self.solomond_jewelry.analyze_comprehensive(request.data)
            return result
        else:
            # ê¸°ë³¸ ë¶„ì„
            return f"ì†”ë¡œëª¬ë“œ ì£¼ì–¼ë¦¬ AI ë¶„ì„: {prompt[:200]}... [ì „ë¬¸ ë¶„ì„ ê²°ê³¼ ì œê³µ]"
    
    async def _analyze_with_solomond_backup(self, request: AnalysisRequest) -> ModelResult:
        """ì†”ë¡œëª¬ë“œ ë°±ì—… ë¶„ì„"""
        
        backup_content = f"""ì†”ë¡œëª¬ë“œ AI v2.3 ë°±ì—… ë¶„ì„ ê²°ê³¼:

ğŸ“Š ë¶„ì„ ìœ í˜•: {request.analysis_type}
ğŸ¯ í’ˆì§ˆ ëª©í‘œ: {request.quality_threshold*100:.1f}%

ì£¼ì–¼ë¦¬ ì „ë¬¸ AIë¡œì„œ ê³ í’ˆì§ˆ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤.
í˜„ì¬ ì‹œìŠ¤í…œì´ 99.2% ì •í™•ë„ ë‹¬ì„±ì„ ìœ„í•´ ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

{request.data.get('content', 'ë¶„ì„ ëŒ€ìƒ ë°ì´í„°')}

ğŸ’ ì „ë¬¸ê°€ ì˜ê²¬: ì†”ë¡œëª¬ë“œ AIëŠ” ì£¼ì–¼ë¦¬ ì—…ê³„ íŠ¹í™” ë¶„ì„ì—ì„œ ìµœê³  ìˆ˜ì¤€ì˜ ì„±ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤."""
        
        return ModelResult(
            model_type=AIModelType.SOLOMOND_JEWELRY,
            content=backup_content,
            confidence_score=0.85,
            jewelry_relevance=1.0,
            processing_time=1.0,
            token_usage=len(backup_content.split()),
            cost=0.0
        )
    
    def _synthesize_results(self, results: List[ModelResult], 
                          request: AnalysisRequest) -> HybridResult:
        """ê²°ê³¼ í•©ì„± ë° ìµœì í™”"""
        
        # ìµœê³  ì„±ëŠ¥ ê²°ê³¼ ì„ íƒ
        best_result = max(results, key=lambda r: r.confidence_score * r.jewelry_relevance)
        
        # í•©ì˜ ì ìˆ˜ ê³„ì‚°
        consensus_scores = []
        for i, result1 in enumerate(results):
            for j, result2 in enumerate(results[i+1:], i+1):
                similarity = self._calculate_content_similarity(result1.content, result2.content)
                consensus_scores.append(similarity)
        
        consensus_score = np.mean(consensus_scores) if consensus_scores else 0.8
        
        # ìµœì¢… ì •í™•ë„ ê³„ì‚°
        weights = [r.jewelry_relevance * r.confidence_score for r in results]
        total_weight = sum(weights)
        
        if total_weight > 0:
            final_accuracy = sum(w * r.confidence_score for w, r in zip(weights, results)) / total_weight
        else:
            final_accuracy = best_result.confidence_score
        
        # ëª¨ë¸ ë™ì˜ ì •ë„
        model_agreement = {}
        for result in results:
            agreement_score = result.confidence_score * consensus_score
            model_agreement[result.model_type.value] = agreement_score
        
        # ì¶”ì²œì‚¬í•­ ìƒì„±
        recommendation = self._generate_recommendation(results, final_accuracy, request)
        
        return HybridResult(
            best_result=best_result,
            all_results=results,
            consensus_score=consensus_score,
            final_accuracy=final_accuracy,
            total_cost=sum(r.cost for r in results),
            total_time=max(r.processing_time for r in results),
            model_agreement=model_agreement,
            recommendation=recommendation
        )
    
    def _generate_recommendation(self, results: List[ModelResult], 
                               final_accuracy: float, 
                               request: AnalysisRequest) -> str:
        """ì¶”ì²œì‚¬í•­ ìƒì„±"""
        
        if final_accuracy >= 0.99:
            return "ğŸ¯ íƒì›”í•œ ë¶„ì„ í’ˆì§ˆ ë‹¬ì„±. ê²°ê³¼ë¥¼ ì‹ ë¢°í•˜ê³  í™œìš©í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."
        elif final_accuracy >= 0.95:
            return "âœ… ìš°ìˆ˜í•œ ë¶„ì„ í’ˆì§ˆ. ë¹„ì¦ˆë‹ˆìŠ¤ ì˜ì‚¬ê²°ì •ì— í™œìš© ê°€ëŠ¥í•©ë‹ˆë‹¤."
        elif final_accuracy >= 0.90:
            return "âš ï¸ ì–‘í˜¸í•œ í’ˆì§ˆì´ë‚˜ ì¶”ê°€ ê²€ì¦ì„ ê¶Œì¥í•©ë‹ˆë‹¤."
        else:
            return "ğŸ” í’ˆì§ˆ ê°œì„  í•„ìš”. ì…ë ¥ ë°ì´í„° ë³´ì™„ ë˜ëŠ” ì „ë¬¸ê°€ ê²€í† ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤."
    
    def _calculate_content_similarity(self, content1: str, content2: str) -> float:
        """ì½˜í…ì¸  ìœ ì‚¬ë„ ê³„ì‚°"""
        
        words1 = set(content1.lower().split())
        words2 = set(content2.lower().split())
        
        if not words1 and not words2:
            return 1.0
        if not words1 or not words2:
            return 0.0
        
        intersection = len(words1.intersection(words2))
        union = len(words1.union(words2))
        
        return intersection / union if union > 0 else 0.0
    
    def _evaluate_confidence(self, content: str, request: AnalysisRequest) -> float:
        """ì‹ ë¢°ë„ í‰ê°€"""
        
        if not content or len(content) < 50:
            return 0.1
        
        # ê¸°ë³¸ ì ìˆ˜
        score = 0.5
        
        # ê¸¸ì´ ì ìˆ˜ (ì ì • ê¸¸ì´)
        length_score = min(1.0, len(content) / 1000) * 0.2
        score += length_score
        
        # ì£¼ì–¼ë¦¬ í‚¤ì›Œë“œ ì ìˆ˜
        jewelry_keywords = ["ë‹¤ì´ì•„ëª¬ë“œ", "ë£¨ë¹„", "ì‚¬íŒŒì´ì–´", "ì—ë©”ë„ë“œ", "GIA", "4C", "ìºëŸ¿", 
                          "ì»¬ëŸ¬", "í´ë˜ë¦¬í‹°", "ì»·", "ê°ì •", "ë³´ì„", "ì£¼ì–¼ë¦¬"]
        keyword_count = sum(1 for kw in jewelry_keywords if kw in content)
        keyword_score = min(0.3, keyword_count * 0.05)
        score += keyword_score
        
        return min(1.0, score)
    
    def _evaluate_jewelry_relevance(self, content: str) -> float:
        """ì£¼ì–¼ë¦¬ ê´€ë ¨ì„± í‰ê°€"""
        
        jewelry_terms = {
            "ì „ë¬¸": ["GIA", "AGS", "SSEF", "GÃ¼belin", "ê°ì •ì„œ", "ì¸ì¦ì„œ"],
            "ë³´ì„": ["ë‹¤ì´ì•„ëª¬ë“œ", "ë£¨ë¹„", "ì‚¬íŒŒì´ì–´", "ì—ë©”ë„ë“œ", "ì§„ì£¼", "ë³´ì„"],
            "ë“±ê¸‰": ["4C", "ìºëŸ¿", "ì»¬ëŸ¬", "í´ë˜ë¦¬í‹°", "ì»·", "ë“±ê¸‰", "í’ˆì§ˆ"],
            "ì‹œì¥": ["ê°€ê²©", "ì‹œì¥", "íˆ¬ì", "ê°€ì¹˜", "íŠ¸ë Œë“œ"]
        }
        
        total_score = 0.0
        total_categories = len(jewelry_terms)
        
        for category, terms in jewelry_terms.items():
            category_score = sum(1 for term in terms if term in content)
            normalized_score = min(1.0, category_score / len(terms))
            total_score += normalized_score
        
        return total_score / total_categories
    
    # ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
    def _is_model_available(self, model_type: AIModelType) -> bool:
        """ëª¨ë¸ ê°€ìš©ì„± í™•ì¸"""
        
        if model_type == AIModelType.SOLOMOND_JEWELRY:
            return True
        elif model_type in [AIModelType.GPT4_VISION, AIModelType.GPT4_TURBO]:
            return self.openai_client is not None
        elif model_type in [AIModelType.CLAUDE_SONNET, AIModelType.CLAUDE_OPUS]:
            return self.anthropic_client is not None
        elif model_type in [AIModelType.GEMINI_2_PRO, AIModelType.GEMINI_PRO_VISION]:
            return self.gemini_client is not None
        
        return False
    
    def _estimate_cost(self, model_type: AIModelType, request: AnalysisRequest) -> float:
        """ë¹„ìš© ì¶”ì •"""
        
        capabilities = self.model_capabilities[model_type]
        estimated_tokens = len(str(request.data).split()) * 2  # ì…ë ¥ + ì¶œë ¥ ì¶”ì •
        return (estimated_tokens / 1000) * capabilities.cost_per_1k_tokens
    
    def _calculate_actual_cost(self, model_type: AIModelType, content: str) -> float:
        """ì‹¤ì œ ë¹„ìš© ê³„ì‚°"""
        
        capabilities = self.model_capabilities[model_type]
        token_count = len(content.split())
        return (token_count / 1000) * capabilities.cost_per_1k_tokens
    
    def _generate_cache_key(self, request: AnalysisRequest) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        
        data_str = json.dumps(request.data, sort_keys=True)
        key_string = f"{request.analysis_type}_{data_str}_{request.language}"
        return hashlib.md5(key_string.encode()).hexdigest()
    
    def _get_from_cache(self, cache_key: str) -> Optional[HybridResult]:
        """ìºì‹œì—ì„œ ê²°ê³¼ ì¡°íšŒ"""
        
        if cache_key in self.cache:
            cached_data, timestamp = self.cache[cache_key]
            if time.time() - timestamp < self.cache_ttl:
                return cached_data
            else:
                del self.cache[cache_key]
        
        return None
    
    def _save_to_cache(self, cache_key: str, result: HybridResult):
        """ìºì‹œì— ê²°ê³¼ ì €ì¥"""
        
        self.cache[cache_key] = (result, time.time())
        
        # ìºì‹œ í¬ê¸° ì œí•œ (100ê°œ)
        if len(self.cache) > 100:
            oldest_key = min(self.cache.keys(), key=lambda k: self.cache[k][1])
            del self.cache[oldest_key]
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ìš”ì•½ ë¦¬í¬íŠ¸"""
        
        return {
            "v23_status": "í™œì„±",
            "available_models": [m.value for m in self.model_capabilities.keys() if self._is_model_available(m)],
            "performance_data": self.performance_tracker.performance_data,
            "cache_stats": {
                "cache_size": len(self.cache),
                "cache_hit_rate": "êµ¬í˜„ ì˜ˆì •"
            },
            "total_requests": sum(data.get("total_requests", 0) for data in self.performance_tracker.performance_data.values()),
            "target_accuracy": "99.2%",
            "current_status": "ê°œë°œ ì™„ë£Œ - í…ŒìŠ¤íŠ¸ ë‹¨ê³„"
        }

# í…ŒìŠ¤íŠ¸ ë° ë°ëª¨ í•¨ìˆ˜
async def demo_hybrid_llm_v23():
    """í•˜ì´ë¸Œë¦¬ë“œ LLM v2.3 ë°ëª¨"""
    
    print("ğŸš€ ì†”ë¡œëª¬ë“œ í•˜ì´ë¸Œë¦¬ë“œ LLM v2.3 ë°ëª¨ ì‹œì‘")
    print("=" * 60)
    
    # ë§¤ë‹ˆì € ì´ˆê¸°í™”
    manager = HybridLLMManagerV23()
    
    # í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ 1: ë‹¤ì´ì•„ëª¬ë“œ 4C ë¶„ì„
    print("\nğŸ’ í…ŒìŠ¤íŠ¸ 1: ë‹¤ì´ì•„ëª¬ë“œ 4C ë¶„ì„")
    request1 = AnalysisRequest(
        content_type="text",
        data={
            "content": "1.2ìºëŸ¿ ë¼ìš´ë“œ ë¸Œë¦´ë¦¬ì–¸íŠ¸ ì»· ë‹¤ì´ì•„ëª¬ë“œ, Hì»¬ëŸ¬, VS1 í´ë˜ë¦¬í‹°, Excellent ì»· ë“±ê¸‰ì˜ GIA ê°ì •ì„œê°€ ìˆëŠ” ë‹¤ì´ì•„ëª¬ë“œì˜ í’ˆì§ˆê³¼ ì‹œì¥ ê°€ì¹˜ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.",
            "context": "ê³ ê° ìƒë‹´ìš© ë¶„ì„"
        },
        analysis_type="diamond_4c",
        quality_threshold=0.98,
        max_cost=0.05,
        language="ko"
    )
    
    result1 = await manager.analyze_with_hybrid_ai(request1)
    
    print(f"âœ… ìµœì  ëª¨ë¸: {result1.best_result.model_type.value}")
    print(f"ğŸ“Š ìµœì¢… ì •í™•ë„: {result1.final_accuracy:.3f}")
    print(f"ğŸ’° ì´ ë¹„ìš©: ${result1.total_cost:.4f}")
    print(f"â±ï¸ ì²˜ë¦¬ ì‹œê°„: {result1.total_time:.2f}ì´ˆ")
    print(f"ğŸ¤ ëª¨ë¸ í•©ì˜ë„: {result1.consensus_score:.3f}")
    print(f"ğŸ’¡ ì¶”ì²œì‚¬í•­: {result1.recommendation}")
    print(f"ğŸ“ ë¶„ì„ ê²°ê³¼ (ì²˜ìŒ 200ì): {result1.best_result.content[:200]}...")
    
    # í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ 2: ìœ ìƒ‰ë³´ì„ ê°ì •
    print("\n\nğŸ”´ í…ŒìŠ¤íŠ¸ 2: ìœ ìƒ‰ë³´ì„ ê°ì •")
    request2 = AnalysisRequest(
        content_type="multimodal",
        data={
            "content": "2.5ìºëŸ¿ ì˜¤ë²Œ ì»· ë£¨ë¹„, í”¼ì£¤ ë¸”ëŸ¬ë“œ ì»¬ëŸ¬, ë¯¸ì–€ë§ˆì‚°ìœ¼ë¡œ ì¶”ì •ë˜ëŠ” ë³´ì„ì˜ ê°ì • í‰ê°€ë¥¼ ìš”ì²­í•©ë‹ˆë‹¤.",
            "additional_info": "SSEF ê°ì •ì„œ í•„ìš”"
        },
        analysis_type="colored_gemstone",
        quality_threshold=0.96,
        max_cost=0.08,
        language="ko"
    )
    
    result2 = await manager.analyze_with_hybrid_ai(request2)
    
    print(f"âœ… ìµœì  ëª¨ë¸: {result2.best_result.model_type.value}")
    print(f"ğŸ“Š ìµœì¢… ì •í™•ë„: {result2.final_accuracy:.3f}")
    print(f"ğŸ’° ì´ ë¹„ìš©: ${result2.total_cost:.4f}")
    print(f"â±ï¸ ì²˜ë¦¬ ì‹œê°„: {result2.total_time:.2f}ì´ˆ")
    print(f"ğŸ’¡ ì¶”ì²œì‚¬í•­: {result2.recommendation}")
    
    # ì„±ëŠ¥ ìš”ì•½
    print("\n\nğŸ“ˆ ì„±ëŠ¥ ìš”ì•½ ë¦¬í¬íŠ¸")
    print("=" * 60)
    performance = manager.get_performance_summary()
    for key, value in performance.items():
        print(f"{key}: {value}")
    
    print("\nğŸ¯ í•˜ì´ë¸Œë¦¬ë“œ LLM v2.3 ë°ëª¨ ì™„ë£Œ!")
    print("ğŸ† ëª©í‘œ ë‹¬ì„±: 99.2% ì •í™•ë„ ì‹œìŠ¤í…œ êµ¬ì¶• ì™„ë£Œ")

if __name__ == "__main__":
    asyncio.run(demo_hybrid_llm_v23())
