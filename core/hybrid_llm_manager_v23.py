"""
ğŸ§  ì†”ë¡œëª¬ë“œ í•˜ì´ë¸Œë¦¬ë“œ LLM ë§¤ë‹ˆì € v2.3
ì°¨ì„¸ëŒ€ ì£¼ì–¼ë¦¬ AI ë¶„ì„ì„ ìœ„í•œ ë‹¤ì¤‘ LLM í†µí•© ì‹œìŠ¤í…œ

ğŸ“… ê°œë°œì¼: 2025.07.13
ğŸ¯ ëª©í‘œ: 99.2% ì •í™•ë„ ë‹¬ì„±
ğŸ”¥ ì£¼ìš” ê¸°ëŠ¥:
- GPT-4V + Claude Vision + Gemini 2.0 ë™ì‹œ í™œìš©
- ì‹¤ì‹œê°„ ëª¨ë¸ ì„ íƒ ì•Œê³ ë¦¬ì¦˜
- ì£¼ì–¼ë¦¬ íŠ¹í™” í”„ë¡¬í”„íŠ¸ ìµœì í™”
- ì‹¤ì‹œê°„ í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ
- A/B í…ŒìŠ¤íŠ¸ ìë™í™”

ê¸°ì¡´ ì‹œìŠ¤í…œê³¼ ì™„ì „ í˜¸í™˜:
- core/jewelry_ai_engine.py (37KB)
- core/multimodal_integrator.py (31KB) 
- core/advanced_llm_summarizer_complete.py (17KB)
"""

import asyncio
import logging
import time
import json
import hashlib
import statistics
from typing import Dict, List, Optional, Any, Union, Tuple
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
import numpy as np
from datetime import datetime, timedelta

# ê³ ê¸‰ ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('HybridLLM_v23')

class AIModelType(Enum):
    """ì°¨ì„¸ëŒ€ AI ëª¨ë¸ íƒ€ì…"""
    GPT4V = "gpt-4-vision-preview"
    CLAUDE_VISION = "claude-3-sonnet-20240229"
    GEMINI_2_0 = "gemini-2.0-flash-exp"
    JEWELRY_SPECIALIZED = "jewelry_ai_v22"
    OPENAI_GPT4_TURBO = "gpt-4-turbo-preview"
    QUALITY_VALIDATOR = "quality_validation_ai"
    BUSINESS_INSIGHTS = "business_intelligence_ai"

@dataclass
class ModelCapability:
    """ëª¨ë¸ ì—­ëŸ‰ ì •ì˜"""
    vision_analysis: float = 0.0
    text_processing: float = 0.0
    jewelry_expertise: float = 0.0
    speed: float = 0.0
    cost_efficiency: float = 0.0
    reliability: float = 0.0
    multimodal_fusion: float = 0.0

@dataclass 
class AIModelConfig:
    """ê³ ê¸‰ AI ëª¨ë¸ ì„¤ì •"""
    model_type: AIModelType
    api_endpoint: str
    api_key: Optional[str] = None
    max_tokens: int = 8000
    temperature: float = 0.1  # ì •í™•ë„ ìš°ì„ 
    top_p: float = 0.9
    frequency_penalty: float = 0.2
    presence_penalty: float = 0.1
    
    # v2.3 ìƒˆë¡œìš´ ë§¤ê°œë³€ìˆ˜
    jewelry_weight: float = 1.0
    accuracy_threshold: float = 0.99
    response_time_limit: float = 15.0
    cost_per_1k_tokens: float = 0.01
    
    # ëª¨ë¸ íŠ¹í™” ì—­ëŸ‰
    capabilities: ModelCapability = field(default_factory=ModelCapability)
    
    # í”„ë¡¬í”„íŠ¸ ìµœì í™”
    system_prompt_template: str = ""
    user_prompt_template: str = ""
    
    # í’ˆì§ˆ ê²€ì¦
    quality_validation_enabled: bool = True
    retry_on_failure: bool = True
    max_retries: int = 3

@dataclass
class AnalysisRequest:
    """ë¶„ì„ ìš”ì²­ êµ¬ì¡°"""
    request_id: str
    input_data: Dict[str, Any]
    analysis_type: str
    priority: str = "normal"  # low, normal, high, critical
    quality_requirement: float = 0.95
    time_limit: float = 30.0
    cost_budget: float = 0.10
    
    # ì»¨í…ìŠ¤íŠ¸ ì •ë³´
    user_context: Dict[str, Any] = field(default_factory=dict)
    business_context: Dict[str, Any] = field(default_factory=dict)
    historical_context: List[Dict[str, Any]] = field(default_factory=list)

@dataclass
class AnalysisResult:
    """ê³ ê¸‰ ë¶„ì„ ê²°ê³¼"""
    request_id: str
    model_type: AIModelType
    content: str
    
    # í’ˆì§ˆ ì§€í‘œ
    confidence_score: float
    jewelry_relevance_score: float
    accuracy_prediction: float
    completeness_score: float
    coherence_score: float
    
    # ì„±ëŠ¥ ì§€í‘œ
    processing_time: float
    token_usage: int
    cost: float
    
    # ë©”íƒ€ë°ì´í„°
    timestamp: datetime
    model_version: str
    quality_checks_passed: bool
    validation_results: Dict[str, Any] = field(default_factory=dict)
    
    # ê°œì„  ì œì•ˆ
    improvement_suggestions: List[str] = field(default_factory=list)
    alternative_models: List[AIModelType] = field(default_factory=list)

@dataclass
class QualityMetrics:
    """í’ˆì§ˆ ì¸¡ì • ì§€í‘œ"""
    accuracy: float = 0.0
    precision: float = 0.0
    recall: float = 0.0
    f1_score: float = 0.0
    jewelry_domain_accuracy: float = 0.0
    user_satisfaction: float = 0.0
    expert_validation: float = 0.0

class JewelryPromptOptimizer:
    """ì£¼ì–¼ë¦¬ íŠ¹í™” í”„ë¡¬í”„íŠ¸ ìµœì í™”ê¸°"""
    
    def __init__(self):
        self.jewelry_terminology = {
            "diamond": ["ë‹¤ì´ì•„ëª¬ë“œ", "diamond", "carat", "ìºëŸ¿", "4C", "GIA", "cut", "color", "clarity"],
            "ruby": ["ë£¨ë¹„", "ruby", "pigeon blood", "ë²„ë§ˆ", "Myanmar"],
            "sapphire": ["ì‚¬íŒŒì´ì–´", "sapphire", "Kashmir", "Ceylon", "cornflower"],
            "emerald": ["ì—ë©”ë„ë“œ", "emerald", "Colombia", "Zambia", "jardin"],
            "general": ["ë³´ì„", "gemstone", "jewelry", "ì£¼ì–¼ë¦¬", "ê°ì •", "appraisal", "certification"]
        }
        
        self.grading_standards = {
            "GIA": "Gemological Institute of America í‘œì¤€",
            "AGS": "American Gem Society í‘œì¤€", 
            "SSEF": "Swiss Gemmological Institute í‘œì¤€",
            "GÃ¼belin": "GÃ¼belin Gem Lab í‘œì¤€"
        }
        
        self.market_context = {
            "investment": "íˆ¬ì ê´€ì  ë¶„ì„",
            "retail": "ì†Œë§¤ íŒë§¤ ê´€ì ",
            "insurance": "ë³´í—˜ í‰ê°€ ê´€ì ",
            "collection": "ìˆ˜ì§‘ ê°€ì¹˜ ê´€ì "
        }
    
    def optimize_prompt(self, base_prompt: str, context: Dict[str, Any]) -> str:
        """í”„ë¡¬í”„íŠ¸ ìµœì í™”"""
        
        # 1. ì£¼ì–¼ë¦¬ ì „ë¬¸ ìš©ì–´ ê°•í™”
        enhanced_prompt = self._enhance_jewelry_terminology(base_prompt, context)
        
        # 2. ë¶„ì„ ê¸°ì¤€ ëª…í™•í™”
        enhanced_prompt = self._add_grading_standards(enhanced_prompt, context)
        
        # 3. ë¹„ì¦ˆë‹ˆìŠ¤ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
        enhanced_prompt = self._add_business_context(enhanced_prompt, context)
        
        # 4. ì •í™•ë„ í–¥ìƒ ì§€ì‹œì‚¬í•­
        enhanced_prompt = self._add_accuracy_instructions(enhanced_prompt)
        
        return enhanced_prompt
    
    def _enhance_jewelry_terminology(self, prompt: str, context: Dict[str, Any]) -> str:
        """ì£¼ì–¼ë¦¬ ì „ë¬¸ ìš©ì–´ ê°•í™”"""
        gem_type = context.get("gem_type", "general")
        terminology = self.jewelry_terminology.get(gem_type, self.jewelry_terminology["general"])
        
        enhancement = f"""
        
### ì£¼ì–¼ë¦¬ ì „ë¬¸ ë¶„ì„ ìš”êµ¬ì‚¬í•­:
- ê´€ë ¨ ì „ë¬¸ ìš©ì–´ í™œìš©: {', '.join(terminology)}
- ì—…ê³„ í‘œì¤€ ê¸°ì¤€ ì ìš©
- ì •í™•í•œ ê°ì • ìš©ì–´ ì‚¬ìš©
        """
        
        return prompt + enhancement
    
    def _add_grading_standards(self, prompt: str, context: Dict[str, Any]) -> str:
        """ê°ì • ê¸°ì¤€ ëª…í™•í™”"""
        preferred_standard = context.get("grading_standard", "GIA")
        standard_desc = self.grading_standards.get(preferred_standard, "êµ­ì œ í‘œì¤€")
        
        enhancement = f"""
        
### ê°ì • ê¸°ì¤€:
- ì£¼ìš” ê¸°ì¤€: {standard_desc}
- ì •í™•ë„ ìš”êµ¬ìˆ˜ì¤€: 99.2% ì´ìƒ
- ê·¼ê±° ì œì‹œ í•„ìˆ˜
        """
        
        return prompt + enhancement
    
    def _add_business_context(self, prompt: str, context: Dict[str, Any]) -> str:
        """ë¹„ì¦ˆë‹ˆìŠ¤ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€"""
        market_context = context.get("market_context", "general")
        business_focus = self.market_context.get(market_context, "ì¢…í•©ì  ê´€ì ")
        
        enhancement = f"""
        
### ë¹„ì¦ˆë‹ˆìŠ¤ ê´€ì :
- ë¶„ì„ ê´€ì : {business_focus}
- ì‹œì¥ ê°€ì¹˜ ê³ ë ¤
- ì‹¤ë¬´ì  ì¡°ì–¸ í¬í•¨
        """
        
        return prompt + enhancement
    
    def _add_accuracy_instructions(self, prompt: str) -> str:
        """ì •í™•ë„ í–¥ìƒ ì§€ì‹œì‚¬í•­"""
        enhancement = """
        
### ì •í™•ë„ ìµœì í™” ì§€ì‹œì‚¬í•­:
1. ëª¨ë“  ë¶„ì„ì€ ëª…í™•í•œ ê·¼ê±°ì™€ í•¨ê»˜ ì œì‹œ
2. ë¶ˆí™•ì‹¤í•œ ë‚´ìš©ì€ ëª…ì‹œì ìœ¼ë¡œ í‘œê¸°
3. ì—¬ëŸ¬ ê°€ëŠ¥ì„±ì´ ìˆëŠ” ê²½ìš° í™•ë¥ ê³¼ í•¨ê»˜ ì œì‹œ
4. ì „ë¬¸ê°€ ìˆ˜ì¤€ì˜ ê¹Šì´ ìˆëŠ” ë¶„ì„ ìˆ˜í–‰
5. ì—…ê³„ ìµœì‹  ë™í–¥ ë° í‘œì¤€ ë°˜ì˜
        
**ëª©í‘œ ì •í™•ë„: 99.2% ì´ìƒ**
        """
        
        return prompt + enhancement

class QualityValidationSystem:
    """ì‹¤ì‹œê°„ í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.quality_threshold = 0.95
        self.validation_models = ["content_validator", "jewelry_expert_validator", "consistency_checker"]
        
    async def validate_result(self, result: AnalysisResult, original_request: AnalysisRequest) -> Dict[str, Any]:
        """ì¢…í•©ì  í’ˆì§ˆ ê²€ì¦"""
        
        validation_results = {
            "overall_quality": 0.0,
            "content_quality": 0.0,
            "jewelry_expertise": 0.0,
            "consistency": 0.0,
            "completeness": 0.0,
            "accuracy_prediction": 0.0,
            "validation_passed": False,
            "improvement_areas": [],
            "confidence_level": "low"
        }
        
        # 1. ì½˜í…ì¸  í’ˆì§ˆ ê²€ì¦
        content_score = await self._validate_content_quality(result.content)
        validation_results["content_quality"] = content_score
        
        # 2. ì£¼ì–¼ë¦¬ ì „ë¬¸ì„± ê²€ì¦
        expertise_score = await self._validate_jewelry_expertise(result.content, original_request)
        validation_results["jewelry_expertise"] = expertise_score
        
        # 3. ì¼ê´€ì„± ê²€ì¦
        consistency_score = await self._validate_consistency(result, original_request)
        validation_results["consistency"] = consistency_score
        
        # 4. ì™„ì„±ë„ ê²€ì¦
        completeness_score = await self._validate_completeness(result.content, original_request)
        validation_results["completeness"] = completeness_score
        
        # 5. ì •í™•ë„ ì˜ˆì¸¡
        accuracy_prediction = await self._predict_accuracy(result)
        validation_results["accuracy_prediction"] = accuracy_prediction
        
        # ì¢…í•© í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        overall_quality = (
            content_score * 0.25 + 
            expertise_score * 0.30 + 
            consistency_score * 0.20 + 
            completeness_score * 0.15 + 
            accuracy_prediction * 0.10
        )
        
        validation_results["overall_quality"] = overall_quality
        validation_results["validation_passed"] = overall_quality >= self.quality_threshold
        
        # ì‹ ë¢°ë„ ìˆ˜ì¤€ ê²°ì •
        if overall_quality >= 0.95:
            validation_results["confidence_level"] = "very_high"
        elif overall_quality >= 0.90:
            validation_results["confidence_level"] = "high"
        elif overall_quality >= 0.80:
            validation_results["confidence_level"] = "medium"
        else:
            validation_results["confidence_level"] = "low"
        
        return validation_results
    
    async def _validate_content_quality(self, content: str) -> float:
        """ì½˜í…ì¸  í’ˆì§ˆ ê²€ì¦"""
        if not content or len(content.strip()) < 50:
            return 0.1
        
        # ê¸°ë³¸ í’ˆì§ˆ ì§€í‘œ
        score = 0.5
        
        # ê¸¸ì´ ì ì •ì„± (100-2000ì ì ì •)
        content_length = len(content)
        if 100 <= content_length <= 2000:
            score += 0.2
        elif content_length > 2000:
            score += 0.1
        
        # êµ¬ì¡°ì  ì™„ì„±ë„ (ë‹¨ë½, ë¬¸ì¥ êµ¬ì¡°)
        paragraphs = content.split('\n\n')
        if len(paragraphs) >= 2:
            score += 0.15
        
        # ì „ë¬¸ ìš©ì–´ ì‚¬ìš©ë„
        jewelry_terms = ["ë‹¤ì´ì•„ëª¬ë“œ", "ë£¨ë¹„", "ì‚¬íŒŒì´ì–´", "ì—ë©”ë„ë“œ", "GIA", "4C", "ìºëŸ¿", "ë“±ê¸‰"]
        term_count = sum(1 for term in jewelry_terms if term in content)
        if term_count >= 3:
            score += 0.15
        
        return min(1.0, score)
    
    async def _validate_jewelry_expertise(self, content: str, request: AnalysisRequest) -> float:
        """ì£¼ì–¼ë¦¬ ì „ë¬¸ì„± ê²€ì¦"""
        expertise_indicators = [
            "ê°ì •", "ë“±ê¸‰", "í’ˆì§ˆ", "ê°€ì¹˜", "ì‹œì¥", "íˆ¬ì",
            "GIA", "AGS", "SSEF", "4C", "ìºëŸ¿", "ì»·", "ì»¬ëŸ¬", "í´ë˜ë¦¬í‹°"
        ]
        
        content_lower = content.lower()
        indicator_count = sum(1 for indicator in expertise_indicators 
                            if indicator.lower() in content_lower)
        
        # ê¸°ë³¸ ì ìˆ˜
        expertise_score = min(1.0, indicator_count / 8)
        
        # ë¶„ì„ íƒ€ì…ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜
        analysis_type = request.analysis_type
        if analysis_type in ["diamond_grading", "jewelry_appraisal", "gemstone_analysis"]:
            if indicator_count >= 5:
                expertise_score += 0.2
        
        return min(1.0, expertise_score)
    
    async def _validate_consistency(self, result: AnalysisResult, request: AnalysisRequest) -> float:
        """ì¼ê´€ì„± ê²€ì¦"""
        # ìš”ì²­ê³¼ ê²°ê³¼ì˜ ì¼ê´€ì„± í™•ì¸
        consistency_score = 0.8  # ê¸°ë³¸ê°’
        
        # ë¶„ì„ íƒ€ì… ì¼ì¹˜ í™•ì¸
        analysis_type = request.analysis_type
        content = result.content.lower()
        
        type_keywords = {
            "diamond_analysis": ["ë‹¤ì´ì•„ëª¬ë“œ", "diamond", "4c"],
            "ruby_analysis": ["ë£¨ë¹„", "ruby"],
            "jewelry_appraisal": ["ê°ì •", "appraisal", "ê°€ì¹˜"],
            "market_analysis": ["ì‹œì¥", "market", "ê°€ê²©"]
        }
        
        if analysis_type in type_keywords:
            keywords = type_keywords[analysis_type]
            if any(keyword in content for keyword in keywords):
                consistency_score += 0.2
        
        return min(1.0, consistency_score)
    
    async def _validate_completeness(self, content: str, request: AnalysisRequest) -> float:
        """ì™„ì„±ë„ ê²€ì¦"""
        required_elements = {
            "jewelry_analysis": ["íŠ¹ì„±", "í’ˆì§ˆ", "ê°€ì¹˜", "ê¶Œì¥ì‚¬í•­"],
            "diamond_grading": ["ì»·", "ì»¬ëŸ¬", "í´ë˜ë¦¬í‹°", "ìºëŸ¿", "ë“±ê¸‰"],
            "market_analysis": ["í˜„ì¬ê°€ì¹˜", "ì‹œì¥ë™í–¥", "íˆ¬ìì „ë§"]
        }
        
        analysis_type = request.analysis_type
        if analysis_type not in required_elements:
            return 0.8  # ê¸°ë³¸ ì™„ì„±ë„
        
        required = required_elements[analysis_type]
        content_lower = content.lower()
        
        found_elements = sum(1 for element in required 
                           if element in content_lower)
        
        completeness_score = found_elements / len(required)
        return completeness_score
    
    async def _predict_accuracy(self, result: AnalysisResult) -> float:
        """ì •í™•ë„ ì˜ˆì¸¡ (ê³ ê¸‰ ì•Œê³ ë¦¬ì¦˜)"""
        # ì—¬ëŸ¬ ì§€í‘œë¥¼ ì¢…í•©í•œ ì •í™•ë„ ì˜ˆì¸¡
        factors = []
        
        # 1. ëª¨ë¸ ì‹ ë¢°ë„
        factors.append(result.confidence_score)
        
        # 2. ì£¼ì–¼ë¦¬ ê´€ë ¨ì„±
        factors.append(result.jewelry_relevance_score)
        
        # 3. ì½˜í…ì¸  ê¸¸ì´ ì ì •ì„±
        content_length = len(result.content)
        length_score = min(1.0, content_length / 1000) if content_length < 1000 else 1.0
        factors.append(length_score)
        
        # 4. ì²˜ë¦¬ ì‹œê°„ (ë„ˆë¬´ ë¹ ë¥´ë©´ ë¶€ì‹¤í•  ìˆ˜ ìˆìŒ)
        time_score = min(1.0, result.processing_time / 5.0) if result.processing_time < 5.0 else 1.0
        factors.append(time_score)
        
        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ì •í™•ë„ ì˜ˆì¸¡
        weights = [0.4, 0.3, 0.2, 0.1]
        predicted_accuracy = sum(f * w for f, w in zip(factors, weights))
        
        return min(0.99, predicted_accuracy)  # ìµœëŒ€ 99% ì˜ˆì¸¡

class PerformanceBenchmark:
    """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë° A/B í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.benchmark_results = {}
        self.a_b_test_results = {}
        self.performance_history = []
        
    async def run_model_benchmark(self, models: List[AIModelType], 
                                test_cases: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ëª¨ë¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        
        benchmark_results = {
            "test_timestamp": datetime.now().isoformat(),
            "models_tested": [model.value for model in models],
            "test_cases_count": len(test_cases),
            "results": {},
            "performance_ranking": [],
            "recommendations": []
        }
        
        for model in models:
            model_results = {
                "accuracy_scores": [],
                "response_times": [],
                "jewelry_relevance_scores": [],
                "cost_efficiency": [],
                "overall_score": 0.0
            }
            
            for test_case in test_cases:
                # ì—¬ê¸°ì„œ ì‹¤ì œ ëª¨ë¸ í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰
                # (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ê° ëª¨ë¸ì— ëŒ€í•´ ì‹¤ì œ API í˜¸ì¶œ)
                
                # ì‹œë®¬ë ˆì´ì…˜ëœ ê²°ê³¼
                accuracy = np.random.normal(0.92, 0.05)  # í‰ê·  92%, í‘œì¤€í¸ì°¨ 5%
                response_time = np.random.normal(8.0, 2.0)  # í‰ê·  8ì´ˆ
                jewelry_relevance = np.random.normal(0.85, 0.1)
                cost = np.random.normal(0.05, 0.01)
                
                model_results["accuracy_scores"].append(accuracy)
                model_results["response_times"].append(response_time)
                model_results["jewelry_relevance_scores"].append(jewelry_relevance)
                model_results["cost_efficiency"].append(1.0 / cost if cost > 0 else 1.0)
            
            # í†µê³„ ê³„ì‚°
            model_results["avg_accuracy"] = statistics.mean(model_results["accuracy_scores"])
            model_results["avg_response_time"] = statistics.mean(model_results["response_times"])
            model_results["avg_jewelry_relevance"] = statistics.mean(model_results["jewelry_relevance_scores"])
            model_results["avg_cost_efficiency"] = statistics.mean(model_results["cost_efficiency"])
            
            # ì¢…í•© ì ìˆ˜ ê³„ì‚° (99.2% ëª©í‘œ ê¸°ì¤€)
            accuracy_score = min(1.0, model_results["avg_accuracy"] / 0.992)
            speed_score = max(0.0, 1.0 - (model_results["avg_response_time"] - 5.0) / 10.0)
            relevance_score = model_results["avg_jewelry_relevance"]
            cost_score = min(1.0, model_results["avg_cost_efficiency"] / 20.0)
            
            overall_score = (
                accuracy_score * 0.4 +
                speed_score * 0.25 +
                relevance_score * 0.25 +
                cost_score * 0.1
            )
            
            model_results["overall_score"] = overall_score
            benchmark_results["results"][model.value] = model_results
        
        # ì„±ëŠ¥ ìˆœìœ„
        sorted_models = sorted(benchmark_results["results"].items(), 
                             key=lambda x: x[1]["overall_score"], reverse=True)
        benchmark_results["performance_ranking"] = [
            {"model": model, "score": results["overall_score"]} 
            for model, results in sorted_models
        ]
        
        # ê¶Œì¥ì‚¬í•­ ìƒì„±
        best_model = sorted_models[0][0]
        best_score = sorted_models[0][1]["overall_score"]
        
        recommendations = [
            f"ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model} (ì ìˆ˜: {best_score:.3f})",
            f"99.2% ì •í™•ë„ ëª©í‘œ {'ë‹¬ì„±' if best_score >= 0.95 else 'ë¯¸ë‹¬ì„±'}",
        ]
        
        if best_score < 0.95:
            recommendations.append("ì¶”ê°€ ìµœì í™” í•„ìš”: í”„ë¡¬í”„íŠ¸ ê°œì„ , ëª¨ë¸ íŒŒì¸íŠœë‹ ê²€í† ")
        
        benchmark_results["recommendations"] = recommendations
        
        return benchmark_results
    
    async def run_a_b_test(self, model_a: AIModelType, model_b: AIModelType,
                          test_duration_days: int = 7) -> Dict[str, Any]:
        """A/B í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        
        test_results = {
            "test_start": datetime.now().isoformat(),
            "model_a": model_a.value,
            "model_b": model_b.value,
            "duration_days": test_duration_days,
            "model_a_metrics": {},
            "model_b_metrics": {},
            "statistical_significance": False,
            "winner": None,
            "confidence_level": 0.0
        }
        
        # ì‹œë®¬ë ˆì´ì…˜ëœ A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼
        # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì‹¤ì œ ì‚¬ìš©ì ë°ì´í„°ë¥¼ ìˆ˜ì§‘
        
        # ëª¨ë¸ A ê²°ê³¼ (ì‹œë®¬ë ˆì´ì…˜)
        model_a_accuracy = np.random.normal(0.94, 0.02, 100)
        model_a_satisfaction = np.random.normal(0.87, 0.05, 100)
        model_a_response_time = np.random.normal(7.5, 1.5, 100)
        
        # ëª¨ë¸ B ê²°ê³¼ (ì‹œë®¬ë ˆì´ì…˜)
        model_b_accuracy = np.random.normal(0.96, 0.015, 100)
        model_b_satisfaction = np.random.normal(0.89, 0.04, 100)
        model_b_response_time = np.random.normal(8.2, 1.2, 100)
        
        test_results["model_a_metrics"] = {
            "avg_accuracy": float(np.mean(model_a_accuracy)),
            "avg_satisfaction": float(np.mean(model_a_satisfaction)),
            "avg_response_time": float(np.mean(model_a_response_time)),
            "sample_size": len(model_a_accuracy)
        }
        
        test_results["model_b_metrics"] = {
            "avg_accuracy": float(np.mean(model_b_accuracy)),
            "avg_satisfaction": float(np.mean(model_b_satisfaction)),
            "avg_response_time": float(np.mean(model_b_response_time)),
            "sample_size": len(model_b_accuracy)
        }
        
        # í†µê³„ì  ìœ ì˜ì„± ê²€ì • (ê°„ë‹¨í•œ ë²„ì „)
        accuracy_diff = abs(test_results["model_a_metrics"]["avg_accuracy"] - 
                          test_results["model_b_metrics"]["avg_accuracy"])
        
        if accuracy_diff > 0.01:  # 1% ì´ìƒ ì°¨ì´
            test_results["statistical_significance"] = True
            test_results["confidence_level"] = 0.95
            
            if test_results["model_a_metrics"]["avg_accuracy"] > test_results["model_b_metrics"]["avg_accuracy"]:
                test_results["winner"] = model_a.value
            else:
                test_results["winner"] = model_b.value
        
        return test_results

class HybridLLMManagerV23:
    """ì°¨ì„¸ëŒ€ í•˜ì´ë¸Œë¦¬ë“œ LLM ë§¤ë‹ˆì € v2.3"""
    
    def __init__(self, config_path: Optional[str] = None):
        self.version = "2.3.0"
        self.target_accuracy = 0.992  # 99.2% ëª©í‘œ
        
        # í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.prompt_optimizer = JewelryPromptOptimizer()
        self.quality_validator = QualityValidationSystem()
        self.benchmark_system = PerformanceBenchmark()
        
        # ëª¨ë¸ ì„¤ì •
        self.models = self._initialize_model_configs()
        self.active_models = {}
        
        # ì„±ëŠ¥ ì¶”ì 
        self.performance_metrics = {
            "total_requests": 0,
            "successful_requests": 0,
            "accuracy_scores": [],
            "response_times": [],
            "cost_tracking": {},
            "model_performance": {},
            "quality_improvements": []
        }
        
        # ê¸°ì¡´ ì‹œìŠ¤í…œê³¼ì˜ í†µí•©
        self.legacy_integration = self._setup_legacy_integration()
        
        logger.info(f"ğŸš€ í•˜ì´ë¸Œë¦¬ë“œ LLM ë§¤ë‹ˆì € v{self.version} ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"ğŸ¯ ëª©í‘œ ì •í™•ë„: {self.target_accuracy * 100}%")
    
    def _initialize_model_configs(self) -> Dict[AIModelType, AIModelConfig]:
        """ê³ ê¸‰ ëª¨ë¸ ì„¤ì • ì´ˆê¸°í™”"""
        
        configs = {
            AIModelType.GPT4V: AIModelConfig(
                model_type=AIModelType.GPT4V,
                api_endpoint="https://api.openai.com/v1/chat/completions",
                max_tokens=4000,
                temperature=0.1,
                jewelry_weight=1.5,
                cost_per_1k_tokens=0.03,
                capabilities=ModelCapability(
                    vision_analysis=0.95,
                    text_processing=0.92,
                    jewelry_expertise=0.75,
                    speed=0.80,
                    cost_efficiency=0.70,
                    reliability=0.93,
                    multimodal_fusion=0.90
                )
            ),
            
            AIModelType.CLAUDE_VISION: AIModelConfig(
                model_type=AIModelType.CLAUDE_VISION,
                api_endpoint="https://api.anthropic.com/v1/messages",
                max_tokens=4000,
                temperature=0.1,
                jewelry_weight=1.3,
                cost_per_1k_tokens=0.015,
                capabilities=ModelCapability(
                    vision_analysis=0.88,
                    text_processing=0.95,
                    jewelry_expertise=0.70,
                    speed=0.85,
                    cost_efficiency=0.85,
                    reliability=0.91,
                    multimodal_fusion=0.85
                )
            ),
            
            AIModelType.GEMINI_2_0: AIModelConfig(
                model_type=AIModelType.GEMINI_2_0,
                api_endpoint="https://generativelanguage.googleapis.com/v1/models/gemini-2.0-flash-exp",
                max_tokens=8000,
                temperature=0.1,
                jewelry_weight=1.2,
                cost_per_1k_tokens=0.002,
                capabilities=ModelCapability(
                    vision_analysis=0.87,
                    text_processing=0.89,
                    jewelry_expertise=0.60,
                    speed=0.95,
                    cost_efficiency=0.98,
                    reliability=0.88,
                    multimodal_fusion=0.92
                )
            ),
            
            AIModelType.JEWELRY_SPECIALIZED: AIModelConfig(
                model_type=AIModelType.JEWELRY_SPECIALIZED,
                api_endpoint="local://jewelry_ai_v22",
                max_tokens=6000,
                temperature=0.05,
                jewelry_weight=3.0,
                cost_per_1k_tokens=0.001,
                capabilities=ModelCapability(
                    vision_analysis=0.75,
                    text_processing=0.85,
                    jewelry_expertise=0.98,
                    speed=0.90,
                    cost_efficiency=0.95,
                    reliability=0.94,
                    multimodal_fusion=0.80
                )
            )
        }
        
        return configs
    
    def _setup_legacy_integration(self) -> Dict[str, Any]:
        """ê¸°ì¡´ ì‹œìŠ¤í…œê³¼ì˜ í†µí•© ì„¤ì •"""
        integration = {
            "jewelry_ai_engine": None,
            "multimodal_integrator": None, 
            "summarizer": None,
            "available": False
        }
        
        try:
            # ê¸°ì¡´ ëª¨ë“ˆ ë™ì  import ì‹œë„
            from core.jewelry_ai_engine import JewelryAIEngine
            from core.multimodal_integrator import MultimodalIntegrator
            from core.advanced_llm_summarizer_complete import AdvancedLLMSummarizer
            
            integration["jewelry_ai_engine"] = JewelryAIEngine()
            integration["multimodal_integrator"] = MultimodalIntegrator()
            integration["summarizer"] = AdvancedLLMSummarizer()
            integration["available"] = True
            
            logger.info("âœ… ê¸°ì¡´ ì‹œìŠ¤í…œê³¼ì˜ í†µí•© ì™„ë£Œ")
            
        except ImportError as e:
            logger.warning(f"âš ï¸ ê¸°ì¡´ ëª¨ë“ˆ import ì‹¤íŒ¨: {e}")
            logger.info("ğŸ”„ ë…ë¦½ ì‹¤í–‰ ëª¨ë“œë¡œ ì „í™˜")
        
        return integration
    
    async def analyze_with_optimal_strategy(self, request: AnalysisRequest) -> AnalysisResult:
        """ìµœì  ì „ëµìœ¼ë¡œ ë¶„ì„ ìˆ˜í–‰"""
        
        start_time = time.time()
        request_id = request.request_id
        
        logger.info(f"ğŸ” ë¶„ì„ ì‹œì‘: {request_id} (íƒ€ì…: {request.analysis_type})")
        
        try:
            # 1. ì…ë ¥ ë¶„ì„ ë° ìµœì  ì „ëµ ê²°ì •
            analysis_strategy = await self._determine_analysis_strategy(request)
            
            # 2. ë‹¤ì¤‘ ëª¨ë¸ ë™ì‹œ ë¶„ì„ (í•„ìš”ì‹œ)
            if analysis_strategy["use_ensemble"]:
                result = await self._ensemble_analysis(request, analysis_strategy)
            else:
                result = await self._single_model_analysis(request, analysis_strategy)
            
            # 3. ì‹¤ì‹œê°„ í’ˆì§ˆ ê²€ì¦
            validation_results = await self.quality_validator.validate_result(result, request)
            result.validation_results = validation_results
            result.quality_checks_passed = validation_results["validation_passed"]
            
            # 4. í’ˆì§ˆ ê¸°ì¤€ ë¯¸ë‹¬ì‹œ ì¬ë¶„ì„
            if not result.quality_checks_passed and request.priority in ["high", "critical"]:
                logger.warning(f"âš ï¸ í’ˆì§ˆ ê¸°ì¤€ ë¯¸ë‹¬, ì¬ë¶„ì„ ìˆ˜í–‰: {request_id}")
                result = await self._retry_analysis_with_fallback(request, result)
            
            # 5. ì„±ëŠ¥ ì§€í‘œ ì—…ë°ì´íŠ¸
            self._update_performance_metrics(result)
            
            # 6. ê°œì„  ì œì•ˆ ìƒì„±
            result.improvement_suggestions = await self._generate_improvement_suggestions(result, validation_results)
            
            logger.info(f"âœ… ë¶„ì„ ì™„ë£Œ: {request_id} (í’ˆì§ˆ: {validation_results['overall_quality']:.3f})")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {request_id} - {e}")
            return self._create_error_result(request, str(e))
    
    async def _determine_analysis_strategy(self, request: AnalysisRequest) -> Dict[str, Any]:
        """ë¶„ì„ ì „ëµ ê²°ì •"""
        
        strategy = {
            "primary_model": AIModelType.JEWELRY_SPECIALIZED,
            "secondary_models": [],
            "use_ensemble": False,
            "quality_requirement": request.quality_requirement,
            "optimization_level": "standard"
        }
        
        # ì…ë ¥ ë°ì´í„° ë¶„ì„
        data_complexity = self._analyze_data_complexity(request.input_data)
        
        # ìš°ì„ ìˆœìœ„ë³„ ì „ëµ
        if request.priority == "critical":
            strategy["use_ensemble"] = True
            strategy["secondary_models"] = [AIModelType.GPT4V, AIModelType.CLAUDE_VISION]
            strategy["optimization_level"] = "maximum"
        elif request.priority == "high":
            if data_complexity > 0.7:
                strategy["use_ensemble"] = True
                strategy["secondary_models"] = [AIModelType.GPT4V]
            strategy["optimization_level"] = "high"
        
        # í’ˆì§ˆ ìš”êµ¬ì‚¬í•­ë³„ ì¡°ì •
        if request.quality_requirement >= 0.98:
            strategy["use_ensemble"] = True
            if AIModelType.CLAUDE_VISION not in strategy["secondary_models"]:
                strategy["secondary_models"].append(AIModelType.CLAUDE_VISION)
        
        # ë°ì´í„° íƒ€ì…ë³„ ìµœì  ëª¨ë¸ ì„ íƒ
        if "image" in request.input_data or "video" in request.input_data:
            strategy["primary_model"] = AIModelType.GPT4V
        elif "complex_analysis" in request.analysis_type:
            strategy["primary_model"] = AIModelType.CLAUDE_VISION
        
        return strategy
    
    async def _ensemble_analysis(self, request: AnalysisRequest, strategy: Dict[str, Any]) -> AnalysisResult:
        """ì•™ìƒë¸” ë¶„ì„ (ë‹¤ì¤‘ ëª¨ë¸ ë™ì‹œ í™œìš©)"""
        
        primary_model = strategy["primary_model"]
        secondary_models = strategy["secondary_models"]
        all_models = [primary_model] + secondary_models
        
        logger.info(f"ğŸ”„ ì•™ìƒë¸” ë¶„ì„ ì‹œì‘: {len(all_models)}ê°œ ëª¨ë¸ ë™ì‹œ ì‹¤í–‰")
        
        # ë™ì‹œ ë¶„ì„ ì‹¤í–‰
        tasks = []
        for model in all_models:
            task = self._execute_single_model_analysis(request, model)
            tasks.append(task)
        
        # ëª¨ë“  ëª¨ë¸ ê²°ê³¼ ìˆ˜ì§‘
        individual_results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ê²°ê³¼ í†µí•© ë° ìµœì í™”
        final_result = await self._combine_ensemble_results(
            individual_results, all_models, primary_model, request
        )
        
        return final_result
    
    async def _single_model_analysis(self, request: AnalysisRequest, strategy: Dict[str, Any]) -> AnalysisResult:
        """ë‹¨ì¼ ëª¨ë¸ ë¶„ì„"""
        
        primary_model = strategy["primary_model"]
        return await self._execute_single_model_analysis(request, primary_model)
    
    async def _execute_single_model_analysis(self, request: AnalysisRequest, model_type: AIModelType) -> AnalysisResult:
        """ê°œë³„ ëª¨ë¸ ë¶„ì„ ì‹¤í–‰"""
        
        start_time = time.time()
        
        try:
            # 1. ëª¨ë¸ë³„ ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±
            optimized_prompt = await self._generate_optimized_prompt(request, model_type)
            
            # 2. ëª¨ë¸ ì‹¤í–‰ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ê° ëª¨ë¸ì˜ API í˜¸ì¶œ)
            if model_type == AIModelType.JEWELRY_SPECIALIZED and self.legacy_integration["available"]:
                # ê¸°ì¡´ ì£¼ì–¼ë¦¬ AI ì—”ì§„ í™œìš©
                content = await self._call_jewelry_specialized_model(request, optimized_prompt)
            else:
                # ì‹œë®¬ë ˆì´ì…˜ëœ ë¶„ì„ ê²°ê³¼
                content = await self._simulate_model_response(request, model_type, optimized_prompt)
            
            # 3. ê²°ê³¼ í›„ì²˜ë¦¬ ë° í’ˆì§ˆ í–¥ìƒ
            enhanced_content = await self._enhance_analysis_result(content, request, model_type)
            
            # 4. ë©”íŠ¸ë¦­ ê³„ì‚°
            processing_time = time.time() - start_time
            confidence_score = self._calculate_confidence_score(enhanced_content, model_type)
            jewelry_relevance = self._calculate_jewelry_relevance(enhanced_content, request)
            
            result = AnalysisResult(
                request_id=request.request_id,
                model_type=model_type,
                content=enhanced_content,
                confidence_score=confidence_score,
                jewelry_relevance_score=jewelry_relevance,
                accuracy_prediction=min(0.99, confidence_score * 1.05),
                completeness_score=self._calculate_completeness_score(enhanced_content, request),
                coherence_score=self._calculate_coherence_score(enhanced_content),
                processing_time=processing_time,
                token_usage=len(enhanced_content.split()),
                cost=self._calculate_cost(model_type, enhanced_content),
                timestamp=datetime.now(),
                model_version=self.version,
                quality_checks_passed=False  # ì¶”í›„ ê²€ì¦ì—ì„œ ì„¤ì •
            )
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ {model_type.value} ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return self._create_error_result(request, str(e), model_type)
    
    async def _generate_optimized_prompt(self, request: AnalysisRequest, model_type: AIModelType) -> str:
        """ëª¨ë¸ë³„ ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        base_prompt = self._get_base_prompt_template(request.analysis_type)
        
        # ì£¼ì–¼ë¦¬ íŠ¹í™” ìµœì í™”
        context = {
            "analysis_type": request.analysis_type,
            "gem_type": request.input_data.get("gem_type", "general"),
            "grading_standard": request.user_context.get("preferred_standard", "GIA"),
            "market_context": request.business_context.get("context", "general"),
            "quality_requirement": request.quality_requirement
        }
        
        optimized_prompt = self.prompt_optimizer.optimize_prompt(base_prompt, context)
        
        # ëª¨ë¸ë³„ íŠ¹í™” ì¡°ì •
        model_specific_prompt = self._adjust_prompt_for_model(optimized_prompt, model_type)
        
        return model_specific_prompt
    
    def _get_base_prompt_template(self, analysis_type: str) -> str:
        """ë¶„ì„ íƒ€ì…ë³„ ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿"""
        
        templates = {
            "jewelry_analysis": """
ì£¼ì–¼ë¦¬ ì „ë¬¸ê°€ë¡œì„œ ë‹¤ìŒ í•­ëª©ì— ëŒ€í•œ ì¢…í•©ì ì¸ ë¶„ì„ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”:

{input_data}

ë¶„ì„ í•­ëª©:
1. ë³´ì„ì˜ íŠ¹ì„± ë° í’ˆì§ˆ í‰ê°€
2. ì‹œì¥ ê°€ì¹˜ ë° íˆ¬ì ê°€ì¹˜
3. ê°ì • ë° ë“±ê¸‰ ì˜ê²¬
4. êµ¬ë§¤/íˆ¬ì ê¶Œì¥ì‚¬í•­

ì „ë¬¸ì ì´ê³  ì •í™•í•œ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”.
            """,
            
            "diamond_grading": """
ë‹¤ì´ì•„ëª¬ë“œ ê°ì • ì „ë¬¸ê°€ë¡œì„œ 4C ê¸°ì¤€ì— ë”°ë¥¸ ì •í™•í•œ ë“±ê¸‰ì„ ì œì‹œí•´ì£¼ì„¸ìš”:

{input_data}

í‰ê°€ ê¸°ì¤€:
- Cut (ì»·): ê´‘íƒ, ëŒ€ì¹­ì„±, ë§ˆê°ë„
- Color (ì»¬ëŸ¬): GIA í‘œì¤€ ìƒ‰ìƒ ë“±ê¸‰
- Clarity (í´ë˜ë¦¬í‹°): ë‚´/ì™¸ë¶€ íŠ¹ì„±
- Carat (ìºëŸ¿): ì •í™•í•œ ì¤‘ëŸ‰

ê° í•­ëª©ë³„ ìƒì„¸ ë¶„ì„ê³¼ ì¢…í•© ë“±ê¸‰ì„ ì œì‹œí•´ì£¼ì„¸ìš”.
            """,
            
            "market_analysis": """
ì£¼ì–¼ë¦¬ ì‹œì¥ ë¶„ì„ ì „ë¬¸ê°€ë¡œì„œ ë‹¤ìŒì— ëŒ€í•œ ì‹œì¥ ë¶„ì„ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”:

{input_data}

ë¶„ì„ ë²”ìœ„:
1. í˜„ì¬ ì‹œì¥ ê°€ì¹˜ ë° ë™í–¥
2. íˆ¬ì ì „ë§ ë° ë¦¬ìŠ¤í¬
3. ìœ ì‚¬ ì œí’ˆ ë¹„êµ ë¶„ì„
4. ë§¤ë§¤ ì¶”ì²œ ê°€ê²©ëŒ€

ë°ì´í„° ê¸°ë°˜ì˜ ê°ê´€ì  ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”.
            """
        }
        
        return templates.get(analysis_type, templates["jewelry_analysis"])
    
    def _adjust_prompt_for_model(self, prompt: str, model_type: AIModelType) -> str:
        """ëª¨ë¸ë³„ í”„ë¡¬í”„íŠ¸ íŠ¹í™” ì¡°ì •"""
        
        model_adjustments = {
            AIModelType.GPT4V: {
                "prefix": "ì‹œê°ì  ì •ë³´ì™€ í…ìŠ¤íŠ¸ë¥¼ ì¢…í•©í•˜ì—¬ ë¶„ì„í•´ì£¼ì„¸ìš”.\n\n",
                "suffix": "\n\n**ì •í™•ì„±ê³¼ ì „ë¬¸ì„±ì„ ìµœìš°ì„ ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”.**"
            },
            AIModelType.CLAUDE_VISION: {
                "prefix": "ë…¼ë¦¬ì ì´ê³  ì²´ê³„ì ì¸ ë¶„ì„ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”.\n\n", 
                "suffix": "\n\n**ë‹¨ê³„ë³„ë¡œ ëª…í™•í•œ ê·¼ê±°ì™€ í•¨ê»˜ ì„¤ëª…í•´ì£¼ì„¸ìš”.**"
            },
            AIModelType.GEMINI_2_0: {
                "prefix": "ë¹ ë¥´ê³  ì •í™•í•œ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”.\n\n",
                "suffix": "\n\n**í•µì‹¬ í¬ì¸íŠ¸ë¥¼ ëª…í™•í•˜ê²Œ ì •ë¦¬í•´ì£¼ì„¸ìš”.**"
            },
            AIModelType.JEWELRY_SPECIALIZED: {
                "prefix": "ì£¼ì–¼ë¦¬ ì—…ê³„ ìµœê³  ìˆ˜ì¤€ì˜ ì „ë¬¸ ë¶„ì„ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”.\n\n",
                "suffix": "\n\n**99.2% ì •í™•ë„ ìˆ˜ì¤€ì˜ ê°ì • ì˜ê²¬ì„ ì œì‹œí•´ì£¼ì„¸ìš”.**"
            }
        }
        
        adjustment = model_adjustments.get(model_type, {"prefix": "", "suffix": ""})
        
        return adjustment["prefix"] + prompt + adjustment["suffix"]
    
    async def _call_jewelry_specialized_model(self, request: AnalysisRequest, prompt: str) -> str:
        """ì£¼ì–¼ë¦¬ íŠ¹í™” ëª¨ë¸ í˜¸ì¶œ"""
        
        if not self.legacy_integration["available"]:
            return await self._simulate_model_response(request, AIModelType.JEWELRY_SPECIALIZED, prompt)
        
        try:
            jewelry_engine = self.legacy_integration["jewelry_ai_engine"]
            
            # ê¸°ì¡´ ì—”ì§„ì˜ ë©”ì„œë“œ í™œìš©
            if hasattr(jewelry_engine, 'analyze_comprehensive'):
                return await jewelry_engine.analyze_comprehensive(request.input_data)
            elif hasattr(jewelry_engine, 'analyze'):
                return await jewelry_engine.analyze(request.input_data)
            else:
                # ê¸°ë³¸ ë¶„ì„ ë©”ì„œë“œ
                return f"ì£¼ì–¼ë¦¬ íŠ¹í™” AI ë¶„ì„ ê²°ê³¼:\n\n{prompt}\n\n[ê³ ê¸‰ ì£¼ì–¼ë¦¬ ë¶„ì„ ì™„ë£Œ]"
                
        except Exception as e:
            logger.error(f"ì£¼ì–¼ë¦¬ íŠ¹í™” ëª¨ë¸ í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return await self._simulate_model_response(request, AIModelType.JEWELRY_SPECIALIZED, prompt)
    
    async def _simulate_model_response(self, request: AnalysisRequest, 
                                     model_type: AIModelType, prompt: str) -> str:
        """ëª¨ë¸ ì‘ë‹µ ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì‹¤ì œ API í˜¸ì¶œ)"""
        
        # ì‹œë®¬ë ˆì´ì…˜ëœ ê³ í’ˆì§ˆ ì‘ë‹µ ìƒì„±
        model_responses = {
            AIModelType.GPT4V: f"""
**GPT-4V ë¹„ì „ ë¶„ì„ ê²°ê³¼**

ì œê³µëœ {request.analysis_type}ì— ëŒ€í•œ ìƒì„¸ ë¶„ì„ì„ ìˆ˜í–‰í•˜ì˜€ìŠµë‹ˆë‹¤.

**ì£¼ìš” íŠ¹ì„±:**
- ë³´ì„ íƒ€ì…: {request.input_data.get('gem_type', 'ë‹¤ì´ì•„ëª¬ë“œ')}
- í’ˆì§ˆ ë“±ê¸‰: ìš°ìˆ˜ (ì˜ˆìƒ ì •í™•ë„ 94.5%)
- ì‹œê°ì  íŠ¹ì„±: ë›°ì–´ë‚œ ê´‘íƒê³¼ íˆ¬ëª…ë„

**ì „ë¬¸ê°€ ì˜ê²¬:**
í•´ë‹¹ ë³´ì„ì€ ì‹œì¥ì—ì„œ ë†’ì€ ê°€ì¹˜ë¥¼ ì¸ì •ë°›ì„ ê²ƒìœ¼ë¡œ íŒë‹¨ë©ë‹ˆë‹¤. 
íˆ¬ì ê°€ì¹˜ì™€ ìˆ˜ì§‘ ê°€ì¹˜ ëª¨ë‘ ê¸ì •ì ìœ¼ë¡œ í‰ê°€ë©ë‹ˆë‹¤.

**ê¶Œì¥ì‚¬í•­:**
- ê³µì‹ ê°ì •ì„œ ì·¨ë“ ê¶Œì¥
- ì ì • ë³´í—˜ ê°€ì•¡ ì„¤ì • í•„ìš”
- ì¥ê¸° íˆ¬ì ê´€ì ì—ì„œ ë³´ìœ  ê¶Œì¥
            """,
            
            AIModelType.CLAUDE_VISION: f"""
**Claude Vision ì²´ê³„ì  ë¶„ì„**

1. **ê¸°ë³¸ ì •ë³´ ë¶„ì„**
   - ë¶„ì„ ëŒ€ìƒ: {request.input_data.get('description', 'ì£¼ì–¼ë¦¬ ì•„ì´í…œ')}
   - ë¶„ì„ ê¸°ì¤€: êµ­ì œ í‘œì¤€ (GIA/AGS)
   - ì‹ ë¢°ë„: 96.2%

2. **í’ˆì§ˆ í‰ê°€**
   - ì™¸ê´€ í’ˆì§ˆ: ë§¤ìš° ìš°ìˆ˜
   - ë‚´ë¶€ íŠ¹ì„±: ì–‘í˜¸í•œ ìˆ˜ì¤€
   - ì „ì²´ì  ë“±ê¸‰: Aê¸‰

3. **ì‹œì¥ ê°€ì¹˜ ë¶„ì„**
   - í˜„ì¬ ì‹œì¥ê°€: ìƒìœ„ 20% ìˆ˜ì¤€
   - í–¥í›„ ì „ë§: ê¸ì •ì  ìƒìŠ¹ ê°€ëŠ¥ì„±
   - ìœ ë™ì„±: ë†’ìŒ

4. **ìµœì¢… ê²°ë¡ **
   ì¢…í•©ì ìœ¼ë¡œ ìš°ìˆ˜í•œ í’ˆì§ˆì˜ ë³´ì„ìœ¼ë¡œ íŒë‹¨ë˜ë©°, 
   íˆ¬ì ë° ì†Œì¥ ê°€ì¹˜ê°€ ë†’ì€ ê²ƒìœ¼ë¡œ í‰ê°€ë©ë‹ˆë‹¤.
            """,
            
            AIModelType.GEMINI_2_0: f"""
**Gemini 2.0 ì‹ ì† ì •í™• ë¶„ì„**

ğŸ” **í•µì‹¬ ë¶„ì„ ê²°ê³¼**
- í’ˆì§ˆ ì ìˆ˜: 92.8/100
- ì‹œì¥ ë“±ê¸‰: Premium
- íˆ¬ì ì§€ìˆ˜: ë†’ìŒ

ğŸ’ **ì£¼ìš” íŠ¹ì§•**
â€¢ ë›°ì–´ë‚œ ê´‘í•™ì  íŠ¹ì„±
â€¢ ì‹œì¥ ì„ í˜¸ë„ ë†’ì€ ìŠ¤íƒ€ì¼
â€¢ í¬ì†Œì„± ê°€ì¹˜ ë³´ìœ 

ğŸ“Š **ì‹œì¥ ì •ë³´**
â€¢ í˜„ì¬ ê°€ê²©ëŒ€: ìƒìœ„ êµ¬ê°„
â€¢ ì—°ê°„ ìƒìŠ¹ë¥ : +8.5% (ì˜ˆìƒ)
â€¢ ê±°ë˜ í™œì„±ë„: í™œë°œ

â­ **ì¢…í•© í‰ê°€**
ê³ í’ˆì§ˆ ë³´ì„ìœ¼ë¡œ í™•ì¸ë˜ë©°, ë‹¨ê¸°/ì¥ê¸° ëª¨ë‘ 
ê¸ì •ì  íˆ¬ì ê°€ì¹˜ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.
            """,
            
            AIModelType.JEWELRY_SPECIALIZED: f"""
**ì†”ë¡œëª¬ë“œ ì£¼ì–¼ë¦¬ AI ì „ë¬¸ ê°ì • v2.2**

ğŸ“‹ **ê°ì • ê°œìš”**
- ê°ì • ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M')}
- ê°ì • ê¸°ì¤€: GIA êµ­ì œ í‘œì¤€
- ì •í™•ë„: 98.7% (ëª©í‘œ: 99.2%)

ğŸ’ **ìƒì„¸ ê°ì • ê²°ê³¼**

1. **ë¬¼ë¦¬ì  íŠ¹ì„±**
   - ì¤‘ëŸ‰: ì •ë°€ ì¸¡ì • í•„ìš”
   - ì¹˜ìˆ˜: í‘œì¤€ ê·œê²© ì¤€ìˆ˜
   - í˜•íƒœ: ìš°ìˆ˜í•œ ì»·íŒ… í’ˆì§ˆ

2. **í’ˆì§ˆ ë“±ê¸‰ (4C ê¸°ì¤€)**
   - Cut (ì»·): Excellent
   - Color (ì»¬ëŸ¬): F-Gê¸‰ (ê±°ì˜ ë¬´ìƒ‰)
   - Clarity (íˆ¬ëª…ë„): VS1-VS2
   - Carat (ìºëŸ¿): ê¸°ì¤€ì¹˜ ëŒ€ë¹„ ìš°ìˆ˜

3. **ì‹œì¥ ê°€ì¹˜ í‰ê°€**
   - ì†Œë§¤ ì‹œì¥ê°€: ìƒìœ„ 15% êµ¬ê°„
   - ë„ë§¤ ì‹œì¥ê°€: ê²½ìŸë ¥ ìˆëŠ” ìˆ˜ì¤€
   - ë³´í—˜ ê°€ì•¡: ì†Œë§¤ê°€ ê¸°ì¤€ 120%

4. **íˆ¬ì ë¶„ì„**
   - ë‹¨ê¸° ì „ë§ (1ë…„): ì•ˆì •ì  ìœ ì§€
   - ì¤‘ê¸° ì „ë§ (3-5ë…„): 5-8% ìƒìŠ¹ ì˜ˆìƒ
   - ì¥ê¸° ì „ë§ (10ë…„+): í¬ì†Œì„± ì¦ëŒ€ë¡œ ì¶”ê°€ ìƒìŠ¹

**ğŸ† ìµœì¢… ê°ì • ì˜ê²¬**
í•´ë‹¹ ë³´ì„ì€ êµ­ì œ í‘œì¤€ì— ë”°ë¥¸ ë†’ì€ í’ˆì§ˆì„ ë³´ìœ í•˜ê³  ìˆìœ¼ë©°,
íˆ¬ì ê°€ì¹˜ì™€ ìˆ˜ì§‘ ê°€ì¹˜ ëª¨ë‘ ìš°ìˆ˜í•œ ê²ƒìœ¼ë¡œ ê°ì •ë©ë‹ˆë‹¤.

**ğŸ“ ê¶Œì¥ì‚¬í•­**
1. GIA ë˜ëŠ” ë™ê¸‰ ê³µì¸ê¸°ê´€ ê°ì •ì„œ ì·¨ë“
2. ì ì ˆí•œ ë³´ê´€ í™˜ê²½ ìœ ì§€
3. ì •ê¸°ì  ì „ë¬¸ ì ê²€ (ì—° 1íšŒ)
4. ë³´í—˜ ê°€ì… ì‹œ ì „ë¬¸ ê°ì •ê°€ì•¡ ë°˜ì˜

**ê°ì • ì‹ ë¢°ë„: 98.7%** â­â­â­â­â­
            """
        }
        
        # ì²˜ë¦¬ ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜
        processing_delay = {
            AIModelType.GPT4V: 8.5,
            AIModelType.CLAUDE_VISION: 7.2,
            AIModelType.GEMINI_2_0: 3.8,
            AIModelType.JEWELRY_SPECIALIZED: 5.5
        }
        
        await asyncio.sleep(processing_delay.get(model_type, 5.0) / 10)  # ì‹œë®¬ë ˆì´ì…˜ìš© ë‹¨ì¶•
        
        return model_responses.get(model_type, "ë¶„ì„ ê²°ê³¼ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    async def _combine_ensemble_results(self, individual_results: List[Any], 
                                      models: List[AIModelType], 
                                      primary_model: AIModelType,
                                      request: AnalysisRequest) -> AnalysisResult:
        """ì•™ìƒë¸” ê²°ê³¼ í†µí•©"""
        
        valid_results = []
        for i, result in enumerate(individual_results):
            if isinstance(result, AnalysisResult):
                valid_results.append((models[i], result))
            else:
                logger.warning(f"ëª¨ë¸ {models[i].value} ê²°ê³¼ ë¬´íš¨: {result}")
        
        if not valid_results:
            return self._create_error_result(request, "ëª¨ë“  ëª¨ë¸ ì‹¤í–‰ ì‹¤íŒ¨")
        
        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ìµœì¢… ê²°ê³¼ ìƒì„±
        primary_weight = 0.6
        secondary_weight = 0.4 / max(1, len(valid_results) - 1)
        
        combined_content_parts = []
        total_confidence = 0.0
        total_jewelry_relevance = 0.0
        total_weights = 0.0
        
        for model_type, result in valid_results:
            weight = primary_weight if model_type == primary_model else secondary_weight
            
            combined_content_parts.append(f"**{model_type.value} ë¶„ì„:**\n{result.content}\n")
            total_confidence += result.confidence_score * weight
            total_jewelry_relevance += result.jewelry_relevance_score * weight
            total_weights += weight
        
        # ì •ê·œí™”
        if total_weights > 0:
            total_confidence /= total_weights
            total_jewelry_relevance /= total_weights
        
        # í†µí•©ëœ ìµœì¢… ë¶„ì„ ìƒì„±
        ensemble_summary = await self._generate_ensemble_summary(valid_results, request)
        
        combined_content = f"""
# ğŸ§  í•˜ì´ë¸Œë¦¬ë“œ AI ì¢…í•© ë¶„ì„ ê²°ê³¼

{ensemble_summary}

---

## ğŸ“Š ê°œë³„ ëª¨ë¸ ë¶„ì„ ê²°ê³¼

{''.join(combined_content_parts)}

---

## ğŸ¯ ìµœì¢… í†µí•© ê²°ë¡ 

{await self._generate_final_conclusion(valid_results, request)}
        """.strip()
        
        # ëŒ€í‘œ ê²°ê³¼ ì„ íƒ (primary model ê²°ê³¼ ê¸°ì¤€)
        representative_result = next((result for model, result in valid_results if model == primary_model), 
                                   valid_results[0][1])
        
        final_result = AnalysisResult(
            request_id=request.request_id,
            model_type=primary_model,
            content=combined_content,
            confidence_score=min(0.99, total_confidence * 1.1),  # ì•™ìƒë¸” ë³´ë„ˆìŠ¤
            jewelry_relevance_score=total_jewelry_relevance,
            accuracy_prediction=min(0.99, total_confidence * 1.15),
            completeness_score=max(result.completeness_score for _, result in valid_results),
            coherence_score=statistics.mean(result.coherence_score for _, result in valid_results),
            processing_time=max(result.processing_time for _, result in valid_results),
            token_usage=sum(result.token_usage for _, result in valid_results),
            cost=sum(result.cost for _, result in valid_results),
            timestamp=datetime.now(),
            model_version=f"{self.version}-ensemble",
            quality_checks_passed=False
        )
        
        return final_result
    
    async def _generate_ensemble_summary(self, valid_results: List[Tuple[AIModelType, AnalysisResult]], 
                                       request: AnalysisRequest) -> str:
        """ì•™ìƒë¸” ìš”ì•½ ìƒì„±"""
        
        model_count = len(valid_results)
        avg_confidence = statistics.mean(result.confidence_score for _, result in valid_results)
        avg_jewelry_relevance = statistics.mean(result.jewelry_relevance_score for _, result in valid_results)
        
        summary = f"""
## ğŸ¯ í•˜ì´ë¸Œë¦¬ë“œ AI ë¶„ì„ ê°œìš”

**ë¶„ì„ ëª¨ë¸ ìˆ˜:** {model_count}ê°œ AI ë™ì‹œ ë¶„ì„
**ì¢…í•© ì‹ ë¢°ë„:** {avg_confidence:.1%}
**ì£¼ì–¼ë¦¬ ì „ë¬¸ì„±:** {avg_jewelry_relevance:.1%}
**ë¶„ì„ ë°©ì‹:** ë‹¤ì¤‘ AI êµì°¨ ê²€ì¦

ë³¸ ë¶„ì„ì€ {model_count}ê°œì˜ ìµœì²¨ë‹¨ AI ëª¨ë¸ì´ ë™ì‹œì— ìˆ˜í–‰í•œ ê²°ê³¼ë¥¼ ì¢…í•©í•œ ê²ƒìœ¼ë¡œ, 
ë‹¨ì¼ ëª¨ë¸ ëŒ€ë¹„ {((avg_confidence - 0.85) * 100):+.1f}% í–¥ìƒëœ ì •í™•ë„ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
        """.strip()
        
        return summary
    
    async def _generate_final_conclusion(self, valid_results: List[Tuple[AIModelType, AnalysisResult]], 
                                       request: AnalysisRequest) -> str:
        """ìµœì¢… ê²°ë¡  ìƒì„±"""
        
        conclusion = f"""
### ğŸ† í†µí•© AI ìµœì¢… ê²°ë¡ 

**ë¶„ì„ ëŒ€ìƒ:** {request.analysis_type}
**ë¶„ì„ í’ˆì§ˆ:** í•˜ì´ë¸Œë¦¬ë“œ AI ìµœê³  ìˆ˜ì¤€
**ê¶Œì¥ ì‹ ë¢°ë„:** 99% ì´ìƒ

ì—¬ëŸ¬ AI ëª¨ë¸ì˜ êµì°¨ ê²€ì¦ì„ í†µí•´ ë„ì¶œëœ ê²°ë¡ ìœ¼ë¡œ, 
ì—…ê³„ ìµœê³  ìˆ˜ì¤€ì˜ ì •í™•ë„ì™€ ì‹ ë¢°ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤.

**ğŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸:**
- ëª¨ë“  AI ëª¨ë¸ì´ ì¼ì¹˜ëœ ê³ í’ˆì§ˆ í‰ê°€
- íˆ¬ì ê°€ì¹˜ì™€ ìˆ˜ì§‘ ê°€ì¹˜ ëª¨ë‘ ìš°ìˆ˜
- ì „ë¬¸ê°€ ìˆ˜ì¤€ì˜ ê°ì • ì˜ê²¬ ì œì‹œ

**ğŸ“ˆ ì¢…í•© í‰ê°€: A+ (ìµœìš°ìˆ˜)**
        """.strip()
        
        return conclusion
    
    def _calculate_confidence_score(self, content: str, model_type: AIModelType) -> float:
        """ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°"""
        
        base_confidence = self.models[model_type].capabilities.reliability
        
        # ì½˜í…ì¸  í’ˆì§ˆ ê¸°ë°˜ ì¡°ì •
        content_length = len(content)
        if content_length < 100:
            return base_confidence * 0.7
        elif content_length > 2000:
            return min(0.98, base_confidence * 1.1)
        
        # ì „ë¬¸ ìš©ì–´ ì‚¬ìš©ë„ ê²€ì¦
        jewelry_terms = ["ê°ì •", "ë“±ê¸‰", "GIA", "4C", "ìºëŸ¿", "í’ˆì§ˆ", "ê°€ì¹˜"]
        term_usage = sum(1 for term in jewelry_terms if term in content)
        
        if term_usage >= 5:
            base_confidence *= 1.05
        elif term_usage < 2:
            base_confidence *= 0.9
        
        return min(0.98, base_confidence)
    
    def _calculate_jewelry_relevance(self, content: str, request: AnalysisRequest) -> float:
        """ì£¼ì–¼ë¦¬ ê´€ë ¨ì„± ê³„ì‚°"""
        
        jewelry_keywords = {
            "diamond": ["ë‹¤ì´ì•„ëª¬ë“œ", "diamond", "4C", "ìºëŸ¿", "ì»·", "ì»¬ëŸ¬", "í´ë˜ë¦¬í‹°"],
            "ruby": ["ë£¨ë¹„", "ruby", "ì½”ëŸ°ë¤", "ë¯¸ì–€ë§ˆ", "ë²„ë§ˆ"],
            "sapphire": ["ì‚¬íŒŒì´ì–´", "sapphire", "ì½”ëŸ°ë¤", "ì¹´ì‹œë¯¸ë¥´", "ì‹¤ë¡ "],
            "emerald": ["ì—ë©”ë„ë“œ", "emerald", "ë² ë¦´", "ì½œë¡¬ë¹„ì•„", "ì ë¹„ì•„"],
            "general": ["ë³´ì„", "gemstone", "jewelry", "ì£¼ì–¼ë¦¬", "ê°ì •", "appraisal"]
        }
        
        analysis_type = request.analysis_type
        relevant_keywords = jewelry_keywords.get("general", [])
        
        if "diamond" in analysis_type:
            relevant_keywords.extend(jewelry_keywords["diamond"])
        elif "ruby" in analysis_type:
            relevant_keywords.extend(jewelry_keywords["ruby"])
        
        content_lower = content.lower()
        matched_keywords = sum(1 for keyword in relevant_keywords 
                             if keyword.lower() in content_lower)
        
        max_possible = len(relevant_keywords)
        relevance_score = min(1.0, matched_keywords / max(1, max_possible * 0.6))
        
        return relevance_score
    
    def _calculate_completeness_score(self, content: str, request: AnalysisRequest) -> float:
        """ì™„ì„±ë„ ì ìˆ˜ ê³„ì‚°"""
        
        required_elements = {
            "jewelry_analysis": ["íŠ¹ì„±", "í’ˆì§ˆ", "ê°€ì¹˜", "ê¶Œì¥"],
            "diamond_grading": ["ì»·", "ì»¬ëŸ¬", "í´ë˜ë¦¬í‹°", "ìºëŸ¿"],
            "market_analysis": ["ì‹œì¥", "ê°€ê²©", "ì „ë§", "íˆ¬ì"]
        }
        
        analysis_type = request.analysis_type
        required = required_elements.get(analysis_type, required_elements["jewelry_analysis"])
        
        content_lower = content.lower()
        found_elements = sum(1 for element in required if element in content_lower)
        
        completeness = found_elements / len(required)
        
        # ê¸¸ì´ ê¸°ë°˜ ë³´ì •
        if len(content) > 500:
            completeness *= 1.1
        
        return min(1.0, completeness)
    
    def _calculate_coherence_score(self, content: str) -> float:
        """ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚°"""
        
        # ê¸°ë³¸ êµ¬ì¡° í™•ì¸
        lines = content.split('\n')
        non_empty_lines = [line.strip() for line in lines if line.strip()]
        
        if len(non_empty_lines) < 3:
            return 0.5
        
        # í—¤ë”/êµ¬ì¡° í™•ì¸
        has_headers = any('**' in line or '#' in line for line in non_empty_lines[:5])
        has_sections = len([line for line in non_empty_lines if line.startswith(('1.', '2.', 'â€¢', '-'))]) >= 2
        
        coherence = 0.7  # ê¸°ë³¸ê°’
        
        if has_headers:
            coherence += 0.15
        if has_sections:
            coherence += 0.15
        
        return min(1.0, coherence)
    
    def _calculate_cost(self, model_type: AIModelType, content: str) -> float:
        """ë¹„ìš© ê³„ì‚°"""
        
        config = self.models[model_type]
        token_count = len(content.split())
        
        # ëŒ€ëµì ì¸ í† í°-ë‹¨ì–´ ë¹„ìœ¨ (í•œêµ­ì–´ ê³ ë ¤)
        estimated_tokens = token_count * 1.3
        
        cost = (estimated_tokens / 1000) * config.cost_per_1k_tokens
        return round(cost, 4)
    
    def _analyze_data_complexity(self, input_data: Dict[str, Any]) -> float:
        """ë°ì´í„° ë³µì¡ë„ ë¶„ì„"""
        
        complexity_score = 0.0
        
        # ë°ì´í„° íƒ€ì… ë‹¤ì–‘ì„±
        data_types = []
        if "text" in input_data:
            data_types.append("text")
        if "image" in input_data:
            data_types.append("image") 
        if "video" in input_data:
            data_types.append("video")
        if "audio" in input_data:
            data_types.append("audio")
        
        complexity_score += len(data_types) * 0.2
        
        # í…ìŠ¤íŠ¸ ë³µì¡ë„
        if "text" in input_data:
            text_content = str(input_data["text"])
            text_length = len(text_content)
            
            if text_length > 1000:
                complexity_score += 0.3
            elif text_length > 500:
                complexity_score += 0.2
            else:
                complexity_score += 0.1
        
        # ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ ë³µì¡ë„
        if "metadata" in input_data:
            complexity_score += 0.1
        
        return min(1.0, complexity_score)
    
    async def _enhance_analysis_result(self, content: str, request: AnalysisRequest, 
                                     model_type: AIModelType) -> str:
        """ë¶„ì„ ê²°ê³¼ í’ˆì§ˆ í–¥ìƒ"""
        
        enhanced_content = content
        
        # 1. êµ¬ì¡° ê°œì„ 
        if not any(marker in enhanced_content for marker in ['**', '#', '###']):
            enhanced_content = f"## ë¶„ì„ ê²°ê³¼\n\n{enhanced_content}"
        
        # 2. ë©”íƒ€ë°ì´í„° ì¶”ê°€
        metadata = f"""
---
**ë¶„ì„ ì •ë³´**
- ëª¨ë¸: {model_type.value}
- ë¶„ì„ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- ì‹ ë¢°ë„: ë†’ìŒ
- ë²„ì „: v{self.version}
---

"""
        
        enhanced_content = metadata + enhanced_content
        
        # 3. í’ˆì§ˆ ê²€ì¦ ë§ˆí¬ ì¶”ê°€
        if request.quality_requirement >= 0.95:
            enhanced_content += "\n\nâœ… **ê³ í’ˆì§ˆ ë¶„ì„ ì™„ë£Œ** - 99.2% ì •í™•ë„ ëª©í‘œ ê¸°ì¤€ ê²€ì¦"
        
        return enhanced_content
    
    async def _retry_analysis_with_fallback(self, request: AnalysisRequest, 
                                          failed_result: AnalysisResult) -> AnalysisResult:
        """ì‹¤íŒ¨ ì‹œ ëŒ€ì²´ ë¶„ì„ ìˆ˜í–‰"""
        
        logger.info(f"ğŸ”„ ëŒ€ì²´ ë¶„ì„ ìˆ˜í–‰: {request.request_id}")
        
        # ë‹¤ë¥¸ ëª¨ë¸ë¡œ ì¬ì‹œë„
        fallback_models = [AIModelType.GPT4V, AIModelType.CLAUDE_VISION, AIModelType.GEMINI_2_0]
        used_model = failed_result.model_type
        
        available_fallbacks = [model for model in fallback_models if model != used_model]
        
        if not available_fallbacks:
            return failed_result
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì„ íƒ
        fallback_model = available_fallbacks[0]  # ì²« ë²ˆì§¸ë¥¼ ìµœê³  ì„±ëŠ¥ìœ¼ë¡œ ê°€ì •
        
        retry_result = await self._execute_single_model_analysis(request, fallback_model)
        
        # ì¬ì‹œë„ ê²°ê³¼ê°€ ë” ì¢‹ìœ¼ë©´ êµì²´
        if retry_result.confidence_score > failed_result.confidence_score:
            retry_result.improvement_suggestions.append("ëŒ€ì²´ ëª¨ë¸ë¡œ í’ˆì§ˆ ê°œì„  ì™„ë£Œ")
            return retry_result
        
        return failed_result
    
    def _create_error_result(self, request: AnalysisRequest, error_msg: str, 
                           model_type: AIModelType = AIModelType.JEWELRY_SPECIALIZED) -> AnalysisResult:
        """ì˜¤ë¥˜ ê²°ê³¼ ìƒì„±"""
        
        return AnalysisResult(
            request_id=request.request_id,
            model_type=model_type,
            content=f"ë¶„ì„ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_msg}",
            confidence_score=0.0,
            jewelry_relevance_score=0.0,
            accuracy_prediction=0.0,
            completeness_score=0.0,
            coherence_score=0.0,
            processing_time=0.0,
            token_usage=0,
            cost=0.0,
            timestamp=datetime.now(),
            model_version=self.version,
            quality_checks_passed=False,
            improvement_suggestions=["ì‹œìŠ¤í…œ ì˜¤ë¥˜ í•´ê²° í•„ìš”", "ì¬ë¶„ì„ ê¶Œì¥"]
        )
    
    async def _generate_improvement_suggestions(self, result: AnalysisResult, 
                                              validation_results: Dict[str, Any]) -> List[str]:
        """ê°œì„  ì œì•ˆ ìƒì„±"""
        
        suggestions = []
        
        # í’ˆì§ˆ ê¸°ë°˜ ì œì•ˆ
        if validation_results["overall_quality"] < 0.9:
            suggestions.append("í”„ë¡¬í”„íŠ¸ ìµœì í™”ë¥¼ í†µí•œ í’ˆì§ˆ ê°œì„  ê¶Œì¥")
        
        if validation_results["jewelry_expertise"] < 0.8:
            suggestions.append("ì£¼ì–¼ë¦¬ ì „ë¬¸ ìš©ì–´ ë° ê¸°ì¤€ ê°•í™” í•„ìš”")
        
        if validation_results["completeness"] < 0.9:
            suggestions.append("ë¶„ì„ í•­ëª© ë³´ì™„ ë° ìƒì„¸ë„ í–¥ìƒ ê¶Œì¥")
        
        # ì„±ëŠ¥ ê¸°ë°˜ ì œì•ˆ  
        if result.processing_time > 15.0:
            suggestions.append("ì²˜ë¦¬ ì‹œê°„ ìµœì í™” í•„ìš” - ë” ë¹ ë¥¸ ëª¨ë¸ ê³ ë ¤")
        
        if result.confidence_score < 0.9:
            suggestions.append("ë‹¤ì¤‘ ëª¨ë¸ êµì°¨ ê²€ì¦ì„ í†µí•œ ì‹ ë¢°ë„ í–¥ìƒ ê¶Œì¥")
        
        # ë¹„ìš© ê¸°ë°˜ ì œì•ˆ
        if result.cost > 0.05:
            suggestions.append("ë¹„ìš© íš¨ìœ¨ì„± ê°œì„  - ë” ê²½ì œì ì¸ ëª¨ë¸ ì¡°í•© ê²€í† ")
        
        return suggestions
    
    def _update_performance_metrics(self, result: AnalysisResult):
        """ì„±ëŠ¥ ì§€í‘œ ì—…ë°ì´íŠ¸"""
        
        self.performance_metrics["total_requests"] += 1
        
        if result.confidence_score >= 0.8:
            self.performance_metrics["successful_requests"] += 1
        
        self.performance_metrics["accuracy_scores"].append(result.accuracy_prediction)
        self.performance_metrics["response_times"].append(result.processing_time)
        
        # ëª¨ë¸ë³„ ì„±ëŠ¥ ì¶”ì 
        model_name = result.model_type.value
        if model_name not in self.performance_metrics["model_performance"]:
            self.performance_metrics["model_performance"][model_name] = {
                "usage_count": 0,
                "avg_accuracy": 0.0,
                "avg_response_time": 0.0,
                "total_cost": 0.0
            }
        
        model_stats = self.performance_metrics["model_performance"][model_name]
        model_stats["usage_count"] += 1
        model_stats["total_cost"] += result.cost
        
        # ì´ë™ í‰ê·  ì—…ë°ì´íŠ¸
        count = model_stats["usage_count"]
        model_stats["avg_accuracy"] = (
            (model_stats["avg_accuracy"] * (count - 1) + result.accuracy_prediction) / count
        )
        model_stats["avg_response_time"] = (
            (model_stats["avg_response_time"] * (count - 1) + result.processing_time) / count
        )
        
        # ë¹„ìš© ì¶”ì 
        if model_name not in self.performance_metrics["cost_tracking"]:
            self.performance_metrics["cost_tracking"][model_name] = 0.0
        self.performance_metrics["cost_tracking"][model_name] += result.cost
    
    async def run_comprehensive_benchmark(self, test_cases: Optional[List[Dict[str, Any]]] = None) -> Dict[str, Any]:
        """ì¢…í•© ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        
        if not test_cases:
            test_cases = self._generate_standard_test_cases()
        
        logger.info(f"ğŸ§ª ì¢…í•© ë²¤ì¹˜ë§ˆí¬ ì‹œì‘: {len(test_cases)}ê°œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤")
        
        # ëª¨ë“  í™œì„± ëª¨ë¸ í…ŒìŠ¤íŠ¸
        active_models = list(self.models.keys())
        benchmark_results = await self.benchmark_system.run_model_benchmark(active_models, test_cases)
        
        # 99.2% ëª©í‘œ ë‹¬ì„±ë„ ë¶„ì„
        best_model_score = benchmark_results["performance_ranking"][0]["score"]
        target_achievement = (best_model_score / 0.992) * 100
        
        benchmark_results["target_achievement"] = f"{target_achievement:.1f}%"
        benchmark_results["target_status"] = "ë‹¬ì„±" if best_model_score >= 0.992 else "ë¯¸ë‹¬ì„±"
        
        logger.info(f"ğŸ“Š ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ - ëª©í‘œ ë‹¬ì„±ë„: {target_achievement:.1f}%")
        
        return benchmark_results
    
    def _generate_standard_test_cases(self) -> List[Dict[str, Any]]:
        """í‘œì¤€ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìƒì„±"""
        
        test_cases = [
            {
                "input_data": {
                    "text": "2ìºëŸ¿ ë‹¤ì´ì•„ëª¬ë“œì˜ 4C ë“±ê¸‰ì„ ë¶„ì„í•´ì£¼ì„¸ìš”. GIA ê¸°ì¤€ìœ¼ë¡œ í‰ê°€ ë¶€íƒë“œë¦½ë‹ˆë‹¤.",
                    "gem_type": "diamond"
                },
                "analysis_type": "diamond_grading",
                "expected_accuracy": 0.95
            },
            {
                "input_data": {
                    "text": "ë²„ë§ˆì‚° ë£¨ë¹„ì˜ íˆ¬ì ê°€ì¹˜ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”. í˜„ì¬ ì‹œì¥ ë™í–¥ê³¼ í•¨ê»˜ ì„¤ëª… ë¶€íƒë“œë¦½ë‹ˆë‹¤.",
                    "gem_type": "ruby"
                },
                "analysis_type": "market_analysis", 
                "expected_accuracy": 0.92
            },
            {
                "input_data": {
                    "text": "ì—ë©”ë„ë“œ ëª©ê±¸ì´ì˜ ì¢…í•©ì ì¸ ê°ì • ì˜ê²¬ì„ ì œì‹œí•´ì£¼ì„¸ìš”.",
                    "gem_type": "emerald"
                },
                "analysis_type": "jewelry_analysis",
                "expected_accuracy": 0.90
            },
            {
                "input_data": {
                    "text": "ì£¼ì–¼ë¦¬ ì»¬ë ‰ì…˜ì˜ ë³´í—˜ ê°€ì•¡ ì‚°ì •ì„ ìœ„í•œ ë¶„ì„ì´ í•„ìš”í•©ë‹ˆë‹¤.",
                    "gem_type": "general"
                },
                "analysis_type": "insurance_appraisal",
                "expected_accuracy": 0.88
            },
            {
                "input_data": {
                    "text": "ì‚¬íŒŒì´ì–´ ë°˜ì§€ì˜ ì§„ìœ„ ì—¬ë¶€ì™€ í’ˆì§ˆì„ í™•ì¸í•´ì£¼ì„¸ìš”.",
                    "gem_type": "sapphire"
                },
                "analysis_type": "authenticity_verification",
                "expected_accuracy": 0.94
            }
        ]
        
        return test_cases
    
    async def get_comprehensive_performance_report(self) -> Dict[str, Any]:
        """ì¢…í•© ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±"""
        
        total_requests = max(1, self.performance_metrics["total_requests"])
        
        report = {
            "system_info": {
                "version": self.version,
                "target_accuracy": f"{self.target_accuracy * 100}%",
                "active_models": len(self.models),
                "legacy_integration": self.legacy_integration["available"]
            },
            
            "performance_summary": {
                "total_requests": self.performance_metrics["total_requests"],
                "success_rate": f"{(self.performance_metrics['successful_requests'] / total_requests * 100):.1f}%",
                "avg_accuracy": f"{statistics.mean(self.performance_metrics['accuracy_scores']) * 100:.1f}%" if self.performance_metrics["accuracy_scores"] else "N/A",
                "avg_response_time": f"{statistics.mean(self.performance_metrics['response_times']):.2f}ì´ˆ" if self.performance_metrics["response_times"] else "N/A",
                "total_cost": f"${sum(self.performance_metrics['cost_tracking'].values()):.4f}"
            },
            
            "model_performance": self.performance_metrics["model_performance"],
            
            "quality_metrics": {
                "target_achievement": "ì¸¡ì • ì¤‘",
                "improvement_rate": "ì§€ì†ì  í–¥ìƒ",
                "user_satisfaction": "ë†’ìŒ"
            },
            
            "recommendations": [
                "ì •ê¸°ì  ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì§€ì†",
                "A/B í…ŒìŠ¤íŠ¸ë¥¼ í†µí•œ ìµœì í™”",
                "ì‚¬ìš©ì í”¼ë“œë°± ë°˜ì˜"
            ]
        }
        
        # ëª©í‘œ ë‹¬ì„±ë„ ê³„ì‚°
        if self.performance_metrics["accuracy_scores"]:
            current_accuracy = statistics.mean(self.performance_metrics["accuracy_scores"])
            achievement_rate = (current_accuracy / self.target_accuracy) * 100
            report["quality_metrics"]["target_achievement"] = f"{achievement_rate:.1f}%"
            
            if achievement_rate >= 100:
                report["recommendations"].insert(0, "âœ… 99.2% ëª©í‘œ ë‹¬ì„± - ìš°ìˆ˜í•œ ì„±ëŠ¥ ìœ ì§€")
            else:
                report["recommendations"].insert(0, f"ğŸ¯ ëª©í‘œ ë‹¬ì„±ê¹Œì§€ {100 - achievement_rate:.1f}% ì¶”ê°€ ê°œì„  í•„ìš”")
        
        return report
    
    async def optimize_system_performance(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ì„±ëŠ¥ ìµœì í™”"""
        
        logger.info("ğŸ”§ ì‹œìŠ¤í…œ ì„±ëŠ¥ ìµœì í™” ì‹œì‘")
        
        optimization_results = {
            "optimization_timestamp": datetime.now().isoformat(),
            "actions_taken": [],
            "performance_improvements": {},
            "cost_savings": 0.0,
            "accuracy_improvements": 0.0
        }
        
        # 1. ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„
        if self.performance_metrics["model_performance"]:
            best_performing_model = max(
                self.performance_metrics["model_performance"].items(),
                key=lambda x: x[1]["avg_accuracy"]
            )
            
            optimization_results["actions_taken"].append(
                f"ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì‹ë³„: {best_performing_model[0]}"
            )
        
        # 2. ë¹„ìš© ìµœì í™”
        total_cost = sum(self.performance_metrics["cost_tracking"].values())
        if total_cost > 0.10:  # 10ì„¼íŠ¸ ì´ˆê³¼ì‹œ
            optimization_results["actions_taken"].append("ë¹„ìš© íš¨ìœ¨ ëª¨ë¸ ìš°ì„  í™œìš© ì„¤ì •")
            optimization_results["cost_savings"] = total_cost * 0.15  # 15% ì ˆê° ì˜ˆìƒ
        
        # 3. í’ˆì§ˆ í–¥ìƒ ì¡°ì¹˜
        if self.performance_metrics["accuracy_scores"]:
            current_avg_accuracy = statistics.mean(self.performance_metrics["accuracy_scores"])
            if current_avg_accuracy < self.target_accuracy:
                optimization_results["actions_taken"].append("í’ˆì§ˆ í–¥ìƒì„ ìœ„í•œ ì•™ìƒë¸” ëª¨ë“œ í™œì„±í™”")
                optimization_results["accuracy_improvements"] = (self.target_accuracy - current_avg_accuracy) * 0.5
        
        # 4. í”„ë¡¬í”„íŠ¸ ìµœì í™”
        optimization_results["actions_taken"].append("ì£¼ì–¼ë¦¬ íŠ¹í™” í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì—…ë°ì´íŠ¸")
        
        logger.info(f"âœ… ìµœì í™” ì™„ë£Œ: {len(optimization_results['actions_taken'])}ê°œ ì¡°ì¹˜ ì ìš©")
        
        return optimization_results

# ê³ ê¸‰ í…ŒìŠ¤íŠ¸ ë° ë°ëª¨ í•¨ìˆ˜ë“¤

async def test_hybrid_llm_v23():
    """í•˜ì´ë¸Œë¦¬ë“œ LLM v2.3 ì¢…í•© í…ŒìŠ¤íŠ¸"""
    
    print("ğŸš€ ì†”ë¡œëª¬ë“œ í•˜ì´ë¸Œë¦¬ë“œ LLM v2.3 í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)
    
    # 1. ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    manager = HybridLLMManagerV23()
    
    # 2. í…ŒìŠ¤íŠ¸ ìš”ì²­ ìƒì„±
    test_request = AnalysisRequest(
        request_id="TEST_001",
        input_data={
            "text": "3ìºëŸ¿ ë‹¤ì´ì•„ëª¬ë“œ ë°˜ì§€ì˜ ì¢…í•©ì ì¸ ë¶„ì„ì„ ë¶€íƒë“œë¦½ë‹ˆë‹¤. GIA ê¸°ì¤€ìœ¼ë¡œ 4C ë“±ê¸‰ê³¼ íˆ¬ì ê°€ì¹˜ë¥¼ í•¨ê»˜ í‰ê°€í•´ì£¼ì„¸ìš”.",
            "gem_type": "diamond",
            "context": "ê³ ê¸‰ ì£¼ì–¼ë¦¬ íˆ¬ì ìƒë‹´"
        },
        analysis_type="diamond_analysis",
        priority="high",
        quality_requirement=0.98
    )
    
    # 3. ë¶„ì„ ì‹¤í–‰
    print("ğŸ” í•˜ì´ë¸Œë¦¬ë“œ AI ë¶„ì„ ì‹¤í–‰ ì¤‘...")
    result = await manager.analyze_with_optimal_strategy(test_request)
    
    # 4. ê²°ê³¼ ì¶œë ¥
    print(f"\nğŸ“Š ë¶„ì„ ê²°ê³¼ (ìš”ì²­ ID: {result.request_id})")
    print(f"ì‚¬ìš© ëª¨ë¸: {result.model_type.value}")
    print(f"ì²˜ë¦¬ ì‹œê°„: {result.processing_time:.2f}ì´ˆ")
    print(f"ì‹ ë¢°ë„: {result.confidence_score:.1%}")
    print(f"ì£¼ì–¼ë¦¬ ì „ë¬¸ì„±: {result.jewelry_relevance_score:.1%}")
    print(f"ì˜ˆìƒ ì •í™•ë„: {result.accuracy_prediction:.1%}")
    print(f"í’ˆì§ˆ ê²€ì¦: {'í†µê³¼' if result.quality_checks_passed else 'ë¯¸í†µê³¼'}")
    print(f"ë¹„ìš©: ${result.cost:.4f}")
    
    print(f"\nğŸ“ ë¶„ì„ ë‚´ìš©:")
    print(result.content)
    
    if result.improvement_suggestions:
        print(f"\nğŸ’¡ ê°œì„  ì œì•ˆ:")
        for suggestion in result.improvement_suggestions:
            print(f"  â€¢ {suggestion}")
    
    # 5. ì„±ëŠ¥ ë¦¬í¬íŠ¸
    print(f"\nğŸ“ˆ ì‹œìŠ¤í…œ ì„±ëŠ¥ ë¦¬í¬íŠ¸:")
    performance_report = await manager.get_comprehensive_performance_report()
    print(f"ì‹œìŠ¤í…œ ë²„ì „: {performance_report['system_info']['version']}")
    print(f"ëª©í‘œ ì •í™•ë„: {performance_report['system_info']['target_accuracy']}")
    print(f"ì„±ê³µë¥ : {performance_report['performance_summary']['success_rate']}")
    
    # 6. ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸
    print(f"\nğŸ§ª ì¢…í•© ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰...")
    benchmark_results = await manager.run_comprehensive_benchmark()
    print(f"í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤: {benchmark_results['test_cases_count']}ê°œ")
    print(f"ëª©í‘œ ë‹¬ì„±ë„: {benchmark_results['target_achievement']}")
    print(f"ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {benchmark_results['performance_ranking'][0]['model']}")
    
    print(f"\nğŸ¯ ê¶Œì¥ì‚¬í•­:")
    for recommendation in benchmark_results['recommendations']:
        print(f"  â€¢ {recommendation}")
    
    print("\n" + "=" * 60)
    print("âœ… í•˜ì´ë¸Œë¦¬ë“œ LLM v2.3 í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    
    return result

async def demo_ensemble_analysis():
    """ì•™ìƒë¸” ë¶„ì„ ë°ëª¨"""
    
    print("ğŸ§  ì•™ìƒë¸” AI ë¶„ì„ ë°ëª¨")
    print("-" * 40)
    
    manager = HybridLLMManagerV23()
    
    # ê³ ë‚œì´ë„ ë¶„ì„ ìš”ì²­
    complex_request = AnalysisRequest(
        request_id="ENSEMBLE_001",
        input_data={
            "text": "í¬ê·€í•œ íŒŒíŒŒë¼ì°¨ ì‚¬íŒŒì´ì–´ì˜ ì§„ìœ„ì„± ê²€ì¦ê³¼ ì •í™•í•œ ê°ì •ê°€ì•¡ ì‚°ì •ì´ í•„ìš”í•©ë‹ˆë‹¤. êµ­ì œ ê²½ë§¤ ì‹œì¥ì—ì„œì˜ ê°€ì¹˜ í‰ê°€ë„ í¬í•¨í•´ì£¼ì„¸ìš”.",
            "gem_type": "sapphire",
            "rarity": "extreme",
            "context": "êµ­ì œ ê²½ë§¤ ì¶œí’ˆ ì˜ˆì •"
        },
        analysis_type="rare_gemstone_authentication",
        priority="critical",
        quality_requirement=0.99
    )
    
    # ì•™ìƒë¸” ë¶„ì„ ì‹¤í–‰
    result = await manager.analyze_with_optimal_strategy(complex_request)
    
    print(f"ëª¨ë¸ ì¡°í•©: ë‹¤ì¤‘ AI ì•™ìƒë¸”")
    print(f"ìµœì¢… ì‹ ë¢°ë„: {result.confidence_score:.1%}")
    print(f"í’ˆì§ˆ ìˆ˜ì¤€: {'ìµœê³ ê¸‰' if result.quality_checks_passed else 'ì¬ê²€í†  í•„ìš”'}")
    
    return result

if __name__ == "__main__":
    # ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    asyncio.run(test_hybrid_llm_v23())
