#!/usr/bin/env python3
"""
ì¢…í•© ë©”ì‹œì§€ ì¶”ì¶œ ì—”ì§„ - í´ë¡œë°” ë…¸íŠ¸ + ChatGPT ìˆ˜ì¤€ì˜ ë¶„ì„ ì‹œìŠ¤í…œ
ê°•ì—°ìê°€ ì „ë‹¬í•˜ê³ ì í•˜ëŠ” í•µì‹¬ ë©”ì‹œì§€ë¥¼ ì •í™•íˆ íŒŒì•…í•˜ëŠ” ì‹œìŠ¤í…œ
"""

import os
import re
import logging
import time
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
import json

# ê³ ê¸‰ ì–¸ì–´ ëª¨ë¸ë“¤
try:
    import openai
    openai_available = True
except ImportError:
    openai_available = False

try:
    import google.generativeai as genai
    gemini_available = True
except ImportError:
    gemini_available = False

try:
    from transformers import pipeline, AutoTokenizer, AutoModel
    transformers_available = True
except ImportError:
    transformers_available = False

class ComprehensiveMessageExtractor:
    """ì¢…í•© ë©”ì‹œì§€ ì¶”ì¶œ ì—”ì§„ - ê°•ì—°ìì˜ ì˜ë„ë¥¼ ì •í™•íˆ íŒŒì•…"""
    
    def __init__(self):
        self.logger = self._setup_logging()
        
        # ê³ ê¸‰ ì–¸ì–´ ëª¨ë¸ ì´ˆê¸°í™”
        self.advanced_llm = None
        self.korean_llm = None
        self.context_analyzer = None
        
        # ê°•ì—°/í”„ë ˆì  í…Œì´ì…˜ íŠ¹í™” ì§€ì‹
        self.presentation_patterns = self._build_presentation_patterns()
        self.message_extraction_rules = self._build_message_extraction_rules()
        self.context_enhancement_rules = self._build_context_enhancement_rules()
        
        self.logger.info("ğŸ¯ ì¢…í•© ë©”ì‹œì§€ ì¶”ì¶œ ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _setup_logging(self) -> logging.Logger:
        """ë¡œê¹… ì„¤ì •"""
        logger = logging.getLogger(__name__)
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            logger.setLevel(logging.INFO)
        return logger
    
    def _build_presentation_patterns(self) -> Dict[str, Any]:
        """í”„ë ˆì  í…Œì´ì…˜ íŒ¨í„´ ë¶„ì„ ê·œì¹™"""
        return {
            "opening_patterns": [
                "ì˜¤ëŠ˜ ë§ì”€ë“œë¦´", "ë°œí‘œí• ", "ì†Œê°œí•˜ê² ìŠµë‹ˆë‹¤", "ì‹œì‘í•˜ê² ìŠµë‹ˆë‹¤",
                "ì£¼ì œëŠ”", "í…Œë§ˆëŠ”", "ë‹¤ë£¨ê³ ì", "ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤"
            ],
            "key_point_indicators": [
                "ì¤‘ìš”í•œ ê²ƒì€", "í•µì‹¬ì€", "í¬ì¸íŠ¸ëŠ”", "ê°•ì¡°í•˜ê³  ì‹¶ì€",
                "ê¸°ì–µí•´ì•¼ í• ", "ì£¼ëª©í• ", "íŠ¹íˆ", "ë¬´ì—‡ë³´ë‹¤ë„"
            ],
            "transition_phrases": [
                "ë‹¤ìŒìœ¼ë¡œ", "ì´ì–´ì„œ", "ê·¸ë¦¬ê³ ", "ë˜í•œ", "í•œí¸",
                "ë°˜ë©´ì—", "ê·¸ëŸ¬ë‚˜", "ê²°ë¡ ì ìœ¼ë¡œ", "ë§ˆì§€ë§‰ìœ¼ë¡œ"
            ],
            "emphasis_markers": [
                "ë°˜ë“œì‹œ", "ê¼­", "ì ˆëŒ€", "ë§¤ìš°", "ì •ë§", "ì§„ì§œ",
                "í™•ì‹¤íˆ", "ë¶„ëª…íˆ", "ë‹¹ì—°íˆ", "ë¬¼ë¡ "
            ],
            "conclusion_patterns": [
                "ê²°ë¡ ì€", "ìš”ì•½í•˜ë©´", "ì •ë¦¬í•˜ìë©´", "ë§ˆë¬´ë¦¬í•˜ë©°",
                "ëìœ¼ë¡œ", "ë§ˆì§€ë§‰ìœ¼ë¡œ", "ê²°ê³¼ì ìœ¼ë¡œ", "ë”°ë¼ì„œ"
            ],
            "question_patterns": [
                "ê¶ê¸ˆí•˜ì‹œì£ ?", "ì–´ë–»ê²Œ ìƒê°í•˜ì„¸ìš”?", "ì§ˆë¬¸ì´ ìˆìœ¼ì‹¤ê¹Œìš”?",
                "ì´í•´ë˜ì‹œë‚˜ìš”?", "ë§ì£ ?", "ê·¸ë ‡ì§€ ì•Šë‚˜ìš”?"
            ]
        }
    
    def _build_message_extraction_rules(self) -> Dict[str, Any]:
        """ë©”ì‹œì§€ ì¶”ì¶œ ê·œì¹™"""
        return {
            "primary_message_weights": {
                "title_mentions": 3.0,      # ì œëª©/ì£¼ì œ ì–¸ê¸‰
                "key_indicators": 2.5,      # í•µì‹¬ ì§€ì‹œì–´
                "emphasis_markers": 2.0,    # ê°•ì¡° í‘œí˜„
                "conclusion_statements": 2.5, # ê²°ë¡  ì§„ìˆ 
                "repetition": 1.5,          # ë°˜ë³µëœ ë‚´ìš©
                "question_answers": 2.0     # ì§ˆë¬¸-ë‹µë³€ íŒ¨í„´
            },
            "supporting_message_weights": {
                "examples": 1.5,            # ì˜ˆì‹œ/ì‚¬ë¡€
                "statistics": 2.0,          # í†µê³„/ìˆ˜ì¹˜
                "quotes": 1.8,              # ì¸ìš©
                "analogies": 1.3,           # ë¹„ìœ /ì€ìœ 
                "stories": 1.4              # ìŠ¤í† ë¦¬/ì¼í™”
            },
            "context_modifiers": {
                "time_references": 1.2,     # ì‹œê°„ ì–¸ê¸‰
                "place_references": 1.1,    # ì¥ì†Œ ì–¸ê¸‰
                "people_references": 1.3,   # ì¸ë¬¼ ì–¸ê¸‰
                "data_references": 1.8      # ë°ì´í„° ì–¸ê¸‰
            }
        }
    
    def _build_context_enhancement_rules(self) -> Dict[str, Any]:
        """ì»¨í…ìŠ¤íŠ¸ í–¥ìƒ ê·œì¹™"""
        return {
            "slide_context_clues": [
                "ìŠ¬ë¼ì´ë“œ", "í™”ë©´", "ë³´ì‹œëŠ” ë°”ì™€ ê°™ì´", "ê·¸ë¦¼", "í‘œ", "ì°¨íŠ¸",
                "ë‹¤ìŒ í˜ì´ì§€", "ì´ ë¶€ë¶„ì„", "ì—¬ê¸°ì„œ", "ì´ê²ƒì€"
            ],
            "audience_interaction": [
                "ì—¬ëŸ¬ë¶„", "ì²­ì¤‘", "ì°¸ì„ì", "ê³ ê°", "ë™ë£Œ", "íŒ€ì›",
                "í•¨ê»˜", "ê°™ì´", "ìš°ë¦¬ê°€", "ëª¨ë‘"
            ],
            "temporal_markers": [
                "ê³¼ê±°ì—", "í˜„ì¬", "ë¯¸ë˜ì—", "ì§€ê¸ˆ", "ì´ì „", "ë‹¤ìŒ",
                "ì˜¬í•´", "ë‚´ë…„", "ìµœê·¼", "ì•ìœ¼ë¡œ"
            ],
            "causality_markers": [
                "ë”°ë¼ì„œ", "ê·¸ë˜ì„œ", "ê²°ê³¼ì ìœ¼ë¡œ", "ì´ë¡œ ì¸í•´", "ë•Œë¬¸ì—",
                "ë•ë¶„ì—", "ì˜í•´", "ìœ¼ë¡œë¶€í„°", "ì—ì„œ ë¹„ë¡¯ëœ"
            ]
        }
    
    def extract_comprehensive_message(self, multimodal_data: Dict[str, Any], context: Dict[str, Any] = None) -> Dict[str, Any]:
        """ë‹¤ì¤‘ ëª¨ë‹¬ ë°ì´í„°ì—ì„œ ì¢…í•© ë©”ì‹œì§€ ì¶”ì¶œ"""
        self.logger.info("ğŸ¯ ì¢…í•© ë©”ì‹œì§€ ì¶”ì¶œ ì‹œì‘")
        
        # 1. ë‹¤ì¤‘ ëª¨ë‹¬ ë°ì´í„° í†µí•©
        integrated_content = self._integrate_multimodal_content(multimodal_data)
        
        # 2. ì»¨í…ìŠ¤íŠ¸ ê°•í™” ì „ì²˜ë¦¬
        enhanced_content = self._enhance_context(integrated_content, context)
        
        # 3. ê°•ì—°ì ì˜ë„ ë¶„ì„
        speaker_intent = self._analyze_speaker_intent(enhanced_content)
        
        # 4. í•µì‹¬ ë©”ì‹œì§€ ì¶”ì¶œ
        key_messages = self._extract_key_messages(enhanced_content, speaker_intent)
        
        # 5. ë©”ì‹œì§€ ê³„ì¸µ êµ¬ì¡°í™”
        message_hierarchy = self._structure_message_hierarchy(key_messages)
        
        # 6. ì‹¤í–‰ ê°€ëŠ¥í•œ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ
        actionable_insights = self._derive_actionable_insights(message_hierarchy, context)
        
        # 7. í´ë¡œë°” ë…¸íŠ¸ ìŠ¤íƒ€ì¼ ìš”ì•½ ìƒì„±
        clova_style_summary = self._generate_clova_style_summary(message_hierarchy, speaker_intent)
        
        return {
            "comprehensive_analysis": {
                "speaker_intent": speaker_intent,
                "key_messages": key_messages,
                "message_hierarchy": message_hierarchy,
                "actionable_insights": actionable_insights,
                "clova_style_summary": clova_style_summary
            },
            "technical_details": {
                "integrated_content": integrated_content,
                "enhancement_applied": enhanced_content != integrated_content,
                "processing_time": time.time(),
                "confidence_score": self._calculate_overall_confidence(message_hierarchy)
            }
        }
    
    def _integrate_multimodal_content(self, multimodal_data: Dict[str, Any]) -> Dict[str, Any]:
        """ë‹¤ì¤‘ ëª¨ë‹¬ ë°ì´í„° í†µí•©"""
        integrated = {
            "audio_content": "",
            "visual_content": "",
            "temporal_sync": [],
            "metadata": {}
        }
        
        # ìŒì„± ë°ì´í„° í†µí•©
        if 'audio_analysis' in multimodal_data:
            audio = multimodal_data['audio_analysis']
            if audio.get('status') == 'success':
                # í–¥ìƒëœ í…ìŠ¤íŠ¸ ìš°ì„  ì‚¬ìš©
                integrated["audio_content"] = audio.get('enhanced_text', audio.get('full_text', ''))
                
                # ì‹œê°„ëŒ€ë³„ ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´
                if audio.get('segments'):
                    for segment in audio['segments']:
                        integrated["temporal_sync"].append({
                            "type": "audio",
                            "start": segment.get('start', 0),
                            "end": segment.get('end', 0),
                            "content": segment.get('text', ''),
                            "confidence": segment.get('avg_logprob', 0)
                        })
        
        # ì‹œê° ë°ì´í„° í†µí•© (OCR + í‚¤í”„ë ˆì„)
        visual_texts = []
        
        # ì´ë¯¸ì§€ OCR ê²°ê³¼
        if 'image_analysis' in multimodal_data:
            for image_result in multimodal_data['image_analysis']:
                if image_result.get('status') == 'success':
                    visual_texts.append(image_result.get('enhanced_text', image_result.get('full_text', '')))
        
        # ë¹„ë””ì˜¤ í‚¤í”„ë ˆì„ OCR ê²°ê³¼
        if 'video_analysis' in multimodal_data:
            video = multimodal_data['video_analysis']
            if video.get('visual_analysis', {}).get('status') == 'success':
                visual_analysis = video['visual_analysis']
                combined_visual = visual_analysis.get('combined_visual_text', '')
                if combined_visual:
                    visual_texts.append(combined_visual)
                
                # ì‹œê°„ëŒ€ë³„ ì‹œê° ì •ë³´
                if visual_analysis.get('frame_details'):
                    for frame in visual_analysis['frame_details']:
                        if frame.get('enhanced_text', '').strip():
                            integrated["temporal_sync"].append({
                                "type": "visual",
                                "timestamp": frame.get('timestamp_seconds', 0),
                                "content": frame['enhanced_text'],
                                "confidence": frame.get('average_confidence', 0)
                            })
        
        integrated["visual_content"] = ' '.join(filter(None, visual_texts))
        
        # ë©”íƒ€ë°ì´í„° í†µí•©
        integrated["metadata"] = {
            "has_audio": bool(integrated["audio_content"]),
            "has_visual": bool(integrated["visual_content"]),
            "temporal_mappings": len(integrated["temporal_sync"]),
            "content_richness": self._assess_content_richness(integrated)
        }
        
        return integrated
    
    def _enhance_context(self, integrated_content: Dict[str, Any], context: Dict[str, Any] = None) -> Dict[str, Any]:
        """ì»¨í…ìŠ¤íŠ¸ ê°•í™” ì²˜ë¦¬"""
        enhanced = integrated_content.copy()
        
        if not context:
            return enhanced
        
        # 1. ì°¸ì„ì/ë°œí‘œì ì •ë³´ í™œìš©
        if context.get('speakers') or context.get('participants'):
            enhanced = self._apply_speaker_context(enhanced, context)
        
        # 2. ì´ë²¤íŠ¸ ì»¨í…ìŠ¤íŠ¸ í™œìš©
        if context.get('event_context'):
            enhanced = self._apply_event_context(enhanced, context)
        
        # 3. ì£¼ì œ í‚¤ì›Œë“œ ê°•í™”
        if context.get('topic_keywords'):
            enhanced = self._apply_topic_enhancement(enhanced, context)
        
        # 4. ëª©ì  ê¸°ë°˜ í•„í„°ë§
        if context.get('objective'):
            enhanced = self._apply_objective_filtering(enhanced, context)
        
        return enhanced
    
    def _analyze_speaker_intent(self, content: Dict[str, Any]) -> Dict[str, Any]:
        """ê°•ì—°ì ì˜ë„ ë¶„ì„"""
        audio_text = content.get("audio_content", "")
        visual_text = content.get("visual_content", "")
        combined_text = f"{audio_text} {visual_text}".strip()
        
        if not combined_text:
            return {"intent_type": "unknown", "confidence": 0.0}
        
        intent_analysis = {
            "primary_intent": self._identify_primary_intent(combined_text),
            "communication_style": self._analyze_communication_style(combined_text),
            "audience_engagement": self._assess_audience_engagement(combined_text),
            "content_structure": self._analyze_content_structure(combined_text),
            "emotional_tone": self._analyze_emotional_tone(combined_text)
        }
        
        return intent_analysis
    
    def _extract_key_messages(self, content: Dict[str, Any], speaker_intent: Dict[str, Any]) -> List[Dict[str, Any]]:
        """í•µì‹¬ ë©”ì‹œì§€ ì¶”ì¶œ"""
        audio_text = content.get("audio_content", "")
        visual_text = content.get("visual_content", "")
        
        key_messages = []
        
        # 1. ìŒì„±ì—ì„œ í•µì‹¬ ë©”ì‹œì§€ ì¶”ì¶œ
        if audio_text:
            audio_messages = self._extract_messages_from_text(audio_text, "audio")
            key_messages.extend(audio_messages)
        
        # 2. ì‹œê° ìë£Œì—ì„œ í•µì‹¬ ë©”ì‹œì§€ ì¶”ì¶œ
        if visual_text:
            visual_messages = self._extract_messages_from_text(visual_text, "visual")
            key_messages.extend(visual_messages)
        
        # 3. ì‹œê°„ ë™ê¸°í™” ì •ë³´ í™œìš©
        if content.get("temporal_sync"):
            temporal_messages = self._extract_temporal_messages(content["temporal_sync"])
            key_messages.extend(temporal_messages)
        
        # 4. ë©”ì‹œì§€ ì¤‘ìš”ë„ ê³„ì‚° ë° ì •ë ¬
        for message in key_messages:
            message["importance_score"] = self._calculate_message_importance(message, speaker_intent)
        
        # ì¤‘ìš”ë„ ê¸°ì¤€ ì •ë ¬
        key_messages.sort(key=lambda x: x["importance_score"], reverse=True)
        
        return key_messages[:10]  # ìƒìœ„ 10ê°œ ë©”ì‹œì§€
    
    def _extract_messages_from_text(self, text: str, source_type: str) -> List[Dict[str, Any]]:
        """í…ìŠ¤íŠ¸ì—ì„œ ë©”ì‹œì§€ ì¶”ì¶œ"""
        messages = []
        sentences = re.split(r'[.!?]\s+', text)
        
        for i, sentence in enumerate(sentences):
            if len(sentence.strip()) < 10:
                continue
            
            # ë©”ì‹œì§€ í›„ë³´ ì ìˆ˜ ê³„ì‚°
            score = 0
            matched_patterns = []
            
            # íŒ¨í„´ ë§¤ì¹­
            for pattern_type, patterns in self.presentation_patterns.items():
                for pattern in patterns:
                    if pattern in sentence:
                        weight = self.message_extraction_rules["primary_message_weights"].get(pattern_type, 1.0)
                        score += weight
                        matched_patterns.append(f"{pattern_type}:{pattern}")
            
            # ì¼ì • ì ìˆ˜ ì´ìƒë§Œ ë©”ì‹œì§€ë¡œ ê°„ì£¼
            if score >= 1.5:
                messages.append({
                    "content": sentence.strip(),
                    "source_type": source_type,
                    "position": i,
                    "raw_score": score,
                    "matched_patterns": matched_patterns,
                    "message_type": self._classify_message_type(sentence)
                })
        
        return messages
    
    def _extract_temporal_messages(self, temporal_sync: List[Dict]) -> List[Dict[str, Any]]:
        """ì‹œê°„ ë™ê¸°í™” ì •ë³´ì—ì„œ ë©”ì‹œì§€ ì¶”ì¶œ"""
        messages = []
        
        # ì‹œê°„ëŒ€ë³„ ê·¸ë£¹í™”
        time_groups = {}
        for item in temporal_sync:
            time_key = int(item.get('timestamp', item.get('start', 0)) // 30)  # 30ì´ˆ ë‹¨ìœ„
            if time_key not in time_groups:
                time_groups[time_key] = []
            time_groups[time_key].append(item)
        
        # ê° ê·¸ë£¹ì—ì„œ ë©”ì‹œì§€ ì¶”ì¶œ
        for time_key, group in time_groups.items():
            audio_content = " ".join([item['content'] for item in group if item['type'] == 'audio'])
            visual_content = " ".join([item['content'] for item in group if item['type'] == 'visual'])
            
            if audio_content and visual_content:
                # ìŒì„±ê³¼ ì‹œê° ì •ë³´ê°€ ëª¨ë‘ ìˆëŠ” ê²½ìš°
                combined_content = f"{audio_content} [í™”ë©´: {visual_content}]"
                messages.append({
                    "content": combined_content,
                    "source_type": "multimodal",
                    "timestamp": time_key * 30,
                    "message_type": "synchronized",
                    "raw_score": 2.0  # ë©€í‹°ëª¨ë‹¬ ë³´ë„ˆìŠ¤
                })
        
        return messages
    
    def _structure_message_hierarchy(self, key_messages: List[Dict]) -> Dict[str, Any]:
        """ë©”ì‹œì§€ ê³„ì¸µ êµ¬ì¡°í™”"""
        hierarchy = {
            "main_theme": None,
            "key_points": [],
            "supporting_details": [],
            "conclusions": [],
            "call_to_actions": []
        }
        
        for message in key_messages:
            msg_type = message.get("message_type", "general")
            importance = message.get("importance_score", 0)
            
            if importance >= 4.0:
                if not hierarchy["main_theme"]:
                    hierarchy["main_theme"] = message
                else:
                    hierarchy["key_points"].append(message)
            elif importance >= 2.5:
                hierarchy["key_points"].append(message)
            elif importance >= 1.5:
                hierarchy["supporting_details"].append(message)
            
            # ë©”ì‹œì§€ íƒ€ì…ë³„ ë¶„ë¥˜
            if msg_type == "conclusion":
                hierarchy["conclusions"].append(message)
            elif msg_type == "action":
                hierarchy["call_to_actions"].append(message)
        
        return hierarchy
    
    def _generate_clova_style_summary(self, message_hierarchy: Dict, speaker_intent: Dict) -> Dict[str, Any]:
        """í´ë¡œë°” ë…¸íŠ¸ ìŠ¤íƒ€ì¼ ìš”ì•½ ìƒì„±"""
        
        # 1. í•µì‹¬ ë©”ì‹œì§€ (í´ë¡œë°” ë…¸íŠ¸ì˜ "ìš”ì•½" ì„¹ì…˜)
        main_summary = ""
        if message_hierarchy.get("main_theme"):
            main_summary = message_hierarchy["main_theme"]["content"]
        elif message_hierarchy.get("key_points"):
            main_summary = message_hierarchy["key_points"][0]["content"]
        
        # 2. ì£¼ìš” í¬ì¸íŠ¸ (í´ë¡œë°” ë…¸íŠ¸ì˜ "í‚¤ì›Œë“œ" ì„¹ì…˜)
        key_points = []
        for point in message_hierarchy.get("key_points", [])[:5]:
            key_points.append({
                "point": point["content"][:100] + "..." if len(point["content"]) > 100 else point["content"],
                "importance": point.get("importance_score", 0)
            })
        
        # 3. ì‹¤í–‰ í•­ëª© (í´ë¡œë°” ë…¸íŠ¸ì˜ "ì•¡ì…˜ ì•„ì´í…œ" ì„¹ì…˜)
        action_items = []
        for action in message_hierarchy.get("call_to_actions", []):
            action_items.append(action["content"])
        
        # 4. ì¸ì‚¬ì´íŠ¸ (í´ë¡œë°” ë…¸íŠ¸ì˜ "ì¸ì‚¬ì´íŠ¸" ì„¹ì…˜)
        insights = self._generate_insights_from_intent(speaker_intent)
        
        return {
            "executive_summary": main_summary,
            "key_takeaways": key_points,
            "action_items": action_items,
            "speaker_insights": insights,
            "presentation_structure": {
                "style": speaker_intent.get("communication_style", {}),
                "engagement_level": speaker_intent.get("audience_engagement", {}),
                "emotional_tone": speaker_intent.get("emotional_tone", {})
            },
            "clova_compatibility_score": self._calculate_clova_compatibility(message_hierarchy, speaker_intent)
        }
    
    def _identify_primary_intent(self, text: str) -> Dict[str, Any]:
        """ì£¼ìš” ì˜ë„ ì‹ë³„"""
        intent_indicators = {
            "inform": ["ì„¤ëª…", "ì†Œê°œ", "ì•Œë ¤ë“œë¦¬", "ë³´ì—¬ë“œë¦¬", "ë§ì”€ë“œë¦¬"],
            "persuade": ["ì„¤ë“", "ì œì•ˆ", "ì¶”ì²œ", "ê¶Œìœ ", "ì„ íƒ"],
            "educate": ["êµìœ¡", "í•™ìŠµ", "ë°°ìš°", "ì´í•´", "ìŠµë“"],
            "inspire": ["ì˜ê°", "ë™ê¸°", "ê²©ë ¤", "ì‘ì›", "ìê·¹"],
            "demonstrate": ["ì‹œì—°", "ë³´ì—¬ì£¼", "ë°ëª¨", "ì‹¤ìŠµ", "ì‹¤ì œ"],
            "analyze": ["ë¶„ì„", "ê²€í† ", "í‰ê°€", "ë¹„êµ", "ì—°êµ¬"]
        }
        
        intent_scores = {}
        for intent, indicators in intent_indicators.items():
            score = sum(1 for indicator in indicators if indicator in text)
            if score > 0:
                intent_scores[intent] = score
        
        if intent_scores:
            primary = max(intent_scores, key=intent_scores.get)
            return {
                "type": primary,
                "confidence": intent_scores[primary] / 10,
                "all_scores": intent_scores
            }
        
        return {"type": "general", "confidence": 0.1, "all_scores": {}}
    
    def _analyze_communication_style(self, text: str) -> Dict[str, Any]:
        """ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ìŠ¤íƒ€ì¼ ë¶„ì„"""
        style_indicators = {
            "formal": ["ì¡´ê²½í•˜ëŠ”", "ë§ì”€ë“œë¦¬ê² ìŠµë‹ˆë‹¤", "ê°ì‚¬í•©ë‹ˆë‹¤", "ì •ì¤‘íˆ"],
            "casual": ["ì—¬ëŸ¬ë¶„", "ìš°ë¦¬", "ê°™ì´", "í•¨ê»˜", "ê·¸ì£ "],
            "technical": ["ë°ì´í„°", "ë¶„ì„", "ê²°ê³¼", "ì—°êµ¬", "ë°©ë²•ë¡ "],
            "storytelling": ["ì´ì•¼ê¸°", "ê²½í—˜", "ì‚¬ë¡€", "ì˜ˆë¥¼ ë“¤ì–´", "í•œë²ˆì€"],
            "interactive": ["ì§ˆë¬¸", "ì˜ê²¬", "ìƒê°í•´ë³´ì„¸ìš”", "ì–´ë–»ê²Œ ìƒê°í•˜ì„¸ìš”"]
        }
        
        style_scores = {}
        for style, indicators in style_indicators.items():
            score = sum(1 for indicator in indicators if indicator in text)
            if score > 0:
                style_scores[style] = score
        
        return {
            "dominant_style": max(style_scores, key=style_scores.get) if style_scores else "neutral",
            "style_mix": style_scores
        }
    
    def _assess_audience_engagement(self, text: str) -> Dict[str, Any]:
        """ì²­ì¤‘ ì°¸ì—¬ë„ í‰ê°€"""
        engagement_indicators = {
            "high": ["ì—¬ëŸ¬ë¶„", "í•¨ê»˜", "ì§ˆë¬¸", "ì°¸ì—¬", "ìƒí˜¸ì‘ìš©"],
            "medium": ["ë³´ì‹œë“¯ì´", "ì•Œ ìˆ˜ ìˆë“¯", "ì´í•´í•˜ì‹œê² ì§€ë§Œ"],
            "low": ["ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤", "ë³´ì—¬ë“œë¦¬ê² ìŠµë‹ˆë‹¤"]
        }
        
        engagement_scores = {}
        for level, indicators in engagement_indicators.items():
            score = sum(1 for indicator in indicators if indicator in text)
            engagement_scores[level] = score
        
        total_score = sum(engagement_scores.values())
        if total_score == 0:
            return {"level": "unknown", "score": 0}
        
        # ê°€ì¤‘ í‰ê·  ê³„ì‚°
        weighted_score = (engagement_scores["high"] * 3 + engagement_scores["medium"] * 2 + engagement_scores["low"] * 1) / total_score
        
        if weighted_score >= 2.5:
            level = "high"
        elif weighted_score >= 1.5:
            level = "medium"
        else:
            level = "low"
        
        return {"level": level, "score": weighted_score, "details": engagement_scores}
    
    def _analyze_content_structure(self, text: str) -> Dict[str, Any]:
        """ë‚´ìš© êµ¬ì¡° ë¶„ì„"""
        structure_elements = {
            "has_introduction": any(pattern in text for pattern in self.presentation_patterns["opening_patterns"]),
            "has_main_points": any(pattern in text for pattern in self.presentation_patterns["key_point_indicators"]),
            "has_transitions": any(pattern in text for pattern in self.presentation_patterns["transition_phrases"]),
            "has_conclusion": any(pattern in text for pattern in self.presentation_patterns["conclusion_patterns"]),
            "has_emphasis": any(pattern in text for pattern in self.presentation_patterns["emphasis_markers"])
        }
        
        structure_score = sum(structure_elements.values()) / len(structure_elements)
        
        return {
            "structure_completeness": structure_score,
            "elements_present": structure_elements,
            "organization_level": "well_structured" if structure_score >= 0.6 else "moderately_structured" if structure_score >= 0.3 else "poorly_structured"
        }
    
    def _analyze_emotional_tone(self, text: str) -> Dict[str, Any]:
        """ê°ì • í†¤ ë¶„ì„"""
        emotional_indicators = {
            "enthusiastic": ["ì •ë§", "êµ‰ì¥íˆ", "ë§¤ìš°", "ë†€ë¼ìš´", "í™˜ìƒì "],
            "confident": ["í™•ì‹ ", "ë¶„ëª…", "ë‹¹ì—°íˆ", "í™•ì‹¤íˆ", "ëª…í™•íˆ"],
            "cautious": ["ì•„ë§ˆë„", "ê°€ëŠ¥ì„±", "ê³ ë ¤í•´ì•¼", "ì£¼ì˜í•´ì•¼", "ì‹ ì¤‘íˆ"],
            "urgent": ["ê¸´ê¸‰", "ë¹¨ë¦¬", "ì¦‰ì‹œ", "ë°˜ë“œì‹œ", "ì¤‘ìš”"],
            "neutral": ["ì…ë‹ˆë‹¤", "ìˆìŠµë‹ˆë‹¤", "ë©ë‹ˆë‹¤", "ê²ƒì…ë‹ˆë‹¤"]
        }
        
        tone_scores = {}
        for tone, indicators in emotional_indicators.items():
            score = sum(1 for indicator in indicators if indicator in text)
            if score > 0:
                tone_scores[tone] = score
        
        dominant_tone = max(tone_scores, key=tone_scores.get) if tone_scores else "neutral"
        
        return {
            "dominant_tone": dominant_tone,
            "tone_distribution": tone_scores,
            "emotional_intensity": sum(tone_scores.values()) / len(text.split()) * 100 if text else 0
        }
    
    def _classify_message_type(self, message: str) -> str:
        """ë©”ì‹œì§€ íƒ€ì… ë¶„ë¥˜"""
        message_lower = message.lower()
        
        if any(pattern in message_lower for pattern in self.presentation_patterns["conclusion_patterns"]):
            return "conclusion"
        elif any(pattern in message_lower for pattern in self.presentation_patterns["key_point_indicators"]):
            return "key_point"
        elif any(pattern in message_lower for pattern in self.presentation_patterns["opening_patterns"]):
            return "introduction"
        elif "í•´ì•¼" in message_lower or "í•„ìš”" in message_lower or "ê¶Œì¥" in message_lower:
            return "action"
        elif "?" in message:
            return "question"
        else:
            return "general"
    
    def _calculate_message_importance(self, message: Dict, speaker_intent: Dict) -> float:
        """ë©”ì‹œì§€ ì¤‘ìš”ë„ ê³„ì‚°"""
        base_score = message.get("raw_score", 0)
        
        # ë©”ì‹œì§€ íƒ€ì…ë³„ ë³´ë„ˆìŠ¤
        type_bonus = {
            "key_point": 1.5,
            "conclusion": 1.3,
            "action": 1.2,
            "introduction": 1.1,
            "question": 1.0,
            "general": 0.8
        }
        
        msg_type = message.get("message_type", "general")
        score = base_score * type_bonus.get(msg_type, 1.0)
        
        # ì†ŒìŠ¤ íƒ€ì…ë³„ ë³´ë„ˆìŠ¤
        source_bonus = {
            "multimodal": 1.3,  # ìŒì„±+ì‹œê° ë™ì‹œ
            "audio": 1.0,
            "visual": 1.1
        }
        
        source_type = message.get("source_type", "audio")
        score *= source_bonus.get(source_type, 1.0)
        
        # ìŠ¤í”¼ì»¤ ì˜ë„ì™€ì˜ ì¼ì¹˜ë„ ë³´ë„ˆìŠ¤
        intent_type = speaker_intent.get("primary_intent", {}).get("type", "general")
        if intent_type == "inform" and msg_type == "key_point":
            score *= 1.2
        elif intent_type == "persuade" and msg_type == "action":
            score *= 1.3
        
        return score
    
    def _derive_actionable_insights(self, message_hierarchy: Dict, context: Dict = None) -> List[Dict[str, Any]]:
        """ì‹¤í–‰ ê°€ëŠ¥í•œ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ"""
        insights = []
        
        # ì£¼ìš” ë©”ì‹œì§€ ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸
        if message_hierarchy.get("main_theme"):
            main_theme = message_hierarchy["main_theme"]["content"]
            insights.append({
                "type": "main_message",
                "insight": f"ê°•ì—°ìì˜ í•µì‹¬ ë©”ì‹œì§€: {main_theme[:100]}...",
                "action": "ì´ ë©”ì‹œì§€ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ í›„ì† ë…¼ì˜ë‚˜ ì‹¤í–‰ ê³„íš ìˆ˜ë¦½",
                "priority": "high"
            })
        
        # ì‹¤í–‰ í•­ëª© ì¸ì‚¬ì´íŠ¸
        if message_hierarchy.get("call_to_actions"):
            insights.append({
                "type": "action_items",
                "insight": f"{len(message_hierarchy['call_to_actions'])}ê°œì˜ êµ¬ì²´ì ì¸ ì‹¤í–‰ í•­ëª© ì œì‹œë¨",
                "action": "ì œì‹œëœ ì‹¤í–‰ í•­ëª©ë“¤ì„ ì²´í¬ë¦¬ìŠ¤íŠ¸ë¡œ ì •ë¦¬í•˜ì—¬ ë‹¨ê³„ë³„ ì‹¤í–‰",
                "priority": "high"
            })
        
        # ì§€ì‹ ê³µìœ  ì¸ì‚¬ì´íŠ¸
        if len(message_hierarchy.get("supporting_details", [])) > 3:
            insights.append({
                "type": "knowledge_sharing",
                "insight": "í’ë¶€í•œ ì„¸ë¶€ ì •ë³´ì™€ ë°°ê²½ ì§€ì‹ ì œê³µë¨",
                "action": "ìƒì„¸ ë‚´ìš©ì„ ì •ë¦¬í•˜ì—¬ íŒ€ ë‚´ ì§€ì‹ ê³µìœ  ìë£Œë¡œ í™œìš©",
                "priority": "medium"
            })
        
        return insights
    
    def _generate_insights_from_intent(self, speaker_intent: Dict) -> List[str]:
        """ì˜ë„ ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        insights = []
        
        intent_type = speaker_intent.get("primary_intent", {}).get("type", "general")
        
        if intent_type == "inform":
            insights.append("ğŸ’¡ ì •ë³´ ì „ë‹¬ ì¤‘ì‹¬ì˜ ë°œí‘œ - í•µì‹¬ ì •ë³´ ìŠµë“ì— ì§‘ì¤‘")
        elif intent_type == "persuade":
            insights.append("ğŸ¯ ì„¤ë“ì„ ìœ„í•œ ë°œí‘œ - ì œì•ˆì‚¬í•­ì— ëŒ€í•œ ì˜ì‚¬ê²°ì • í•„ìš”")
        elif intent_type == "educate":
            insights.append("ğŸ“š êµìœ¡ ëª©ì ì˜ ë°œí‘œ - í•™ìŠµí•œ ë‚´ìš©ì„ ì‹¤ë¬´ì— ì ìš© ê²€í† ")
        elif intent_type == "inspire":
            insights.append("ğŸ”¥ ë™ê¸°ë¶€ì—¬ ì¤‘ì‹¬ì˜ ë°œí‘œ - ê°œì¸/íŒ€ ëª©í‘œ ì¬ì •ë¹„ ê¸°íšŒ")
        
        # ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ìŠ¤íƒ€ì¼ ì¸ì‚¬ì´íŠ¸
        comm_style = speaker_intent.get("communication_style", {}).get("dominant_style", "neutral")
        if comm_style == "interactive":
            insights.append("ğŸ¤ ìƒí˜¸ì‘ìš©ì  ë°œí‘œ ìŠ¤íƒ€ì¼ - ì¶”ê°€ ì§ˆì˜ì‘ë‹µ ì‹œê°„ í™•ë³´ ê¶Œì¥")
        elif comm_style == "technical":
            insights.append("ğŸ”¬ ê¸°ìˆ ì  ë°œí‘œ ë‚´ìš© - ì „ë¬¸ ìš©ì–´ ë° ì„¸ë¶€ì‚¬í•­ ì¬ê²€í†  í•„ìš”")
        
        return insights
    
    def _calculate_overall_confidence(self, message_hierarchy: Dict) -> float:
        """ì „ì²´ ì‹ ë¢°ë„ ê³„ì‚°"""
        factors = [
            message_hierarchy.get("main_theme") is not None,
            len(message_hierarchy.get("key_points", [])) >= 2,
            len(message_hierarchy.get("supporting_details", [])) >= 1,
            len(message_hierarchy.get("conclusions", [])) >= 1
        ]
        
        confidence = sum(factors) / len(factors)
        return confidence
    
    def _calculate_clova_compatibility(self, message_hierarchy: Dict, speaker_intent: Dict) -> float:
        """í´ë¡œë°” ë…¸íŠ¸ í˜¸í™˜ì„± ì ìˆ˜"""
        compatibility_factors = [
            message_hierarchy.get("main_theme") is not None,  # ëª…í™•í•œ ì£¼ì œ
            len(message_hierarchy.get("key_points", [])) >= 1,  # í•µì‹¬ í¬ì¸íŠ¸
            speaker_intent.get("content_structure", {}).get("structure_completeness", 0) > 0.5,  # êµ¬ì¡°í™”
            speaker_intent.get("primary_intent", {}).get("confidence", 0) > 0.3  # ì˜ë„ ëª…í™•ì„±
        ]
        
        score = sum(compatibility_factors) / len(compatibility_factors)
        return score
    
    # Context application methods (helper methods)
    def _apply_speaker_context(self, content: Dict, context: Dict) -> Dict:
        """ë°œí‘œì ì»¨í…ìŠ¤íŠ¸ ì ìš©"""
        # ê°„ë‹¨í•œ êµ¬í˜„ - ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ë¡œì§ í•„ìš”
        return content
    
    def _apply_event_context(self, content: Dict, context: Dict) -> Dict:
        """ì´ë²¤íŠ¸ ì»¨í…ìŠ¤íŠ¸ ì ìš©"""
        return content
    
    def _apply_topic_enhancement(self, content: Dict, context: Dict) -> Dict:
        """ì£¼ì œ í‚¤ì›Œë“œ ê°•í™”"""
        return content
    
    def _apply_objective_filtering(self, content: Dict, context: Dict) -> Dict:
        """ëª©ì  ê¸°ë°˜ í•„í„°ë§"""
        return content
    
    def _assess_content_richness(self, content: Dict) -> float:
        """ì½˜í…ì¸  í’ë¶€ë„ í‰ê°€"""
        richness = 0
        if content.get("audio_content"):
            richness += 0.5
        if content.get("visual_content"):
            richness += 0.3
        if len(content.get("temporal_sync", [])) > 0:
            richness += 0.2
        return min(richness, 1.0)

# ì „ì—­ ë©”ì‹œì§€ ì¶”ì¶œ ì—”ì§„
global_message_extractor = ComprehensiveMessageExtractor()

def extract_speaker_message(analysis_results: Dict[str, Any], context: Dict[str, Any] = None) -> Dict[str, Any]:
    """ê°•ì—°ì ë©”ì‹œì§€ ì¶”ì¶œ í†µí•© í•¨ìˆ˜"""
    return global_message_extractor.extract_comprehensive_message(analysis_results, context)

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    print("ğŸ¯ ì¢…í•© ë©”ì‹œì§€ ì¶”ì¶œ ì—”ì§„ í…ŒìŠ¤íŠ¸")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_data = {
        "audio_analysis": {
            "status": "success",
            "enhanced_text": "ì˜¤ëŠ˜ ë§ì”€ë“œë¦´ ì£¼ì œëŠ” ë””ì§€í„¸ ì „í™˜ì…ë‹ˆë‹¤. ê°€ì¥ ì¤‘ìš”í•œ ê²ƒì€ ê³ ê° ì¤‘ì‹¬ì˜ ì‚¬ê³ ì…ë‹ˆë‹¤. ê²°ë¡ ì ìœ¼ë¡œ ìš°ë¦¬ëŠ” ì¦‰ì‹œ í–‰ë™í•´ì•¼ í•©ë‹ˆë‹¤.",
            "segments": [
                {"start": 0, "end": 10, "text": "ì˜¤ëŠ˜ ë§ì”€ë“œë¦´ ì£¼ì œëŠ” ë””ì§€í„¸ ì „í™˜ì…ë‹ˆë‹¤"},
                {"start": 10, "end": 20, "text": "ê°€ì¥ ì¤‘ìš”í•œ ê²ƒì€ ê³ ê° ì¤‘ì‹¬ì˜ ì‚¬ê³ ì…ë‹ˆë‹¤"}
            ]
        },
        "visual_analysis": {
            "status": "success",
            "combined_visual_text": "ë””ì§€í„¸ ì „í™˜ ì „ëµ ìŠ¬ë¼ì´ë“œ ê³ ê° ë§Œì¡±ë„ 95% ì¦ê°€"
        }
    }
    
    test_context = {
        "event_context": "ê¸°ì—… ì „ëµ ë°œí‘œíšŒ",
        "speakers": "CEO ê¹€ì² ìˆ˜",
        "objective": "ë””ì§€í„¸ ì „í™˜ ê³„íš ê³µìœ "
    }
    
    extractor = ComprehensiveMessageExtractor()
    result = extractor.extract_comprehensive_message(test_data, test_context)
    
    print(f"ë©”ì¸ ë©”ì‹œì§€: {result['comprehensive_analysis']['clova_style_summary']['executive_summary']}")
    print(f"í•µì‹¬ í¬ì¸íŠ¸: {len(result['comprehensive_analysis']['key_messages'])}ê°œ")
    print(f"í´ë¡œë°” í˜¸í™˜ì„±: {result['comprehensive_analysis']['clova_style_summary']['clova_compatibility_score']:.2f}")