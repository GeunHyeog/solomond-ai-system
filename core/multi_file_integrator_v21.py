"""
ğŸ“Š Solomond AI v2.1 - ë‹¤ì¤‘ íŒŒì¼ í†µí•© ë¶„ì„ê¸°
ì‹œê³„ì—´ ê¸°ë°˜ ë‹¤ì¤‘ íŒŒì¼ í†µí•©, ë‚´ìš© ì—°ê²°, ìƒí™©ë³„ ë¶„ë¥˜ ë° ì¢…í•© ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ

Author: ì „ê·¼í˜ (Solomond)
Created: 2025.07.11
Version: 2.1.0
"""

import os
import json
import time
import hashlib
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any, Union
from dataclasses import dataclass, asdict
import logging
import numpy as np
import pandas as pd
from collections import defaultdict, Counter
import re
from difflib import SequenceMatcher
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed

@dataclass
class FileMetadata:
    """íŒŒì¼ ë©”íƒ€ë°ì´í„°"""
    file_path: str
    file_name: str
    file_type: str          # audio, image, document, video
    file_size: int
    creation_time: float
    modification_time: float
    estimated_duration: float  # ìŒì„±/ë¹„ë””ì˜¤ì˜ ê²½ìš°
    content_hash: str       # ë‚´ìš© ì¤‘ë³µ ê²€ì‚¬ìš©
    quality_score: float    # í’ˆì§ˆ ì ìˆ˜
    language: str          # ê°ì§€ëœ ì–¸ì–´
    content_preview: str   # ë‚´ìš© ë¯¸ë¦¬ë³´ê¸° (ì²« 100ì)

@dataclass
class ContentSegment:
    """ë‚´ìš© ì„¸ê·¸ë¨¼íŠ¸"""
    segment_id: str
    source_file: str
    content_type: str      # text, audio_transcript, ocr_text, metadata
    content: str
    start_time: float      # íŒŒì¼ ë‚´ ì‹œì‘ ì‹œê°„
    end_time: float        # íŒŒì¼ ë‚´ ì¢…ë£Œ ì‹œê°„
    confidence: float      # ë‚´ìš© ì‹ ë¢°ë„
    language: str
    keywords: List[str]    # ì¶”ì¶œëœ í‚¤ì›Œë“œ
    jewelry_terms: List[str]  # ì£¼ì–¼ë¦¬ ì „ë¬¸ìš©ì–´
    timestamp: float       # ì‹¤ì œ ì‹œê°„ (Unix timestamp)

@dataclass
class IntegratedSession:
    """í†µí•© ì„¸ì…˜"""
    session_id: str
    session_type: str      # meeting, seminar, lecture, trade_show, conference
    title: str
    start_time: float
    end_time: float
    participants: List[str]
    files: List[FileMetadata]
    segments: List[ContentSegment]
    merged_content: str    # ë³‘í•©ëœ ë‚´ìš©
    key_insights: List[str]
    action_items: List[str]
    summary: str
    confidence_score: float
    processing_details: Dict[str, Any]

class ContentClassifier:
    """ë‚´ìš© ë¶„ë¥˜ê¸° - íšŒì˜/ì„¸ë¯¸ë‚˜/ê°•ì˜ ë“± ìƒí™© ë¶„ë¥˜"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.classification_keywords = {
            'meeting': [
                'íšŒì˜', 'ë¯¸íŒ…', 'ë…¼ì˜', 'ê²°ì •', 'ì•ˆê±´', 'ì˜ê²¬', 'ë™ì˜', 'ë°˜ëŒ€',
                'meeting', 'discussion', 'agenda', 'decision', 'vote'
            ],
            'seminar': [
                'ì„¸ë¯¸ë‚˜', 'ì›Œí¬ìˆ', 'êµìœ¡', 'ê°•ì—°', 'ë°œí‘œ', 'í”„ë ˆì  í…Œì´ì…˜',
                'seminar', 'workshop', 'presentation', 'training', 'lecture'
            ],
            'lecture': [
                'ê°•ì˜', 'ìˆ˜ì—…', 'í•™ìŠµ', 'ì„¤ëª…', 'ì´ë¡ ', 'ì›ë¦¬', 'ë°©ë²•',
                'lecture', 'class', 'learning', 'explanation', 'theory'
            ],
            'trade_show': [
                'ì „ì‹œíšŒ', 'ë°•ëŒíšŒ', 'ì‡¼ë£¸', 'ë¶€ìŠ¤', 'ì „ì‹œ', 'ìƒí’ˆ', 'ì‹ ì œí’ˆ',
                'trade show', 'exhibition', 'expo', 'booth', 'showcase'
            ],
            'conference': [
                'ì»¨í¼ëŸ°ìŠ¤', 'ì‹¬í¬ì§€ì—„', 'í¬ëŸ¼', 'ì´íšŒ', 'ëŒ€íšŒ', 'í•™íšŒ',
                'conference', 'symposium', 'forum', 'convention', 'summit'
            ]
        }
        
    def classify_session_type(self, content_segments: List[ContentSegment]) -> Tuple[str, float]:
        """ì„¸ì…˜ íƒ€ì… ë¶„ë¥˜"""
        try:
            # ëª¨ë“  ì»¨í…ì¸  í†µí•©
            all_content = ' '.join([seg.content for seg in content_segments]).lower()
            
            # íƒ€ì…ë³„ ì ìˆ˜ ê³„ì‚°
            type_scores = {}
            
            for session_type, keywords in self.classification_keywords.items():
                score = 0
                for keyword in keywords:
                    count = all_content.count(keyword.lower())
                    score += count
                
                # ì •ê·œí™” (í‚¤ì›Œë“œ ìˆ˜ ëŒ€ë¹„)
                type_scores[session_type] = score / len(keywords)
            
            # ìµœê³  ì ìˆ˜ íƒ€ì… ì„ íƒ
            if not type_scores or max(type_scores.values()) == 0:
                return 'meeting', 0.5  # ê¸°ë³¸ê°’
            
            best_type = max(type_scores, key=type_scores.get)
            confidence = min(1.0, type_scores[best_type] / 10)  # 0-1 ì •ê·œí™”
            
            return best_type, confidence
            
        except Exception as e:
            self.logger.error(f"ì„¸ì…˜ íƒ€ì… ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
            return 'meeting', 0.5

class TimelineAnalyzer:
    """ì‹œê³„ì—´ ë¶„ì„ê¸° - íŒŒì¼ë“¤ì˜ ì‹œê°„ ìˆœì„œ ë¶„ì„"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
    def create_timeline(self, files_metadata: List[FileMetadata]) -> List[FileMetadata]:
        """íŒŒì¼ë“¤ì˜ ì‹œê°„ìˆœ íƒ€ì„ë¼ì¸ ìƒì„±"""
        try:
            # ì‹œê°„ ê¸°ì¤€ ì •ë ¬ (ìƒì„±ì‹œê°„ ìš°ì„ , ìˆ˜ì •ì‹œê°„ ë³´ì¡°)
            sorted_files = sorted(
                files_metadata,
                key=lambda f: (f.creation_time, f.modification_time)
            )
            
            return sorted_files
            
        except Exception as e:
            self.logger.error(f"íƒ€ì„ë¼ì¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return files_metadata
    
    def detect_time_gaps(self, timeline: List[FileMetadata], max_gap_hours: float = 2.0) -> List[Tuple[int, int, float]]:
        """ì‹œê°„ ê°„ê²© ê°ì§€ (ì„¸ì…˜ êµ¬ë¶„ìš©)"""
        try:
            gaps = []
            
            for i in range(len(timeline) - 1):
                current_end = timeline[i].modification_time
                next_start = timeline[i + 1].creation_time
                
                gap_hours = (next_start - current_end) / 3600
                
                if gap_hours > max_gap_hours:
                    gaps.append((i, i + 1, gap_hours))
            
            return gaps
            
        except Exception as e:
            self.logger.error(f"ì‹œê°„ ê°„ê²© ê°ì§€ ì‹¤íŒ¨: {e}")
            return []
    
    def group_files_by_session(self, timeline: List[FileMetadata]) -> List[List[FileMetadata]]:
        """ì‹œê°„ ê°„ê²© ê¸°ì¤€ìœ¼ë¡œ íŒŒì¼ë“¤ì„ ì„¸ì…˜ë³„ë¡œ ê·¸ë£¹í™”"""
        try:
            if not timeline:
                return []
            
            gaps = self.detect_time_gaps(timeline)
            sessions = []
            
            current_session = []
            gap_indices = [gap[0] + 1 for gap in gaps]  # ìƒˆ ì„¸ì…˜ ì‹œì‘ ì¸ë±ìŠ¤
            
            for i, file_meta in enumerate(timeline):
                current_session.append(file_meta)
                
                # ë‹¤ìŒ íŒŒì¼ì´ ìƒˆ ì„¸ì…˜ ì‹œì‘ì´ê±°ë‚˜ ë§ˆì§€ë§‰ íŒŒì¼ì¸ ê²½ìš°
                if i + 1 in gap_indices or i == len(timeline) - 1:
                    if current_session:
                        sessions.append(current_session)
                        current_session = []
            
            return sessions
            
        except Exception as e:
            self.logger.error(f"ì„¸ì…˜ë³„ ê·¸ë£¹í™” ì‹¤íŒ¨: {e}")
            return [timeline]  # ì‹¤íŒ¨ ì‹œ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ì„¸ì…˜ìœ¼ë¡œ

class ContentDeduplicator:
    """ë‚´ìš© ì¤‘ë³µ ì œê±°ê¸°"""
    
    def __init__(self, similarity_threshold: float = 0.8):
        self.similarity_threshold = similarity_threshold
        self.logger = logging.getLogger(__name__)
    
    def calculate_content_similarity(self, content1: str, content2: str) -> float:
        """ë‘ ë‚´ìš© ê°„ ìœ ì‚¬ë„ ê³„ì‚°"""
        try:
            if not content1.strip() or not content2.strip():
                return 0.0
            
            # í…ìŠ¤íŠ¸ ì •ê·œí™”
            normalized1 = self._normalize_text(content1)
            normalized2 = self._normalize_text(content2)
            
            # SequenceMatcherë¥¼ ì‚¬ìš©í•œ ìœ ì‚¬ë„ ê³„ì‚°
            similarity = SequenceMatcher(None, normalized1, normalized2).ratio()
            
            return similarity
            
        except Exception as e:
            self.logger.error(f"ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    def _normalize_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ê·œí™”"""
        # ì†Œë¬¸ì ë³€í™˜, ê³µë°± ì •ë¦¬, íŠ¹ìˆ˜ë¬¸ì ì œê±°
        normalized = re.sub(r'[^\w\sê°€-í£]', '', text.lower())
        normalized = re.sub(r'\s+', ' ', normalized).strip()
        return normalized
    
    def remove_duplicate_segments(self, segments: List[ContentSegment]) -> List[ContentSegment]:
        """ì¤‘ë³µ ì„¸ê·¸ë¨¼íŠ¸ ì œê±°"""
        try:
            unique_segments = []
            
            for current_segment in segments:
                is_duplicate = False
                
                for existing_segment in unique_segments:
                    similarity = self.calculate_content_similarity(
                        current_segment.content,
                        existing_segment.content
                    )
                    
                    if similarity >= self.similarity_threshold:
                        # ë” ì‹ ë¢°ë„ê°€ ë†’ì€ ì„¸ê·¸ë¨¼íŠ¸ ìœ ì§€
                        if current_segment.confidence > existing_segment.confidence:
                            unique_segments.remove(existing_segment)
                            unique_segments.append(current_segment)
                        is_duplicate = True
                        break
                
                if not is_duplicate:
                    unique_segments.append(current_segment)
            
            self.logger.info(f"ì¤‘ë³µ ì œê±°: {len(segments)} â†’ {len(unique_segments)} ì„¸ê·¸ë¨¼íŠ¸")
            return unique_segments
            
        except Exception as e:
            self.logger.error(f"ì¤‘ë³µ ì œê±° ì‹¤íŒ¨: {e}")
            return segments

class KeywordExtractor:
    """í‚¤ì›Œë“œ ì¶”ì¶œê¸°"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
        # ì£¼ì–¼ë¦¬ ì—…ê³„ ì£¼ìš” í‚¤ì›Œë“œ
        self.jewelry_keywords = {
            'ì œí’ˆ': ['ë‹¤ì´ì•„ëª¬ë“œ', 'ë£¨ë¹„', 'ì‚¬íŒŒì´ì–´', 'ì—ë©”ë„ë“œ', 'ì§„ì£¼', 'ê¸ˆ', 'ì€', 'í”Œë˜í‹°ë„˜'],
            'í’ˆì§ˆ': ['ìºëŸ¿', 'íˆ¬ëª…ë„', 'ì»¬ëŸ¬', 'ì»¤íŒ…', '4C', 'ë“±ê¸‰', 'ì¸ì¦', 'GIA'],
            'ë¹„ì¦ˆë‹ˆìŠ¤': ['ê°€ê²©', 'ì‹œì„¸', 'ë„ë§¤', 'ì†Œë§¤', 'ìˆ˜ì…', 'ìˆ˜ì¶œ', 'ë§ˆì§„', 'ìˆ˜ìµ'],
            'ê¸°ìˆ ': ['ì„¸íŒ…', 'ê°€ê³µ', 'ì—°ë§ˆ', 'ì¡°ê°', 'ë””ìì¸', 'ì œì‘', 'ìˆ˜ë¦¬'],
            'ì‹œì¥': ['íŠ¸ë Œë“œ', 'ìˆ˜ìš”', 'ê³µê¸‰', 'ê²½ìŸ', 'ë¸Œëœë“œ', 'ë§ˆì¼€íŒ…', 'ê³ ê°']
        }
        
        # ë¶ˆìš©ì–´ (ì œì™¸í•  ë‹¨ì–´ë“¤)
        self.stop_words = {
            'ê·¸', 'ì´', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ìˆ', 'í•˜', 'ë˜', 'ì˜', 'ê°€', 'ì„', 'ë¥¼',
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for'
        }
    
    def extract_keywords(self, content: str, max_keywords: int = 20) -> Tuple[List[str], List[str]]:
        """í‚¤ì›Œë“œ ë° ì£¼ì–¼ë¦¬ ì „ë¬¸ìš©ì–´ ì¶”ì¶œ"""
        try:
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            content = re.sub(r'[^\w\sê°€-í£]', ' ', content)
            words = content.split()
            
            # ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
            word_freq = Counter()
            jewelry_terms = []
            
            for word in words:
                word = word.strip().lower()
                
                if len(word) > 1 and word not in self.stop_words:
                    word_freq[word] += 1
                    
                    # ì£¼ì–¼ë¦¬ ìš©ì–´ í™•ì¸
                    for category, terms in self.jewelry_keywords.items():
                        if any(term.lower() in word or word in term.lower() for term in terms):
                            if word not in jewelry_terms:
                                jewelry_terms.append(word)
            
            # ìƒìœ„ í‚¤ì›Œë“œ ì„ íƒ
            top_keywords = [word for word, freq in word_freq.most_common(max_keywords)]
            
            return top_keywords, jewelry_terms
            
        except Exception as e:
            self.logger.error(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return [], []

class MultiFileIntegratorV21:
    """v2.1 ë‹¤ì¤‘ íŒŒì¼ í†µí•© ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.classifier = ContentClassifier()
        self.timeline_analyzer = TimelineAnalyzer()
        self.deduplicator = ContentDeduplicator()
        self.keyword_extractor = KeywordExtractor()
        
    def integrate_multiple_files(self, file_paths: List[str], stt_results: Dict = None, ocr_results: Dict = None) -> Dict[str, Any]:
        """ë‹¤ì¤‘ íŒŒì¼ í†µí•© ë¶„ì„"""
        try:
            start_time = time.time()
            
            # 1. íŒŒì¼ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
            files_metadata = self._collect_files_metadata(file_paths)
            
            # 2. ì‹œê°„ìˆœ ì •ë ¬ ë° ì„¸ì…˜ ê·¸ë£¹í™”
            timeline = self.timeline_analyzer.create_timeline(files_metadata)
            sessions = self.timeline_analyzer.group_files_by_session(timeline)
            
            self.logger.info(f"ê°ì§€ëœ ì„¸ì…˜ ìˆ˜: {len(sessions)}")
            
            # 3. ê° ì„¸ì…˜ë³„ í†µí•© ë¶„ì„
            integrated_sessions = []
            
            for i, session_files in enumerate(sessions):
                session_result = self._integrate_single_session(
                    session_files, i, stt_results, ocr_results
                )
                integrated_sessions.append(session_result)
            
            # 4. ì „ì²´ í†µí•© ê²°ê³¼ ìƒì„±
            overall_result = self._create_overall_integration(integrated_sessions)
            
            processing_time = time.time() - start_time
            
            return {
                'individual_sessions': integrated_sessions,
                'overall_integration': overall_result,
                'processing_statistics': {
                    'total_files': len(file_paths),
                    'total_sessions': len(sessions),
                    'processing_time': processing_time,
                    'files_per_session': [len(session) for session in sessions]
                },
                'timeline_analysis': {
                    'first_file_time': min(f.creation_time for f in files_metadata) if files_metadata else 0,
                    'last_file_time': max(f.modification_time for f in files_metadata) if files_metadata else 0,
                    'total_duration_hours': (max(f.modification_time for f in files_metadata) - 
                                           min(f.creation_time for f in files_metadata)) / 3600 if files_metadata else 0
                }
            }
            
        except Exception as e:
            self.logger.error(f"ë‹¤ì¤‘ íŒŒì¼ í†µí•© ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'error': str(e),
                'processing_complete': False
            }
    
    def _collect_files_metadata(self, file_paths: List[str]) -> List[FileMetadata]:
        """íŒŒì¼ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘"""
        try:
            metadata_list = []
            
            for file_path in file_paths:
                try:
                    path_obj = Path(file_path)
                    
                    if not path_obj.exists():
                        self.logger.warning(f"íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {file_path}")
                        continue
                    
                    stat = path_obj.stat()
                    
                    # íŒŒì¼ íƒ€ì… ê²°ì •
                    file_ext = path_obj.suffix.lower()
                    if file_ext in ['.mp3', '.wav', '.m4a', '.aac']:
                        file_type = 'audio'
                        duration = self._estimate_audio_duration(file_path)
                    elif file_ext in ['.mp4', '.avi', '.mov', '.mkv']:
                        file_type = 'video'
                        duration = self._estimate_video_duration(file_path)
                    elif file_ext in ['.jpg', '.jpeg', '.png', '.bmp', '.gif']:
                        file_type = 'image'
                        duration = 0
                    else:
                        file_type = 'document'
                        duration = 0
                    
                    # ì»¨í…ì¸  í•´ì‹œ ìƒì„±
                    content_hash = self._calculate_file_hash(file_path)
                    
                    # ë‚´ìš© ë¯¸ë¦¬ë³´ê¸° (ê°€ëŠ¥í•œ ê²½ìš°)
                    content_preview = self._get_content_preview(file_path, file_type)
                    
                    metadata = FileMetadata(
                        file_path=file_path,
                        file_name=path_obj.name,
                        file_type=file_type,
                        file_size=stat.st_size,
                        creation_time=stat.st_ctime,
                        modification_time=stat.st_mtime,
                        estimated_duration=duration,
                        content_hash=content_hash,
                        quality_score=75.0,  # ê¸°ë³¸ê°’, ë‚˜ì¤‘ì— í’ˆì§ˆ ë¶„ì„ ê²°ê³¼ë¡œ ì—…ë°ì´íŠ¸
                        language='ko',       # ê¸°ë³¸ê°’, ë‚˜ì¤‘ì— ì–¸ì–´ ê°ì§€ ê²°ê³¼ë¡œ ì—…ë°ì´íŠ¸
                        content_preview=content_preview
                    )
                    
                    metadata_list.append(metadata)
                    
                except Exception as e:
                    self.logger.error(f"íŒŒì¼ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨ ({file_path}): {e}")
            
            return metadata_list
            
        except Exception as e:
            self.logger.error(f"ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []
    
    def _estimate_audio_duration(self, audio_path: str) -> float:
        """ìŒì„± íŒŒì¼ ê¸¸ì´ ì¶”ì •"""
        try:
            import librosa
            y, sr = librosa.load(audio_path, sr=None)
            return len(y) / sr
        except Exception:
            # librosa ì‹¤íŒ¨ ì‹œ íŒŒì¼ í¬ê¸° ê¸°ë°˜ ì¶”ì •
            try:
                file_size = Path(audio_path).stat().st_size
                # MP3 í‰ê·  ë¹„íŠ¸ë ˆì´íŠ¸ 128kbps ê°€ì •
                estimated_duration = file_size / (128 * 1000 / 8)
                return estimated_duration
            except Exception:
                return 0.0
    
    def _estimate_video_duration(self, video_path: str) -> float:
        """ë¹„ë””ì˜¤ íŒŒì¼ ê¸¸ì´ ì¶”ì •"""
        try:
            # OpenCV ì‚¬ìš© ì‹œë„
            import cv2
            cap = cv2.VideoCapture(video_path)
            fps = cap.get(cv2.CAP_PROP_FPS)
            frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)
            cap.release()
            
            if fps > 0:
                return frame_count / fps
            else:
                return 0.0
        except Exception:
            # íŒŒì¼ í¬ê¸° ê¸°ë°˜ ì¶”ì •
            try:
                file_size = Path(video_path).stat().st_size
                # í‰ê·  ë¹„íŠ¸ë ˆì´íŠ¸ 1Mbps ê°€ì •
                estimated_duration = file_size / (1000 * 1000 / 8)
                return estimated_duration
            except Exception:
                return 0.0
    
    def _calculate_file_hash(self, file_path: str) -> str:
        """íŒŒì¼ í•´ì‹œ ê³„ì‚° (ì¤‘ë³µ ê²€ì‚¬ìš©)"""
        try:
            hash_md5 = hashlib.md5()
            with open(file_path, "rb") as f:
                # í° íŒŒì¼ì˜ ê²½ìš° ì¼ë¶€ë§Œ í•´ì‹œ ê³„ì‚°
                chunk = f.read(8192)
                while chunk:
                    hash_md5.update(chunk)
                    chunk = f.read(8192)
            return hash_md5.hexdigest()
        except Exception as e:
            self.logger.error(f"íŒŒì¼ í•´ì‹œ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return ""
    
    def _get_content_preview(self, file_path: str, file_type: str) -> str:
        """ë‚´ìš© ë¯¸ë¦¬ë³´ê¸° ìƒì„±"""
        try:
            if file_type == 'document' and file_path.endswith('.txt'):
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read(200)
                    return content.strip()
            
            # ë‹¤ë¥¸ íŒŒì¼ íƒ€ì…ì˜ ê²½ìš° íŒŒì¼ëª…ê³¼ íƒ€ì… ì •ë³´ ë°˜í™˜
            return f"[{file_type.upper()}] {Path(file_path).stem}"
            
        except Exception as e:
            self.logger.error(f"ë‚´ìš© ë¯¸ë¦¬ë³´ê¸° ìƒì„± ì‹¤íŒ¨: {e}")
            return f"[{file_type.upper()}] {Path(file_path).name}"
    
    def _integrate_single_session(self, session_files: List[FileMetadata], session_index: int,
                                 stt_results: Dict = None, ocr_results: Dict = None) -> IntegratedSession:
        """ë‹¨ì¼ ì„¸ì…˜ í†µí•© ë¶„ì„"""
        try:
            session_id = f"session_{session_index}_{int(time.time())}"
            
            # 1. ì»¨í…ì¸  ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±
            segments = self._create_content_segments(session_files, stt_results, ocr_results)
            
            # 2. ì¤‘ë³µ ì œê±°
            unique_segments = self.deduplicator.remove_duplicate_segments(segments)
            
            # 3. ì„¸ì…˜ íƒ€ì… ë¶„ë¥˜
            session_type, type_confidence = self.classifier.classify_session_type(unique_segments)
            
            # 4. ë‚´ìš© ë³‘í•©
            merged_content = self._merge_segments_content(unique_segments)
            
            # 5. í‚¤ì›Œë“œ ë° ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ
            keywords, jewelry_terms = self.keyword_extractor.extract_keywords(merged_content)
            key_insights = self._extract_session_insights(merged_content, session_type)
            action_items = self._extract_action_items(merged_content)
            
            # 6. ìš”ì•½ ìƒì„±
            summary = self._generate_session_summary(merged_content, session_type, key_insights)
            
            # 7. ì „ì²´ ì‹ ë¢°ë„ ê³„ì‚°
            confidence_score = self._calculate_session_confidence(unique_segments, type_confidence)
            
            # ì„¸ì…˜ ì‹œê°„ ê³„ì‚°
            start_time = min(f.creation_time for f in session_files)
            end_time = max(f.modification_time for f in session_files)
            
            return IntegratedSession(
                session_id=session_id,
                session_type=session_type,
                title=self._generate_session_title(session_type, key_insights),
                start_time=start_time,
                end_time=end_time,
                participants=self._extract_participants(merged_content),
                files=session_files,
                segments=unique_segments,
                merged_content=merged_content,
                key_insights=key_insights,
                action_items=action_items,
                summary=summary,
                confidence_score=confidence_score,
                processing_details={
                    'total_segments': len(segments),
                    'unique_segments': len(unique_segments),
                    'keywords_count': len(keywords),
                    'jewelry_terms_count': len(jewelry_terms),
                    'type_confidence': type_confidence
                }
            )
            
        except Exception as e:
            self.logger.error(f"ì„¸ì…˜ í†µí•© ë¶„ì„ ì‹¤íŒ¨: {e}")
            return IntegratedSession(
                session_id=f"error_session_{session_index}",
                session_type='unknown',
                title='ë¶„ì„ ì‹¤íŒ¨',
                start_time=time.time(),
                end_time=time.time(),
                participants=[],
                files=session_files,
                segments=[],
                merged_content="",
                key_insights=[],
                action_items=[],
                summary="ì„¸ì…˜ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                confidence_score=0.0,
                processing_details={'error': str(e)}
            )
    
    def _create_content_segments(self, files: List[FileMetadata], stt_results: Dict = None, 
                                ocr_results: Dict = None) -> List[ContentSegment]:
        """ì»¨í…ì¸  ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±"""
        segments = []
        
        try:
            for file_meta in files:
                file_path = file_meta.file_path
                
                # STT ê²°ê³¼ ì²˜ë¦¬
                if stt_results and file_path in stt_results:
                    stt_data = stt_results[file_path]
                    content = stt_data.get('text', '')
                    language = stt_data.get('language', 'ko')
                    confidence = stt_data.get('confidence', 0.8)
                    
                    if content.strip():
                        keywords, jewelry_terms = self.keyword_extractor.extract_keywords(content)
                        
                        segment = ContentSegment(
                            segment_id=f"stt_{hashlib.md5(file_path.encode()).hexdigest()[:8]}",
                            source_file=file_path,
                            content_type='audio_transcript',
                            content=content,
                            start_time=0,
                            end_time=file_meta.estimated_duration,
                            confidence=confidence,
                            language=language,
                            keywords=keywords,
                            jewelry_terms=jewelry_terms,
                            timestamp=file_meta.creation_time
                        )
                        segments.append(segment)
                
                # OCR ê²°ê³¼ ì²˜ë¦¬
                if ocr_results and file_path in ocr_results:
                    ocr_data = ocr_results[file_path]
                    content = ocr_data.get('text', '')
                    confidence = ocr_data.get('confidence', 0.8)
                    
                    if content.strip():
                        keywords, jewelry_terms = self.keyword_extractor.extract_keywords(content)
                        
                        segment = ContentSegment(
                            segment_id=f"ocr_{hashlib.md5(file_path.encode()).hexdigest()[:8]}",
                            source_file=file_path,
                            content_type='ocr_text',
                            content=content,
                            start_time=0,
                            end_time=0,
                            confidence=confidence,
                            language='ko',  # OCRì€ ê¸°ë³¸ì ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ê°€ì •
                            keywords=keywords,
                            jewelry_terms=jewelry_terms,
                            timestamp=file_meta.creation_time
                        )
                        segments.append(segment)
                
                # ë©”íƒ€ë°ì´í„° ì„¸ê·¸ë¨¼íŠ¸
                if file_meta.content_preview:
                    keywords, jewelry_terms = self.keyword_extractor.extract_keywords(file_meta.content_preview)
                    
                    segment = ContentSegment(
                        segment_id=f"meta_{hashlib.md5(file_path.encode()).hexdigest()[:8]}",
                        source_file=file_path,
                        content_type='metadata',
                        content=file_meta.content_preview,
                        start_time=0,
                        end_time=0,
                        confidence=0.6,
                        language='ko',
                        keywords=keywords,
                        jewelry_terms=jewelry_terms,
                        timestamp=file_meta.creation_time
                    )
                    segments.append(segment)
            
            # ì‹œê°„ìˆœ ì •ë ¬
            segments.sort(key=lambda x: x.timestamp)
            
            return segments
            
        except Exception as e:
            self.logger.error(f"ì»¨í…ì¸  ì„¸ê·¸ë¨¼íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return []
    
    def _merge_segments_content(self, segments: List[ContentSegment]) -> str:
        """ì„¸ê·¸ë¨¼íŠ¸ ë‚´ìš© ë³‘í•©"""
        try:
            # íƒ€ì…ë³„ë¡œ ê·¸ë£¹í™”
            content_by_type = defaultdict(list)
            
            for segment in segments:
                content_by_type[segment.content_type].append(segment.content)
            
            # ë³‘í•© ìˆœì„œ: ë©”íƒ€ë°ì´í„° â†’ OCR â†’ ìŒì„±ì „ì‚¬
            merged_parts = []
            
            if content_by_type['metadata']:
                merged_parts.append("=== íŒŒì¼ ì •ë³´ ===")
                merged_parts.extend(content_by_type['metadata'])
                merged_parts.append("")
            
            if content_by_type['ocr_text']:
                merged_parts.append("=== ë¬¸ì„œ/ì´ë¯¸ì§€ ë‚´ìš© ===")
                merged_parts.extend(content_by_type['ocr_text'])
                merged_parts.append("")
            
            if content_by_type['audio_transcript']:
                merged_parts.append("=== ìŒì„± ë‚´ìš© ===")
                merged_parts.extend(content_by_type['audio_transcript'])
            
            return '\n'.join(merged_parts)
            
        except Exception as e:
            self.logger.error(f"ì„¸ê·¸ë¨¼íŠ¸ ë‚´ìš© ë³‘í•© ì‹¤íŒ¨: {e}")
            return ""
    
    def _extract_session_insights(self, content: str, session_type: str) -> List[str]:
        """ì„¸ì…˜ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ"""
        try:
            insights = []
            
            # ì„¸ì…˜ íƒ€ì…ë³„ íŠ¹í™” ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ
            if session_type == 'meeting':
                patterns = [
                    r'ê²°ì •.*?[\.!?]',
                    r'í•©ì˜.*?[\.!?]',
                    r'ë‹¤ìŒ.*?ê³„íš.*?[\.!?]',
                    r'ë¬¸ì œ.*?í•´ê²°.*?[\.!?]'
                ]
            elif session_type == 'trade_show':
                patterns = [
                    r'ì‹ ì œí’ˆ.*?[\.!?]',
                    r'íŠ¸ë Œë“œ.*?[\.!?]',
                    r'ì‹œì¥.*?ì „ë§.*?[\.!?]',
                    r'ê³ ê°.*?ë°˜ì‘.*?[\.!?]'
                ]
            else:
                patterns = [
                    r'í•µì‹¬.*?[\.!?]',
                    r'ì¤‘ìš”.*?[\.!?]',
                    r'ì£¼ëª©.*?[\.!?]',
                    r'íŠ¹ì§•.*?[\.!?]'
                ]
            
            for pattern in patterns:
                matches = re.findall(pattern, content, re.IGNORECASE)
                insights.extend(matches[:2])  # ê° íŒ¨í„´ì—ì„œ ìµœëŒ€ 2ê°œ
            
            # ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
            unique_insights = list(set(insights))[:5]
            
            return unique_insights if unique_insights else ['ì£¼ìš” ë‚´ìš©ì´ ì„±ê³µì ìœ¼ë¡œ ë¶„ì„ë˜ì—ˆìŠµë‹ˆë‹¤.']
            
        except Exception as e:
            self.logger.error(f"ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return ['ë‚´ìš© ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.']
    
    def _extract_action_items(self, content: str) -> List[str]:
        """ì•¡ì…˜ ì•„ì´í…œ ì¶”ì¶œ"""
        try:
            action_patterns = [
                r'í• \s*ì¼.*?[\.!?]',
                r'ê³¼ì œ.*?[\.!?]',
                r'ì¤€ë¹„.*?[\.!?]',
                r'ê²€í† .*?[\.!?]',
                r'í™•ì¸.*?[\.!?]',
                r'follow.*?up.*?[\.!?]',
                r'action.*?item.*?[\.!?]',
                r'to.*?do.*?[\.!?]'
            ]
            
            action_items = []
            
            for pattern in action_patterns:
                matches = re.findall(pattern, content, re.IGNORECASE)
                action_items.extend(matches[:2])
            
            return list(set(action_items))[:5]
            
        except Exception as e:
            self.logger.error(f"ì•¡ì…˜ ì•„ì´í…œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def _generate_session_summary(self, content: str, session_type: str, insights: List[str]) -> str:
        """ì„¸ì…˜ ìš”ì•½ ìƒì„±"""
        try:
            # ìš”ì•½ í…œí”Œë¦¿
            summary_parts = []
            
            # ì„¸ì…˜ íƒ€ì…ë³„ ìš”ì•½ ì‹œì‘
            type_descriptions = {
                'meeting': 'íšŒì˜ ë‚´ìš©',
                'seminar': 'ì„¸ë¯¸ë‚˜ ì£¼ìš” ë‚´ìš©',
                'lecture': 'ê°•ì˜ í•µì‹¬ ì‚¬í•­',
                'trade_show': 'ì „ì‹œíšŒ ì°¸ê´€ ê²°ê³¼',
                'conference': 'ì»¨í¼ëŸ°ìŠ¤ ì£¼ìš” ì‚¬í•­'
            }
            
            summary_parts.append(f"**{type_descriptions.get(session_type, 'ì„¸ì…˜')} ìš”ì•½**")
            summary_parts.append("")
            
            # í•µì‹¬ ì¸ì‚¬ì´íŠ¸ í¬í•¨
            if insights:
                summary_parts.append("**ì£¼ìš” ë‚´ìš©:**")
                for i, insight in enumerate(insights[:3], 1):
                    summary_parts.append(f"{i}. {insight}")
                summary_parts.append("")
            
            # ë‚´ìš© ê¸¸ì´ì— ë”°ë¥¸ ì¶”ê°€ ìš”ì•½
            content_length = len(content)
            if content_length > 1000:
                summary_parts.append("**ìƒì„¸ ë¶„ì„:**")
                summary_parts.append(f"ì´ {content_length:,}ìì˜ ë‚´ìš©ì´ ë¶„ì„ë˜ì—ˆìœ¼ë©°, ì£¼ì–¼ë¦¬ ì—…ê³„ ê´€ë ¨ ì „ë¬¸ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            
            return '\n'.join(summary_parts)
            
        except Exception as e:
            self.logger.error(f"ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
            return "ì„¸ì…˜ì´ ì„±ê³µì ìœ¼ë¡œ ë¶„ì„ë˜ì—ˆìŠµë‹ˆë‹¤."
    
    def _generate_session_title(self, session_type: str, insights: List[str]) -> str:
        """ì„¸ì…˜ ì œëª© ìƒì„±"""
        try:
            date_str = datetime.now().strftime("%Y.%m.%d")
            
            if insights and len(insights[0]) < 50:
                # ì²« ë²ˆì§¸ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œëª©ìœ¼ë¡œ í™œìš©
                title = f"{date_str} {insights[0]}"
            else:
                # ì„¸ì…˜ íƒ€ì… ê¸°ë°˜ ê¸°ë³¸ ì œëª©
                type_titles = {
                    'meeting': 'ì£¼ì–¼ë¦¬ ì—…ê³„ íšŒì˜',
                    'seminar': 'ì£¼ì–¼ë¦¬ ì„¸ë¯¸ë‚˜',
                    'lecture': 'ì£¼ì–¼ë¦¬ ê°•ì˜',
                    'trade_show': 'ì£¼ì–¼ë¦¬ ì „ì‹œíšŒ',
                    'conference': 'ì£¼ì–¼ë¦¬ ì»¨í¼ëŸ°ìŠ¤'
                }
                title = f"{date_str} {type_titles.get(session_type, 'ì£¼ì–¼ë¦¬ ì„¸ì…˜')}"
            
            return title
            
        except Exception as e:
            self.logger.error(f"ì œëª© ìƒì„± ì‹¤íŒ¨: {e}")
            return f"ì£¼ì–¼ë¦¬ ì„¸ì…˜ {datetime.now().strftime('%Y.%m.%d')}"
    
    def _extract_participants(self, content: str) -> List[str]:
        """ì°¸ê°€ì ì¶”ì¶œ"""
        try:
            # ê°„ë‹¨í•œ íŒ¨í„´ìœ¼ë¡œ ì´ë¦„ ì¶”ì¶œ
            name_patterns = [
                r'[ê°€-í£]{2,4}(?:\s*ëŒ€í‘œ|\s*íšŒì¥|\s*ì´ì‚¬|\s*íŒ€ì¥|\s*ê³¼ì¥)',
                r'Mr\.\s*[A-Za-z]+',
                r'Ms\.\s*[A-Za-z]+',
                r'[A-Z][a-z]+\s+[A-Z][a-z]+'
            ]
            
            participants = []
            
            for pattern in name_patterns:
                matches = re.findall(pattern, content)
                participants.extend(matches)
            
            # ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
            unique_participants = list(set(participants))[:10]
            
            return unique_participants
            
        except Exception as e:
            self.logger.error(f"ì°¸ê°€ì ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def _calculate_session_confidence(self, segments: List[ContentSegment], type_confidence: float) -> float:
        """ì„¸ì…˜ ì‹ ë¢°ë„ ê³„ì‚°"""
        try:
            if not segments:
                return 0.0
            
            # ì„¸ê·¸ë¨¼íŠ¸ í‰ê·  ì‹ ë¢°ë„
            avg_segment_confidence = np.mean([seg.confidence for seg in segments])
            
            # ë‚´ìš© ì–‘ ê¸°ë°˜ ë³´ì •
            total_content_length = sum(len(seg.content) for seg in segments)
            content_factor = min(1.0, total_content_length / 1000)
            
            # ì£¼ì–¼ë¦¬ ìš©ì–´ í¬í•¨ ë³´ë„ˆìŠ¤
            total_jewelry_terms = sum(len(seg.jewelry_terms) for seg in segments)
            jewelry_bonus = min(0.2, total_jewelry_terms * 0.05)
            
            # ì¢…í•© ì‹ ë¢°ë„
            final_confidence = (
                avg_segment_confidence * 0.5 +
                type_confidence * 0.3 +
                content_factor * 0.2 +
                jewelry_bonus
            )
            
            return max(0.0, min(1.0, final_confidence))
            
        except Exception as e:
            self.logger.error(f"ì„¸ì…˜ ì‹ ë¢°ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _create_overall_integration(self, sessions: List[IntegratedSession]) -> Dict[str, Any]:
        """ì „ì²´ í†µí•© ê²°ê³¼ ìƒì„±"""
        try:
            if not sessions:
                return {'error': 'ë¶„ì„í•  ì„¸ì…˜ì´ ì—†ìŠµë‹ˆë‹¤.'}
            
            # ì „ì²´ í†µê³„
            total_files = sum(len(session.files) for session in sessions)
            total_segments = sum(len(session.segments) for session in sessions)
            avg_confidence = np.mean([session.confidence_score for session in sessions])
            
            # ëª¨ë“  ë‚´ìš© í†µí•©
            all_content = '\n\n'.join([session.merged_content for session in sessions])
            
            # ì „ì²´ í‚¤ì›Œë“œ ë° ì¸ì‚¬ì´íŠ¸
            all_insights = []
            for session in sessions:
                all_insights.extend(session.key_insights)
            
            # ì„¸ì…˜ íƒ€ì… ë¶„í¬
            session_types = Counter([session.session_type for session in sessions])
            
            # ì‹œê°„ ë²”ìœ„
            start_time = min(session.start_time for session in sessions)
            end_time = max(session.end_time for session in sessions)
            duration_hours = (end_time - start_time) / 3600
            
            return {
                'total_statistics': {
                    'total_sessions': len(sessions),
                    'total_files': total_files,
                    'total_segments': total_segments,
                    'average_confidence': avg_confidence,
                    'duration_hours': duration_hours
                },
                'session_distribution': dict(session_types),
                'time_range': {
                    'start': datetime.fromtimestamp(start_time).isoformat(),
                    'end': datetime.fromtimestamp(end_time).isoformat(),
                    'duration_hours': duration_hours
                },
                'integrated_content': all_content,
                'overall_insights': list(set(all_insights))[:10],  # ì¤‘ë³µ ì œê±° í›„ ìƒìœ„ 10ê°œ
                'processing_success': True
            }
            
        except Exception as e:
            self.logger.error(f"ì „ì²´ í†µí•© ê²°ê³¼ ìƒì„± ì‹¤íŒ¨: {e}")
            return {'error': str(e)}

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(level=logging.INFO)
    
    # ë‹¤ì¤‘ íŒŒì¼ í†µí•© ë¶„ì„ê¸° ì´ˆê¸°í™”
    integrator = MultiFileIntegratorV21()
    
    # ìƒ˜í”Œ íŒŒì¼ë“¤ ë¶„ì„
    # file_paths = ["meeting_audio.mp3", "presentation.pdf", "notes.jpg"]
    # result = integrator.integrate_multiple_files(file_paths)
    # print(f"í†µí•© ë¶„ì„ ì™„ë£Œ: {result['processing_statistics']}")
    
    print("âœ… ë‹¤ì¤‘ íŒŒì¼ í†µí•© ë¶„ì„ê¸° v2.1 ë¡œë“œ ì™„ë£Œ!")
