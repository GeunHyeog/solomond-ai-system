#!/usr/bin/env python3
"""
ìºì‹œ ë¶„ì„ ë° ìµœì í™” ì‹œìŠ¤í…œ v2.6
ìºì‹œ ì„±ëŠ¥ ë¶„ì„, íŒ¨í„´ ê°ì§€, ìë™ ìµœì í™” ì œì•ˆ
"""

import time
import json
import numpy as np
import pandas as pd
from typing import Dict, List, Any, Optional, Tuple, Set
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from collections import defaultdict, deque
import logging
import threading
from pathlib import Path
import pickle
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

@dataclass
class CacheAccessPattern:
    """ìºì‹œ ì ‘ê·¼ íŒ¨í„´"""
    key_pattern: str
    access_frequency: float
    temporal_pattern: str  # 'peak', 'off-peak', 'steady'
    size_category: str  # 'small', 'medium', 'large'
    ttl_effectiveness: float
    hit_rate: float
    avg_response_time_ms: float
    category: str  # 'audio', 'image', 'model', 'config'

@dataclass
class CacheOptimizationSuggestion:
    """ìºì‹œ ìµœì í™” ì œì•ˆ"""
    suggestion_type: str
    priority: str  # 'high', 'medium', 'low'
    description: str
    expected_improvement: float
    implementation_effort: str  # 'easy', 'medium', 'hard'
    affected_keys: List[str]
    parameters: Dict[str, Any]

@dataclass
class CachePerformanceReport:
    """ìºì‹œ ì„±ëŠ¥ ë³´ê³ ì„œ"""
    analysis_timestamp: str
    overall_hit_rate: float
    avg_response_time_ms: float
    memory_efficiency: float
    cache_pollution_score: float
    optimization_opportunities: int
    patterns_detected: List[CacheAccessPattern]
    suggestions: List[CacheOptimizationSuggestion]
    performance_trends: Dict[str, List[float]]

class CacheAnalyzer:
    """ìºì‹œ ë¶„ì„ê¸°"""
    
    def __init__(self, cache_manager, analysis_window_hours: int = 24):
        self.cache_manager = cache_manager
        self.analysis_window_hours = analysis_window_hours
        self.logger = self._setup_logging()
        
        # ë¶„ì„ ë°ì´í„° ì €ì¥ì†Œ
        self.access_logs = deque(maxlen=10000)
        self.performance_metrics = deque(maxlen=1000)
        self.pattern_history = {}
        
        # ë¶„ì„ ì„¤ì •
        self.key_clustering_threshold = 0.8
        self.pattern_detection_min_samples = 10
        self.optimization_confidence_threshold = 0.7
        
        # ì„±ëŠ¥ ê¸°ì¤€ì„ 
        self.baseline_metrics = {
            'target_hit_rate': 0.8,
            'target_response_time_ms': 50.0,
            'target_memory_efficiency': 0.7,
            'acceptable_pollution_score': 0.3
        }
        
        # íŒ¨í„´ ë¶„ë¥˜ê¸°
        self.pattern_classifier = None
        self.is_trained = False
        
        # ìŠ¤ë ˆë“œ ì•ˆì „ì„±
        self.lock = threading.RLock()
        
        # ë¶„ì„ ìºì‹œ (ë¶„ì„ ê²°ê³¼ ìºì‹±)
        self.analysis_cache = {}
        self.last_analysis_time = 0
        self.analysis_cache_ttl = 300  # 5ë¶„
        
        self.logger.info("ğŸ” ìºì‹œ ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _setup_logging(self) -> logging.Logger:
        """ë¡œê¹… ì„¤ì •"""
        logger = logging.getLogger(f'{__name__}.CacheAnalyzer')
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s | %(name)s | %(levelname)s | %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            logger.setLevel(logging.INFO)
        return logger
    
    def log_access(self, key: str, hit: bool, response_time_ms: float, 
                   cache_level: str, operation: str, data_size: int = 0) -> None:
        """ì ‘ê·¼ ë¡œê·¸ ê¸°ë¡"""
        with self.lock:
            access_log = {
                'timestamp': time.time(),
                'key': key,
                'hit': hit,
                'response_time_ms': response_time_ms,
                'cache_level': cache_level,
                'operation': operation,  # 'get', 'set', 'delete'
                'data_size': data_size,
                'hour_of_day': datetime.now().hour,
                'day_of_week': datetime.now().weekday()
            }
            
            self.access_logs.append(access_log)
            
            # ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            self._update_realtime_metrics(access_log)
    
    def _update_realtime_metrics(self, access_log: Dict[str, Any]) -> None:
        """ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        current_time = time.time()
        
        # ìµœê·¼ 1ë¶„ê°„ ë©”íŠ¸ë¦­ ê³„ì‚°
        recent_logs = [
            log for log in self.access_logs 
            if current_time - log['timestamp'] <= 60
        ]
        
        if recent_logs:
            hit_rate = sum(1 for log in recent_logs if log['hit']) / len(recent_logs)
            avg_response_time = np.mean([log['response_time_ms'] for log in recent_logs])
            
            metric = {
                'timestamp': current_time,
                'hit_rate': hit_rate,
                'avg_response_time_ms': avg_response_time,
                'total_requests': len(recent_logs),
                'cache_size_mb': self.cache_manager.get_size()
            }
            
            self.performance_metrics.append(metric)
    
    def analyze_cache_performance(self, force_refresh: bool = False) -> CachePerformanceReport:
        """ìºì‹œ ì„±ëŠ¥ ë¶„ì„"""
        current_time = time.time()
        
        # ìºì‹œëœ ë¶„ì„ ê²°ê³¼ í™•ì¸
        if not force_refresh and \
           current_time - self.last_analysis_time < self.analysis_cache_ttl and \
           'performance_report' in self.analysis_cache:
            return self.analysis_cache['performance_report']
        
        with self.lock:
            try:
                self.logger.info("ğŸ” ìºì‹œ ì„±ëŠ¥ ë¶„ì„ ì‹œì‘")
                
                # ë¶„ì„ ë°ì´í„° ì¤€ë¹„
                analysis_data = self._prepare_analysis_data()
                
                if not analysis_data:
                    return self._create_empty_report()
                
                # 1. ì „ì²´ ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
                overall_metrics = self._calculate_overall_metrics(analysis_data)
                
                # 2. ì ‘ê·¼ íŒ¨í„´ ë¶„ì„
                patterns = self._analyze_access_patterns(analysis_data)
                
                # 3. ì„±ëŠ¥ ì¶”ì„¸ ë¶„ì„
                trends = self._analyze_performance_trends()
                
                # 4. ìµœì í™” ê¸°íšŒ ì‹ë³„
                suggestions = self._generate_optimization_suggestions(
                    overall_metrics, patterns, trends
                )
                
                # 5. ë³´ê³ ì„œ ìƒì„±
                report = CachePerformanceReport(
                    analysis_timestamp=datetime.now().isoformat(),
                    overall_hit_rate=overall_metrics['hit_rate'],
                    avg_response_time_ms=overall_metrics['avg_response_time_ms'],
                    memory_efficiency=overall_metrics['memory_efficiency'],
                    cache_pollution_score=overall_metrics['pollution_score'],
                    optimization_opportunities=len(suggestions),
                    patterns_detected=patterns,
                    suggestions=suggestions,
                    performance_trends=trends
                )
                
                # ê²°ê³¼ ìºì‹±
                self.analysis_cache['performance_report'] = report
                self.last_analysis_time = current_time
                
                self.logger.info(f"âœ… ìºì‹œ ì„±ëŠ¥ ë¶„ì„ ì™„ë£Œ: íˆíŠ¸ìœ¨ {report.overall_hit_rate:.1%}")
                return report
                
            except Exception as e:
                self.logger.error(f"âŒ ìºì‹œ ì„±ëŠ¥ ë¶„ì„ ì‹¤íŒ¨: {e}")
                return self._create_empty_report()
    
    def _prepare_analysis_data(self) -> List[Dict[str, Any]]:
        """ë¶„ì„ ë°ì´í„° ì¤€ë¹„"""
        cutoff_time = time.time() - (self.analysis_window_hours * 3600)
        
        # ë¶„ì„ ìœˆë„ìš° ë‚´ ë°ì´í„° í•„í„°ë§
        analysis_data = [
            log for log in self.access_logs 
            if log['timestamp'] > cutoff_time
        ]
        
        return analysis_data
    
    def _calculate_overall_metrics(self, data: List[Dict[str, Any]]) -> Dict[str, float]:
        """ì „ì²´ ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°"""
        if not data:
            return {
                'hit_rate': 0.0,
                'avg_response_time_ms': 0.0,
                'memory_efficiency': 0.0,
                'pollution_score': 0.0
            }
        
        # íˆíŠ¸ìœ¨
        hit_rate = sum(1 for log in data if log['hit']) / len(data)
        
        # í‰ê·  ì‘ë‹µ ì‹œê°„
        avg_response_time = np.mean([log['response_time_ms'] for log in data])
        
        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± (íˆíŠ¸ìœ¨ ëŒ€ë¹„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰)
        cache_size_mb = self.cache_manager.get_size()
        memory_efficiency = hit_rate / max(cache_size_mb, 1.0)
        
        # ìºì‹œ ì˜¤ì—¼ ì ìˆ˜ (ë¯¸ì‚¬ìš© ë°ì´í„° ë¹„ìœ¨)
        pollution_score = self._calculate_cache_pollution(data)
        
        return {
            'hit_rate': hit_rate,
            'avg_response_time_ms': avg_response_time,
            'memory_efficiency': memory_efficiency,
            'pollution_score': pollution_score
        }
    
    def _calculate_cache_pollution(self, data: List[Dict[str, Any]]) -> float:
        """ìºì‹œ ì˜¤ì—¼ ì ìˆ˜ ê³„ì‚°"""
        try:
            # í‚¤ë³„ ë§ˆì§€ë§‰ ì ‘ê·¼ ì‹œê°„ ê³„ì‚°
            key_last_access = {}
            for log in data:
                key = log['key']
                if key not in key_last_access or log['timestamp'] > key_last_access[key]:
                    key_last_access[key] = log['timestamp']
            
            # í˜„ì¬ ìºì‹œì˜ ëª¨ë“  í‚¤
            cached_keys = set(self.cache_manager.keys())
            
            # ìµœê·¼ ì ‘ê·¼ë˜ì§€ ì•Šì€ í‚¤ì˜ ë¹„ìœ¨
            current_time = time.time()
            stale_threshold = 3600  # 1ì‹œê°„
            
            stale_keys = 0
            for key in cached_keys:
                if key not in key_last_access or \
                   (current_time - key_last_access[key]) > stale_threshold:
                    stale_keys += 1
            
            pollution_score = stale_keys / max(len(cached_keys), 1)
            return min(1.0, pollution_score)
            
        except Exception as e:
            self.logger.error(f"âŒ ìºì‹œ ì˜¤ì—¼ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    def _analyze_access_patterns(self, data: List[Dict[str, Any]]) -> List[CacheAccessPattern]:
        """ì ‘ê·¼ íŒ¨í„´ ë¶„ì„"""
        patterns = []
        
        try:
            # í‚¤ë³„ í†µê³„ ê³„ì‚°
            key_stats = self._calculate_key_statistics(data)
            
            # íŒ¨í„´ ê°ì§€
            for key_pattern, stats in key_stats.items():
                if stats['access_count'] >= self.pattern_detection_min_samples:
                    
                    # ì‹œê°„ì  íŒ¨í„´ ë¶„ì„
                    temporal_pattern = self._analyze_temporal_pattern(
                        key_pattern, data
                    )
                    
                    # í¬ê¸° ì¹´í…Œê³ ë¦¬ ê²°ì •
                    size_category = self._categorize_size(stats['avg_size'])
                    
                    # ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
                    category = self._classify_key_category(key_pattern)
                    
                    pattern = CacheAccessPattern(
                        key_pattern=key_pattern,
                        access_frequency=stats['access_frequency'],
                        temporal_pattern=temporal_pattern,
                        size_category=size_category,
                        ttl_effectiveness=stats['ttl_effectiveness'],
                        hit_rate=stats['hit_rate'],
                        avg_response_time_ms=stats['avg_response_time'],
                        category=category
                    )
                    
                    patterns.append(pattern)
            
            self.logger.info(f"ğŸ” ê°ì§€ëœ íŒ¨í„´: {len(patterns)}ê°œ")
            return patterns
            
        except Exception as e:
            self.logger.error(f"âŒ ì ‘ê·¼ íŒ¨í„´ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return []
    
    def _calculate_key_statistics(self, data: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:
        """í‚¤ë³„ í†µê³„ ê³„ì‚°"""
        key_stats = defaultdict(lambda: {
            'access_count': 0,
            'hits': 0,
            'total_response_time': 0.0,
            'total_size': 0,
            'access_times': [],
            'ttl_hits': 0,
            'ttl_total': 0
        })
        
        # í‚¤ íŒ¨í„´ ê·¸ë£¹í•‘ (ìœ ì‚¬í•œ í‚¤ë“¤ì„ í•˜ë‚˜ì˜ íŒ¨í„´ìœ¼ë¡œ)
        key_patterns = self._group_similar_keys(data)
        
        for log in data:
            key = log['key']
            pattern = self._find_key_pattern(key, key_patterns)
            
            stats = key_stats[pattern]
            stats['access_count'] += 1
            stats['total_response_time'] += log['response_time_ms']
            stats['total_size'] += log['data_size']
            stats['access_times'].append(log['timestamp'])
            
            if log['hit']:
                stats['hits'] += 1
                
                # TTL íš¨ê³¼ì„± (ìºì‹œì—ì„œ ì°¾ì€ ê²½ìš°)
                if log.get('ttl_remaining', 0) > 0:
                    stats['ttl_hits'] += 1
                stats['ttl_total'] += 1
        
        # í†µê³„ ê³„ì‚°
        processed_stats = {}
        for pattern, stats in key_stats.items():
            if stats['access_count'] > 0:
                processed_stats[pattern] = {
                    'access_count': stats['access_count'],
                    'access_frequency': stats['access_count'] / self.analysis_window_hours,
                    'hit_rate': stats['hits'] / stats['access_count'],
                    'avg_response_time': stats['total_response_time'] / stats['access_count'],
                    'avg_size': stats['total_size'] / stats['access_count'] if stats['total_size'] > 0 else 0,
                    'ttl_effectiveness': stats['ttl_hits'] / max(stats['ttl_total'], 1),
                    'access_times': stats['access_times']
                }
        
        return processed_stats
    
    def _group_similar_keys(self, data: List[Dict[str, Any]]) -> Dict[str, str]:
        """ìœ ì‚¬í•œ í‚¤ë“¤ì„ íŒ¨í„´ìœ¼ë¡œ ê·¸ë£¹í•‘"""
        keys = list(set(log['key'] for log in data))
        key_patterns = {}
        
        # ê°„ë‹¨í•œ íŒ¨í„´ ë§¤ì¹­ (ì •ê·œì‹ ê¸°ë°˜)
        patterns = {
            'audio_analysis': r'audio_.*_\d+',
            'image_ocr': r'image_.*_ocr',
            'model_inference': r'model_.*_inference',
            'user_config': r'user_.*_config',
            'temp_file': r'temp_.*',
            'analysis_result': r'.*_analysis_.*'
        }
        
        import re
        for key in keys:
            matched_pattern = 'other'
            for pattern_name, pattern_regex in patterns.items():
                if re.match(pattern_regex, key):
                    matched_pattern = pattern_name
                    break
            key_patterns[key] = matched_pattern
        
        return key_patterns
    
    def _find_key_pattern(self, key: str, key_patterns: Dict[str, str]) -> str:
        """í‚¤ì˜ íŒ¨í„´ ì°¾ê¸°"""
        return key_patterns.get(key, 'other')
    
    def _analyze_temporal_pattern(self, key_pattern: str, data: List[Dict[str, Any]]) -> str:
        """ì‹œê°„ì  íŒ¨í„´ ë¶„ì„"""
        try:
            # í•´ë‹¹ íŒ¨í„´ì˜ ì ‘ê·¼ ì‹œê°„ë“¤
            pattern_logs = [
                log for log in data 
                if self._find_key_pattern(log['key'], self._group_similar_keys(data)) == key_pattern
            ]
            
            if len(pattern_logs) < 10:
                return 'insufficient_data'
            
            # ì‹œê°„ëŒ€ë³„ ì ‘ê·¼ ë¹ˆë„
            hourly_access = defaultdict(int)
            for log in pattern_logs:
                hour = datetime.fromtimestamp(log['timestamp']).hour
                hourly_access[hour] += 1
            
            # í”¼í¬ ì‹œê°„ ì‹ë³„
            max_access = max(hourly_access.values())
            avg_access = np.mean(list(hourly_access.values()))
            
            # íŒ¨í„´ ë¶„ë¥˜
            if max_access > avg_access * 2:
                return 'peak'
            elif max_access < avg_access * 1.2:
                return 'steady'
            else:
                return 'variable'
                
        except Exception as e:
            self.logger.error(f"âŒ ì‹œê°„ì  íŒ¨í„´ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 'unknown'
    
    def _categorize_size(self, avg_size: float) -> str:
        """í¬ê¸° ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
        if avg_size < 1024:  # 1KB ë¯¸ë§Œ
            return 'small'
        elif avg_size < 1024 * 1024:  # 1MB ë¯¸ë§Œ
            return 'medium'
        else:
            return 'large'
    
    def _classify_key_category(self, key_pattern: str) -> str:
        """í‚¤ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
        category_map = {
            'audio_analysis': 'audio',
            'image_ocr': 'image',
            'model_inference': 'model',
            'user_config': 'config',
            'temp_file': 'temp',
            'analysis_result': 'analysis'
        }
        return category_map.get(key_pattern, 'other')
    
    def _analyze_performance_trends(self) -> Dict[str, List[float]]:
        """ì„±ëŠ¥ ì¶”ì„¸ ë¶„ì„"""
        try:
            trends = {}
            
            if len(self.performance_metrics) < 10:
                return trends
            
            # ì‹œê°„ ìˆœìœ¼ë¡œ ì •ë ¬ëœ ë©”íŠ¸ë¦­
            sorted_metrics = sorted(self.performance_metrics, key=lambda x: x['timestamp'])
            
            # ì¶”ì„¸ ë°ì´í„° ì¶”ì¶œ
            trends['hit_rate'] = [m['hit_rate'] for m in sorted_metrics]
            trends['response_time'] = [m['avg_response_time_ms'] for m in sorted_metrics]
            trends['cache_size'] = [m['cache_size_mb'] for m in sorted_metrics]
            trends['request_rate'] = [m['total_requests'] for m in sorted_metrics]
            
            # ì´ë™ í‰ê·  ê³„ì‚° (ìŠ¤ë¬´ë”©)
            window_size = min(10, len(sorted_metrics) // 2)
            if window_size > 1:
                for metric_name, values in trends.items():
                    smoothed = []
                    for i in range(len(values)):
                        start_idx = max(0, i - window_size // 2)
                        end_idx = min(len(values), i + window_size // 2 + 1)
                        smoothed.append(np.mean(values[start_idx:end_idx]))
                    trends[f'{metric_name}_smoothed'] = smoothed
            
            return trends
            
        except Exception as e:
            self.logger.error(f"âŒ ì„±ëŠ¥ ì¶”ì„¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {}
    
    def _generate_optimization_suggestions(self, 
                                         overall_metrics: Dict[str, float],
                                         patterns: List[CacheAccessPattern],
                                         trends: Dict[str, List[float]]) -> List[CacheOptimizationSuggestion]:
        """ìµœì í™” ì œì•ˆ ìƒì„±"""
        suggestions = []
        
        try:
            # 1. íˆíŠ¸ìœ¨ ê°œì„  ì œì•ˆ
            if overall_metrics['hit_rate'] < self.baseline_metrics['target_hit_rate']:
                suggestions.extend(self._suggest_hit_rate_improvements(overall_metrics, patterns))
            
            # 2. ì‘ë‹µ ì‹œê°„ ê°œì„  ì œì•ˆ
            if overall_metrics['avg_response_time_ms'] > self.baseline_metrics['target_response_time_ms']:
                suggestions.extend(self._suggest_response_time_improvements(overall_metrics, patterns))
            
            # 3. ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê°œì„  ì œì•ˆ
            if overall_metrics['memory_efficiency'] < self.baseline_metrics['target_memory_efficiency']:
                suggestions.extend(self._suggest_memory_efficiency_improvements(overall_metrics, patterns))
            
            # 4. ìºì‹œ ì˜¤ì—¼ í•´ê²° ì œì•ˆ
            if overall_metrics['pollution_score'] > self.baseline_metrics['acceptable_pollution_score']:
                suggestions.extend(self._suggest_pollution_cleanup(overall_metrics, patterns))
            
            # 5. íŒ¨í„´ ê¸°ë°˜ ìµœì í™” ì œì•ˆ
            suggestions.extend(self._suggest_pattern_optimizations(patterns))
            
            # 6. ì¶”ì„¸ ê¸°ë°˜ ì˜ˆì¸¡ ì œì•ˆ
            suggestions.extend(self._suggest_trend_based_optimizations(trends))
            
            # ìš°ì„ ìˆœìœ„ ì •ë ¬
            suggestions.sort(key=lambda x: {
                'high': 3, 'medium': 2, 'low': 1
            }.get(x.priority, 0), reverse=True)
            
            self.logger.info(f"ğŸ’¡ ìƒì„±ëœ ìµœì í™” ì œì•ˆ: {len(suggestions)}ê°œ")
            return suggestions
            
        except Exception as e:
            self.logger.error(f"âŒ ìµœì í™” ì œì•ˆ ìƒì„± ì‹¤íŒ¨: {e}")
            return []
    
    def _suggest_hit_rate_improvements(self, metrics: Dict[str, float], patterns: List[CacheAccessPattern]) -> List[CacheOptimizationSuggestion]:
        """íˆíŠ¸ìœ¨ ê°œì„  ì œì•ˆ"""
        suggestions = []
        
        # ë‚®ì€ íˆíŠ¸ìœ¨ íŒ¨í„´ ì‹ë³„
        low_hit_patterns = [p for p in patterns if p.hit_rate < 0.5]
        
        if low_hit_patterns:
            suggestions.append(CacheOptimizationSuggestion(
                suggestion_type='increase_cache_size',
                priority='high',
                description=f'íˆíŠ¸ìœ¨ì´ ë‚®ì€ {len(low_hit_patterns)}ê°œ íŒ¨í„´ì˜ ìºì‹œ í¬ê¸° ì¦ê°€',
                expected_improvement=0.15,
                implementation_effort='easy',
                affected_keys=[p.key_pattern for p in low_hit_patterns],
                parameters={'size_increase_factor': 1.5}
            ))
        
        # TTL ìµœì í™”
        poor_ttl_patterns = [p for p in patterns if p.ttl_effectiveness < 0.3]
        if poor_ttl_patterns:
            suggestions.append(CacheOptimizationSuggestion(
                suggestion_type='optimize_ttl',
                priority='medium',
                description=f'TTL íš¨ìœ¨ì„±ì´ ë‚®ì€ {len(poor_ttl_patterns)}ê°œ íŒ¨í„´ì˜ TTL ì¡°ì •',
                expected_improvement=0.1,
                implementation_effort='easy',
                affected_keys=[p.key_pattern for p in poor_ttl_patterns],
                parameters={'ttl_multiplier': 2.0}
            ))
        
        return suggestions
    
    def _suggest_response_time_improvements(self, metrics: Dict[str, float], patterns: List[CacheAccessPattern]) -> List[CacheOptimizationSuggestion]:
        """ì‘ë‹µ ì‹œê°„ ê°œì„  ì œì•ˆ"""
        suggestions = []
        
        # ëŠë¦° íŒ¨í„´ ì‹ë³„
        slow_patterns = [p for p in patterns if p.avg_response_time_ms > 100]
        
        if slow_patterns:
            suggestions.append(CacheOptimizationSuggestion(
                suggestion_type='enable_compression',
                priority='medium',
                description=f'ì‘ë‹µ ì‹œê°„ì´ ëŠë¦° {len(slow_patterns)}ê°œ íŒ¨í„´ì— ì••ì¶• ì ìš©',
                expected_improvement=0.3,
                implementation_effort='medium',
                affected_keys=[p.key_pattern for p in slow_patterns],
                parameters={'compression_type': 'lz4'}
            ))
        
        # í° ë°ì´í„° íŒ¨í„´
        large_data_patterns = [p for p in patterns if p.size_category == 'large']
        if large_data_patterns:
            suggestions.append(CacheOptimizationSuggestion(
                suggestion_type='implement_streaming',
                priority='high',
                description=f'ëŒ€ìš©ëŸ‰ ë°ì´í„° {len(large_data_patterns)}ê°œ íŒ¨í„´ì— ìŠ¤íŠ¸ë¦¬ë° ì ìš©',
                expected_improvement=0.5,
                implementation_effort='hard',
                affected_keys=[p.key_pattern for p in large_data_patterns],
                parameters={'chunk_size_mb': 1.0}
            ))
        
        return suggestions
    
    def _suggest_memory_efficiency_improvements(self, metrics: Dict[str, float], patterns: List[CacheAccessPattern]) -> List[CacheOptimizationSuggestion]:
        """ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê°œì„  ì œì•ˆ"""
        suggestions = []
        
        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì´ ë‚®ì€ ê²½ìš°
        if metrics['memory_efficiency'] < 0.5:
            suggestions.append(CacheOptimizationSuggestion(
                suggestion_type='adjust_cache_levels',
                priority='high',
                description='L1/L2 ìºì‹œ ë ˆë²¨ ê°„ ë°ì´í„° ì¬ë¶„ë°°',
                expected_improvement=0.2,
                implementation_effort='medium',
                affected_keys=['all'],
                parameters={'l1_ratio': 0.6, 'l2_ratio': 0.4}
            ))
        
        return suggestions
    
    def _suggest_pollution_cleanup(self, metrics: Dict[str, float], patterns: List[CacheAccessPattern]) -> List[CacheOptimizationSuggestion]:
        """ìºì‹œ ì˜¤ì—¼ ì •ë¦¬ ì œì•ˆ"""
        suggestions = []
        
        if metrics['pollution_score'] > 0.3:
            suggestions.append(CacheOptimizationSuggestion(
                suggestion_type='aggressive_cleanup',
                priority='medium',
                description=f'ìºì‹œ ì˜¤ì—¼ë„ {metrics["pollution_score"]:.1%} - ì ê·¹ì  ì •ë¦¬ í•„ìš”',
                expected_improvement=0.15,
                implementation_effort='easy',
                affected_keys=['stale_keys'],
                parameters={'cleanup_threshold_hours': 2}
            ))
        
        return suggestions
    
    def _suggest_pattern_optimizations(self, patterns: List[CacheAccessPattern]) -> List[CacheOptimizationSuggestion]:
        """íŒ¨í„´ ê¸°ë°˜ ìµœì í™” ì œì•ˆ"""
        suggestions = []
        
        # í”¼í¬ ì‹œê°„ íŒ¨í„´
        peak_patterns = [p for p in patterns if p.temporal_pattern == 'peak']
        if peak_patterns:
            suggestions.append(CacheOptimizationSuggestion(
                suggestion_type='implement_preloading',
                priority='medium',
                description=f'í”¼í¬ ì‹œê°„ íŒ¨í„´ {len(peak_patterns)}ê°œì— ì‚¬ì „ ë¡œë”© ì ìš©',
                expected_improvement=0.2,
                implementation_effort='medium',
                affected_keys=[p.key_pattern for p in peak_patterns],
                parameters={'preload_window_minutes': 30}
            ))
        
        # ì¹´í…Œê³ ë¦¬ë³„ ìµœì í™”
        audio_patterns = [p for p in patterns if p.category == 'audio']
        if len(audio_patterns) > 3:
            suggestions.append(CacheOptimizationSuggestion(
                suggestion_type='category_specific_strategy',
                priority='low',
                description='ì˜¤ë””ì˜¤ ë¶„ì„ ì „ìš© ìºì‹œ ì „ëµ ì ìš©',
                expected_improvement=0.1,
                implementation_effort='medium',
                affected_keys=[p.key_pattern for p in audio_patterns],
                parameters={'strategy': 'audio_optimized'}
            ))
        
        return suggestions
    
    def _suggest_trend_based_optimizations(self, trends: Dict[str, List[float]]) -> List[CacheOptimizationSuggestion]:
        """ì¶”ì„¸ ê¸°ë°˜ ìµœì í™” ì œì•ˆ"""
        suggestions = []
        
        try:
            # íˆíŠ¸ìœ¨ í•˜ë½ ì¶”ì„¸
            if 'hit_rate' in trends and len(trends['hit_rate']) > 5:
                recent_hit_rate = np.mean(trends['hit_rate'][-3:])
                older_hit_rate = np.mean(trends['hit_rate'][:3])
                
                if recent_hit_rate < older_hit_rate * 0.9:  # 10% ì´ìƒ í•˜ë½
                    suggestions.append(CacheOptimizationSuggestion(
                        suggestion_type='investigate_hit_rate_decline',
                        priority='high',
                        description=f'íˆíŠ¸ìœ¨ í•˜ë½ ì¶”ì„¸ ê°ì§€ ({older_hit_rate:.1%} â†’ {recent_hit_rate:.1%})',
                        expected_improvement=0.15,
                        implementation_effort='medium',
                        affected_keys=['all'],
                        parameters={'analysis_required': True}
                    ))
            
            # ì‘ë‹µ ì‹œê°„ ì¦ê°€ ì¶”ì„¸
            if 'response_time' in trends and len(trends['response_time']) > 5:
                recent_rt = np.mean(trends['response_time'][-3:])
                older_rt = np.mean(trends['response_time'][:3])
                
                if recent_rt > older_rt * 1.2:  # 20% ì´ìƒ ì¦ê°€
                    suggestions.append(CacheOptimizationSuggestion(
                        suggestion_type='address_performance_degradation',
                        priority='high',
                        description=f'ì‘ë‹µ ì‹œê°„ ì¦ê°€ ì¶”ì„¸ ê°ì§€ ({older_rt:.1f}ms â†’ {recent_rt:.1f}ms)',
                        expected_improvement=0.25,
                        implementation_effort='medium',
                        affected_keys=['all'],
                        parameters={'performance_analysis_required': True}
                    ))
            
        except Exception as e:
            self.logger.error(f"âŒ ì¶”ì„¸ ê¸°ë°˜ ì œì•ˆ ìƒì„± ì‹¤íŒ¨: {e}")
        
        return suggestions
    
    def _create_empty_report(self) -> CachePerformanceReport:
        """ë¹ˆ ë³´ê³ ì„œ ìƒì„±"""
        return CachePerformanceReport(
            analysis_timestamp=datetime.now().isoformat(),
            overall_hit_rate=0.0,
            avg_response_time_ms=0.0,
            memory_efficiency=0.0,
            cache_pollution_score=0.0,
            optimization_opportunities=0,
            patterns_detected=[],
            suggestions=[],
            performance_trends={}
        )
    
    def generate_visual_report(self, report: CachePerformanceReport, output_path: str) -> None:
        """ì‹œê°ì  ë³´ê³ ì„œ ìƒì„±"""
        try:
            plt.style.use('seaborn-v0_8')
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            fig.suptitle('ìºì‹œ ì„±ëŠ¥ ë¶„ì„ ë³´ê³ ì„œ', fontsize=16, fontweight='bold')
            
            # 1. ì „ì²´ ì„±ëŠ¥ ì§€í‘œ
            ax1 = axes[0, 0]
            metrics = ['íˆíŠ¸ìœ¨', 'ì‘ë‹µì‹œê°„', 'ë©”ëª¨ë¦¬íš¨ìœ¨', 'ì˜¤ì—¼ì ìˆ˜']
            values = [
                report.overall_hit_rate * 100,
                min(100, report.avg_response_time_ms),  # ìŠ¤ì¼€ì¼ ì¡°ì •
                report.memory_efficiency * 100,
                report.cache_pollution_score * 100
            ]
            targets = [80, 50, 70, 30]  # ëª©í‘œê°’
            
            x = np.arange(len(metrics))
            width = 0.35
            
            ax1.bar(x - width/2, values, width, label='í˜„ì¬ê°’', alpha=0.8)
            ax1.bar(x + width/2, targets, width, label='ëª©í‘œê°’', alpha=0.6)
            ax1.set_ylabel('ì ìˆ˜')
            ax1.set_title('ì„±ëŠ¥ ì§€í‘œ ë¹„êµ')
            ax1.set_xticks(x)
            ax1.set_xticklabels(metrics)
            ax1.legend()
            
            # 2. íŒ¨í„´ë³„ íˆíŠ¸ìœ¨
            ax2 = axes[0, 1]
            if report.patterns_detected:
                pattern_names = [p.key_pattern[:10] + '...' if len(p.key_pattern) > 10 else p.key_pattern 
                               for p in report.patterns_detected[:8]]  # ìƒìœ„ 8ê°œ
                hit_rates = [p.hit_rate * 100 for p in report.patterns_detected[:8]]
                
                bars = ax2.bar(pattern_names, hit_rates)
                ax2.set_ylabel('íˆíŠ¸ìœ¨ (%)')
                ax2.set_title('íŒ¨í„´ë³„ íˆíŠ¸ìœ¨')
                ax2.tick_params(axis='x', rotation=45)
                
                # ìƒ‰ìƒ êµ¬ë¶„ (íˆíŠ¸ìœ¨ì— ë”°ë¼)
                for bar, hit_rate in zip(bars, hit_rates):
                    if hit_rate >= 80:
                        bar.set_color('green')
                    elif hit_rate >= 60:
                        bar.set_color('orange')
                    else:
                        bar.set_color('red')
            else:
                ax2.text(0.5, 0.5, 'íŒ¨í„´ ë°ì´í„° ì—†ìŒ', ha='center', va='center', transform=ax2.transAxes)
                ax2.set_title('íŒ¨í„´ë³„ íˆíŠ¸ìœ¨')
            
            # 3. ì„±ëŠ¥ ì¶”ì„¸
            ax3 = axes[1, 0]
            if 'hit_rate' in report.performance_trends:
                hit_trend = report.performance_trends['hit_rate']
                if len(hit_trend) > 1:
                    ax3.plot(hit_trend, label='íˆíŠ¸ìœ¨', marker='o')
                    ax3.set_ylabel('íˆíŠ¸ìœ¨')
                    ax3.set_xlabel('ì‹œê°„ ìˆœì„œ')
                    ax3.set_title('íˆíŠ¸ìœ¨ ì¶”ì„¸')
                    ax3.legend()
                else:
                    ax3.text(0.5, 0.5, 'ì¶”ì„¸ ë°ì´í„° ë¶€ì¡±', ha='center', va='center', transform=ax3.transAxes)
            else:
                ax3.text(0.5, 0.5, 'ì¶”ì„¸ ë°ì´í„° ì—†ìŒ', ha='center', va='center', transform=ax3.transAxes)
                ax3.set_title('íˆíŠ¸ìœ¨ ì¶”ì„¸')
            
            # 4. ìµœì í™” ì œì•ˆ ìš”ì•½
            ax4 = axes[1, 1]
            if report.suggestions:
                priority_counts = defaultdict(int)
                for suggestion in report.suggestions:
                    priority_counts[suggestion.priority] += 1
                
                priorities = list(priority_counts.keys())
                counts = list(priority_counts.values())
                colors = {'high': 'red', 'medium': 'orange', 'low': 'green'}
                
                ax4.pie(counts, labels=priorities, autopct='%1.1f%%', 
                       colors=[colors.get(p, 'gray') for p in priorities])
                ax4.set_title('ìµœì í™” ì œì•ˆ ìš°ì„ ìˆœìœ„')
            else:
                ax4.text(0.5, 0.5, 'ìµœì í™” ì œì•ˆ ì—†ìŒ', ha='center', va='center', transform=ax4.transAxes)
                ax4.set_title('ìµœì í™” ì œì•ˆ ìš°ì„ ìˆœìœ„')
            
            plt.tight_layout()
            plt.savefig(output_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            self.logger.info(f"ğŸ“Š ì‹œê°ì  ë³´ê³ ì„œ ìƒì„±: {output_path}")
            
        except Exception as e:
            self.logger.error(f"âŒ ì‹œê°ì  ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {e}")
    
    def export_analysis_data(self, output_path: str) -> None:
        """ë¶„ì„ ë°ì´í„° ë‚´ë³´ë‚´ê¸°"""
        try:
            export_data = {
                'export_timestamp': datetime.now().isoformat(),
                'analysis_window_hours': self.analysis_window_hours,
                'access_logs_count': len(self.access_logs),
                'performance_metrics_count': len(self.performance_metrics),
                'baseline_metrics': self.baseline_metrics,
                'recent_access_logs': list(self.access_logs)[-1000:],  # ìµœê·¼ 1000ê°œ
                'recent_performance_metrics': list(self.performance_metrics)[-100:],  # ìµœê·¼ 100ê°œ
                'pattern_history': dict(self.pattern_history)
            }
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"ğŸ“Š ë¶„ì„ ë°ì´í„° ë‚´ë³´ë‚´ê¸°: {output_path}")
            
        except Exception as e:
            self.logger.error(f"âŒ ë¶„ì„ ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ê°€ìƒì˜ ìºì‹œ ë§¤ë‹ˆì € (í…ŒìŠ¤íŠ¸ìš©)
    class MockCacheManager:
        def __init__(self):
            self.size = 100.0
            
        def get_size(self):
            return self.size
            
        def keys(self):
            return ['audio_001', 'image_002', 'model_003', 'config_004']
    
    # ìºì‹œ ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸
    cache_manager = MockCacheManager()
    analyzer = CacheAnalyzer(cache_manager)
    
    print("ğŸ” ìºì‹œ ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸:")
    
    # ê°€ìƒì˜ ì ‘ê·¼ ë¡œê·¸ ìƒì„±
    import random
    for i in range(100):
        analyzer.log_access(
            key=f"test_key_{i % 10}",
            hit=random.choice([True, False]),
            response_time_ms=random.uniform(10, 200),
            cache_level='l1_memory',
            operation='get',
            data_size=random.randint(1024, 1024*1024)
        )
    
    # ì„±ëŠ¥ ë¶„ì„ ì‹¤í–‰
    report = analyzer.analyze_cache_performance()
    
    print(f"âœ… ë¶„ì„ ì™„ë£Œ:")
    print(f"   íˆíŠ¸ìœ¨: {report.overall_hit_rate:.1%}")
    print(f"   í‰ê·  ì‘ë‹µì‹œê°„: {report.avg_response_time_ms:.1f}ms")
    print(f"   ê°ì§€ëœ íŒ¨í„´: {len(report.patterns_detected)}ê°œ")
    print(f"   ìµœì í™” ì œì•ˆ: {len(report.suggestions)}ê°œ")
    
    # ë¶„ì„ ë°ì´í„° ë‚´ë³´ë‚´ê¸°
    analyzer.export_analysis_data("cache_analysis_data.json")
    
    print("âœ… ìºì‹œ ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸ ì™„ë£Œ")