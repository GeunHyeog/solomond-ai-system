#!/usr/bin/env python3
"""
ìŠ¤ë§ˆíŠ¸ ìºì‹œ ê´€ë¦¬ ì‹œìŠ¤í…œ v2.6
ë‹¤ì¤‘ ë ˆë²¨ ìºì‹œ ë° ì§€ëŠ¥ì  ë©”ëª¨ë¦¬ ì—°ë™ ê´€ë¦¬
"""

import os
import time
import json
import pickle
import hashlib
import threading
import weakref
from typing import Dict, List, Any, Optional, Union, Tuple, Callable
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from pathlib import Path
from collections import OrderedDict, deque
import logging
import psutil
import gzip
import lz4.frame
from enum import Enum
import asyncio

class CacheLevel(Enum):
    """ìºì‹œ ë ˆë²¨"""
    L1_MEMORY = "l1_memory"
    L2_DISK = "l2_disk"
    L3_NETWORK = "l3_network"

class CacheStrategy(Enum):
    """ìºì‹œ ì „ëµ"""
    LRU = "lru"  # Least Recently Used
    LFU = "lfu"  # Least Frequently Used
    TTL = "ttl"  # Time To Live
    ADAPTIVE = "adaptive"  # ì ì‘í˜•

class CompressionType(Enum):
    """ì••ì¶• íƒ€ì…"""
    NONE = "none"
    GZIP = "gzip"
    LZ4 = "lz4"
    PICKLE = "pickle"

@dataclass
class CacheEntry:
    """ìºì‹œ ì—”íŠ¸ë¦¬"""
    key: str
    data: Any
    level: CacheLevel
    created_at: float
    last_accessed: float
    access_count: int
    ttl: Optional[float]
    size_bytes: int
    compressed: bool
    compression_type: CompressionType
    metadata: Dict[str, Any]

@dataclass
class CacheStats:
    """ìºì‹œ í†µê³„"""
    total_entries: int
    total_size_mb: float
    hit_count: int
    miss_count: int
    hit_rate: float
    l1_entries: int
    l2_entries: int
    l3_entries: int
    eviction_count: int
    compression_ratio: float
    last_cleanup: str

@dataclass
class CacheConfiguration:
    """ìºì‹œ ì„¤ì •"""
    l1_max_size_mb: float = 512.0  # L1 ìµœëŒ€ í¬ê¸°
    l2_max_size_mb: float = 2048.0  # L2 ìµœëŒ€ í¬ê¸°
    l3_max_size_mb: float = 8192.0  # L3 ìµœëŒ€ í¬ê¸°
    default_ttl_seconds: float = 3600.0  # ê¸°ë³¸ TTL (1ì‹œê°„)
    cleanup_interval_seconds: float = 300.0  # ì •ë¦¬ ê°„ê²© (5ë¶„)
    compression_threshold_kb: float = 100.0  # ì••ì¶• ì„ê³„ê°’ (100KB)
    memory_pressure_threshold: float = 80.0  # ë©”ëª¨ë¦¬ ì••ë°• ì„ê³„ê°’ (80%)
    enable_preloading: bool = True
    enable_analytics: bool = True

class SmartCacheManager:
    """ìŠ¤ë§ˆíŠ¸ ìºì‹œ ê´€ë¦¬ì"""
    
    def __init__(self, 
                 config: Optional[CacheConfiguration] = None,
                 cache_dir: str = "./cache"):
        
        self.config = config or CacheConfiguration()
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        
        self.logger = self._setup_logging()
        
        # ìºì‹œ ì €ì¥ì†Œ (ë ˆë²¨ë³„)
        self.l1_cache = OrderedDict()  # ë©”ëª¨ë¦¬ ìºì‹œ
        self.l2_index = {}  # ë””ìŠ¤í¬ ìºì‹œ ì¸ë±ìŠ¤
        self.l3_index = {}  # ë„¤íŠ¸ì›Œí¬ ìºì‹œ ì¸ë±ìŠ¤
        
        # ìºì‹œ í†µê³„
        self.stats = CacheStats(
            total_entries=0,
            total_size_mb=0.0,
            hit_count=0,
            miss_count=0,
            hit_rate=0.0,
            l1_entries=0,
            l2_entries=0,
            l3_entries=0,
            eviction_count=0,
            compression_ratio=1.0,
            last_cleanup=datetime.now().isoformat()
        )
        
        # ì ‘ê·¼ ë¹ˆë„ ì¶”ì  (LFUìš©)
        self.access_frequency = {}
        
        # ìºì‹œ í‚¤ íŒ¨í„´ ë¶„ì„
        self.key_patterns = {}
        self.prediction_model = {}
        
        # ìŠ¤ë ˆë“œ ì•ˆì „ì„±
        self.lock = threading.RLock()
        
        # ìë™ ì •ë¦¬ ìŠ¤ë ˆë“œ
        self.cleanup_thread = None
        self.is_running = False
        
        # ë©”ëª¨ë¦¬ ìµœì í™” ì—”ì§„ ì—°ë™
        self.memory_optimizer = None
        self.memory_callbacks = []
        
        # ì„±ëŠ¥ ë¶„ì„
        self.performance_history = deque(maxlen=1000)
        
        # ì••ì¶• ì—”ì§„
        self.compression_engines = {
            CompressionType.GZIP: self._gzip_compress,
            CompressionType.LZ4: self._lz4_compress,
            CompressionType.PICKLE: self._pickle_compress
        }
        
        self.decompression_engines = {
            CompressionType.GZIP: self._gzip_decompress,
            CompressionType.LZ4: self._lz4_decompress,
            CompressionType.PICKLE: self._pickle_decompress
        }
        
        # ë””ìŠ¤í¬ ìºì‹œ ì´ˆê¸°í™”
        self._initialize_disk_cache()
        
        self.logger.info("ğŸš€ ìŠ¤ë§ˆíŠ¸ ìºì‹œ ê´€ë¦¬ì ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _setup_logging(self) -> logging.Logger:
        """ë¡œê¹… ì„¤ì •"""
        logger = logging.getLogger(f'{__name__}.SmartCacheManager')
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s | %(name)s | %(levelname)s | %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            logger.setLevel(logging.INFO)
        return logger
    
    def _initialize_disk_cache(self) -> None:
        """ë””ìŠ¤í¬ ìºì‹œ ì´ˆê¸°í™”"""
        try:
            # L2 ë””ìŠ¤í¬ ìºì‹œ ë””ë ‰í† ë¦¬
            self.l2_dir = self.cache_dir / "l2_disk"
            self.l2_dir.mkdir(exist_ok=True)
            
            # L3 ë„¤íŠ¸ì›Œí¬ ìºì‹œ ë””ë ‰í† ë¦¬
            self.l3_dir = self.cache_dir / "l3_network"
            self.l3_dir.mkdir(exist_ok=True)
            
            # ê¸°ì¡´ ìºì‹œ ì¸ë±ìŠ¤ ë¡œë“œ
            self._load_cache_indexes()
            
        except Exception as e:
            self.logger.error(f"âŒ ë””ìŠ¤í¬ ìºì‹œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def _load_cache_indexes(self) -> None:
        """ìºì‹œ ì¸ë±ìŠ¤ ë¡œë“œ"""
        try:
            # L2 ì¸ë±ìŠ¤ ë¡œë“œ
            l2_index_file = self.cache_dir / "l2_index.json"
            if l2_index_file.exists():
                with open(l2_index_file, 'r', encoding='utf-8') as f:
                    self.l2_index = json.load(f)
            
            # L3 ì¸ë±ìŠ¤ ë¡œë“œ
            l3_index_file = self.cache_dir / "l3_index.json"
            if l3_index_file.exists():
                with open(l3_index_file, 'r', encoding='utf-8') as f:
                    self.l3_index = json.load(f)
            
            self.logger.info(f"ğŸ“‹ ìºì‹œ ì¸ë±ìŠ¤ ë¡œë“œ: L2={len(self.l2_index)}, L3={len(self.l3_index)}")
            
        except Exception as e:
            self.logger.error(f"âŒ ìºì‹œ ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def _save_cache_indexes(self) -> None:
        """ìºì‹œ ì¸ë±ìŠ¤ ì €ì¥"""
        try:
            # L2 ì¸ë±ìŠ¤ ì €ì¥
            l2_index_file = self.cache_dir / "l2_index.json"
            with open(l2_index_file, 'w', encoding='utf-8') as f:
                json.dump(self.l2_index, f, ensure_ascii=False, indent=2)
            
            # L3 ì¸ë±ìŠ¤ ì €ì¥
            l3_index_file = self.cache_dir / "l3_index.json"
            with open(l3_index_file, 'w', encoding='utf-8') as f:
                json.dump(self.l3_index, f, ensure_ascii=False, indent=2)
            
        except Exception as e:
            self.logger.error(f"âŒ ìºì‹œ ì¸ë±ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def start_auto_cleanup(self) -> None:
        """ìë™ ì •ë¦¬ ì‹œì‘"""
        if self.is_running:
            return
        
        self.is_running = True
        self.cleanup_thread = threading.Thread(
            target=self._auto_cleanup_loop,
            daemon=True
        )
        self.cleanup_thread.start()
        self.logger.info("ğŸ”„ ìë™ ìºì‹œ ì •ë¦¬ ì‹œì‘")
    
    def stop_auto_cleanup(self) -> None:
        """ìë™ ì •ë¦¬ ì¤‘ì§€"""
        self.is_running = False
        if self.cleanup_thread:
            self.cleanup_thread.join(timeout=5.0)
        self.logger.info("â¹ï¸ ìë™ ìºì‹œ ì •ë¦¬ ì¤‘ì§€")
    
    def _auto_cleanup_loop(self) -> None:
        """ìë™ ì •ë¦¬ ë£¨í”„"""
        while self.is_running:
            try:
                time.sleep(self.config.cleanup_interval_seconds)
                if self.is_running:
                    self.cleanup_expired_entries()
                    self._check_memory_pressure()
                    self._update_statistics()
                    
            except Exception as e:
                self.logger.error(f"âŒ ìë™ ì •ë¦¬ ë£¨í”„ ì˜¤ë¥˜: {e}")
    
    def set(self, 
            key: str, 
            data: Any, 
            level: CacheLevel = CacheLevel.L1_MEMORY,
            ttl: Optional[float] = None,
            strategy: CacheStrategy = CacheStrategy.ADAPTIVE,
            metadata: Optional[Dict[str, Any]] = None) -> bool:
        """ìºì‹œ í•­ëª© ì„¤ì •"""
        
        start_time = time.time()
        
        try:
            with self.lock:
                # TTL ì„¤ì •
                if ttl is None:
                    ttl = self.config.default_ttl_seconds
                
                # ë°ì´í„° ì••ì¶• ê²°ì •
                raw_size = self._calculate_size(data)
                should_compress = raw_size > (self.config.compression_threshold_kb * 1024)
                
                # ë°ì´í„° ì²˜ë¦¬
                processed_data, compression_type, compressed_size = self._process_data_for_storage(
                    data, should_compress
                )
                
                # ìºì‹œ ì—”íŠ¸ë¦¬ ìƒì„±
                entry = CacheEntry(
                    key=key,
                    data=processed_data,
                    level=level,
                    created_at=time.time(),
                    last_accessed=time.time(),
                    access_count=1,
                    ttl=ttl,
                    size_bytes=compressed_size,
                    compressed=should_compress,
                    compression_type=compression_type,
                    metadata=metadata or {}
                )
                
                # ë ˆë²¨ë³„ ì €ì¥
                success = self._store_by_level(entry, strategy)
                
                if success:
                    # í†µê³„ ì—…ë°ì´íŠ¸
                    self.stats.total_entries += 1
                    self.stats.total_size_mb += compressed_size / (1024**2)
                    
                    # ì„±ëŠ¥ ê¸°ë¡
                    self.performance_history.append({
                        'operation': 'set',
                        'key': key,
                        'level': level.value,
                        'size_bytes': compressed_size,
                        'duration_ms': (time.time() - start_time) * 1000,
                        'timestamp': time.time()
                    })
                    
                    self.logger.debug(f"âœ… ìºì‹œ ì €ì¥: {key} ({level.value}, {compressed_size}bytes)")
                    return True
                else:
                    self.logger.warning(f"âš ï¸ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {key}")
                    return False
                    
        except Exception as e:
            self.logger.error(f"âŒ ìºì‹œ ì €ì¥ ì˜¤ë¥˜: {e}")
            return False
    
    def get(self, key: str, default: Any = None) -> Any:
        """ìºì‹œ í•­ëª© ì¡°íšŒ"""
        
        start_time = time.time()
        
        try:
            with self.lock:
                # L1 ë©”ëª¨ë¦¬ ìºì‹œì—ì„œ ì¡°íšŒ
                if key in self.l1_cache:
                    entry = self.l1_cache[key]
                    
                    # TTL í™•ì¸
                    if self._is_expired(entry):
                        self._remove_entry(key, CacheLevel.L1_MEMORY)
                        self.stats.miss_count += 1
                        return default
                    
                    # ì ‘ê·¼ ì •ë³´ ì—…ë°ì´íŠ¸
                    entry.last_accessed = time.time()
                    entry.access_count += 1
                    self.access_frequency[key] = self.access_frequency.get(key, 0) + 1
                    
                    # LRU ìˆœì„œ ì—…ë°ì´íŠ¸
                    self.l1_cache.move_to_end(key)
                    
                    # ë°ì´í„° ë³µì›
                    data = self._restore_data_from_storage(entry)
                    
                    # í†µê³„ ì—…ë°ì´íŠ¸
                    self.stats.hit_count += 1
                    self._update_hit_rate()
                    
                    # ì„±ëŠ¥ ê¸°ë¡
                    self.performance_history.append({
                        'operation': 'get',
                        'key': key,
                        'level': 'l1_memory',
                        'hit': True,
                        'duration_ms': (time.time() - start_time) * 1000,
                        'timestamp': time.time()
                    })
                    
                    self.logger.debug(f"ğŸ¯ L1 ìºì‹œ íˆíŠ¸: {key}")
                    return data
                
                # L2 ë””ìŠ¤í¬ ìºì‹œì—ì„œ ì¡°íšŒ
                if key in self.l2_index:
                    entry_info = self.l2_index[key]
                    
                    # TTL í™•ì¸
                    if time.time() - entry_info['created_at'] > entry_info['ttl']:
                        self._remove_entry(key, CacheLevel.L2_DISK)
                        self.stats.miss_count += 1
                        return default
                    
                    # ë””ìŠ¤í¬ì—ì„œ ë¡œë“œ
                    data = self._load_from_disk(key, entry_info)
                    
                    if data is not None:
                        # L1ìœ¼ë¡œ ìŠ¹ê²©
                        self._promote_to_l1(key, data, entry_info)
                        
                        # í†µê³„ ì—…ë°ì´íŠ¸
                        self.stats.hit_count += 1
                        self._update_hit_rate()
                        
                        # ì„±ëŠ¥ ê¸°ë¡
                        self.performance_history.append({
                            'operation': 'get',
                            'key': key,
                            'level': 'l2_disk',
                            'hit': True,
                            'duration_ms': (time.time() - start_time) * 1000,
                            'timestamp': time.time()
                        })
                        
                        self.logger.debug(f"ğŸ’¾ L2 ìºì‹œ íˆíŠ¸: {key}")
                        return data
                
                # L3 ë„¤íŠ¸ì›Œí¬ ìºì‹œì—ì„œ ì¡°íšŒ (í–¥í›„ êµ¬í˜„)
                # if key in self.l3_index:
                #     ...
                
                # ìºì‹œ ë¯¸ìŠ¤
                self.stats.miss_count += 1
                self._update_hit_rate()
                
                # ì„±ëŠ¥ ê¸°ë¡
                self.performance_history.append({
                    'operation': 'get',
                    'key': key,
                    'level': 'miss',
                    'hit': False,
                    'duration_ms': (time.time() - start_time) * 1000,
                    'timestamp': time.time()
                })
                
                self.logger.debug(f"âŒ ìºì‹œ ë¯¸ìŠ¤: {key}")
                return default
                
        except Exception as e:
            self.logger.error(f"âŒ ìºì‹œ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return default
    
    def delete(self, key: str) -> bool:
        """ìºì‹œ í•­ëª© ì‚­ì œ"""
        try:
            with self.lock:
                removed = False
                
                # L1ì—ì„œ ì‚­ì œ
                if key in self.l1_cache:
                    del self.l1_cache[key]
                    removed = True
                
                # L2ì—ì„œ ì‚­ì œ
                if key in self.l2_index:
                    self._remove_from_disk(key)
                    del self.l2_index[key]
                    removed = True
                
                # L3ì—ì„œ ì‚­ì œ (í–¥í›„ êµ¬í˜„)
                if key in self.l3_index:
                    del self.l3_index[key]
                    removed = True
                
                # ì ‘ê·¼ ë¹ˆë„ì—ì„œ ì‚­ì œ
                if key in self.access_frequency:
                    del self.access_frequency[key]
                
                if removed:
                    self.stats.total_entries -= 1
                    self.logger.debug(f"ğŸ—‘ï¸ ìºì‹œ ì‚­ì œ: {key}")
                
                return removed
                
        except Exception as e:
            self.logger.error(f"âŒ ìºì‹œ ì‚­ì œ ì˜¤ë¥˜: {e}")
            return False
    
    def clear(self, level: Optional[CacheLevel] = None) -> None:
        """ìºì‹œ ì •ë¦¬"""
        try:
            with self.lock:
                if level is None or level == CacheLevel.L1_MEMORY:
                    self.l1_cache.clear()
                    self.stats.l1_entries = 0
                
                if level is None or level == CacheLevel.L2_DISK:
                    self.l2_index.clear()
                    self.stats.l2_entries = 0
                    # ë””ìŠ¤í¬ íŒŒì¼ë“¤ ì‚­ì œ
                    for file_path in self.l2_dir.glob("*.cache"):
                        file_path.unlink()
                
                if level is None or level == CacheLevel.L3_NETWORK:
                    self.l3_index.clear()
                    self.stats.l3_entries = 0
                
                if level is None:
                    self.access_frequency.clear()
                    self.stats.total_entries = 0
                    self.stats.total_size_mb = 0.0
                
                self.logger.info(f"ğŸ§¹ ìºì‹œ ì •ë¦¬ ì™„ë£Œ: {level.value if level else 'ALL'}")
                
        except Exception as e:
            self.logger.error(f"âŒ ìºì‹œ ì •ë¦¬ ì˜¤ë¥˜: {e}")
    
    def cleanup_expired_entries(self) -> int:
        """ë§Œë£Œëœ í•­ëª© ì •ë¦¬"""
        try:
            with self.lock:
                removed_count = 0
                current_time = time.time()
                
                # L1 ë§Œë£Œ í•­ëª© ì •ë¦¬
                expired_l1_keys = []
                for key, entry in self.l1_cache.items():
                    if self._is_expired(entry):
                        expired_l1_keys.append(key)
                
                for key in expired_l1_keys:
                    del self.l1_cache[key]
                    removed_count += 1
                
                # L2 ë§Œë£Œ í•­ëª© ì •ë¦¬
                expired_l2_keys = []
                for key, entry_info in self.l2_index.items():
                    if current_time - entry_info['created_at'] > entry_info['ttl']:
                        expired_l2_keys.append(key)
                
                for key in expired_l2_keys:
                    self._remove_from_disk(key)
                    del self.l2_index[key]
                    removed_count += 1
                
                if removed_count > 0:
                    self.stats.total_entries -= removed_count
                    self.stats.last_cleanup = datetime.now().isoformat()
                    self.logger.info(f"ğŸ§¹ ë§Œë£Œ í•­ëª© ì •ë¦¬: {removed_count}ê°œ")
                
                return removed_count
                
        except Exception as e:
            self.logger.error(f"âŒ ë§Œë£Œ í•­ëª© ì •ë¦¬ ì˜¤ë¥˜: {e}")
            return 0
    
    def _process_data_for_storage(self, data: Any, should_compress: bool) -> Tuple[Any, CompressionType, int]:
        """ì €ì¥ìš© ë°ì´í„° ì²˜ë¦¬"""
        if not should_compress:
            return data, CompressionType.NONE, self._calculate_size(data)
        
        # ìµœì  ì••ì¶• ë°©ì‹ ì„ íƒ
        compression_type = self._select_compression_type(data)
        
        # ì••ì¶• ì‹¤í–‰
        compressed_data = self.compression_engines[compression_type](data)
        compressed_size = self._calculate_size(compressed_data)
        
        return compressed_data, compression_type, compressed_size
    
    def _restore_data_from_storage(self, entry: CacheEntry) -> Any:
        """ì €ì¥ëœ ë°ì´í„° ë³µì›"""
        if not entry.compressed:
            return entry.data
        
        return self.decompression_engines[entry.compression_type](entry.data)
    
    def _select_compression_type(self, data: Any) -> CompressionType:
        """ìµœì  ì••ì¶• íƒ€ì… ì„ íƒ"""
        # ë°ì´í„° íƒ€ì…ì— ë”°ë¥¸ ì••ì¶• ë°©ì‹ ì„ íƒ
        if isinstance(data, (dict, list)):
            return CompressionType.LZ4  # JSON ë°ì´í„°ì— LZ4ê°€ íš¨ìœ¨ì 
        elif isinstance(data, bytes):
            return CompressionType.GZIP  # ë°”ì´ë„ˆë¦¬ ë°ì´í„°ì— GZIP íš¨ìœ¨ì 
        else:
            return CompressionType.PICKLE  # ê¸°íƒ€ ê°ì²´ëŠ” pickle + gzip
    
    def _gzip_compress(self, data: Any) -> bytes:
        """GZIP ì••ì¶•"""
        if isinstance(data, str):
            data = data.encode('utf-8')
        elif not isinstance(data, bytes):
            data = pickle.dumps(data)
        return gzip.compress(data)
    
    def _gzip_decompress(self, data: bytes) -> Any:
        """GZIP ì••ì¶• í•´ì œ"""
        decompressed = gzip.decompress(data)
        try:
            return pickle.loads(decompressed)
        except:
            return decompressed.decode('utf-8')
    
    def _lz4_compress(self, data: Any) -> bytes:
        """LZ4 ì••ì¶•"""
        if not isinstance(data, bytes):
            data = json.dumps(data, ensure_ascii=False).encode('utf-8')
        return lz4.frame.compress(data)
    
    def _lz4_decompress(self, data: bytes) -> Any:
        """LZ4 ì••ì¶• í•´ì œ"""
        decompressed = lz4.frame.decompress(data)
        try:
            return json.loads(decompressed.decode('utf-8'))
        except:
            return decompressed
    
    def _pickle_compress(self, data: Any) -> bytes:
        """Pickle + GZIP ì••ì¶•"""
        pickled = pickle.dumps(data)
        return gzip.compress(pickled)
    
    def _pickle_decompress(self, data: bytes) -> Any:
        """Pickle + GZIP ì••ì¶• í•´ì œ"""
        decompressed = gzip.decompress(data)
        return pickle.loads(decompressed)
    
    def _calculate_size(self, data: Any) -> int:
        """ë°ì´í„° í¬ê¸° ê³„ì‚°"""
        if isinstance(data, bytes):
            return len(data)
        elif isinstance(data, str):
            return len(data.encode('utf-8'))
        else:
            try:
                return len(pickle.dumps(data))
            except:
                return 1024  # ì¶”ì •ê°’
    
    def _is_expired(self, entry: CacheEntry) -> bool:
        """ë§Œë£Œ ì—¬ë¶€ í™•ì¸"""
        if entry.ttl is None:
            return False
        return (time.time() - entry.created_at) > entry.ttl
    
    def _store_by_level(self, entry: CacheEntry, strategy: CacheStrategy) -> bool:
        """ë ˆë²¨ë³„ ì €ì¥"""
        if entry.level == CacheLevel.L1_MEMORY:
            return self._store_to_l1(entry, strategy)
        elif entry.level == CacheLevel.L2_DISK:
            return self._store_to_l2(entry)
        elif entry.level == CacheLevel.L3_NETWORK:
            return self._store_to_l3(entry)
        return False
    
    def _store_to_l1(self, entry: CacheEntry, strategy: CacheStrategy) -> bool:
        """L1 ë©”ëª¨ë¦¬ì— ì €ì¥"""
        try:
            # ë©”ëª¨ë¦¬ í•œê³„ í™•ì¸
            current_size = sum(e.size_bytes for e in self.l1_cache.values())
            max_size = self.config.l1_max_size_mb * 1024 * 1024
            
            # ê³µê°„ í™•ë³´ í•„ìš” ì‹œ ì •ë¦¬
            if current_size + entry.size_bytes > max_size:
                self._evict_l1_entries(strategy, entry.size_bytes)
            
            # ì €ì¥
            self.l1_cache[entry.key] = entry
            self.stats.l1_entries = len(self.l1_cache)
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ L1 ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def _store_to_l2(self, entry: CacheEntry) -> bool:
        """L2 ë””ìŠ¤í¬ì— ì €ì¥"""
        try:
            # íŒŒì¼ ê²½ë¡œ
            file_path = self.l2_dir / f"{entry.key}.cache"
            
            # ë°ì´í„° ì €ì¥
            with open(file_path, 'wb') as f:
                pickle.dump(entry, f)
            
            # ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
            self.l2_index[entry.key] = {
                'file_path': str(file_path),
                'created_at': entry.created_at,
                'ttl': entry.ttl,
                'size_bytes': entry.size_bytes,
                'compressed': entry.compressed,
                'compression_type': entry.compression_type.value
            }
            
            self.stats.l2_entries = len(self.l2_index)
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ L2 ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def _store_to_l3(self, entry: CacheEntry) -> bool:
        """L3 ë„¤íŠ¸ì›Œí¬ì— ì €ì¥ (í–¥í›„ êµ¬í˜„)"""
        # í–¥í›„ í´ë¼ìš°ë“œ ì €ì¥ì†Œ ì—°ë™ êµ¬í˜„
        return True
    
    def _evict_l1_entries(self, strategy: CacheStrategy, needed_space: int) -> None:
        """L1 ì—”íŠ¸ë¦¬ ì¶•ì¶œ"""
        if strategy == CacheStrategy.LRU:
            self._evict_lru()
        elif strategy == CacheStrategy.LFU:
            self._evict_lfu()
        elif strategy == CacheStrategy.TTL:
            self._evict_expired()
        else:  # ADAPTIVE
            self._evict_adaptive(needed_space)
    
    def _evict_lru(self) -> None:
        """LRU ì¶•ì¶œ"""
        if self.l1_cache:
            key, _ = self.l1_cache.popitem(last=False)  # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª©
            self.stats.eviction_count += 1
            self.logger.debug(f"ğŸ”„ LRU ì¶•ì¶œ: {key}")
    
    def _evict_lfu(self) -> None:
        """LFU ì¶•ì¶œ"""
        if self.l1_cache:
            # ê°€ì¥ ì ê²Œ ì ‘ê·¼ëœ í•­ëª© ì°¾ê¸°
            min_access = min(entry.access_count for entry in self.l1_cache.values())
            for key, entry in self.l1_cache.items():
                if entry.access_count == min_access:
                    del self.l1_cache[key]
                    self.stats.eviction_count += 1
                    self.logger.debug(f"ğŸ”„ LFU ì¶•ì¶œ: {key}")
                    break
    
    def _evict_expired(self) -> None:
        """ë§Œë£Œëœ í•­ëª© ì¶•ì¶œ"""
        expired_keys = []
        for key, entry in self.l1_cache.items():
            if self._is_expired(entry):
                expired_keys.append(key)
        
        for key in expired_keys:
            del self.l1_cache[key]
            self.stats.eviction_count += 1
            self.logger.debug(f"ğŸ”„ TTL ì¶•ì¶œ: {key}")
    
    def _evict_adaptive(self, needed_space: int) -> None:
        """ì ì‘í˜• ì¶•ì¶œ"""
        # 1. ë§Œë£Œëœ í•­ëª© ìš°ì„  ì œê±°
        self._evict_expired()
        
        # 2. í•„ìš”í•œ ê³µê°„ì´ í™•ë³´ë˜ì—ˆëŠ”ì§€ í™•ì¸
        current_size = sum(e.size_bytes for e in self.l1_cache.values())
        max_size = self.config.l1_max_size_mb * 1024 * 1024
        
        if current_size + needed_space <= max_size:
            return
        
        # 3. LFUì™€ LRU ì¡°í•©
        entries_by_score = []
        for key, entry in self.l1_cache.items():
            # ì ìˆ˜ = ì ‘ê·¼ ë¹ˆë„ * ìµœê·¼ ì ‘ê·¼ ì‹œê°„ ê°€ì¤‘ì¹˜
            time_weight = 1.0 / (1.0 + (time.time() - entry.last_accessed) / 3600)
            score = entry.access_count * time_weight
            entries_by_score.append((score, key))
        
        # ë‚®ì€ ì ìˆ˜ë¶€í„° ì •ë ¬
        entries_by_score.sort()
        
        # í•„ìš”í•œ ê³µê°„ê¹Œì§€ ì œê±°
        freed_space = 0
        for score, key in entries_by_score:
            if freed_space >= needed_space:
                break
            
            entry = self.l1_cache[key]
            freed_space += entry.size_bytes
            del self.l1_cache[key]
            self.stats.eviction_count += 1
            self.logger.debug(f"ğŸ”„ ì ì‘í˜• ì¶•ì¶œ: {key} (ì ìˆ˜: {score:.2f})")
    
    def _load_from_disk(self, key: str, entry_info: Dict) -> Any:
        """ë””ìŠ¤í¬ì—ì„œ ë¡œë“œ"""
        try:
            file_path = Path(entry_info['file_path'])
            if not file_path.exists():
                return None
            
            with open(file_path, 'rb') as f:
                entry = pickle.load(f)
            
            return self._restore_data_from_storage(entry)
            
        except Exception as e:
            self.logger.error(f"âŒ ë””ìŠ¤í¬ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def _remove_from_disk(self, key: str) -> None:
        """ë””ìŠ¤í¬ì—ì„œ ì œê±°"""
        try:
            if key in self.l2_index:
                file_path = Path(self.l2_index[key]['file_path'])
                if file_path.exists():
                    file_path.unlink()
        except Exception as e:
            self.logger.error(f"âŒ ë””ìŠ¤í¬ ì œê±° ì‹¤íŒ¨: {e}")
    
    def _promote_to_l1(self, key: str, data: Any, entry_info: Dict) -> None:
        """L1ìœ¼ë¡œ ìŠ¹ê²©"""
        try:
            # L1 ì—”íŠ¸ë¦¬ ìƒì„±
            entry = CacheEntry(
                key=key,
                data=data,
                level=CacheLevel.L1_MEMORY,
                created_at=entry_info['created_at'],
                last_accessed=time.time(),
                access_count=1,
                ttl=entry_info['ttl'],
                size_bytes=entry_info['size_bytes'],
                compressed=False,  # L1ì—ì„œëŠ” ì••ì¶• í•´ì œ ìƒíƒœë¡œ ë³´ê´€
                compression_type=CompressionType.NONE,
                metadata={}
            )
            
            # L1ì— ì €ì¥
            self._store_to_l1(entry, CacheStrategy.ADAPTIVE)
            
            self.logger.debug(f"â¬†ï¸ L1 ìŠ¹ê²©: {key}")
            
        except Exception as e:
            self.logger.error(f"âŒ L1 ìŠ¹ê²© ì‹¤íŒ¨: {e}")
    
    def _remove_entry(self, key: str, level: CacheLevel) -> None:
        """ì—”íŠ¸ë¦¬ ì œê±°"""
        if level == CacheLevel.L1_MEMORY and key in self.l1_cache:
            del self.l1_cache[key]
        elif level == CacheLevel.L2_DISK and key in self.l2_index:
            self._remove_from_disk(key)
            del self.l2_index[key]
        elif level == CacheLevel.L3_NETWORK and key in self.l3_index:
            del self.l3_index[key]
    
    def _check_memory_pressure(self) -> None:
        """ë©”ëª¨ë¦¬ ì••ë°• í™•ì¸ ë° ëŒ€ì‘"""
        try:
            # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ í™•ì¸
            memory = psutil.virtual_memory()
            
            if memory.percent > self.config.memory_pressure_threshold:
                # ë©”ëª¨ë¦¬ ì••ë°• ìƒí™© - ìºì‹œ ì •ë¦¬
                self._handle_memory_pressure(memory.percent)
                
        except Exception as e:
            self.logger.error(f"âŒ ë©”ëª¨ë¦¬ ì••ë°• í™•ì¸ ì‹¤íŒ¨: {e}")
    
    def _handle_memory_pressure(self, memory_percent: float) -> None:
        """ë©”ëª¨ë¦¬ ì••ë°• ì²˜ë¦¬"""
        if memory_percent > 95:
            # ì‹¬ê°: L1 ìºì‹œ 50% ì •ë¦¬
            self._reduce_l1_cache(0.5)
            self.logger.warning(f"ğŸš¨ ì‹¬ê°í•œ ë©”ëª¨ë¦¬ ì••ë°• ({memory_percent:.1f}%) - L1 ìºì‹œ 50% ì •ë¦¬")
        elif memory_percent > 90:
            # ë†’ìŒ: L1 ìºì‹œ 25% ì •ë¦¬
            self._reduce_l1_cache(0.25)
            self.logger.warning(f"âš ï¸ ë†’ì€ ë©”ëª¨ë¦¬ ì••ë°• ({memory_percent:.1f}%) - L1 ìºì‹œ 25% ì •ë¦¬")
        elif memory_percent > 85:
            # ë³´í†µ: ë§Œë£Œëœ í•­ëª©ë§Œ ì •ë¦¬
            self._evict_expired()
            self.logger.info(f"â„¹ï¸ ë©”ëª¨ë¦¬ ì••ë°• ({memory_percent:.1f}%) - ë§Œë£Œ í•­ëª© ì •ë¦¬")
    
    def _reduce_l1_cache(self, ratio: float) -> None:
        """L1 ìºì‹œ í¬ê¸° ê°ì†Œ"""
        target_count = int(len(self.l1_cache) * (1 - ratio))
        current_count = len(self.l1_cache)
        
        while len(self.l1_cache) > target_count:
            self._evict_adaptive(0)
        
        removed = current_count - len(self.l1_cache)
        self.logger.info(f"ğŸ§¹ L1 ìºì‹œ ì¶•ì†Œ: {removed}ê°œ í•­ëª© ì œê±°")
    
    def _update_statistics(self) -> None:
        """í†µê³„ ì—…ë°ì´íŠ¸"""
        try:
            with self.lock:
                # ì—”íŠ¸ë¦¬ ìˆ˜ ì—…ë°ì´íŠ¸
                self.stats.l1_entries = len(self.l1_cache)
                self.stats.l2_entries = len(self.l2_index)
                self.stats.l3_entries = len(self.l3_index)
                self.stats.total_entries = self.stats.l1_entries + self.stats.l2_entries + self.stats.l3_entries
                
                # ì´ í¬ê¸° ê³„ì‚°
                l1_size = sum(entry.size_bytes for entry in self.l1_cache.values())
                l2_size = sum(info['size_bytes'] for info in self.l2_index.values())
                self.stats.total_size_mb = (l1_size + l2_size) / (1024**2)
                
                # ì••ì¶•ë¥  ê³„ì‚°
                self._calculate_compression_ratio()
                
        except Exception as e:
            self.logger.error(f"âŒ í†µê³„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    def _calculate_compression_ratio(self) -> None:
        """ì••ì¶•ë¥  ê³„ì‚°"""
        try:
            total_original = 0
            total_compressed = 0
            
            for entry in self.l1_cache.values():
                if entry.compressed:
                    # ì••ì¶•ëœ í¬ê¸°ì™€ ì›ë³¸ í¬ê¸° ë¹„êµ (ì¶”ì •)
                    total_compressed += entry.size_bytes
                    total_original += entry.size_bytes * 2  # ì¶”ì •ê°’
                else:
                    total_original += entry.size_bytes
                    total_compressed += entry.size_bytes
            
            if total_original > 0:
                self.stats.compression_ratio = total_compressed / total_original
            else:
                self.stats.compression_ratio = 1.0
                
        except Exception as e:
            self.logger.error(f"âŒ ì••ì¶•ë¥  ê³„ì‚° ì‹¤íŒ¨: {e}")
            self.stats.compression_ratio = 1.0
    
    def _update_hit_rate(self) -> None:
        """íˆíŠ¸ìœ¨ ì—…ë°ì´íŠ¸"""
        total_requests = self.stats.hit_count + self.stats.miss_count
        if total_requests > 0:
            self.stats.hit_rate = self.stats.hit_count / total_requests
        else:
            self.stats.hit_rate = 0.0
    
    def get_statistics(self) -> CacheStats:
        """ìºì‹œ í†µê³„ ë°˜í™˜"""
        with self.lock:
            self._update_statistics()
            return self.stats
    
    def get_performance_report(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±"""
        try:
            with self.lock:
                recent_ops = list(self.performance_history)[-100:]  # ìµœê·¼ 100ê°œ ì‘ì—…
                
                if not recent_ops:
                    return {"status": "no_data"}
                
                # ì‘ì—…ë³„ í†µê³„
                set_ops = [op for op in recent_ops if op['operation'] == 'set']
                get_ops = [op for op in recent_ops if op['operation'] == 'get']
                hit_ops = [op for op in get_ops if op.get('hit', False)]
                
                return {
                    "timestamp": datetime.now().isoformat(),
                    "total_operations": len(recent_ops),
                    "set_operations": len(set_ops),
                    "get_operations": len(get_ops),
                    "cache_hits": len(hit_ops),
                    "hit_rate": len(hit_ops) / len(get_ops) if get_ops else 0.0,
                    "avg_set_time_ms": sum(op['duration_ms'] for op in set_ops) / len(set_ops) if set_ops else 0.0,
                    "avg_get_time_ms": sum(op['duration_ms'] for op in get_ops) / len(get_ops) if get_ops else 0.0,
                    "level_distribution": {
                        "l1_memory": len([op for op in hit_ops if op.get('level') == 'l1_memory']),
                        "l2_disk": len([op for op in hit_ops if op.get('level') == 'l2_disk']),
                        "l3_network": len([op for op in hit_ops if op.get('level') == 'l3_network'])
                    }
                }
                
        except Exception as e:
            self.logger.error(f"âŒ ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {e}")
            return {"status": "error", "message": str(e)}
    
    def register_memory_optimizer(self, memory_optimizer) -> None:
        """ë©”ëª¨ë¦¬ ìµœì í™” ì—”ì§„ ë“±ë¡"""
        self.memory_optimizer = memory_optimizer
        if memory_optimizer:
            memory_optimizer.register_cache(self)
            self.logger.info("ğŸ”— ë©”ëª¨ë¦¬ ìµœì í™” ì—”ì§„ ì—°ë™ ì™„ë£Œ")
    
    def partial_clear(self, ratio: float = 0.5) -> None:
        """ë¶€ë¶„ ìºì‹œ ì •ë¦¬ (ë©”ëª¨ë¦¬ ìµœì í™” ì—”ì§„ìš©)"""
        with self.lock:
            target_count = int(len(self.l1_cache) * (1 - ratio))
            
            while len(self.l1_cache) > target_count:
                self._evict_adaptive(0)
            
            self.logger.info(f"ğŸ§¹ ë¶€ë¶„ ìºì‹œ ì •ë¦¬: {ratio:.1%} ì •ë¦¬ ì™„ë£Œ")
    
    def get_size(self) -> float:
        """ìºì‹œ í¬ê¸° ë°˜í™˜ (MB ë‹¨ìœ„)"""
        return self.stats.total_size_mb
    
    def export_cache_report(self, output_path: str) -> None:
        """ìºì‹œ ë³´ê³ ì„œ ë‚´ë³´ë‚´ê¸°"""
        try:
            report_data = {
                "export_timestamp": datetime.now().isoformat(),
                "cache_statistics": asdict(self.get_statistics()),
                "performance_report": self.get_performance_report(),
                "configuration": asdict(self.config),
                "cache_keys": {
                    "l1_keys": list(self.l1_cache.keys())[:50],  # ìµœëŒ€ 50ê°œ
                    "l2_keys": list(self.l2_index.keys())[:50],
                    "l3_keys": list(self.l3_index.keys())[:50]
                },
                "access_frequency": dict(sorted(
                    self.access_frequency.items(), 
                    key=lambda x: x[1], 
                    reverse=True
                )[:20])  # ìƒìœ„ 20ê°œ
            }
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"ğŸ“Š ìºì‹œ ë³´ê³ ì„œ ìƒì„±: {output_path}")
            
        except Exception as e:
            self.logger.error(f"âŒ ìºì‹œ ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {e}")
    
    def shutdown(self) -> None:
        """ìºì‹œ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        try:
            # ìë™ ì •ë¦¬ ì¤‘ì§€
            self.stop_auto_cleanup()
            
            # ì¸ë±ìŠ¤ ì €ì¥
            self._save_cache_indexes()
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self._update_statistics()
            
            self.logger.info("âœ… ìŠ¤ë§ˆíŠ¸ ìºì‹œ ê´€ë¦¬ì ì¢…ë£Œ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ìºì‹œ ë§¤ë‹ˆì € ì¢…ë£Œ ì‹¤íŒ¨: {e}")

# ì „ì—­ ìºì‹œ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤
_global_cache_manager = None

def get_global_cache_manager() -> SmartCacheManager:
    """ì „ì—­ ìºì‹œ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _global_cache_manager
    if _global_cache_manager is None:
        _global_cache_manager = SmartCacheManager()
        _global_cache_manager.start_auto_cleanup()
    return _global_cache_manager

# í¸ì˜ í•¨ìˆ˜ë“¤
def cache_set(key: str, data: Any, ttl: Optional[float] = None, level: CacheLevel = CacheLevel.L1_MEMORY) -> bool:
    """ìºì‹œ ì„¤ì •"""
    return get_global_cache_manager().set(key, data, level=level, ttl=ttl)

def cache_get(key: str, default: Any = None) -> Any:
    """ìºì‹œ ì¡°íšŒ"""
    return get_global_cache_manager().get(key, default)

def cache_delete(key: str) -> bool:
    """ìºì‹œ ì‚­ì œ"""
    return get_global_cache_manager().delete(key)

def cache_clear(level: Optional[CacheLevel] = None) -> None:
    """ìºì‹œ ì •ë¦¬"""
    get_global_cache_manager().clear(level)

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    cache_manager = SmartCacheManager()
    cache_manager.start_auto_cleanup()
    
    print("ğŸš€ ìŠ¤ë§ˆíŠ¸ ìºì‹œ ê´€ë¦¬ì í…ŒìŠ¤íŠ¸:")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_data = {
        "analysis_result": {
            "transcription": "ì•ˆë…•í•˜ì„¸ìš”, í…ŒìŠ¤íŠ¸ ìŒì„±ì…ë‹ˆë‹¤.",
            "sentiment": "positive",
            "entities": ["í…ŒìŠ¤íŠ¸", "ìŒì„±"],
            "confidence": 0.95
        }
    }
    
    # ìºì‹œ ì„¤ì •
    cache_manager.set("audio_analysis_001", test_data, ttl=3600)
    
    # ìºì‹œ ì¡°íšŒ
    result = cache_manager.get("audio_analysis_001")
    print(f"âœ… ìºì‹œ ì¡°íšŒ ê²°ê³¼: {result is not None}")
    
    # í†µê³„ í™•ì¸
    stats = cache_manager.get_statistics()
    print(f"ğŸ“Š ìºì‹œ í†µê³„:")
    print(f"   ì´ ì—”íŠ¸ë¦¬: {stats.total_entries}ê°œ")
    print(f"   íˆíŠ¸ìœ¨: {stats.hit_rate:.1%}")
    print(f"   ì´ í¬ê¸°: {stats.total_size_mb:.2f}MB")
    
    try:
        time.sleep(2)  # í…ŒìŠ¤íŠ¸ ëŒ€ê¸°
    except KeyboardInterrupt:
        print("\ní…ŒìŠ¤íŠ¸ ì¤‘ë‹¨")
    finally:
        cache_manager.shutdown()
        print("âœ… ìŠ¤ë§ˆíŠ¸ ìºì‹œ ê´€ë¦¬ì í…ŒìŠ¤íŠ¸ ì™„ë£Œ")