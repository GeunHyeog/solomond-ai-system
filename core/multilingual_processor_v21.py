#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì†”ë¡œëª¬ë“œ AI v2.1 - ë‹¤êµ­ì–´ ì²˜ë¦¬ ì—”ì§„
ìë™ ì–¸ì–´ ê°ì§€ + ì–¸ì–´ë³„ STT ìµœì í™” + í•œêµ­ì–´ í†µí•© ë¶„ì„

ì‘ì„±ì: ì „ê·¼í˜ (ì†”ë¡œëª¬ë“œ ëŒ€í‘œ)
ìƒì„±ì¼: 2025.07.11
ëª©ì : í˜„ì¥ì—ì„œ ë‹¤êµ­ì–´ í˜¼ìš© ìƒí™© ì™„ë²½ ì²˜ë¦¬
"""

import re
import json
import numpy as np
from typing import Dict, List, Tuple, Optional
from datetime import datetime
import logging
from collections import Counter

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class LanguageDetector:
    """ì–¸ì–´ ìë™ ê°ì§€ê¸°"""
    
    def __init__(self):
        # ì£¼ì–¼ë¦¬ ì—…ê³„ì—ì„œ ìì£¼ ì‚¬ìš©ë˜ëŠ” ì–¸ì–´ë³„ í‚¤ì›Œë“œ
        self.language_keywords = {
            'korean': {
                'words': ['ë‹¤ì´ì•„ëª¬ë“œ', 'ë°˜ì§€', 'ëª©ê±¸ì´', 'ê·€ê±¸ì´', 'íŒ”ì°Œ', 'ë³´ì„', 'ìºëŸ¿', 'ê¸ˆ', 'ì€', 'í”Œë˜í‹°ë„˜',
                         'ê°€ê²©', 'ì£¼ë¬¸', 'ì œì‘', 'ë””ìì¸', 'ê°ì •ì„œ', 'í’ˆì§ˆ', 'ë“±ê¸‰', 'ë¬´ê²Œ', 'í¬ê¸°', 'ìƒ‰ìƒ'],
                'patterns': [r'[ê°€-í£]', r'ì›$', r'ê°œ$', r'ë²ˆì§¸']
            },
            'english': {
                'words': ['diamond', 'ring', 'necklace', 'earring', 'bracelet', 'jewelry', 'carat', 'gold', 
                         'silver', 'platinum', 'price', 'order', 'design', 'certificate', 'quality', 'grade'],
                'patterns': [r'\b[A-Za-z]+\b', r'\$\d+', r'\d+ct\b', r'\bVVS\d?\b', r'\bGIA\b']
            },
            'chinese': {
                'words': ['é’»çŸ³', 'æˆ’æŒ‡', 'é¡¹é“¾', 'è€³ç¯', 'æ‰‹é•¯', 'ç å®', 'å…‹æ‹‰', 'é»„é‡‘', 'ç™½é“¶', 'é“‚é‡‘',
                         'ä»·æ ¼', 'è®¢å•', 'è®¾è®¡', 'è¯ä¹¦', 'è´¨é‡', 'ç­‰çº§'],
                'patterns': [r'[\u4e00-\u9fff]', r'å…ƒ$', r'å…‹æ‹‰']
            },
            'japanese': {
                'words': ['ãƒ€ã‚¤ãƒ¤ãƒ¢ãƒ³ãƒ‰', 'ãƒªãƒ³ã‚°', 'ãƒãƒƒã‚¯ãƒ¬ã‚¹', 'ãƒ”ã‚¢ã‚¹', 'ãƒ–ãƒ¬ã‚¹ãƒ¬ãƒƒãƒˆ', 'ã‚¸ãƒ¥ã‚¨ãƒªãƒ¼', 
                         'ã‚«ãƒ©ãƒƒãƒˆ', 'ã‚´ãƒ¼ãƒ«ãƒ‰', 'ã‚·ãƒ«ãƒãƒ¼', 'ãƒ—ãƒ©ãƒãƒŠ', 'ä¾¡æ ¼', 'æ³¨æ–‡', 'ãƒ‡ã‚¶ã‚¤ãƒ³'],
                'patterns': [r'[\u3040-\u309f\u30a0-\u30ff\u4e00-\u9fff]', r'å††$', r'ã‚«ãƒ©ãƒƒãƒˆ']
            }
        }
        
        # ê° ì–¸ì–´ë³„ ê°€ì¤‘ì¹˜
        self.language_weights = {
            'korean': 1.2,    # í•œêµ­ì–´ ìš°ëŒ€ (ìµœì¢… ë¶„ì„ì–¸ì–´)
            'english': 1.0,   # êµ­ì œ í‘œì¤€
            'chinese': 0.9,   # ì¤‘êµ­ ì‹œì¥
            'japanese': 0.8   # ì¼ë³¸ ì‹œì¥
        }
    
    def detect_language(self, text: str, confidence_threshold: float = 0.6) -> Dict:
        """í…ìŠ¤íŠ¸ì—ì„œ ì–¸ì–´ ê°ì§€"""
        if not text or not text.strip():
            return {"primary_language": "unknown", "confidence": 0.0, "language_distribution": {}}
        
        # ì–¸ì–´ë³„ ì ìˆ˜ ê³„ì‚°
        language_scores = {}
        
        for lang, config in self.language_keywords.items():
            score = 0.0
            
            # í‚¤ì›Œë“œ ë§¤ì¹­
            for keyword in config['words']:
                if keyword.lower() in text.lower():
                    score += 2.0
            
            # íŒ¨í„´ ë§¤ì¹­
            for pattern in config['patterns']:
                matches = len(re.findall(pattern, text, re.IGNORECASE))
                score += matches * 0.5
            
            # ê°€ì¤‘ì¹˜ ì ìš©
            score *= self.language_weights.get(lang, 1.0)
            
            # í…ìŠ¤íŠ¸ ê¸¸ì´ ëŒ€ë¹„ ì •ê·œí™”
            language_scores[lang] = score / max(len(text.split()), 1)
        
        # ì´ ì ìˆ˜ë¡œ ì •ê·œí™”í•˜ì—¬ ë¶„í¬ ê³„ì‚°
        total_score = sum(language_scores.values())
        if total_score == 0:
            return {"primary_language": "unknown", "confidence": 0.0, "language_distribution": {}}
        
        language_distribution = {lang: score/total_score for lang, score in language_scores.items()}
        
        # ì£¼ìš” ì–¸ì–´ ê²°ì •
        primary_language = max(language_distribution, key=language_distribution.get)
        confidence = language_distribution[primary_language]
        
        result = {
            "primary_language": primary_language,
            "confidence": confidence,
            "language_distribution": language_distribution,
            "is_confident": confidence >= confidence_threshold,
            "is_multilingual": len([lang for lang, score in language_distribution.items() if score > 0.1]) > 1
        }
        
        logger.info(f"ğŸŒ ì–¸ì–´ ê°ì§€ ì™„ë£Œ: {primary_language} ({confidence:.1%})")
        return result
    
    def detect_mixed_languages(self, segments: List[str]) -> List[Dict]:
        """ì—¬ëŸ¬ í…ìŠ¤íŠ¸ ì„¸ê·¸ë¨¼íŠ¸ì—ì„œ ì–¸ì–´ ê°ì§€"""
        results = []
        
        for i, segment in enumerate(segments):
            detection = self.detect_language(segment)
            detection['segment_id'] = i
            detection['text'] = segment
            results.append(detection)
        
        return results


class JewelryTermTranslator:
    """ì£¼ì–¼ë¦¬ ì „ë¬¸ìš©ì–´ ë²ˆì—­ê¸°"""
    
    def __init__(self):
        # ì£¼ì–¼ë¦¬ ì „ë¬¸ìš©ì–´ ë‹¤êµ­ì–´ ì‚¬ì „
        self.jewelry_dictionary = {
            # ë³´ì„ ì¢…ë¥˜
            'diamond': {'ko': 'ë‹¤ì´ì•„ëª¬ë“œ', 'en': 'diamond', 'zh': 'é’»çŸ³', 'ja': 'ãƒ€ã‚¤ãƒ¤ãƒ¢ãƒ³ãƒ‰'},
            'ruby': {'ko': 'ë£¨ë¹„', 'en': 'ruby', 'zh': 'çº¢å®çŸ³', 'ja': 'ãƒ«ãƒ“ãƒ¼'},
            'sapphire': {'ko': 'ì‚¬íŒŒì´ì–´', 'en': 'sapphire', 'zh': 'è“å®çŸ³', 'ja': 'ã‚µãƒ•ã‚¡ã‚¤ã‚¢'},
            'emerald': {'ko': 'ì—ë©”ë„ë“œ', 'en': 'emerald', 'zh': 'ç¥–æ¯ç»¿', 'ja': 'ã‚¨ãƒ¡ãƒ©ãƒ«ãƒ‰'},
            'pearl': {'ko': 'ì§„ì£¼', 'en': 'pearl', 'zh': 'çç ', 'ja': 'çœŸç '},
            
            # í’ˆì§ˆ ë“±ê¸‰
            'carat': {'ko': 'ìºëŸ¿', 'en': 'carat', 'zh': 'å…‹æ‹‰', 'ja': 'ã‚«ãƒ©ãƒƒãƒˆ'},
            'clarity': {'ko': 'íˆ¬ëª…ë„', 'en': 'clarity', 'zh': 'å‡€åº¦', 'ja': 'ã‚¯ãƒ©ãƒªãƒ†ã‚£'},
            'color': {'ko': 'ìƒ‰ìƒ', 'en': 'color', 'zh': 'é¢œè‰²', 'ja': 'ã‚«ãƒ©ãƒ¼'},
            'cut': {'ko': 'ì»·', 'en': 'cut', 'zh': 'åˆ‡å·¥', 'ja': 'ã‚«ãƒƒãƒˆ'},
            
            # ê¸ˆì† ì¢…ë¥˜
            'gold': {'ko': 'ê¸ˆ', 'en': 'gold', 'zh': 'é»„é‡‘', 'ja': 'ã‚´ãƒ¼ãƒ«ãƒ‰'},
            'silver': {'ko': 'ì€', 'en': 'silver', 'zh': 'é“¶', 'ja': 'ã‚·ãƒ«ãƒãƒ¼'},
            'platinum': {'ko': 'í”Œë˜í‹°ë„˜', 'en': 'platinum', 'zh': 'é“‚é‡‘', 'ja': 'ãƒ—ãƒ©ãƒãƒŠ'},
            
            # ì œí’ˆ ì¢…ë¥˜
            'ring': {'ko': 'ë°˜ì§€', 'en': 'ring', 'zh': 'æˆ’æŒ‡', 'ja': 'ãƒªãƒ³ã‚°'},
            'necklace': {'ko': 'ëª©ê±¸ì´', 'en': 'necklace', 'zh': 'é¡¹é“¾', 'ja': 'ãƒãƒƒã‚¯ãƒ¬ã‚¹'},
            'earring': {'ko': 'ê·€ê±¸ì´', 'en': 'earring', 'zh': 'è€³ç¯', 'ja': 'ãƒ”ã‚¢ã‚¹'},
            'bracelet': {'ko': 'íŒ”ì°Œ', 'en': 'bracelet', 'zh': 'æ‰‹é•¯', 'ja': 'ãƒ–ãƒ¬ã‚¹ãƒ¬ãƒƒãƒˆ'},
            
            # ë¹„ì¦ˆë‹ˆìŠ¤ ìš©ì–´
            'price': {'ko': 'ê°€ê²©', 'en': 'price', 'zh': 'ä»·æ ¼', 'ja': 'ä¾¡æ ¼'},
            'quality': {'ko': 'í’ˆì§ˆ', 'en': 'quality', 'zh': 'è´¨é‡', 'ja': 'å“è³ª'},
            'certificate': {'ko': 'ê°ì •ì„œ', 'en': 'certificate', 'zh': 'è¯ä¹¦', 'ja': 'é‘‘å®šæ›¸'},
            'wholesale': {'ko': 'ë„ë§¤', 'en': 'wholesale', 'zh': 'æ‰¹å‘', 'ja': 'å¸å£²'},
            'retail': {'ko': 'ì†Œë§¤', 'en': 'retail', 'zh': 'é›¶å”®', 'ja': 'å°å£²'}
        }
        
        # ì—­ë°©í–¥ ê²€ìƒ‰ì„ ìœ„í•œ ì¸ë±ìŠ¤
        self.reverse_index = {}
        for term, translations in self.jewelry_dictionary.items():
            for lang, translation in translations.items():
                if translation.lower() not in self.reverse_index:
                    self.reverse_index[translation.lower()] = []
                self.reverse_index[translation.lower()].append((term, lang))
    
    def translate_jewelry_terms(self, text: str, target_language: str = 'ko') -> str:
        """ì£¼ì–¼ë¦¬ ì „ë¬¸ìš©ì–´ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­"""
        translated_text = text
        
        # ë°œê²¬ëœ ìš©ì–´ë“¤ ì¶”ì 
        found_terms = []
        
        for term, translations in self.jewelry_dictionary.items():
            for source_lang, source_term in translations.items():
                if source_lang != target_language and source_term.lower() in text.lower():
                    target_term = translations.get(target_language, source_term)
                    
                    # ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´ êµì²´
                    pattern = re.compile(re.escape(source_term), re.IGNORECASE)
                    translated_text = pattern.sub(target_term, translated_text)
                    
                    found_terms.append({
                        'original': source_term,
                        'translated': target_term,
                        'source_language': source_lang,
                        'target_language': target_language
                    })
        
        return translated_text, found_terms
    
    def extract_jewelry_terms(self, text: str) -> List[Dict]:
        """í…ìŠ¤íŠ¸ì—ì„œ ì£¼ì–¼ë¦¬ ì „ë¬¸ìš©ì–´ ì¶”ì¶œ"""
        found_terms = []
        
        words = re.findall(r'\b\w+\b', text.lower())
        
        for word in words:
            if word in self.reverse_index:
                for term, lang in self.reverse_index[word]:
                    found_terms.append({
                        'term': word,
                        'standard_term': term,
                        'language': lang,
                        'translations': self.jewelry_dictionary[term]
                    })
        
        return found_terms


class MultilingualProcessor:
    """í†µí•© ë‹¤êµ­ì–´ ì²˜ë¦¬ê¸°"""
    
    def __init__(self):
        self.language_detector = LanguageDetector()
        self.term_translator = JewelryTermTranslator()
        
        # STT ëª¨ë¸ë³„ ì–¸ì–´ ì§€ì› ì •ë³´
        self.stt_models = {
            'whisper-korean': {'languages': ['korean'], 'accuracy': 0.95, 'specialty': 'korean_native'},
            'whisper-multilingual': {'languages': ['korean', 'english', 'chinese', 'japanese'], 'accuracy': 0.85, 'specialty': 'multilingual'},
            'whisper-english': {'languages': ['english'], 'accuracy': 0.92, 'specialty': 'english_native'}
        }
    
    def process_multilingual_content(self, content: str, content_type: str = 'transcript') -> Dict:
        """ë‹¤êµ­ì–´ ì»¨í…ì¸  í†µí•© ì²˜ë¦¬"""
        
        # 1. ì–¸ì–´ ê°ì§€
        language_detection = self.language_detector.detect_language(content)
        
        # 2. ì£¼ì–¼ë¦¬ ì „ë¬¸ìš©ì–´ ë²ˆì—­ (ëª¨ë“  ì–¸ì–´ë¥¼ í•œêµ­ì–´ë¡œ)
        translated_content, translated_terms = self.term_translator.translate_jewelry_terms(
            content, target_language='ko'
        )
        
        # 3. ì „ë¬¸ìš©ì–´ ì¶”ì¶œ
        extracted_terms = self.term_translator.extract_jewelry_terms(content)
        
        # 4. ìµœì  STT ëª¨ë¸ ì¶”ì²œ
        recommended_model = self._recommend_stt_model(language_detection)
        
        # 5. ë²ˆì—­ í’ˆì§ˆ í‰ê°€
        translation_quality = self._evaluate_translation_quality(content, translated_content, translated_terms)
        
        result = {
            'original_content': content,
            'translated_content': translated_content,
            'language_detection': language_detection,
            'translated_terms': translated_terms,
            'extracted_terms': extracted_terms,
            'recommended_stt_model': recommended_model,
            'translation_quality': translation_quality,
            'processing_timestamp': datetime.now().isoformat(),
            'korean_summary': self._generate_korean_summary(translated_content, language_detection)
        }
        
        logger.info(f"ğŸŒ ë‹¤êµ­ì–´ ì²˜ë¦¬ ì™„ë£Œ: {language_detection['primary_language']} â†’ í•œêµ­ì–´")
        return result
    
    def _recommend_stt_model(self, language_detection: Dict) -> Dict:
        """ì–¸ì–´ ê°ì§€ ê²°ê³¼ì— ë”°ë¥¸ ìµœì  STT ëª¨ë¸ ì¶”ì²œ"""
        primary_lang = language_detection['primary_language']
        confidence = language_detection['confidence']
        is_multilingual = language_detection['is_multilingual']
        
        if is_multilingual:
            # ë‹¤êµ­ì–´ í˜¼ìš© ì‹œ ë‹¤êµ­ì–´ ëª¨ë¸ ì‚¬ìš©
            return {
                'model': 'whisper-multilingual',
                'reason': 'ë‹¤êµ­ì–´ í˜¼ìš© ê°ì§€',
                'expected_accuracy': 0.85,
                'preprocessing_needed': True
            }
        elif primary_lang == 'korean' and confidence > 0.8:
            # í•œêµ­ì–´ ì „ìš© ëª¨ë¸
            return {
                'model': 'whisper-korean',
                'reason': 'í•œêµ­ì–´ ë‹¨ì¼ ì–¸ì–´ (ê³ ì‹ ë¢°ë„)',
                'expected_accuracy': 0.95,
                'preprocessing_needed': False
            }
        elif primary_lang == 'english' and confidence > 0.8:
            # ì˜ì–´ ì „ìš© ëª¨ë¸
            return {
                'model': 'whisper-english',
                'reason': 'ì˜ì–´ ë‹¨ì¼ ì–¸ì–´ (ê³ ì‹ ë¢°ë„)',
                'expected_accuracy': 0.92,
                'preprocessing_needed': False
            }
        else:
            # ê¸°ë³¸ì ìœ¼ë¡œ ë‹¤êµ­ì–´ ëª¨ë¸ ì‚¬ìš©
            return {
                'model': 'whisper-multilingual',
                'reason': 'ì–¸ì–´ ë¶ˆí™•ì‹¤ ë˜ëŠ” ì €ì‹ ë¢°ë„',
                'expected_accuracy': 0.80,
                'preprocessing_needed': True
            }
    
    def _evaluate_translation_quality(self, original: str, translated: str, terms: List[Dict]) -> Dict:
        """ë²ˆì—­ í’ˆì§ˆ í‰ê°€"""
        
        # ë²ˆì—­ëœ ìš©ì–´ ê°œìˆ˜
        translated_count = len(terms)
        
        # í…ìŠ¤íŠ¸ ê¸¸ì´ ë³€í™”ìœ¨
        length_ratio = len(translated) / max(len(original), 1)
        
        # ì£¼ì–¼ë¦¬ ìš©ì–´ ë¹„ìœ¨
        total_words = len(original.split())
        term_ratio = translated_count / max(total_words, 1)
        
        # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        quality_score = min(1.0, (translated_count * 0.3 + term_ratio * 0.4 + min(length_ratio, 2.0) * 0.3))
        
        return {
            'quality_score': round(quality_score, 3),
            'translated_terms_count': translated_count,
            'term_coverage_ratio': round(term_ratio, 3),
            'length_change_ratio': round(length_ratio, 3),
            'quality_level': self._get_quality_level(quality_score)
        }
    
    def _get_quality_level(self, score: float) -> str:
        """í’ˆì§ˆ ì ìˆ˜ë¥¼ ë ˆë²¨ë¡œ ë³€í™˜"""
        if score >= 0.8:
            return "ìš°ìˆ˜"
        elif score >= 0.6:
            return "ì–‘í˜¸"
        elif score >= 0.4:
            return "ë³´í†µ"
        else:
            return "ê°œì„ í•„ìš”"
    
    def _generate_korean_summary(self, translated_content: str, language_detection: Dict) -> str:
        """í•œêµ­ì–´ í†µí•© ìš”ì•½ ìƒì„±"""
        primary_lang = language_detection['primary_language']
        confidence = language_detection['confidence']
        
        summary_parts = []
        
        # ì–¸ì–´ ê°ì§€ ê²°ê³¼ ìš”ì•½
        if language_detection['is_multilingual']:
            lang_dist = language_detection['language_distribution']
            lang_percentages = [f"{lang}: {score:.1%}" for lang, score in lang_dist.items() if score > 0.1]
            summary_parts.append(f"ë‹¤êµ­ì–´ í™˜ê²½ ê°ì§€ ({', '.join(lang_percentages)})")
        else:
            summary_parts.append(f"ì£¼ìš” ì–¸ì–´: {primary_lang} (ì‹ ë¢°ë„: {confidence:.1%})")
        
        # ë²ˆì—­ëœ ë‚´ìš© ìš”ì•½
        summary_parts.append(f"ë²ˆì—­ëœ ë‚´ìš©: {translated_content[:200]}...")
        
        return " | ".join(summary_parts)
    
    def batch_process_multilingual_files(self, files: List[Dict]) -> Dict:
        """ì—¬ëŸ¬ ë‹¤êµ­ì–´ íŒŒì¼ ì¼ê´„ ì²˜ë¦¬"""
        results = {
            'files_processed': len(files),
            'processing_timestamp': datetime.now().isoformat(),
            'individual_results': [],
            'aggregated_analysis': {}
        }
        
        all_language_detections = []
        all_terms = []
        
        for file_info in files:
            file_result = self.process_multilingual_content(
                file_info['content'], 
                file_info.get('type', 'transcript')
            )
            file_result['file_info'] = file_info
            results['individual_results'].append(file_result)
            
            all_language_detections.append(file_result['language_detection'])
            all_terms.extend(file_result['extracted_terms'])
        
        # ì „ì²´ ë¶„ì„
        results['aggregated_analysis'] = self._generate_aggregated_analysis(
            all_language_detections, all_terms
        )
        
        return results
    
    def _generate_aggregated_analysis(self, language_detections: List[Dict], all_terms: List[Dict]) -> Dict:
        """ì „ì²´ íŒŒì¼ë“¤ì˜ í†µí•© ë¶„ì„"""
        
        # ì–¸ì–´ ë¶„í¬ í†µê³„
        primary_languages = [ld['primary_language'] for ld in language_detections]
        language_counter = Counter(primary_languages)
        
        # ë‹¤êµ­ì–´ íŒŒì¼ ë¹„ìœ¨
        multilingual_count = sum(1 for ld in language_detections if ld['is_multilingual'])
        multilingual_ratio = multilingual_count / max(len(language_detections), 1)
        
        # ì „ë¬¸ìš©ì–´ í†µê³„
        term_counter = Counter([term['standard_term'] for term in all_terms])
        
        return {
            'language_distribution': dict(language_counter),
            'multilingual_ratio': round(multilingual_ratio, 3),
            'most_common_language': language_counter.most_common(1)[0] if language_counter else None,
            'total_unique_terms': len(term_counter),
            'most_frequent_terms': term_counter.most_common(10),
            'recommended_strategy': self._recommend_processing_strategy(language_counter, multilingual_ratio)
        }
    
    def _recommend_processing_strategy(self, language_counter: Counter, multilingual_ratio: float) -> Dict:
        """ì²˜ë¦¬ ì „ëµ ì¶”ì²œ"""
        most_common = language_counter.most_common(1)
        
        if multilingual_ratio > 0.5:
            return {
                'strategy': 'multilingual_focus',
                'description': 'ë‹¤êµ­ì–´ í˜¼ìš©ì´ ë¹ˆë²ˆí•˜ë¯€ë¡œ ë‹¤êµ­ì–´ ì²˜ë¦¬ì— íŠ¹í™”ëœ íŒŒì´í”„ë¼ì¸ ì‚¬ìš©',
                'recommended_models': ['whisper-multilingual'],
                'preprocessing': ['language_segmentation', 'term_standardization']
            }
        elif most_common and most_common[0][0] == 'korean':
            return {
                'strategy': 'korean_optimized',
                'description': 'í•œêµ­ì–´ ì¤‘ì‹¬ í™˜ê²½ìœ¼ë¡œ í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸ ì‚¬ìš©',
                'recommended_models': ['whisper-korean'],
                'preprocessing': ['korean_text_normalization']
            }
        else:
            return {
                'strategy': 'balanced_approach',
                'description': 'ê· í˜•ì¡íŒ ë‹¤êµ­ì–´ ì ‘ê·¼ë²• ì‚¬ìš©',
                'recommended_models': ['whisper-multilingual', 'whisper-korean'],
                'preprocessing': ['language_detection', 'adaptive_routing']
            }


# ì‚¬ìš© ì˜ˆì‹œ ë° í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    processor = MultilingualProcessor()
    
    # í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸ (ë‹¤êµ­ì–´ í˜¼ìš©)
    test_texts = [
        "ì•ˆë…•í•˜ì„¸ìš”, ë‹¤ì´ì•„ëª¬ë“œ priceë¥¼ ë¬¸ì˜ë“œë¦½ë‹ˆë‹¤. What's the carat?",
        "è¿™ä¸ªé’»çŸ³æˆ’æŒ‡å¤šå°‘é’±ï¼Ÿ QualityëŠ” ì–´ë–¤ê°€ìš”?",
        "18K gold ring with 1 carat diamond, ê°€ê²©ì€ ì–¼ë§ˆì¸ê°€ìš”?",
        "ì£¼ë¬¸í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤. certificateëŠ” GIA ê°ì •ì„œì¸ê°€ìš”?"
    ]
    
    print("ğŸŒ ë‹¤êµ­ì–´ ì²˜ë¦¬ ì—”ì§„ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    for i, text in enumerate(test_texts, 1):
        print(f"\nğŸ“ í…ŒìŠ¤íŠ¸ {i}: {text}")
        result = processor.process_multilingual_content(text)
        
        print(f"ğŸŒ ê°ì§€ëœ ì–¸ì–´: {result['language_detection']['primary_language']} "
              f"({result['language_detection']['confidence']:.1%})")
        print(f"ğŸ”„ ë²ˆì—­ëœ ë‚´ìš©: {result['translated_content']}")
        print(f"ğŸ’ ë°œê²¬ëœ ìš©ì–´: {len(result['extracted_terms'])}ê°œ")
        print(f"ğŸ¤– ì¶”ì²œ ëª¨ë¸: {result['recommended_stt_model']['model']}")
        print(f"â­ ë²ˆì—­ í’ˆì§ˆ: {result['translation_quality']['quality_level']}")
    
    print("\nâœ… ë‹¤êµ­ì–´ ì²˜ë¦¬ ì—”ì§„ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")