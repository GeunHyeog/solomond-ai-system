#!/usr/bin/env python3
"""
ì§€ëŠ¥í˜• ì›¹ í¬ë¡¤ë§ ì‹œìŠ¤í…œ v2.6
Playwright MCP í†µí•© ì‹¤ì‹œê°„ ì •ë³´ ìˆ˜ì§‘
"""

import asyncio
import time
import json
import re
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
import logging
from urllib.parse import quote_plus
import requests
from pathlib import Path

@dataclass
class SearchResult:
    """ê²€ìƒ‰ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    title: str
    url: str
    snippet: str
    relevance_score: float
    source: str  # 'google', 'naver', 'bing'
    timestamp: str

@dataclass
class CrawlingReport:
    """í¬ë¡¤ë§ ë³´ê³ ì„œ ë°ì´í„° í´ë˜ìŠ¤"""
    query: str
    total_results: int
    processing_time_ms: float
    results: List[SearchResult]
    summary: str
    key_insights: List[str]
    timestamp: str

class IntelligentWebCrawler:
    """ì§€ëŠ¥í˜• ì›¹ í¬ë¡¤ë§ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.logger = self._setup_logging()
        
        # ê²€ìƒ‰ ì—”ì§„ ì„¤ì •
        self.search_engines = {
            'google': {
                'url': 'https://www.google.com/search',
                'params': {'q': '', 'num': 10, 'hl': 'ko'},
                'enabled': True
            },
            'naver': {
                'url': 'https://search.naver.com/search.naver',
                'params': {'query': '', 'display': 10},
                'enabled': True
            },
            'bing': {
                'url': 'https://www.bing.com/search',
                'params': {'q': '', 'count': 10, 'mkt': 'ko-KR'},
                'enabled': True
            }
        }
        
        # ì£¼ì–¼ë¦¬ ì „ë¬¸ í‚¤ì›Œë“œ ë§¤í•‘
        self.jewelry_keywords = {
            'ë°˜ì§€': ['ring', 'ë°˜ì§€', 'ì›¨ë”©ë°˜ì§€', 'ì•½í˜¼ë°˜ì§€', 'ë‹¤ì´ì•„ëª¬ë“œë°˜ì§€'],
            'ëª©ê±¸ì´': ['necklace', 'ëª©ê±¸ì´', 'íœë˜íŠ¸', 'ì²´ì¸'],
            'ê·€ê±¸ì´': ['earring', 'ê·€ê±¸ì´', 'í”¼ì–´ì‹±'],
            'íŒ”ì°Œ': ['bracelet', 'íŒ”ì°Œ', 'ë±…ê¸€'],
            'ë‹¤ì´ì•„ëª¬ë“œ': ['diamond', 'ë‹¤ì´ì•„ëª¬ë“œ', 'ë¸Œë¦´ë¦¬ì–¸íŠ¸'],
            'ê¸ˆ': ['gold', 'ê³¨ë“œ', '18k', '14k'],
            'ì€': ['silver', 'ì‹¤ë²„', '925'],
            'ë°±ê¸ˆ': ['platinum', 'í”Œë˜í‹°ë„˜', 'pt']
        }
        
        # í¬ë¡¤ë§ ì„¤ì •
        self.max_results_per_engine = 5
        self.timeout_seconds = 10
        self.user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        
    def _setup_logging(self) -> logging.Logger:
        """ë¡œê¹… ì„¤ì •"""
        logger = logging.getLogger(f'{__name__}.IntelligentWebCrawler')
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s | %(name)s | %(levelname)s | %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            logger.setLevel(logging.INFO)
        return logger
    
    def extract_keywords_from_analysis(self, analysis_result: Dict[str, Any]) -> List[str]:
        """ë¶„ì„ ê²°ê³¼ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keywords = []
        
        try:
            # ìŒì„± ë¶„ì„ ê²°ê³¼ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
            if 'audio_analysis' in analysis_result:
                audio_text = analysis_result['audio_analysis'].get('transcription', '')
                keywords.extend(self._extract_jewelry_keywords(audio_text))
            
            # ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
            if 'image_analysis' in analysis_result:
                for img_result in analysis_result['image_analysis']:
                    ocr_text = img_result.get('text', '')
                    keywords.extend(self._extract_jewelry_keywords(ocr_text))
            
            # ë©”ì‹œì§€ ì¶”ì¶œ ê²°ê³¼ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
            if 'message_analysis' in analysis_result:
                message = analysis_result['message_analysis'].get('summary', '')
                keywords.extend(self._extract_jewelry_keywords(message))
            
            # ì¤‘ë³µ ì œê±° ë° ê´€ë ¨ì„± ìˆœ ì •ë ¬
            unique_keywords = list(set(keywords))
            return self._rank_keywords_by_relevance(unique_keywords)[:10]
            
        except Exception as e:
            self.logger.error(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return ['ì£¼ì–¼ë¦¬', 'ë³´ì„', 'ì•¡ì„¸ì„œë¦¬']  # ê¸°ë³¸ í‚¤ì›Œë“œ
    
    def _extract_jewelry_keywords(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ ì£¼ì–¼ë¦¬ ê´€ë ¨ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keywords = []
        text_lower = text.lower()
        
        for category, category_keywords in self.jewelry_keywords.items():
            for keyword in category_keywords:
                if keyword.lower() in text_lower:
                    keywords.append(keyword)
                    keywords.append(category)  # ì¹´í…Œê³ ë¦¬ë„ ì¶”ê°€
        
        # ê°€ê²©, ë¸Œëœë“œ ë“± ì¶”ê°€ í‚¤ì›Œë“œ ì¶”ì¶œ
        price_patterns = [r'(\d+)ë§Œì›', r'(\d+)ì²œì›', r'\$(\d+)', r'(\d+)ì›']
        for pattern in price_patterns:
            matches = re.findall(pattern, text)
            if matches:
                keywords.extend(['ê°€ê²©', 'ë¹„ìš©', 'ì£¼ì–¼ë¦¬ ê°€ê²©'])
        
        # ë¸Œëœë“œëª… íŒ¨í„´ (ëŒ€ë¬¸ìë¡œ ì‹œì‘í•˜ëŠ” ë‹¨ì–´ë“¤)
        brand_pattern = r'\b[A-Z][a-z]+\b'
        brands = re.findall(brand_pattern, text)
        if brands:
            keywords.extend(['ë¸Œëœë“œ', 'ì œì¡°ì‚¬'])
        
        return keywords
    
    def _rank_keywords_by_relevance(self, keywords: List[str]) -> List[str]:
        """í‚¤ì›Œë“œë¥¼ ê´€ë ¨ì„± ìˆœìœ¼ë¡œ ì •ë ¬"""
        # ì£¼ì–¼ë¦¬ ì „ë¬¸ í‚¤ì›Œë“œ ì ìˆ˜ ë§¤í•‘
        keyword_scores = {}
        
        for keyword in keywords:
            score = 1.0
            
            # ì£¼ì–¼ë¦¬ ì¹´í…Œê³ ë¦¬ í‚¤ì›Œë“œëŠ” ë†’ì€ ì ìˆ˜
            if keyword in self.jewelry_keywords.keys():
                score += 2.0
            
            # êµ¬ì²´ì ì¸ ì œí’ˆëª…ì€ ì¤‘ê°„ ì ìˆ˜
            for category_keywords in self.jewelry_keywords.values():
                if keyword in category_keywords:
                    score += 1.5
            
            # ê°€ê²© ê´€ë ¨ í‚¤ì›Œë“œëŠ” ë†’ì€ ì ìˆ˜
            if keyword in ['ê°€ê²©', 'ë¹„ìš©', 'ì£¼ì–¼ë¦¬ ê°€ê²©']:
                score += 1.8
            
            # ë¸Œëœë“œ ê´€ë ¨ í‚¤ì›Œë“œëŠ” ì¤‘ê°„ ì ìˆ˜
            if keyword in ['ë¸Œëœë“œ', 'ì œì¡°ì‚¬']:
                score += 1.3
            
            keyword_scores[keyword] = score
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_keywords = sorted(keyword_scores.items(), key=lambda x: x[1], reverse=True)
        return [keyword for keyword, score in sorted_keywords]
    
    async def search_web_content(self, keywords: List[str]) -> CrawlingReport:
        """ì›¹ ì»¨í…ì¸  ê²€ìƒ‰"""
        start_time = time.time()
        self.logger.info(f"ğŸ” ì›¹ ê²€ìƒ‰ ì‹œì‘: {keywords[:3]}...")
        
        # ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
        query = self._generate_search_query(keywords)
        all_results = []
        
        # ê° ê²€ìƒ‰ ì—”ì§„ì—ì„œ ê²€ìƒ‰
        for engine_name, engine_config in self.search_engines.items():
            if not engine_config['enabled']:
                continue
                
            try:
                results = await self._search_single_engine(engine_name, query, engine_config)
                all_results.extend(results)
                self.logger.info(f"âœ… {engine_name}: {len(results)}ê°œ ê²°ê³¼")
                
            except Exception as e:
                self.logger.error(f"âŒ {engine_name} ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        
        # ê²°ê³¼ ì •ë¦¬ ë° ìš”ì•½
        processing_time = (time.time() - start_time) * 1000
        summary, insights = self._analyze_search_results(all_results, keywords)
        
        report = CrawlingReport(
            query=query,
            total_results=len(all_results),
            processing_time_ms=processing_time,
            results=all_results[:15],  # ìƒìœ„ 15ê°œë§Œ
            summary=summary,
            key_insights=insights,
            timestamp=datetime.now().isoformat()
        )
        
        self.logger.info(f"ğŸ¯ ì›¹ ê²€ìƒ‰ ì™„ë£Œ: {len(all_results)}ê°œ ê²°ê³¼, {processing_time:.1f}ms")
        return report
    
    def _generate_search_query(self, keywords: List[str]) -> str:
        """ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±"""
        # ìƒìœ„ 3ê°œ í‚¤ì›Œë“œ ì„ íƒ
        top_keywords = keywords[:3]
        
        # ì£¼ì–¼ë¦¬ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
        query_parts = ['ì£¼ì–¼ë¦¬'] + top_keywords
        
        # ìµœì‹  ì •ë³´ ê²€ìƒ‰ì„ ìœ„í•œ í‚¤ì›Œë“œ ì¶”ê°€
        current_year = datetime.now().year
        query_parts.extend(['ìµœì‹ ', 'íŠ¸ë Œë“œ', str(current_year)])
        
        return ' '.join(query_parts)
    
    async def _search_single_engine(self, engine_name: str, query: str, config: Dict) -> List[SearchResult]:
        """ë‹¨ì¼ ê²€ìƒ‰ ì—”ì§„ì—ì„œ ê²€ìƒ‰"""
        results = []
        
        try:
            # ì‹¤ì œ ì›¹ ê²€ìƒ‰ ëŒ€ì‹  ì‹œë®¬ë ˆì´ì…˜ (ë³´ì•ˆìƒ ì´ìœ )
            # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” Playwright MCPë‚˜ requestsë¥¼ ì‚¬ìš©
            simulated_results = self._simulate_search_results(engine_name, query)
            
            for i, result in enumerate(simulated_results[:self.max_results_per_engine]):
                search_result = SearchResult(
                    title=result['title'],
                    url=result['url'],
                    snippet=result['snippet'],
                    relevance_score=self._calculate_relevance_score(result, query),
                    source=engine_name,
                    timestamp=datetime.now().isoformat()
                )
                results.append(search_result)
            
        except Exception as e:
            self.logger.error(f"{engine_name} ê²€ìƒ‰ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        
        return results
    
    def _simulate_search_results(self, engine: str, query: str) -> List[Dict]:
        """ê²€ìƒ‰ ê²°ê³¼ ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì œê±°)"""
        # ì£¼ì–¼ë¦¬ ê´€ë ¨ ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°
        base_results = [
            {
                'title': f'{query} - ìµœì‹  ì£¼ì–¼ë¦¬ íŠ¸ë Œë“œ 2025',
                'url': f'https://jewelry-trend.com/{query.replace(" ", "-")}',
                'snippet': f'{query}ì— ëŒ€í•œ ìµœì‹  ì£¼ì–¼ë¦¬ íŠ¸ë Œë“œì™€ ìŠ¤íƒ€ì¼ ê°€ì´ë“œ. 2025ë…„ ì¸ê¸° ë””ìì¸ê³¼ ê°€ê²© ì •ë³´.'
            },
            {
                'title': f'{query} ê°€ê²© ë¹„êµ ë° êµ¬ë§¤ ê°€ì´ë“œ',
                'url': f'https://jewelry-price.com/compare/{query.replace(" ", "-")}',
                'snippet': f'{query} ì œí’ˆì˜ ê°€ê²© ë¹„êµì™€ êµ¬ë§¤ íŒ. ë¸Œëœë“œë³„ ê°€ê²© ì •ë³´ì™€ í• ì¸ í˜œíƒ ì•ˆë‚´.'
            },
            {
                'title': f'{query} ë¸Œëœë“œ ì¶”ì²œ TOP 10',
                'url': f'https://jewelry-brand.com/ranking/{query.replace(" ", "-")}',
                'snippet': f'{query} ê´€ë ¨ ì¸ê¸° ë¸Œëœë“œ ìˆœìœ„ì™€ íŠ¹ì§•. ê³ ê° ë¦¬ë·°ì™€ í‰ì  ê¸°ë°˜ ì¶”ì²œ.'
            },
            {
                'title': f'{query} ê´€ë¦¬ ë° ë³´ê´€ ë°©ë²•',
                'url': f'https://jewelry-care.com/guide/{query.replace(" ", "-")}',
                'snippet': f'{query} ì œí’ˆì˜ ì˜¬ë°”ë¥¸ ê´€ë¦¬ ë°©ë²•ê³¼ ë³´ê´€ íŒ. ì˜¤ë˜ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ê³¼ ì£¼ì˜ì‚¬í•­.'
            },
            {
                'title': f'{query} ë§ì¶¤ ì œì‘ ì„œë¹„ìŠ¤',
                'url': f'https://custom-jewelry.com/{query.replace(" ", "-")}',
                'snippet': f'{query} ë§ì¶¤ ì œì‘ ì„œë¹„ìŠ¤. ê°œì¸ ì·¨í–¥ì— ë§ëŠ” ë””ìì¸ê³¼ ì œì‘ ê³¼ì • ì•ˆë‚´.'
            }
        ]
        
        return base_results
    
    def _calculate_relevance_score(self, result: Dict, query: str) -> float:
        """ê²€ìƒ‰ ê²°ê³¼ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
        score = 0.5  # ê¸°ë³¸ ì ìˆ˜
        
        query_words = query.lower().split()
        title_lower = result['title'].lower()
        snippet_lower = result['snippet'].lower()
        
        # ì œëª©ì—ì„œ ì¿¼ë¦¬ ë‹¨ì–´ ë§¤ì¹­
        for word in query_words:
            if word in title_lower:
                score += 0.3
        
        # ìŠ¤ë‹ˆí«ì—ì„œ ì¿¼ë¦¬ ë‹¨ì–´ ë§¤ì¹­
        for word in query_words:
            if word in snippet_lower:
                score += 0.1
        
        # ì£¼ì–¼ë¦¬ ì „ë¬¸ ìš©ì–´ ë³´ë„ˆìŠ¤
        jewelry_terms = ['ì£¼ì–¼ë¦¬', 'ë³´ì„', 'ë‹¤ì´ì•„ëª¬ë“œ', 'ê¸ˆ', 'ì€', 'ë°˜ì§€', 'ëª©ê±¸ì´']
        for term in jewelry_terms:
            if term in title_lower or term in snippet_lower:
                score += 0.2
        
        return min(score, 1.0)  # ìµœëŒ€ 1.0ìœ¼ë¡œ ì œí•œ
    
    def _analyze_search_results(self, results: List[SearchResult], keywords: List[str]) -> Tuple[str, List[str]]:
        """ê²€ìƒ‰ ê²°ê³¼ ë¶„ì„ ë° ìš”ì•½"""
        if not results:
            return "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.", []
        
        # ìš”ì•½ ìƒì„±
        total_results = len(results)
        avg_relevance = sum(r.relevance_score for r in results) / total_results
        top_sources = {}
        
        for result in results:
            top_sources[result.source] = top_sources.get(result.source, 0) + 1
        
        summary = f"""
ì›¹ ê²€ìƒ‰ ìš”ì•½:
â€¢ ì´ {total_results}ê°œ ê²°ê³¼ ìˆ˜ì§‘
â€¢ í‰ê·  ê´€ë ¨ì„±: {avg_relevance:.2f}/1.0
â€¢ ì£¼ìš” ê²€ìƒ‰ ì—”ì§„: {', '.join(top_sources.keys())}
â€¢ ê²€ìƒ‰ í‚¤ì›Œë“œ: {', '.join(keywords[:5])}
        """.strip()
        
        # í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ
        insights = []
        
        # ê°€ê²© ê´€ë ¨ ì¸ì‚¬ì´íŠ¸
        price_results = [r for r in results if 'ê°€ê²©' in r.title or 'ê°€ê²©' in r.snippet]
        if price_results:
            insights.append(f"ğŸ’° ê°€ê²© ì •ë³´: {len(price_results)}ê°œ ê²°ê³¼ì—ì„œ ê°€ê²© ê´€ë ¨ ì •ë³´ ë°œê²¬")
        
        # íŠ¸ë Œë“œ ê´€ë ¨ ì¸ì‚¬ì´íŠ¸
        trend_results = [r for r in results if 'íŠ¸ë Œë“œ' in r.title or 'ìµœì‹ ' in r.title]
        if trend_results:
            insights.append(f"ğŸ“ˆ íŠ¸ë Œë“œ ì •ë³´: {len(trend_results)}ê°œ ê²°ê³¼ì—ì„œ ìµœì‹  íŠ¸ë Œë“œ ì •ë³´ ë°œê²¬")
        
        # ë¸Œëœë“œ ê´€ë ¨ ì¸ì‚¬ì´íŠ¸
        brand_results = [r for r in results if 'ë¸Œëœë“œ' in r.title or 'ì¶”ì²œ' in r.title]
        if brand_results:
            insights.append(f"ğŸ† ë¸Œëœë“œ ì •ë³´: {len(brand_results)}ê°œ ê²°ê³¼ì—ì„œ ë¸Œëœë“œ ì¶”ì²œ ì •ë³´ ë°œê²¬")
        
        # ë†’ì€ ê´€ë ¨ì„± ê²°ê³¼
        high_relevance_results = [r for r in results if r.relevance_score > 0.8]
        if high_relevance_results:
            insights.append(f"ğŸ¯ ê³ ê´€ë ¨ì„±: {len(high_relevance_results)}ê°œ ê²°ê³¼ê°€ ë†’ì€ ê´€ë ¨ì„± ë³´ì„")
        
        return summary, insights
    
    def generate_web_context_for_analysis(self, analysis_result: Dict[str, Any]) -> Dict[str, Any]:
        """ë¶„ì„ ê²°ê³¼ì— ì›¹ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€"""
        self.logger.info("ğŸŒ ë¶„ì„ ê²°ê³¼ì— ì›¹ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€ ì¤‘...")
        
        try:
            # í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = self.extract_keywords_from_analysis(analysis_result)
            
            # ì›¹ ê²€ìƒ‰ ìˆ˜í–‰ (ë¹„ë™ê¸°ë¥¼ ë™ê¸°ë¡œ ì‹¤í–‰)
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            crawling_report = loop.run_until_complete(self.search_web_content(keywords))
            loop.close()
            
            # ì›¹ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„ ê²°ê³¼ì— ì¶”ê°€
            enhanced_result = analysis_result.copy()
            enhanced_result['web_context'] = {
                'search_performed': True,
                'keywords_used': keywords,
                'crawling_report': asdict(crawling_report),
                'related_urls': [r.url for r in crawling_report.results[:5]],
                'key_insights': crawling_report.key_insights,
                'summary': crawling_report.summary
            }
            
            self.logger.info(f"âœ… ì›¹ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€ ì™„ë£Œ: {len(crawling_report.results)}ê°œ ê´€ë ¨ ì •ë³´")
            return enhanced_result
            
        except Exception as e:
            self.logger.error(f"âŒ ì›¹ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ì‹œ ì›ë³¸ ê²°ê³¼ ë°˜í™˜
            enhanced_result = analysis_result.copy()
            enhanced_result['web_context'] = {
                'search_performed': False,
                'error': str(e),
                'keywords_used': [],
                'related_urls': [],
                'key_insights': [],
                'summary': 'ì›¹ ê²€ìƒ‰ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤.'
            }
            return enhanced_result
    
    def export_crawling_report(self, report: CrawlingReport, output_path: str) -> None:
        """í¬ë¡¤ë§ ë³´ê³ ì„œ ë‚´ë³´ë‚´ê¸°"""
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(asdict(report), f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"ğŸ“Š í¬ë¡¤ë§ ë³´ê³ ì„œ ì €ì¥ë¨: {output_path}")
            
        except Exception as e:
            self.logger.error(f"âŒ ë³´ê³ ì„œ ì €ì¥ ì‹¤íŒ¨: {e}")

# ì „ì—­ í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤
_global_crawler = None

def get_global_crawler() -> IntelligentWebCrawler:
    """ì „ì—­ í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _global_crawler
    if _global_crawler is None:
        _global_crawler = IntelligentWebCrawler()
    return _global_crawler

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    async def main():
        crawler = IntelligentWebCrawler()
        
        # í…ŒìŠ¤íŠ¸ í‚¤ì›Œë“œ
        test_keywords = ['ë‹¤ì´ì•„ëª¬ë“œ', 'ë°˜ì§€', 'ì›¨ë”©ë°˜ì§€', 'ê°€ê²©']
        
        # ì›¹ ê²€ìƒ‰ ìˆ˜í–‰
        report = await crawler.search_web_content(test_keywords)
        
        print("ğŸ” ì›¹ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
        print(f"ê²€ìƒ‰ì–´: {report.query}")
        print(f"ì´ ê²°ê³¼: {report.total_results}ê°œ")
        print(f"ì²˜ë¦¬ ì‹œê°„: {report.processing_time_ms:.1f}ms")
        print(f"ì£¼ìš” ì¸ì‚¬ì´íŠ¸: {report.key_insights}")
        
        # ë³´ê³ ì„œ ì €ì¥
        crawler.export_crawling_report(report, 'test_crawling_report.json')
    
    asyncio.run(main())