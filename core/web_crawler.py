"""
ì†”ë¡œëª¬ë“œ AI ì‹œìŠ¤í…œ - ì›¹ í¬ë¡¤ë§ ì—”ì§„
ìœ íŠœë¸Œ, ì›¹ì‚¬ì´íŠ¸, ë‰´ìŠ¤ ë“±ì—ì„œ ì£¼ì–¼ë¦¬ ê´€ë ¨ ì •ë³´ ìë™ ìˆ˜ì§‘ ëª¨ë“ˆ
"""

import os
import re
import asyncio
import aiohttp
import tempfile
from pathlib import Path
from typing import Dict, List, Optional, Union, Tuple, Any
from urllib.parse import urljoin, urlparse, parse_qs
import logging
import json
from datetime import datetime, timedelta
import hashlib

# ì›¹ í¬ë¡¤ë§ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from bs4 import BeautifulSoup
    BS4_AVAILABLE = True
except ImportError:
    BS4_AVAILABLE = False

try:
    import requests
    REQUESTS_AVAILABLE = True
except ImportError:
    REQUESTS_AVAILABLE = False

# ìœ íŠœë¸Œ ë‹¤ìš´ë¡œë”
try:
    import yt_dlp
    YTDLP_AVAILABLE = True
except ImportError:
    YTDLP_AVAILABLE = False

# RSS í”¼ë“œ ì²˜ë¦¬
try:
    import feedparser
    FEEDPARSER_AVAILABLE = True
except ImportError:
    FEEDPARSER_AVAILABLE = False

class WebCrawler:
    """ì›¹ í¬ë¡¤ë§ ë° ì½˜í…ì¸  ìˆ˜ì§‘ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.session = None
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        # ì£¼ì–¼ë¦¬ ê´€ë ¨ í‚¤ì›Œë“œ
        self.jewelry_keywords = [
            'jewelry', 'diamond', 'gold', 'silver', 'platinum', 'gem', 'jewel',
            'ì£¼ì–¼ë¦¬', 'ë‹¤ì´ì•„ëª¬ë“œ', 'ë³´ì„', 'ê¸ˆ', 'ì€', 'ë°˜ì§€', 'ëª©ê±¸ì´', 'ê·€ê±¸ì´',
            'GIA', 'AGS', '4C', 'carat', 'clarity', 'color', 'cut',
            'ìºëŸ¿', 'ì»·', 'ì»¬ëŸ¬', 'í´ë˜ë¦¬í‹°', 'ê°ì •ì„œ'
        ]
        
        # ì£¼ìš” ì£¼ì–¼ë¦¬ ì‚¬ì´íŠ¸ë“¤
        self.jewelry_news_sources = [
            {
                "name": "JCK Magazine",
                "url": "https://www.jckonline.com",
                "rss": "https://www.jckonline.com/rss.xml"
            },
            {
                "name": "National Jeweler",
                "url": "https://www.nationaljeweler.com",
                "rss": "https://www.nationaljeweler.com/rss.xml"
            },
            {
                "name": "Rapaport",
                "url": "https://www.rapaport.com",
                "rss": "https://www.rapaport.com/rss.xml"
            },
            {
                "name": "Korean Jewelry News",
                "url": "https://www.jewelry.co.kr",
                "rss": None
            }
        ]
        
        # ìºì‹œ ë””ë ‰í† ë¦¬
        self.cache_dir = Path("temp_cache")
        self.cache_dir.mkdir(exist_ok=True)
        
        logging.info("ì›¹ í¬ë¡¤ë§ ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
        logging.info(f"ì§€ì› ë¼ì´ë¸ŒëŸ¬ë¦¬: BS4={BS4_AVAILABLE}, Requests={REQUESTS_AVAILABLE}, yt-dlp={YTDLP_AVAILABLE}")
    
    async def __aenter__(self):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì‹œì‘"""
        self.session = aiohttp.ClientSession(headers=self.headers)
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        if self.session:
            await self.session.close()
    
    def get_supported_sources(self) -> Dict[str, List[str]]:
        """ì§€ì›í•˜ëŠ” ì†ŒìŠ¤ íƒ€ì… ë°˜í™˜"""
        return {
            "video": ["youtube", "vimeo"] if YTDLP_AVAILABLE else [],
            "websites": ["general", "jewelry_specific", "news"],
            "feeds": ["rss", "atom"] if FEEDPARSER_AVAILABLE else [],
            "social": ["twitter_public", "instagram_public"]
        }
    
    async def process_url(self, 
                         url: str, 
                         content_type: str = "auto",
                         extract_video: bool = False) -> Dict:
        """
        URL ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜
        
        Args:
            url: ì²˜ë¦¬í•  URL
            content_type: ì½˜í…ì¸  íƒ€ì… ("auto", "video", "article", "feed")
            extract_video: ë¹„ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ì—¬ë¶€
            
        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        try:
            print(f"ğŸŒ URL ì²˜ë¦¬ ì‹œì‘: {url}")
            
            # URL ìœ íš¨ì„± ê²€ì‚¬
            parsed_url = urlparse(url)
            if not parsed_url.scheme or not parsed_url.netloc:
                return {
                    "success": False,
                    "error": "ìœ íš¨í•˜ì§€ ì•Šì€ URL í˜•ì‹",
                    "url": url
                }
            
            # ì½˜í…ì¸  íƒ€ì… ìë™ ê°ì§€
            if content_type == "auto":
                content_type = self._detect_content_type(url)
            
            # íƒ€ì…ë³„ ì²˜ë¦¬
            if content_type == "video":
                return await self.process_video_url(url, extract_video)
            elif content_type == "feed":
                return await self.process_rss_feed(url)
            else:
                return await self.process_web_page(url)
                
        except Exception as e:
            logging.error(f"URL ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return {
                "success": False,
                "error": str(e),
                "url": url
            }
    
    def _detect_content_type(self, url: str) -> str:
        """URLì—ì„œ ì½˜í…ì¸  íƒ€ì… ìë™ ê°ì§€"""
        url_lower = url.lower()
        
        # ë¹„ë””ì˜¤ í”Œë«í¼
        if 'youtube.com' in url_lower or 'youtu.be' in url_lower:
            return "video"
        elif 'vimeo.com' in url_lower:
            return "video"
        
        # RSS/Atom í”¼ë“œ
        if any(feed_word in url_lower for feed_word in ['rss', 'feed', 'atom', '.xml']):
            return "feed"
        
        # ê¸°ë³¸ì ìœ¼ë¡œ ì›¹í˜ì´ì§€
        return "webpage"
    
    async def process_video_url(self, 
                               url: str, 
                               extract_video: bool = False) -> Dict:
        """
        ë¹„ë””ì˜¤ URL ì²˜ë¦¬ (ìœ íŠœë¸Œ ë“±)
        
        Args:
            url: ë¹„ë””ì˜¤ URL
            extract_video: ë¹„ë””ì˜¤ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì—¬ë¶€
            
        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if not YTDLP_AVAILABLE:
            return {
                "success": False,
                "error": "ë¹„ë””ì˜¤ ì²˜ë¦¬ë¥¼ ìœ„í•´ yt-dlpê°€ í•„ìš”í•©ë‹ˆë‹¤. pip install yt-dlpë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.",
                "url": url
            }
        
        try:
            print(f"ğŸ¥ ë¹„ë””ì˜¤ ì²˜ë¦¬ ì‹œì‘: {url}")
            
            # yt-dlp ì˜µì…˜ ì„¤ì •
            ydl_opts = {
                'quiet': True,
                'no_warnings': True,
                'extract_flat': not extract_video,
            }
            
            if extract_video:
                # ìŒì„±ë§Œ ì¶”ì¶œí•˜ëŠ” ê²½ìš°
                ydl_opts.update({
                    'format': 'bestaudio/best',
                    'outtmpl': str(self.cache_dir / '%(title)s.%(ext)s'),
                    'postprocessors': [{
                        'key': 'FFmpegExtractAudio',
                        'preferredcodec': 'wav',
                    }]
                })
            
            # ë¹„ë””ì˜¤ ì •ë³´ ì¶”ì¶œ
            with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                info = ydl.extract_info(url, download=extract_video)
            
            # ê²°ê³¼ êµ¬ì„±
            result = {
                "success": True,
                "url": url,
                "content_type": "video",
                "video_info": {
                    "title": info.get('title', ''),
                    "description": info.get('description', ''),
                    "duration": info.get('duration', 0),
                    "view_count": info.get('view_count', 0),
                    "upload_date": info.get('upload_date', ''),
                    "uploader": info.get('uploader', ''),
                    "channel": info.get('channel', ''),
                },
                "extracted_video": extract_video
            }
            
            # ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ ì •ë³´ ì¶”ê°€
            if extract_video and 'requested_downloads' in info:
                result["downloaded_files"] = info['requested_downloads']
            
            # ì£¼ì–¼ë¦¬ ê´€ë ¨ì„± ë¶„ì„
            text_content = f"{info.get('title', '')} {info.get('description', '')}"
            result["jewelry_analysis"] = self._analyze_jewelry_relevance(text_content)
            
            print(f"âœ… ë¹„ë””ì˜¤ ì²˜ë¦¬ ì™„ë£Œ: {info.get('title', 'Unknown')}")
            return result
            
        except Exception as e:
            logging.error(f"ë¹„ë””ì˜¤ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return {
                "success": False,
                "error": str(e),
                "url": url,
                "content_type": "video"
            }
    
    async def process_web_page(self, url: str) -> Dict:
        """
        ì›¹í˜ì´ì§€ í¬ë¡¤ë§ ë° ì½˜í…ì¸  ì¶”ì¶œ
        
        Args:
            url: ì›¹í˜ì´ì§€ URL
            
        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if not BS4_AVAILABLE:
            return {
                "success": False,
                "error": "ì›¹í˜ì´ì§€ ì²˜ë¦¬ë¥¼ ìœ„í•´ BeautifulSoup4ê°€ í•„ìš”í•©ë‹ˆë‹¤. pip install beautifulsoup4ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.",
                "url": url
            }
        
        try:
            print(f"ğŸ“„ ì›¹í˜ì´ì§€ ì²˜ë¦¬ ì‹œì‘: {url}")
            
            # ì„¸ì…˜ì´ ì—†ìœ¼ë©´ ìƒì„±
            if not self.session:
                self.session = aiohttp.ClientSession(headers=self.headers)
            
            # ì›¹í˜ì´ì§€ ë‹¤ìš´ë¡œë“œ
            async with self.session.get(url, timeout=30) as response:
                if response.status != 200:
                    return {
                        "success": False,
                        "error": f"HTTP {response.status}: {response.reason}",
                        "url": url
                    }
                
                html_content = await response.text()
                charset = response.charset or 'utf-8'
            
            # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            metadata = self._extract_metadata(soup, url)
            
            # ë³¸ë¬¸ ì½˜í…ì¸  ì¶”ì¶œ
            content = self._extract_main_content(soup)
            
            # ë§í¬ ì¶”ì¶œ
            links = self._extract_links(soup, url)
            
            # ì´ë¯¸ì§€ ì¶”ì¶œ
            images = self._extract_images(soup, url)
            
            # ì£¼ì–¼ë¦¬ ê´€ë ¨ì„± ë¶„ì„
            full_text = f"{metadata.get('title', '')} {metadata.get('description', '')} {content}"
            jewelry_analysis = self._analyze_jewelry_relevance(full_text)
            
            result = {
                "success": True,
                "url": url,
                "content_type": "webpage",
                "metadata": metadata,
                "content": content,
                "links": links[:20],  # ìµœëŒ€ 20ê°œ ë§í¬
                "images": images[:10],  # ìµœëŒ€ 10ê°œ ì´ë¯¸ì§€
                "content_length": len(content),
                "jewelry_analysis": jewelry_analysis,
                "charset": charset,
                "processing_time": datetime.now().isoformat()
            }
            
            print(f"âœ… ì›¹í˜ì´ì§€ ì²˜ë¦¬ ì™„ë£Œ: {len(content)}ì ì¶”ì¶œ")
            return result
            
        except asyncio.TimeoutError:
            return {
                "success": False,
                "error": "ì›¹í˜ì´ì§€ ë¡œë”© ì‹œê°„ ì´ˆê³¼ (30ì´ˆ)",
                "url": url
            }
        except Exception as e:
            logging.error(f"ì›¹í˜ì´ì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return {
                "success": False,
                "error": str(e),
                "url": url,
                "content_type": "webpage"
            }
    
    def _extract_metadata(self, soup: BeautifulSoup, url: str) -> Dict:
        """ì›¹í˜ì´ì§€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        metadata = {"url": url}
        
        # íƒ€ì´í‹€
        title_tag = soup.find('title')
        if title_tag:
            metadata["title"] = title_tag.get_text().strip()
        
        # ë©”íƒ€ íƒœê·¸ë“¤
        meta_tags = {
            "description": ["description", "og:description"],
            "keywords": ["keywords"],
            "author": ["author", "article:author"],
            "published_time": ["article:published_time", "published_time"],
            "image": ["og:image", "twitter:image"],
            "site_name": ["og:site_name"],
            "article_section": ["article:section"]
        }
        
        for key, meta_names in meta_tags.items():
            for meta_name in meta_names:
                meta_tag = soup.find('meta', attrs={
                    lambda attr: attr and attr.lower() in ['name', 'property', 'itemprop'] and 
                    soup.find('meta', {attr: meta_name})
                })
                if not meta_tag:
                    meta_tag = soup.find('meta', {'name': meta_name}) or \
                               soup.find('meta', {'property': meta_name})
                
                if meta_tag and meta_tag.get('content'):
                    metadata[key] = meta_tag['content'].strip()
                    break
        
        # h1 íƒœê·¸ (ì œëª© ë³´ì™„)
        if "title" not in metadata:
            h1_tag = soup.find('h1')
            if h1_tag:
                metadata["title"] = h1_tag.get_text().strip()
        
        return metadata
    
    def _extract_main_content(self, soup: BeautifulSoup) -> str:
        """ì›¹í˜ì´ì§€ ë³¸ë¬¸ ì½˜í…ì¸  ì¶”ì¶œ"""
        # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
        for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'iframe']):
            tag.decompose()
        
        # ë³¸ë¬¸ í›„ë³´ íƒœê·¸ë“¤
        main_candidates = [
            soup.find('main'),
            soup.find('article'),
            soup.find('div', class_=re.compile(r'content|main|post|article', re.I)),
            soup.find('div', id=re.compile(r'content|main|post|article', re.I)),
        ]
        
        # ê°€ì¥ ì í•©í•œ ë³¸ë¬¸ ì„ íƒ
        main_content = None
        for candidate in main_candidates:
            if candidate:
                main_content = candidate
                break
        
        # í›„ë³´ê°€ ì—†ìœ¼ë©´ body ì „ì²´ ì‚¬ìš©
        if not main_content:
            main_content = soup.find('body') or soup
        
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì •ë¦¬
        text = main_content.get_text(separator=' ', strip=True)
        
        # ê³µë°± ì •ë¦¬
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'\n\s*\n', '\n', text)
        
        return text.strip()
    
    def _extract_links(self, soup: BeautifulSoup, base_url: str) -> List[Dict]:
        """ë§í¬ ì¶”ì¶œ"""
        links = []
        
        for a_tag in soup.find_all('a', href=True):
            href = a_tag['href'].strip()
            if not href or href.startswith('#'):
                continue
            
            # ì ˆëŒ€ URLë¡œ ë³€í™˜
            full_url = urljoin(base_url, href)
            
            link_text = a_tag.get_text(strip=True)
            if link_text:
                links.append({
                    "url": full_url,
                    "text": link_text,
                    "title": a_tag.get('title', '')
                })
        
        return links
    
    def _extract_images(self, soup: BeautifulSoup, base_url: str) -> List[Dict]:
        """ì´ë¯¸ì§€ ë§í¬ ì¶”ì¶œ"""
        images = []
        
        for img_tag in soup.find_all('img', src=True):
            src = img_tag['src'].strip()
            if not src:
                continue
            
            # ì ˆëŒ€ URLë¡œ ë³€í™˜
            full_url = urljoin(base_url, src)
            
            images.append({
                "url": full_url,
                "alt": img_tag.get('alt', ''),
                "title": img_tag.get('title', ''),
                "width": img_tag.get('width'),
                "height": img_tag.get('height')
            })
        
        return images
    
    async def process_rss_feed(self, feed_url: str) -> Dict:
        """
        RSS/Atom í”¼ë“œ ì²˜ë¦¬
        
        Args:
            feed_url: RSS í”¼ë“œ URL
            
        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if not FEEDPARSER_AVAILABLE:
            return {
                "success": False,
                "error": "RSS í”¼ë“œ ì²˜ë¦¬ë¥¼ ìœ„í•´ feedparserê°€ í•„ìš”í•©ë‹ˆë‹¤. pip install feedparserë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.",
                "url": feed_url
            }
        
        try:
            print(f"ğŸ“¡ RSS í”¼ë“œ ì²˜ë¦¬ ì‹œì‘: {feed_url}")
            
            # í”¼ë“œ íŒŒì‹±
            feed = feedparser.parse(feed_url)
            
            if feed.bozo:
                return {
                    "success": False,
                    "error": f"ì˜ëª»ëœ í”¼ë“œ í˜•ì‹: {feed.bozo_exception}",
                    "url": feed_url
                }
            
            # í”¼ë“œ ì •ë³´
            feed_info = {
                "title": feed.feed.get('title', ''),
                "description": feed.feed.get('description', ''),
                "link": feed.feed.get('link', ''),
                "language": feed.feed.get('language', ''),
                "updated": feed.feed.get('updated', ''),
                "total_entries": len(feed.entries)
            }
            
            # ì—”íŠ¸ë¦¬ ì²˜ë¦¬
            entries = []
            jewelry_entries = []
            
            for entry in feed.entries[:20]:  # ìµœëŒ€ 20ê°œ í•­ëª©
                entry_data = {
                    "title": entry.get('title', ''),
                    "link": entry.get('link', ''),
                    "description": entry.get('description', ''),
                    "published": entry.get('published', ''),
                    "author": entry.get('author', ''),
                    "tags": [tag.term for tag in entry.get('tags', [])]
                }
                
                entries.append(entry_data)
                
                # ì£¼ì–¼ë¦¬ ê´€ë ¨ ì—”íŠ¸ë¦¬ í•„í„°ë§
                content = f"{entry_data['title']} {entry_data['description']}"
                if self._is_jewelry_related(content):
                    jewelry_entries.append(entry_data)
            
            result = {
                "success": True,
                "url": feed_url,
                "content_type": "rss_feed",
                "feed_info": feed_info,
                "entries": entries,
                "jewelry_entries": jewelry_entries,
                "jewelry_count": len(jewelry_entries),
                "total_count": len(entries)
            }
            
            print(f"âœ… RSS í”¼ë“œ ì²˜ë¦¬ ì™„ë£Œ: {len(entries)}ê°œ í•­ëª©, {len(jewelry_entries)}ê°œ ì£¼ì–¼ë¦¬ ê´€ë ¨")
            return result
            
        except Exception as e:
            logging.error(f"RSS í”¼ë“œ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return {
                "success": False,
                "error": str(e),
                "url": feed_url,
                "content_type": "rss_feed"
            }
    
    async def crawl_jewelry_news(self, 
                                max_sources: int = 5,
                                max_articles_per_source: int = 10) -> Dict:
        """
        ì£¼ì–¼ë¦¬ ê´€ë ¨ ë‰´ìŠ¤ ìë™ í¬ë¡¤ë§
        
        Args:
            max_sources: ìµœëŒ€ ì†ŒìŠ¤ ìˆ˜
            max_articles_per_source: ì†ŒìŠ¤ë‹¹ ìµœëŒ€ ê¸°ì‚¬ ìˆ˜
            
        Returns:
            í¬ë¡¤ë§ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        try:
            print("ğŸ“° ì£¼ì–¼ë¦¬ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘")
            
            all_articles = []
            source_results = []
            
            for source in self.jewelry_news_sources[:max_sources]:
                print(f"ğŸ” ì†ŒìŠ¤ ì²˜ë¦¬ ì¤‘: {source['name']}")
                
                try:
                    # RSS í”¼ë“œê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
                    if source.get('rss'):
                        result = await self.process_rss_feed(source['rss'])
                        if result['success']:
                            articles = result.get('jewelry_entries', [])[:max_articles_per_source]
                            all_articles.extend(articles)
                            source_results.append({
                                "source": source['name'],
                                "method": "rss",
                                "articles_found": len(articles),
                                "success": True
                            })
                        else:
                            source_results.append({
                                "source": source['name'],
                                "method": "rss",
                                "success": False,
                                "error": result.get('error', 'Unknown error')
                            })
                    else:
                        # ì¼ë°˜ ì›¹í˜ì´ì§€ í¬ë¡¤ë§
                        result = await self.process_web_page(source['url'])
                        if result['success']:
                            # ê°„ë‹¨í•œ ê¸°ì‚¬ ì¶”ì¶œ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ íŒŒì‹± í•„ìš”)
                            source_results.append({
                                "source": source['name'],
                                "method": "webpage",
                                "content_length": result.get('content_length', 0),
                                "success": True
                            })
                        else:
                            source_results.append({
                                "source": source['name'],
                                "method": "webpage",
                                "success": False,
                                "error": result.get('error', 'Unknown error')
                            })
                
                except Exception as e:
                    source_results.append({
                        "source": source['name'],
                        "success": False,
                        "error": str(e)
                    })
                
                # ì†ŒìŠ¤ ê°„ ë”œë ˆì´
                await asyncio.sleep(1)
            
            # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
            unique_articles = self._deduplicate_articles(all_articles)
            
            result = {
                "success": True,
                "content_type": "jewelry_news_crawl",
                "articles": unique_articles,
                "total_articles": len(unique_articles),
                "sources_processed": len(source_results),
                "source_results": source_results,
                "crawl_time": datetime.now().isoformat()
            }
            
            print(f"âœ… ë‰´ìŠ¤ í¬ë¡¤ë§ ì™„ë£Œ: {len(unique_articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
            return result
            
        except Exception as e:
            logging.error(f"ë‰´ìŠ¤ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            return {
                "success": False,
                "error": str(e),
                "content_type": "jewelry_news_crawl"
            }
    
    def _analyze_jewelry_relevance(self, text: str) -> Dict:
        """í…ìŠ¤íŠ¸ì˜ ì£¼ì–¼ë¦¬ ê´€ë ¨ì„± ë¶„ì„"""
        if not text:
            return {"relevance_score": 0.0, "found_keywords": [], "category": "irrelevant"}
        
        text_lower = text.lower()
        found_keywords = []
        
        # í‚¤ì›Œë“œ ê²€ìƒ‰
        for keyword in self.jewelry_keywords:
            if keyword.lower() in text_lower:
                found_keywords.append(keyword)
        
        # ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
        relevance_score = len(found_keywords) / len(self.jewelry_keywords) * 100
        
        # ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
        if relevance_score >= 20:
            category = "highly_relevant"
        elif relevance_score >= 10:
            category = "moderately_relevant"
        elif relevance_score >= 5:
            category = "slightly_relevant"
        else:
            category = "irrelevant"
        
        return {
            "relevance_score": round(relevance_score, 2),
            "found_keywords": found_keywords,
            "keyword_count": len(found_keywords),
            "category": category,
            "is_jewelry_related": relevance_score >= 5
        }
    
    def _is_jewelry_related(self, text: str, threshold: float = 5.0) -> bool:
        """í…ìŠ¤íŠ¸ê°€ ì£¼ì–¼ë¦¬ ê´€ë ¨ì¸ì§€ íŒë‹¨"""
        analysis = self._analyze_jewelry_relevance(text)
        return analysis["relevance_score"] >= threshold
    
    def _deduplicate_articles(self, articles: List[Dict]) -> List[Dict]:
        """ê¸°ì‚¬ ì¤‘ë³µ ì œê±°"""
        seen_titles = set()
        unique_articles = []
        
        for article in articles:
            title = article.get('title', '').strip()
            if title and title not in seen_titles:
                seen_titles.add(title)
                unique_articles.append(article)
        
        # ë°œí–‰ì¼ ê¸°ì¤€ ì •ë ¬ (ìµœì‹ ìˆœ)
        try:
            unique_articles.sort(
                key=lambda x: x.get('published', ''), 
                reverse=True
            )
        except:
            pass  # ì •ë ¬ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ìˆœì„œ ìœ ì§€
        
        return unique_articles
    
    async def search_youtube_jewelry(self, 
                                    query: str = "jewelry making tutorial",
                                    max_results: int = 10) -> Dict:
        """
        ìœ íŠœë¸Œì—ì„œ ì£¼ì–¼ë¦¬ ê´€ë ¨ ì˜ìƒ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            max_results: ìµœëŒ€ ê²°ê³¼ ìˆ˜
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if not YTDLP_AVAILABLE:
            return {
                "success": False,
                "error": "ìœ íŠœë¸Œ ê²€ìƒ‰ì„ ìœ„í•´ yt-dlpê°€ í•„ìš”í•©ë‹ˆë‹¤.",
                "query": query
            }
        
        try:
            print(f"ğŸ” ìœ íŠœë¸Œ ê²€ìƒ‰: {query}")
            
            # ê²€ìƒ‰ URL êµ¬ì„±
            search_url = f"ytsearch{max_results}:{query}"
            
            ydl_opts = {
                'quiet': True,
                'no_warnings': True,
                'extract_flat': True,
            }
            
            with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                search_results = ydl.extract_info(search_url, download=False)
            
            videos = []
            for entry in search_results.get('entries', []):
                if entry:
                    video_info = {
                        "title": entry.get('title', ''),
                        "url": entry.get('url', ''),
                        "id": entry.get('id', ''),
                        "duration": entry.get('duration', 0),
                        "view_count": entry.get('view_count', 0),
                        "uploader": entry.get('uploader', ''),
                        "description": entry.get('description', '')[:500]  # ì²˜ìŒ 500ìë§Œ
                    }
                    videos.append(video_info)
            
            result = {
                "success": True,
                "query": query,
                "content_type": "youtube_search",
                "videos": videos,
                "total_found": len(videos),
                "search_time": datetime.now().isoformat()
            }
            
            print(f"âœ… ìœ íŠœë¸Œ ê²€ìƒ‰ ì™„ë£Œ: {len(videos)}ê°œ ì˜ìƒ ë°œê²¬")
            return result
            
        except Exception as e:
            logging.error(f"ìœ íŠœë¸Œ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return {
                "success": False,
                "error": str(e),
                "query": query,
                "content_type": "youtube_search"
            }

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
_web_crawler_instance = None

def get_web_crawler() -> WebCrawler:
    """ì „ì—­ ì›¹ í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _web_crawler_instance
    if _web_crawler_instance is None:
        _web_crawler_instance = WebCrawler()
    return _web_crawler_instance

# í¸ì˜ í•¨ìˆ˜ë“¤
async def crawl_url(url: str, **kwargs) -> Dict:
    """URL í¬ë¡¤ë§ í¸ì˜ í•¨ìˆ˜"""
    crawler = get_web_crawler()
    async with crawler:
        return await crawler.process_url(url, **kwargs)

async def crawl_jewelry_news(**kwargs) -> Dict:
    """ì£¼ì–¼ë¦¬ ë‰´ìŠ¤ í¬ë¡¤ë§ í¸ì˜ í•¨ìˆ˜"""
    crawler = get_web_crawler()
    async with crawler:
        return await crawler.crawl_jewelry_news(**kwargs)

async def search_youtube_jewelry(**kwargs) -> Dict:
    """ìœ íŠœë¸Œ ì£¼ì–¼ë¦¬ ê²€ìƒ‰ í¸ì˜ í•¨ìˆ˜"""
    crawler = get_web_crawler()
    async with crawler:
        return await crawler.search_youtube_jewelry(**kwargs)

def check_crawler_support() -> Dict:
    """ì›¹ í¬ë¡¤ëŸ¬ ì§€ì› ìƒíƒœ í™•ì¸"""
    crawler = get_web_crawler()
    return {
        "supported_sources": crawler.get_supported_sources(),
        "libraries": {
            "beautifulsoup4": BS4_AVAILABLE,
            "requests": REQUESTS_AVAILABLE,
            "yt-dlp": YTDLP_AVAILABLE,
            "feedparser": FEEDPARSER_AVAILABLE,
            "aiohttp": True  # í•­ìƒ ì‚¬ìš© ê°€ëŠ¥ (aiohttpëŠ” ê¸°ë³¸ ì„¤ì¹˜ë¨)
        },
        "jewelry_keywords": len(crawler.jewelry_keywords),
        "news_sources": len(crawler.jewelry_news_sources)
    }

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    async def test_crawler():
        print("ì›¹ í¬ë¡¤ë§ ì—”ì§„ í…ŒìŠ¤íŠ¸")
        support_info = check_crawler_support()
        print(f"ì§€ì› ìƒíƒœ: {support_info}")
        
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
        result = await crawl_url("https://www.jckonline.com")
        print(f"í…ŒìŠ¤íŠ¸ ê²°ê³¼: {result.get('success', False)}")
    
    asyncio.run(test_crawler())
