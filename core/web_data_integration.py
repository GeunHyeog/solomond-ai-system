#!/usr/bin/env python3
"""
ì›¹ ë°ì´í„° í†µí•© ì—”ì§„
MCP ë¸Œë¼ìš°ì €ì—ì„œ ìˆ˜ì§‘í•œ ì›¹ ë°ì´í„°ë¥¼ ë¶„ì„ ì›Œí¬í”Œë¡œìš°ì— í†µí•©
"""

import json
import time
from datetime import datetime
from typing import Dict, List, Any, Optional
from pathlib import Path
from utils.logger import get_logger

class WebDataIntegration:
    """ì›¹ ë°ì´í„° í†µí•© ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.logger = get_logger(__name__)
        
        # í†µí•© ê·œì¹™ ì •ì˜
        self.integration_rules = {
            "jewelry_search": {
                "priority": "high",
                "merge_strategy": "contextual",
                "data_types": ["product_info", "price_comparison", "market_analysis"]
            },
            "competitive_analysis": {
                "priority": "medium", 
                "merge_strategy": "comparative",
                "data_types": ["brand_analysis", "feature_comparison", "pricing_strategy"]
            },
            "market_research": {
                "priority": "medium",
                "merge_strategy": "aggregative", 
                "data_types": ["trend_analysis", "consumer_behavior", "market_forecast"]
            }
        }
        
        # ë°ì´í„° í’ˆì§ˆ ê¸°ì¤€
        self.quality_thresholds = {
            "min_confidence": 0.7,
            "min_data_completeness": 0.6,
            "max_processing_time": 30.0,
            "required_fields": ["query", "results", "timestamp"]
        }
        
        self.logger.info("ì›¹ ë°ì´í„° í†µí•© ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def integrate_web_data_to_workflow(self, web_search_result: Dict[str, Any], 
                                     workflow_context: Dict[str, Any]) -> Dict[str, Any]:
        """ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì›Œí¬í”Œë¡œìš°ì— í†µí•©"""
        
        integration_result = {
            "timestamp": datetime.now().isoformat(),
            "integration_success": False,
            "web_data_summary": {},
            "enhanced_context": {},
            "recommendations": [],
            "quality_assessment": {},
            "integration_metadata": {}
        }
        
        try:
            self.logger.info("ì›¹ ë°ì´í„° ì›Œí¬í”Œë¡œìš° í†µí•© ì‹œì‘")
            
            # 1. ì›¹ ë°ì´í„° í’ˆì§ˆ ê²€ì¦
            quality_check = self._validate_web_data_quality(web_search_result)
            integration_result["quality_assessment"] = quality_check
            
            if not quality_check["is_valid"]:
                integration_result["error"] = "ì›¹ ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ì‹¤íŒ¨"
                return integration_result
            
            # 2. ì›¹ ë°ì´í„° ìš”ì•½ ìƒì„±
            web_summary = self._generate_web_data_summary(web_search_result)
            integration_result["web_data_summary"] = web_summary
            
            # 3. ì›Œí¬í”Œë¡œìš° ì»¨í…ìŠ¤íŠ¸ì™€ ê²°í•©
            enhanced_context = self._merge_contexts(web_summary, workflow_context)
            integration_result["enhanced_context"] = enhanced_context
            
            # 4. í†µí•© ì¶”ì²œì‚¬í•­ ìƒì„±
            recommendations = self._generate_integrated_recommendations(
                web_summary, workflow_context, enhanced_context
            )
            integration_result["recommendations"] = recommendations
            
            # 5. ë©”íƒ€ë°ì´í„° ìƒì„±
            integration_result["integration_metadata"] = self._generate_integration_metadata(
                web_search_result, workflow_context
            )
            
            integration_result["integration_success"] = True
            self.logger.info("ì›¹ ë°ì´í„° ì›Œí¬í”Œë¡œìš° í†µí•© ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"ì›¹ ë°ì´í„° í†µí•© ì‹¤íŒ¨: {str(e)}")
            integration_result["error"] = str(e)
        
        return integration_result
    
    def _validate_web_data_quality(self, web_data: Dict[str, Any]) -> Dict[str, Any]:
        """ì›¹ ë°ì´í„° í’ˆì§ˆ ê²€ì¦"""
        
        quality_result = {
            "is_valid": False,
            "quality_score": 0.0,
            "issues": [],
            "strengths": []
        }
        
        score = 0.0
        max_score = 100.0
        
        # í•„ìˆ˜ í•„ë“œ í™•ì¸
        required_fields = self.quality_thresholds["required_fields"]
        missing_fields = [field for field in required_fields if field not in web_data]
        
        if missing_fields:
            quality_result["issues"].append(f"í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {missing_fields}")
        else:
            score += 25.0
            quality_result["strengths"].append("ëª¨ë“  í•„ìˆ˜ í•„ë“œ ì¡´ì¬")
        
        # ì„±ê³µ ì—¬ë¶€ í™•ì¸
        if web_data.get("success", False):
            score += 25.0
            quality_result["strengths"].append("ê²€ìƒ‰ ì„±ê³µ")
        else:
            quality_result["issues"].append("ì›¹ ê²€ìƒ‰ ì‹¤íŒ¨")
        
        # ë°ì´í„° ì™„ì„±ë„ í™•ì¸
        search_results = web_data.get("search_results", {})
        if search_results:
            successful_searches = 0
            total_searches = 0
            
            for category, results in search_results.items():
                if isinstance(results, dict):
                    total_searches += 1
                    if results.get("success"):
                        successful_searches += 1
                elif isinstance(results, list):
                    for result in results:
                        total_searches += 1
                        if result.get("success"):
                            successful_searches += 1
            
            if total_searches > 0:
                completeness = successful_searches / total_searches
                if completeness >= self.quality_thresholds["min_data_completeness"]:
                    score += 25.0
                    quality_result["strengths"].append(f"ë°ì´í„° ì™„ì„±ë„ ì–‘í˜¸ ({completeness:.1%})")
                else:
                    quality_result["issues"].append(f"ë°ì´í„° ì™„ì„±ë„ ë¶€ì¡± ({completeness:.1%})")
        
        # ì¶”ì²œì‚¬í•­ ì¡´ì¬ í™•ì¸
        recommendations = web_data.get("recommendations", [])
        if recommendations and len(recommendations) >= 3:
            score += 25.0
            quality_result["strengths"].append(f"í’ë¶€í•œ ì¶”ì²œì‚¬í•­ ({len(recommendations)}ê°œ)")
        else:
            quality_result["issues"].append("ì¶”ì²œì‚¬í•­ ë¶€ì¡±")
        
        quality_result["quality_score"] = score / max_score
        quality_result["is_valid"] = quality_result["quality_score"] >= self.quality_thresholds["min_confidence"]
        
        return quality_result
    
    def _generate_web_data_summary(self, web_data: Dict[str, Any]) -> Dict[str, Any]:
        """ì›¹ ë°ì´í„° ìš”ì•½ ìƒì„±"""
        
        summary = {
            "search_query": web_data.get("query", ""),
            "search_context": web_data.get("context", {}),
            "total_sites_searched": 0,
            "successful_searches": 0,
            "key_findings": [],
            "price_information": {},
            "brand_insights": [],
            "market_trends": [],
            "search_recommendations": web_data.get("recommendations", [])
        }
        
        # ê²€ìƒ‰ ê²°ê³¼ ë¶„ì„
        search_results = web_data.get("search_results", {})
        
        # êµ¬ê¸€ ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬
        google_result = search_results.get("google", {})
        if google_result.get("success"):
            summary["successful_searches"] += 1
            google_data = google_result.get("data", {})
            if "top_results" in google_data:
                summary["key_findings"].extend([
                    f"êµ¬ê¸€ ê²€ìƒ‰ ìƒìœ„ ê²°ê³¼: {len(google_data['top_results'])}ê°œ",
                    f"ì˜ˆìƒ ê²€ìƒ‰ ê²°ê³¼: {google_data.get('estimated_results', 'N/A')}"
                ])
        summary["total_sites_searched"] += 1
        
        # ì‡¼í•‘ëª° ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬
        shopping_results = search_results.get("shopping", [])
        price_data = []
        
        for shop_result in shopping_results:
            summary["total_sites_searched"] += 1
            if shop_result.get("success"):
                summary["successful_searches"] += 1
                shop_data = shop_result.get("data", {})
                
                # ê°€ê²© ì •ë³´ ìˆ˜ì§‘
                price_range = shop_data.get("price_range", "")
                if price_range:
                    price_data.append({
                        "site": shop_result.get("site", "Unknown"),
                        "price_range": price_range,
                        "product_count": shop_data.get("products_found", "N/A")
                    })
                
                # ë¸Œëœë“œ ì •ë³´ ìˆ˜ì§‘
                brands = shop_data.get("popular_brands", [])
                if brands:
                    summary["brand_insights"].extend(brands)
        
        if price_data:
            summary["price_information"] = {
                "price_sources": len(price_data),
                "price_data": price_data,
                "price_analysis": self._analyze_price_data(price_data)
            }
        
        # ì „ë¬¸ì  ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬
        jewelry_results = search_results.get("jewelry", [])
        
        for jewelry_result in jewelry_results:
            summary["total_sites_searched"] += 1
            if jewelry_result.get("success"):
                summary["successful_searches"] += 1
                jewelry_data = jewelry_result.get("data", {})
                
                # ì „ë¬¸ì  íŠ¹í™” ì •ë³´
                specialty = jewelry_data.get("specialty", "")
                if specialty:
                    summary["market_trends"].append(f"{jewelry_result.get('site', 'Unknown')}: {specialty}")
                
                # ì„œë¹„ìŠ¤ í˜œíƒ ì •ë³´
                benefits = jewelry_data.get("service_benefits", [])
                if benefits:
                    summary["key_findings"].append(f"{jewelry_result.get('site', 'Unknown')} í˜œíƒ: {', '.join(benefits[:3])}")
        
        # ë¸Œëœë“œ ì¸ì‚¬ì´íŠ¸ ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
        summary["brand_insights"] = list(set(summary["brand_insights"]))
        
        return summary
    
    def _analyze_price_data(self, price_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ê°€ê²© ë°ì´í„° ë¶„ì„"""
        
        analysis = {
            "price_range_analysis": "ë‹¤ì–‘í•œ ê°€ê²©ëŒ€ í™•ì¸ë¨",
            "market_positioning": "ì¤‘ê°„ ê°€ê²©ëŒ€",
            "price_competitiveness": "ê²½ìŸì ",
            "recommendations": []
        }
        
        # ê°„ë‹¨í•œ ê°€ê²© ë¶„ì„ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ë¶„ì„ í•„ìš”)
        if len(price_data) >= 3:
            analysis["recommendations"].append("ì—¬ëŸ¬ ë§¤ì¥ ê°€ê²© ë¹„êµ ì™„ë£Œ")
        
        if any("ë§Œì›" in data.get("price_range", "") for data in price_data):
            analysis["market_positioning"] = "í”„ë¦¬ë¯¸ì—„ ì‹œì¥"
            analysis["recommendations"].append("ê³ í’ˆì§ˆ ì œí’ˆêµ° í™•ì¸")
        
        return analysis
    
    def _merge_contexts(self, web_summary: Dict[str, Any], workflow_context: Dict[str, Any]) -> Dict[str, Any]:
        """ì›¹ ë°ì´í„°ì™€ ì›Œí¬í”Œë¡œìš° ì»¨í…ìŠ¤íŠ¸ ê²°í•©"""
        
        enhanced_context = {
            "original_context": workflow_context.copy(),
            "web_insights": web_summary,
            "merged_insights": {},
            "enhanced_recommendations": []
        }
        
        # ê¸°ë³¸ ì •ë³´ ê²°í•©
        search_context = web_summary.get("search_context", {})
        
        # ìƒí™© ì •ë³´ ê²°í•©
        if "situation" in search_context and "customer_situation" in workflow_context:
            enhanced_context["merged_insights"]["situation_alignment"] = {
                "web_search_situation": search_context["situation"],
                "workflow_situation": workflow_context["customer_situation"],
                "consistency": search_context["situation"] == workflow_context.get("customer_situation")
            }
        
        # ì˜ˆì‚° ì •ë³´ ê²°í•©
        if "budget" in search_context and "budget_info" in workflow_context:
            enhanced_context["merged_insights"]["budget_analysis"] = {
                "search_budget": search_context["budget"],
                "workflow_budget": workflow_context["budget_info"],
                "market_price_data": web_summary.get("price_information", {})
            }
        
        # í‚¤ì›Œë“œ ë° ê´€ì‹¬ì‚¬ ê²°í•©
        search_query = web_summary.get("search_query", "")
        workflow_keywords = workflow_context.get("key_topics", [])
        
        enhanced_context["merged_insights"]["topic_relevance"] = {
            "search_terms": search_query.split(),
            "workflow_keywords": workflow_keywords,
            "market_trends": web_summary.get("market_trends", [])
        }
        
        return enhanced_context
    
    def _generate_integrated_recommendations(self, web_summary: Dict[str, Any], 
                                           workflow_context: Dict[str, Any],
                                           enhanced_context: Dict[str, Any]) -> List[str]:
        """í†µí•© ì¶”ì²œì‚¬í•­ ìƒì„±"""
        
        recommendations = []
        
        # ì›¹ ê²€ìƒ‰ ê¸°ë°˜ ì¶”ì²œì‚¬í•­
        web_recommendations = web_summary.get("search_recommendations", [])
        if web_recommendations:
            recommendations.append("ğŸŒ ì˜¨ë¼ì¸ ì‹œì¥ ì¡°ì‚¬ ê²°ê³¼:")
            recommendations.extend([f"  â€¢ {rec}" for rec in web_recommendations[:3]])
        
        # ê°€ê²© ë¶„ì„ ê¸°ë°˜ ì¶”ì²œì‚¬í•­
        price_info = web_summary.get("price_information", {})
        if price_info:
            price_analysis = price_info.get("price_analysis", {})
            price_recs = price_analysis.get("recommendations", [])
            if price_recs:
                recommendations.append("ğŸ’° ê°€ê²© ë¶„ì„ ê²°ê³¼:")
                recommendations.extend([f"  â€¢ {rec}" for rec in price_recs])
        
        # ë¸Œëœë“œ ë¶„ì„ ê¸°ë°˜ ì¶”ì²œì‚¬í•­
        brands = web_summary.get("brand_insights", [])
        if brands:
            recommendations.append(f"ğŸ·ï¸ ì£¼ìš” ë¸Œëœë“œ í™•ì¸: {', '.join(brands[:3])}")
        
        # í†µí•© ë¶„ì„ ê¸°ë°˜ ì¶”ì²œì‚¬í•­
        merged_insights = enhanced_context.get("merged_insights", {})
        
        # ìƒí™©ë³„ ë§ì¶¤ ì¶”ì²œ
        situation_data = merged_insights.get("situation_alignment", {})
        if situation_data.get("consistency"):
            recommendations.append("âœ… ê²€ìƒ‰ ëª©ì ê³¼ ìƒë‹´ ë‚´ìš©ì´ ì¼ì¹˜í•˜ì—¬ ì‹ ë¢°ì„± ë†’ì€ ì •ë³´ ì œê³µ ê°€ëŠ¥")
        
        # ì˜ˆì‚° ë¶„ì„ ê¸°ë°˜ ì¶”ì²œ
        budget_data = merged_insights.get("budget_analysis", {})
        if budget_data:
            recommendations.append("ğŸ’¡ ì˜ˆì‚° ëŒ€ë¹„ ì‹œì¥ ê°€ê²© ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì  êµ¬ë§¤ ì‹œì  ì œì•ˆ")
        
        # ì¢…í•© ì¶”ì²œì‚¬í•­
        recommendations.extend([
            "ğŸ” ì˜¨ë¼ì¸ ì •ë³´ì™€ ì‹¤ì œ ìƒë‹´ ë‚´ìš©ì„ ê²°í•©í•œ ë§ì¶¤í˜• ì†”ë£¨ì…˜ ì œê³µ",
            "ğŸ“Š ì‹œì¥ ë™í–¥ê³¼ ê°œì¸ ìš”êµ¬ì‚¬í•­ì„ ê· í˜•ìˆê²Œ ê³ ë ¤í•œ ì˜ì‚¬ê²°ì • ì§€ì›",
            "ğŸ¯ ì›¹ ê²€ìƒ‰ìœ¼ë¡œ í™•ì¸ëœ ìµœì‹  ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ ì‹¤ì‹œê°„ ìƒë‹´ ì—…ë°ì´íŠ¸"
        ])
        
        return recommendations
    
    def _generate_integration_metadata(self, web_data: Dict[str, Any], 
                                     workflow_context: Dict[str, Any]) -> Dict[str, Any]:
        """í†µí•© ë©”íƒ€ë°ì´í„° ìƒì„±"""
        
        metadata = {
            "integration_timestamp": datetime.now().isoformat(),
            "web_data_source": "MCP Browser Integration",
            "integration_version": "1.0.0",
            "data_sources": {
                "web_search_count": 0,
                "workflow_files": 0,
                "integration_points": []
            },
            "quality_metrics": {
                "web_data_completeness": 0.0,
                "context_alignment": 0.0,
                "recommendation_relevance": 0.0
            },
            "processing_stats": {
                "total_processing_time": 0.0,
                "web_search_time": web_data.get("timestamp", ""),
                "integration_complexity": "medium"
            }
        }
        
        # ë°ì´í„° ì†ŒìŠ¤ ì¹´ìš´íŠ¸
        search_results = web_data.get("search_results", {})
        web_search_count = 0
        
        for category, results in search_results.items():
            if isinstance(results, list):
                web_search_count += len(results)
            else:
                web_search_count += 1
        
        metadata["data_sources"]["web_search_count"] = web_search_count
        metadata["data_sources"]["workflow_files"] = len(workflow_context.get("analyzed_files", []))
        
        # í†µí•© í¬ì¸íŠ¸ ì‹ë³„
        integration_points = []
        if "customer_situation" in workflow_context:
            integration_points.append("situation_context")
        if "budget_info" in workflow_context:
            integration_points.append("budget_analysis")
        if "key_topics" in workflow_context:
            integration_points.append("topic_relevance")
        
        metadata["data_sources"]["integration_points"] = integration_points
        
        return metadata
    
    def create_comprehensive_report(self, integration_result: Dict[str, Any], 
                                  original_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """ì›¹ ë°ì´í„°ê°€ í†µí•©ëœ ì¢…í•© ë³´ê³ ì„œ ìƒì„±"""
        
        comprehensive_report = {
            "report_id": f"comprehensive_{int(time.time())}",
            "timestamp": datetime.now().isoformat(),
            "report_type": "web_integrated_analysis",
            "executive_summary": {},
            "detailed_analysis": {},
            "web_market_insights": {},
            "integrated_recommendations": {},
            "appendix": {}
        }
        
        try:
            # ìš”ì•½ ì„¹ì…˜
            comprehensive_report["executive_summary"] = self._create_executive_summary(
                integration_result, original_analysis
            )
            
            # ìƒì„¸ ë¶„ì„ ì„¹ì…˜
            comprehensive_report["detailed_analysis"] = self._create_detailed_analysis(
                integration_result, original_analysis
            )
            
            # ì›¹ ì‹œì¥ ì¸ì‚¬ì´íŠ¸ ì„¹ì…˜
            comprehensive_report["web_market_insights"] = self._create_market_insights(
                integration_result
            )
            
            # í†µí•© ì¶”ì²œì‚¬í•­ ì„¹ì…˜
            comprehensive_report["integrated_recommendations"] = self._create_integrated_recommendations_section(
                integration_result
            )
            
            # ë¶€ë¡ ì„¹ì…˜
            comprehensive_report["appendix"] = self._create_appendix(
                integration_result, original_analysis
            )
            
            self.logger.info("ì¢…í•© ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"ì¢…í•© ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            comprehensive_report["error"] = str(e)
        
        return comprehensive_report
    
    def _create_executive_summary(self, integration_result: Dict[str, Any], 
                                original_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """ê²½ì˜ì§„ ìš”ì•½ ìƒì„±"""
        
        summary = {
            "overview": "",
            "key_highlights": [],
            "critical_insights": [],
            "action_items": []
        }
        
        # ê°œìš” ìƒì„±
        web_summary = integration_result.get("web_data_summary", {})
        search_query = web_summary.get("search_query", "ì£¼ì–¼ë¦¬ ìƒí’ˆ")
        
        summary["overview"] = (
            f"'{search_query}' ê´€ë ¨ ì¢…í•© ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤. "
            f"ì˜¨ë¼ì¸ ì‹œì¥ ì¡°ì‚¬ì™€ ì‹¤ì œ ìƒë‹´ ë‚´ìš©ì„ í†µí•©í•˜ì—¬ "
            f"ê³ ê° ë§ì¶¤í˜• ì†”ë£¨ì…˜ì„ ì œì•ˆí•©ë‹ˆë‹¤."
        )
        
        # ì£¼ìš” í•˜ì´ë¼ì´íŠ¸
        successful_searches = web_summary.get("successful_searches", 0)
        total_searches = web_summary.get("total_sites_searched", 0)
        
        summary["key_highlights"] = [
            f"ì˜¨ë¼ì¸ ì¡°ì‚¬: {successful_searches}/{total_searches} ì‚¬ì´íŠ¸ ì„±ê³µì  ë¶„ì„",
            f"ì‹œì¥ íŠ¸ë Œë“œ: {len(web_summary.get('market_trends', []))}ê°œ í•µì‹¬ ë™í–¥ íŒŒì•…",
            f"ë¸Œëœë“œ ë¶„ì„: {len(web_summary.get('brand_insights', []))}ê°œ ì£¼ìš” ë¸Œëœë“œ í™•ì¸",
            f"í†µí•© ì¶”ì²œì‚¬í•­: {len(integration_result.get('recommendations', []))}ê°œ ì œì•ˆ"
        ]
        
        # í•µì‹¬ ì¸ì‚¬ì´íŠ¸
        enhanced_context = integration_result.get("enhanced_context", {})
        merged_insights = enhanced_context.get("merged_insights", {})
        
        if merged_insights.get("situation_alignment", {}).get("consistency"):
            summary["critical_insights"].append("âœ… ì˜¨ë¼ì¸ ê²€ìƒ‰ê³¼ ì‹¤ì œ ìƒë‹´ ëª©ì ì´ ì¼ì¹˜í•˜ì—¬ ì‹ ë¢°ì„± ë†’ìŒ")
        
        price_info = web_summary.get("price_information", {})
        if price_info:
            summary["critical_insights"].append(f"ğŸ’° {price_info.get('price_sources', 0)}ê°œ ë§¤ì¥ ê°€ê²© ì •ë³´ í™•ë³´")
        
        # ì•¡ì…˜ ì•„ì´í…œ
        summary["action_items"] = [
            "ì‹œì¥ ì¡°ì‚¬ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ ê³ ê° ë§ì¶¤ ìƒí’ˆ ì œì•ˆ",
            "ê²½ìŸë ¥ ìˆëŠ” ê°€ê²©ëŒ€ í™•ì¸ ë° ìµœì  êµ¬ë§¤ ì‹œì  ì•ˆë‚´", 
            "ë¸Œëœë“œë³„ íŠ¹í™” ì„œë¹„ìŠ¤ ë° í˜œíƒ ì •ë³´ ì œê³µ"
        ]
        
        return summary
    
    def _create_detailed_analysis(self, integration_result: Dict[str, Any], 
                                original_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """ìƒì„¸ ë¶„ì„ ì„¹ì…˜ ìƒì„±"""
        
        detailed = {
            "original_analysis_summary": {},
            "web_research_findings": {},
            "integration_analysis": {},
            "comparative_insights": {}
        }
        
        # ì›ë³¸ ë¶„ì„ ìš”ì•½
        detailed["original_analysis_summary"] = {
            "analysis_type": original_analysis.get("analysis_type", "íŒŒì¼ ë¶„ì„"),
            "file_count": len(original_analysis.get("files_analyzed", [])),
            "key_findings": original_analysis.get("key_insights", [])[:5],
            "processing_time": original_analysis.get("processing_time", "N/A")
        }
        
        # ì›¹ ì¡°ì‚¬ ê²°ê³¼
        web_summary = integration_result.get("web_data_summary", {})
        detailed["web_research_findings"] = {
            "search_scope": f"{web_summary.get('total_sites_searched', 0)}ê°œ ì‚¬ì´íŠ¸ ì¡°ì‚¬",
            "success_rate": f"{web_summary.get('successful_searches', 0)}/{web_summary.get('total_sites_searched', 0)}",
            "price_analysis": web_summary.get("price_information", {}),
            "market_trends": web_summary.get("market_trends", []),
            "brand_landscape": web_summary.get("brand_insights", [])
        }
        
        # í†µí•© ë¶„ì„
        enhanced_context = integration_result.get("enhanced_context", {})
        detailed["integration_analysis"] = {
            "context_alignment": enhanced_context.get("merged_insights", {}),
            "data_quality": integration_result.get("quality_assessment", {}),
            "integration_points": len(enhanced_context.get("merged_insights", {}))
        }
        
        return detailed
    
    def _create_market_insights(self, integration_result: Dict[str, Any]) -> Dict[str, Any]:
        """ì‹œì¥ ì¸ì‚¬ì´íŠ¸ ì„¹ì…˜ ìƒì„±"""
        
        insights = {
            "market_overview": {},
            "competitive_landscape": {},
            "pricing_analysis": {},
            "trend_analysis": {}
        }
        
        web_summary = integration_result.get("web_data_summary", {})
        
        # ì‹œì¥ ê°œìš”
        insights["market_overview"] = {
            "search_query": web_summary.get("search_query", ""),
            "market_coverage": f"{web_summary.get('total_sites_searched', 0)}ê°œ í”Œë«í¼ ì¡°ì‚¬",
            "data_reliability": "ë†’ìŒ" if web_summary.get("successful_searches", 0) > 3 else "ë³´í†µ"
        }
        
        # ê²½ìŸ í™˜ê²½
        brands = web_summary.get("brand_insights", [])
        insights["competitive_landscape"] = {
            "major_brands": brands[:5] if brands else [],
            "market_segments": ["í”„ë¦¬ë¯¸ì—„", "ì¤‘ê°„ê°€", "ì €ê°€"] if brands else [],
            "brand_count": len(brands)
        }
        
        # ê°€ê²© ë¶„ì„
        price_info = web_summary.get("price_information", {})
        insights["pricing_analysis"] = price_info if price_info else {"status": "ë°ì´í„° ë¶€ì¡±"}
        
        # íŠ¸ë Œë“œ ë¶„ì„
        trends = web_summary.get("market_trends", [])
        insights["trend_analysis"] = {
            "identified_trends": trends,
            "trend_count": len(trends),
            "trend_reliability": "ë†’ìŒ" if len(trends) > 2 else "ë³´í†µ"
        }
        
        return insights
    
    def _create_integrated_recommendations_section(self, integration_result: Dict[str, Any]) -> Dict[str, Any]:
        """í†µí•© ì¶”ì²œì‚¬í•­ ì„¹ì…˜ ìƒì„±"""
        
        recommendations_section = {
            "immediate_actions": [],
            "short_term_strategy": [],
            "long_term_considerations": [],
            "risk_assessments": []
        }
        
        recommendations = integration_result.get("recommendations", [])
        
        # ì¶”ì²œì‚¬í•­ì„ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜
        for rec in recommendations:
            if "ì˜¨ë¼ì¸" in rec or "ì‹¤ì‹œê°„" in rec:
                recommendations_section["immediate_actions"].append(rec)
            elif "ì˜ˆì‚°" in rec or "ê°€ê²©" in rec:
                recommendations_section["short_term_strategy"].append(rec)
            elif "ì‹œì¥" in rec or "ë¸Œëœë“œ" in rec:
                recommendations_section["long_term_considerations"].append(rec)
        
        # ê¸°ë³¸ ì¶”ì²œì‚¬í•­ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ ì œê³µ
        if not recommendations_section["immediate_actions"]:
            recommendations_section["immediate_actions"] = [
                "ì›¹ ì¡°ì‚¬ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ ì•¡ì…˜ í”Œëœ ìˆ˜ë¦½"
            ]
        
        if not recommendations_section["short_term_strategy"]:
            recommendations_section["short_term_strategy"] = [
                "ê°€ê²© ê²½ìŸë ¥ í™•ë³´ë¥¼ ìœ„í•œ ë‹¨ê¸° ì „ëµ ìˆ˜ë¦½"
            ]
        
        # ë¦¬ìŠ¤í¬ í‰ê°€
        quality_assessment = integration_result.get("quality_assessment", {})
        if quality_assessment.get("quality_score", 0) < 0.8:
            recommendations_section["risk_assessments"].append(
                "ì›¹ ë°ì´í„° í’ˆì§ˆì´ ì œí•œì ì´ë¯€ë¡œ ì¶”ê°€ ê²€ì¦ í•„ìš”"
            )
        
        return recommendations_section
    
    def _create_appendix(self, integration_result: Dict[str, Any], 
                       original_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """ë¶€ë¡ ì„¹ì…˜ ìƒì„±"""
        
        appendix = {
            "technical_details": integration_result.get("integration_metadata", {}),
            "data_sources": {},
            "quality_metrics": integration_result.get("quality_assessment", {}),
            "processing_logs": [],
            "raw_data_summary": {}
        }
        
        # ë°ì´í„° ì†ŒìŠ¤ ì •ë³´
        web_summary = integration_result.get("web_data_summary", {})
        appendix["data_sources"] = {
            "web_searches": web_summary.get("total_sites_searched", 0),
            "successful_web_searches": web_summary.get("successful_searches", 0),
            "original_files": len(original_analysis.get("files_analyzed", [])),
            "search_query": web_summary.get("search_query", "")
        }
        
        # ì²˜ë¦¬ ë¡œê·¸
        appendix["processing_logs"] = [
            f"ì›¹ ë°ì´í„° í†µí•© ì‹œì‘: {integration_result.get('timestamp', '')}",
            f"í’ˆì§ˆ ê²€ì¦ ì™„ë£Œ: {integration_result.get('quality_assessment', {}).get('is_valid', False)}",
            f"í†µí•© ì¶”ì²œì‚¬í•­ ìƒì„±: {len(integration_result.get('recommendations', []))}ê°œ"
        ]
        
        return appendix

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
_global_web_data_integration = None

def get_web_data_integration():
    """ì „ì—­ ì›¹ ë°ì´í„° í†µí•© ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _global_web_data_integration
    if _global_web_data_integration is None:
        _global_web_data_integration = WebDataIntegration()
    return _global_web_data_integration