"""
ğŸ” ì†”ë¡œëª¬ë“œ AI í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ v2.3
99.2% ì •í™•ë„ ë‹¬ì„±ì„ ìœ„í•œ ì‹¤ì‹œê°„ AI í’ˆì§ˆ ê²€ì¦ ë° ìë™ ê°œì„  ì‹œìŠ¤í…œ

ğŸ“… ê°œë°œì¼: 2025.07.13
ğŸ¯ ëª©í‘œ: ì‹¤ì‹œê°„ í’ˆì§ˆ ê²€ì¦ìœ¼ë¡œ 99.2% ì •í™•ë„ ë³´ì¥
ğŸ”¥ ì£¼ìš” ê¸°ëŠ¥:
- AI ì‘ë‹µ ì¼ê´€ì„± ì‹¤ì‹œê°„ ê²€ì¦
- ì£¼ì–¼ë¦¬ ì „ë¬¸ì„± ì ìˆ˜ ì •ë°€ ì¸¡ì •
- ìë™ ì¬ë¶„ì„ íŠ¸ë¦¬ê±° ì‹œìŠ¤í…œ
- í’ˆì§ˆ ê°œì„  ê¶Œì¥ì‚¬í•­ ìë™ ìƒì„±
- ë‹¤ì°¨ì› í’ˆì§ˆ ë©”íŠ¸ë¦­ ë¶„ì„
- ì‹¤ì‹œê°„ í”¼ë“œë°± ë£¨í”„

ì—°ë™ ì‹œìŠ¤í…œ:
- hybrid_llm_manager_v23.py
- jewelry_specialized_prompts_v23.py
"""

import asyncio
import logging
import time
import json
import statistics
import re
from typing import Dict, List, Optional, Any, Union, Tuple, Callable
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime, timedelta
import numpy as np
from collections import defaultdict, deque
import hashlib

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger('AIQualityValidator_v23')

class QualityDimension(Enum):
    """í’ˆì§ˆ ê²€ì¦ ì°¨ì›"""
    ACCURACY = "accuracy"
    COMPLETENESS = "completeness"
    COHERENCE = "coherence"
    JEWELRY_EXPERTISE = "jewelry_expertise"
    CONSISTENCY = "consistency"
    RELEVANCE = "relevance"
    PROFESSIONAL_TONE = "professional_tone"
    ACTIONABLE_INSIGHTS = "actionable_insights"

class ValidationSeverity(Enum):
    """ê²€ì¦ ì‹¬ê°ë„"""
    CRITICAL = "critical"    # ì¦‰ì‹œ ì¬ë¶„ì„ í•„ìš”
    HIGH = "high"           # ìš°ì„  ê°œì„  í•„ìš”
    MEDIUM = "medium"       # ê°œì„  ê¶Œì¥
    LOW = "low"            # ì°¸ê³ ì‚¬í•­
    INFO = "info"          # ì •ë³´ì„±

class QualityThreshold(Enum):
    """í’ˆì§ˆ ì„ê³„ê°’"""
    MINIMUM = 0.70     # ìµœì†Œ í—ˆìš© í’ˆì§ˆ
    STANDARD = 0.85    # í‘œì¤€ í’ˆì§ˆ
    HIGH = 0.92        # ê³ í’ˆì§ˆ
    EXPERT = 0.96      # ì „ë¬¸ê°€ ìˆ˜ì¤€
    TARGET = 0.992     # ëª©í‘œ í’ˆì§ˆ (99.2%)

@dataclass
class QualityMetric:
    """í’ˆì§ˆ ì§€í‘œ"""
    dimension: QualityDimension
    score: float
    confidence: float
    details: Dict[str, Any] = field(default_factory=dict)
    issues: List[str] = field(default_factory=list)
    suggestions: List[str] = field(default_factory=list)

@dataclass
class ValidationResult:
    """ê²€ì¦ ê²°ê³¼"""
    overall_score: float
    individual_scores: Dict[QualityDimension, QualityMetric]
    validation_passed: bool
    severity: ValidationSeverity
    
    # ìƒì„¸ ë¶„ì„
    strengths: List[str] = field(default_factory=list)
    weaknesses: List[str] = field(default_factory=list)
    improvement_actions: List[str] = field(default_factory=list)
    
    # ë©”íƒ€ë°ì´í„°
    validation_time: datetime = field(default_factory=datetime.now)
    validator_version: str = "2.3.0"
    confidence_level: float = 0.0
    
    # ì¬ë¶„ì„ ì—¬ë¶€
    requires_reanalysis: bool = False
    reanalysis_strategy: Optional[str] = None

@dataclass
class QualityBenchmark:
    """í’ˆì§ˆ ë²¤ì¹˜ë§ˆí¬"""
    gemstone_type: str
    analysis_type: str
    target_scores: Dict[QualityDimension, float]
    weight_distribution: Dict[QualityDimension, float]
    industry_standards: Dict[str, float]

class JewelryExpertiseEvaluator:
    """ì£¼ì–¼ë¦¬ ì „ë¬¸ì„± í‰ê°€ê¸°"""
    
    def __init__(self):
        # ì „ë¬¸ ìš©ì–´ ë°ì´í„°ë² ì´ìŠ¤
        self.professional_terms = {
            "diamond": {
                "essential": ["4C", "ìºëŸ¿", "ì»·", "ì»¬ëŸ¬", "í´ë˜ë¦¬í‹°", "GIA", "AGS", "í˜•ê´‘ì„±"],
                "advanced": ["Hearts and Arrows", "Ideal Cut", "pavilion", "crown", "girdle", "culet", "table"],
                "expert": ["light performance", "scintillation", "fire", "brilliance", "light return"]
            },
            "ruby": {
                "essential": ["ë£¨ë¹„", "ì½”ëŸ°ë¤", "ê°€ì—´", "ë²„ë§ˆ", "ë¯¸ì–€ë§ˆ", "íƒœêµ­"],
                "advanced": ["Pigeon Blood", "ì—´ì²˜ë¦¬", "ì›ì‚°ì§€", "SSEF", "GÃ¼belin"],
                "expert": ["trapiche", "asterism", "pleochroism", "silk inclusions"]
            },
            "sapphire": {
                "essential": ["ì‚¬íŒŒì´ì–´", "ì½”ëŸ°ë¤", "ì¹´ì‹œë¯¸ë¥´", "ì‹¤ë¡ ", "íŒŒë“œíŒŒë¼ì°¨"],
                "advanced": ["cornflower blue", "royal blue", "star sapphire", "color zoning"],
                "expert": ["Kashmir velvet", "Ceylon cornflower", "Padparadscha lotus"]
            },
            "emerald": {
                "essential": ["ì—ë©”ë„ë“œ", "ë² ë¦´", "ì½œë¡¬ë¹„ì•„", "ì ë¹„ì•„", "ì˜¤ì¼"],
                "advanced": ["jardin", "three-phase inclusions", "cedar oil", "opticon"],
                "expert": ["trapiche emerald", "cat's eye emerald", "crystal inclusions"]
            }
        }
        
        # ê°ì • ê¸°ì¤€ ìš©ì–´
        self.grading_terms = {
            "color": ["hue", "tone", "saturation", "ìƒ‰ìƒ", "í†¤", "ì±„ë„", "vivid", "intense"],
            "clarity": ["eye clean", "loupe clean", "inclusion", "ë‚´í¬ë¬¼", "íˆ¬ëª…ë„"],
            "cut": ["proportion", "symmetry", "polish", "ë¹„ìœ¨", "ëŒ€ì¹­ì„±", "ê´‘íƒë„"],
            "treatment": ["natural", "heated", "unheated", "ì²œì—°", "ê°€ì—´", "ë¬´ê°€ì—´", "ì²˜ë¦¬"]
        }
        
        # ì‹œì¥ ìš©ì–´
        self.market_terms = [
            "investment grade", "collector quality", "commercial quality",
            "market value", "auction record", "appreciate", "liquid",
            "íˆ¬ìë“±ê¸‰", "ìˆ˜ì§‘ê°€ê¸‰", "ìƒì—…ì í’ˆì§ˆ", "ì‹œì¥ê°€ì¹˜", "ê²½ë§¤ê¸°ë¡", "ìœ ë™ì„±"
        ]
    
    def evaluate_expertise_level(self, content: str, gemstone_type: str = "general") -> Dict[str, Any]:
        """ì „ë¬¸ì„± ìˆ˜ì¤€ í‰ê°€"""
        
        content_lower = content.lower()
        
        expertise_score = 0.0
        expertise_details = {
            "term_usage": {},
            "expertise_level": "basic",
            "missing_elements": [],
            "advanced_features": []
        }
        
        # 1. ì „ë¬¸ ìš©ì–´ ì‚¬ìš©ë„ í‰ê°€
        if gemstone_type in self.professional_terms:
            terms = self.professional_terms[gemstone_type]
            
            # í•„ìˆ˜ ìš©ì–´
            essential_found = sum(1 for term in terms["essential"] 
                                if term.lower() in content_lower)
            essential_ratio = essential_found / len(terms["essential"])
            
            # ê³ ê¸‰ ìš©ì–´
            advanced_found = sum(1 for term in terms["advanced"] 
                               if term.lower() in content_lower)
            advanced_ratio = advanced_found / len(terms["advanced"])
            
            # ì „ë¬¸ê°€ ìš©ì–´
            expert_found = sum(1 for term in terms["expert"] 
                             if term.lower() in content_lower)
            expert_ratio = expert_found / len(terms["expert"])
            
            expertise_details["term_usage"] = {
                "essential": {"found": essential_found, "ratio": essential_ratio},
                "advanced": {"found": advanced_found, "ratio": advanced_ratio},
                "expert": {"found": expert_found, "ratio": expert_ratio}
            }
            
            # ì „ë¬¸ì„± ì ìˆ˜ ê³„ì‚°
            expertise_score = (
                essential_ratio * 0.5 +
                advanced_ratio * 0.3 +
                expert_ratio * 0.2
            )
        
        # 2. ê°ì • ê¸°ì¤€ ìš©ì–´ ì‚¬ìš©ë„
        grading_score = 0.0
        for category, terms in self.grading_terms.items():
            found_terms = [term for term in terms if term.lower() in content_lower]
            if found_terms:
                grading_score += len(found_terms) / len(terms)
        
        grading_score = min(1.0, grading_score / len(self.grading_terms))
        
        # 3. ì‹œì¥ ìš©ì–´ ì‚¬ìš©ë„
        market_terms_found = sum(1 for term in self.market_terms 
                               if term.lower() in content_lower)
        market_score = min(1.0, market_terms_found / len(self.market_terms))
        
        # 4. ì¢…í•© ì „ë¬¸ì„± ì ìˆ˜
        final_expertise_score = (
            expertise_score * 0.5 +
            grading_score * 0.3 +
            market_score * 0.2
        )
        
        # 5. ì „ë¬¸ì„± ìˆ˜ì¤€ ê²°ì •
        if final_expertise_score >= 0.85:
            expertise_details["expertise_level"] = "expert"
        elif final_expertise_score >= 0.70:
            expertise_details["expertise_level"] = "advanced"
        elif final_expertise_score >= 0.50:
            expertise_details["expertise_level"] = "intermediate"
        else:
            expertise_details["expertise_level"] = "basic"
        
        return {
            "expertise_score": final_expertise_score,
            "details": expertise_details
        }

class ConsistencyAnalyzer:
    """ì¼ê´€ì„± ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.contradiction_patterns = [
            (r"ë†’ì€.*í’ˆì§ˆ.*í•˜ì§€ë§Œ.*ë‚®ì€", "í’ˆì§ˆ í‰ê°€ ëª¨ìˆœ"),
            (r"íˆ¬ì.*ê¶Œì¥.*í•˜ì§€ë§Œ.*ìœ„í—˜", "íˆ¬ì ê¶Œì¥ ëª¨ìˆœ"),
            (r"í¬ê·€.*í•˜ì§€ë§Œ.*ì¼ë°˜ì ", "í¬ì†Œì„± ëª¨ìˆœ"),
            (r"ìµœê³ .*ë“±ê¸‰.*í•˜ì§€ë§Œ.*ê²°í•¨", "ë“±ê¸‰ í‰ê°€ ëª¨ìˆœ")
        ]
        
        self.logical_flow_indicators = [
            "ë”°ë¼ì„œ", "ê·¸ëŸ¬ë¯€ë¡œ", "ê²°ë¡ ì ìœ¼ë¡œ", "ìš”ì•½í•˜ë©´",
            "ë°˜ë©´ì—", "í•˜ì§€ë§Œ", "ê·¸ëŸ¬ë‚˜", "ë‹¤ë§Œ"
        ]
    
    def analyze_consistency(self, content: str) -> Dict[str, Any]:
        """ì¼ê´€ì„± ë¶„ì„"""
        
        consistency_result = {
            "consistency_score": 0.8,  # ê¸°ë³¸ ì ìˆ˜
            "logical_flow_score": 0.0,
            "contradiction_count": 0,
            "contradictions": [],
            "flow_analysis": {}
        }
        
        # 1. ëª¨ìˆœ íŒ¨í„´ ê²€ì¶œ
        for pattern, description in self.contradiction_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            if matches:
                consistency_result["contradiction_count"] += len(matches)
                consistency_result["contradictions"].append({
                    "pattern": description,
                    "matches": matches
                })
        
        # ëª¨ìˆœì´ ìˆìœ¼ë©´ ì¼ê´€ì„± ì ìˆ˜ ê°ì†Œ
        if consistency_result["contradiction_count"] > 0:
            penalty = min(0.3, consistency_result["contradiction_count"] * 0.1)
            consistency_result["consistency_score"] -= penalty
        
        # 2. ë…¼ë¦¬ì  íë¦„ ë¶„ì„
        flow_indicators_found = [
            indicator for indicator in self.logical_flow_indicators
            if indicator in content
        ]
        
        if flow_indicators_found:
            flow_score = min(1.0, len(flow_indicators_found) / 4)
            consistency_result["logical_flow_score"] = flow_score
            consistency_result["consistency_score"] += flow_score * 0.1
        
        # 3. ë¬¸ì¥ êµ¬ì¡° ì¼ê´€ì„± (ê°„ë‹¨í•œ í‰ê°€)
        sentences = content.split('.')
        sentence_lengths = [len(s.strip()) for s in sentences if s.strip()]
        
        if sentence_lengths:
            length_variance = np.var(sentence_lengths)
            if length_variance < 2000:  # ì ì ˆí•œ ë¬¸ì¥ ê¸¸ì´ ë¶„ì‚°
                consistency_result["consistency_score"] += 0.05
        
        consistency_result["consistency_score"] = min(1.0, consistency_result["consistency_score"])
        
        return consistency_result

class RelevanceEvaluator:
    """ê´€ë ¨ì„± í‰ê°€ê¸°"""
    
    def __init__(self):
        self.context_keywords = {
            "diamond_analysis": ["ë‹¤ì´ì•„ëª¬ë“œ", "4C", "ìºëŸ¿", "ì»·", "ì»¬ëŸ¬", "í´ë˜ë¦¬í‹°", "GIA"],
            "ruby_analysis": ["ë£¨ë¹„", "ë²„ë§ˆ", "ê°€ì—´", "ë¬´ê°€ì—´", "ì½”ëŸ°ë¤", "SSEF"],
            "market_analysis": ["ì‹œì¥", "ê°€ê²©", "íˆ¬ì", "ìˆ˜ìµ", "ì „ë§", "ìœ ë™ì„±"],
            "insurance": ["ë³´í—˜", "ê°ì •ê°€ì•¡", "ëŒ€ì²´ë¹„ìš©", "ë¦¬ìŠ¤í¬", "ë³´ì¥"],
            "collection": ["ìˆ˜ì§‘", "í¬ê·€", "ì˜ˆìˆ ì ", "ì—­ì‚¬ì ", "ë¬¸í™”ì "],
            "certification": ["ê°ì •", "ì¸ì¦", "í‘œì¤€", "ë“±ê¸‰", "í’ˆì§ˆ"]
        }
    
    def evaluate_relevance(self, content: str, analysis_type: str, 
                         context_info: Dict[str, Any]) -> Dict[str, Any]:
        """ê´€ë ¨ì„± í‰ê°€"""
        
        relevance_result = {
            "relevance_score": 0.0,
            "context_match_score": 0.0,
            "keyword_coverage": 0.0,
            "off_topic_content": []
        }
        
        content_lower = content.lower()
        
        # 1. ë¶„ì„ íƒ€ì…ë³„ í‚¤ì›Œë“œ ë§¤ì¹­
        relevant_keywords = self.context_keywords.get(analysis_type, [])
        if relevant_keywords:
            matched_keywords = [kw for kw in relevant_keywords if kw.lower() in content_lower]
            keyword_coverage = len(matched_keywords) / len(relevant_keywords)
            relevance_result["keyword_coverage"] = keyword_coverage
        
        # 2. ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ë§¤ì¹­
        context_matches = 0
        total_context_items = 0
        
        if "gemstone_type" in context_info:
            total_context_items += 1
            if context_info["gemstone_type"].lower() in content_lower:
                context_matches += 1
        
        if "purpose" in context_info:
            total_context_items += 1
            purpose_keywords = context_info["purpose"].lower().split()
            if any(kw in content_lower for kw in purpose_keywords):
                context_matches += 1
        
        if total_context_items > 0:
            relevance_result["context_match_score"] = context_matches / total_context_items
        
        # 3. ì „ì²´ ê´€ë ¨ì„± ì ìˆ˜
        relevance_score = (
            relevance_result["keyword_coverage"] * 0.6 +
            relevance_result["context_match_score"] * 0.4
        )
        
        relevance_result["relevance_score"] = relevance_score
        
        return relevance_result

class AIQualityValidatorV23:
    """AI í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ v2.3"""
    
    def __init__(self):
        self.version = "2.3.0"
        self.target_accuracy = 0.992  # 99.2%
        
        # ì „ë¬¸ í‰ê°€ê¸°ë“¤
        self.expertise_evaluator = JewelryExpertiseEvaluator()
        self.consistency_analyzer = ConsistencyAnalyzer()
        self.relevance_evaluator = RelevanceEvaluator()
        
        # í’ˆì§ˆ ì„ê³„ê°’ ì„¤ì •
        self.quality_thresholds = {
            QualityDimension.ACCURACY: QualityThreshold.TARGET.value,
            QualityDimension.COMPLETENESS: QualityThreshold.HIGH.value,
            QualityDimension.COHERENCE: QualityThreshold.HIGH.value,
            QualityDimension.JEWELRY_EXPERTISE: QualityThreshold.EXPERT.value,
            QualityDimension.CONSISTENCY: QualityThreshold.HIGH.value,
            QualityDimension.RELEVANCE: QualityThreshold.HIGH.value,
            QualityDimension.PROFESSIONAL_TONE: QualityThreshold.STANDARD.value,
            QualityDimension.ACTIONABLE_INSIGHTS: QualityThreshold.HIGH.value
        }
        
        # ê°€ì¤‘ì¹˜ ì„¤ì •
        self.dimension_weights = {
            QualityDimension.ACCURACY: 0.25,
            QualityDimension.JEWELRY_EXPERTISE: 0.20,
            QualityDimension.COMPLETENESS: 0.15,
            QualityDimension.COHERENCE: 0.15,
            QualityDimension.CONSISTENCY: 0.10,
            QualityDimension.RELEVANCE: 0.10,
            QualityDimension.PROFESSIONAL_TONE: 0.03,
            QualityDimension.ACTIONABLE_INSIGHTS: 0.02
        }
        
        # ì„±ëŠ¥ ì¶”ì 
        self.validation_history = deque(maxlen=1000)
        self.performance_metrics = {
            "total_validations": 0,
            "passed_validations": 0,
            "failed_validations": 0,
            "reanalysis_triggered": 0,
            "average_scores": defaultdict(list),
            "improvement_trends": defaultdict(list)
        }
        
        logger.info(f"ğŸ” AI í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ v{self.version} ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"ğŸ¯ ëª©í‘œ ì •í™•ë„: {self.target_accuracy * 100}%")
    
    async def validate_comprehensive(self, 
                                   content: str,
                                   analysis_type: str,
                                   context_info: Dict[str, Any],
                                   expected_quality: float = 0.95) -> ValidationResult:
        """ì¢…í•©ì  í’ˆì§ˆ ê²€ì¦"""
        
        start_time = time.time()
        
        logger.info(f"ğŸ” í’ˆì§ˆ ê²€ì¦ ì‹œì‘: {analysis_type}")
        
        # ê°œë³„ ì°¨ì›ë³„ ê²€ì¦
        individual_scores = {}
        
        # 1. ì •í™•ë„ ê²€ì¦
        accuracy_metric = await self._validate_accuracy(content, context_info)
        individual_scores[QualityDimension.ACCURACY] = accuracy_metric
        
        # 2. ì™„ì„±ë„ ê²€ì¦
        completeness_metric = await self._validate_completeness(content, analysis_type, context_info)
        individual_scores[QualityDimension.COMPLETENESS] = completeness_metric
        
        # 3. ì¼ê´€ì„± ê²€ì¦
        coherence_metric = await self._validate_coherence(content)
        individual_scores[QualityDimension.COHERENCE] = coherence_metric
        
        # 4. ì£¼ì–¼ë¦¬ ì „ë¬¸ì„± ê²€ì¦
        expertise_metric = await self._validate_jewelry_expertise(content, context_info)
        individual_scores[QualityDimension.JEWELRY_EXPERTISE] = expertise_metric
        
        # 5. ì¼ê´€ì„± ê²€ì¦
        consistency_metric = await self._validate_consistency(content)
        individual_scores[QualityDimension.CONSISTENCY] = consistency_metric
        
        # 6. ê´€ë ¨ì„± ê²€ì¦
        relevance_metric = await self._validate_relevance(content, analysis_type, context_info)
        individual_scores[QualityDimension.RELEVANCE] = relevance_metric
        
        # 7. ì „ë¬¸ì  ì–´ì¡° ê²€ì¦
        tone_metric = await self._validate_professional_tone(content)
        individual_scores[QualityDimension.PROFESSIONAL_TONE] = tone_metric
        
        # 8. ì‹¤í–‰ ê°€ëŠ¥í•œ ì¸ì‚¬ì´íŠ¸ ê²€ì¦
        insights_metric = await self._validate_actionable_insights(content, analysis_type)
        individual_scores[QualityDimension.ACTIONABLE_INSIGHTS] = insights_metric
        
        # ì¢…í•© ì ìˆ˜ ê³„ì‚°
        overall_score = self._calculate_overall_score(individual_scores)
        
        # ê²€ì¦ í†µê³¼ ì—¬ë¶€ ê²°ì •
        validation_passed = overall_score >= expected_quality
        
        # ì‹¬ê°ë„ ê²°ì •
        severity = self._determine_severity(overall_score, individual_scores)
        
        # ì¬ë¶„ì„ í•„ìš”ì„± íŒë‹¨
        requires_reanalysis = self._should_trigger_reanalysis(overall_score, individual_scores, expected_quality)
        
        # ê°œì„  ì‚¬í•­ ë¶„ì„
        strengths, weaknesses, improvement_actions = self._analyze_improvement_opportunities(individual_scores)
        
        # ì¬ë¶„ì„ ì „ëµ ê²°ì •
        reanalysis_strategy = None
        if requires_reanalysis:
            reanalysis_strategy = self._determine_reanalysis_strategy(individual_scores, context_info)
        
        # ê²°ê³¼ ìƒì„±
        validation_result = ValidationResult(
            overall_score=overall_score,
            individual_scores=individual_scores,
            validation_passed=validation_passed,
            severity=severity,
            strengths=strengths,
            weaknesses=weaknesses,
            improvement_actions=improvement_actions,
            requires_reanalysis=requires_reanalysis,
            reanalysis_strategy=reanalysis_strategy,
            confidence_level=self._calculate_confidence_level(individual_scores)
        )
        
        # ì„±ëŠ¥ ì¶”ì  ì—…ë°ì´íŠ¸
        self._update_performance_tracking(validation_result)
        
        validation_time = time.time() - start_time
        
        logger.info(f"âœ… í’ˆì§ˆ ê²€ì¦ ì™„ë£Œ: {overall_score:.3f} ({validation_time:.2f}ì´ˆ)")
        
        return validation_result
    
    async def _validate_accuracy(self, content: str, context_info: Dict[str, Any]) -> QualityMetric:
        """ì •í™•ë„ ê²€ì¦"""
        
        accuracy_indicators = {
            "specific_values": 0.0,
            "evidence_based": 0.0,
            "uncertainty_acknowledgment": 0.0,
            "standard_compliance": 0.0
        }
        
        # 1. êµ¬ì²´ì  ìˆ˜ì¹˜ ì‚¬ìš©ë„
        number_patterns = re.findall(r'\d+\.?\d*\s*(?:ìºëŸ¿|%|ë“±ê¸‰|ì )', content)
        if number_patterns:
            accuracy_indicators["specific_values"] = min(1.0, len(number_patterns) / 5)
        
        # 2. ê·¼ê±° ê¸°ë°˜ ì„¤ëª…
        evidence_phrases = ["ê·¼ê±°", "ê¸°ì¤€", "í‘œì¤€", "according to", "based on", "ë”°ë¼ì„œ"]
        evidence_count = sum(1 for phrase in evidence_phrases if phrase in content.lower())
        accuracy_indicators["evidence_based"] = min(1.0, evidence_count / 3)
        
        # 3. ë¶ˆí™•ì‹¤ì„± ì¸ì •
        uncertainty_phrases = ["ì¶”ì •", "ì˜ˆìƒ", "ê°€ëŠ¥ì„±", "likely", "estimated", "approximately"]
        uncertainty_count = sum(1 for phrase in uncertainty_phrases if phrase in content.lower())
        accuracy_indicators["uncertainty_acknowledgment"] = min(1.0, uncertainty_count / 2)
        
        # 4. í‘œì¤€ ì¤€ìˆ˜
        standard_mentions = ["GIA", "AGS", "SSEF", "GÃ¼belin", "êµ­ì œí‘œì¤€", "ì—…ê³„í‘œì¤€"]
        standard_count = sum(1 for standard in standard_mentions if standard in content)
        accuracy_indicators["standard_compliance"] = min(1.0, standard_count / 2)
        
        # ì¢…í•© ì •í™•ë„ ì ìˆ˜
        accuracy_score = sum(accuracy_indicators.values()) / len(accuracy_indicators)
        
        # ë³´ì • ìš”ì†Œ
        content_length = len(content)
        if content_length < 200:
            accuracy_score *= 0.8  # ë„ˆë¬´ ì§§ì€ ë‹µë³€ì€ ê°ì 
        elif content_length > 2000:
            accuracy_score *= 1.1  # ìƒì„¸í•œ ë‹µë³€ì€ ê°€ì 
        
        accuracy_score = min(1.0, accuracy_score)
        
        issues = []
        suggestions = []
        
        if accuracy_score < 0.8:
            issues.append("êµ¬ì²´ì  ê·¼ê±° ë¶€ì¡±")
            suggestions.append("ëª…í™•í•œ ìˆ˜ì¹˜ì™€ ê·¼ê±° ì œì‹œ í•„ìš”")
        
        if accuracy_indicators["standard_compliance"] < 0.5:
            issues.append("êµ­ì œ í‘œì¤€ ì–¸ê¸‰ ë¶€ì¡±")
            suggestions.append("GIA, SSEF ë“± ê³µì¸ ê¸°ê´€ ê¸°ì¤€ ëª…ì‹œ í•„ìš”")
        
        return QualityMetric(
            dimension=QualityDimension.ACCURACY,
            score=accuracy_score,
            confidence=0.85,
            details=accuracy_indicators,
            issues=issues,
            suggestions=suggestions
        )
    
    async def _validate_completeness(self, content: str, analysis_type: str, 
                                   context_info: Dict[str, Any]) -> QualityMetric:
        """ì™„ì„±ë„ ê²€ì¦"""
        
        required_elements = {
            "diamond_analysis": ["ì»·", "ì»¬ëŸ¬", "í´ë˜ë¦¬í‹°", "ìºëŸ¿", "ë“±ê¸‰", "ê°€ì¹˜"],
            "ruby_analysis": ["ìƒ‰ìƒ", "íˆ¬ëª…ë„", "ì›ì‚°ì§€", "ì²˜ë¦¬", "í’ˆì§ˆ", "ê°€ì¹˜"],
            "market_analysis": ["í˜„ì¬ê°€ì¹˜", "ì‹œì¥ë™í–¥", "íˆ¬ìì „ë§", "ë¦¬ìŠ¤í¬"],
            "insurance": ["ëŒ€ì²´ë¹„ìš©", "ê°ì •ê°€ì•¡", "ë³´í—˜ë£Œ", "ê°±ì‹ ì£¼ê¸°"],
            "general": ["íŠ¹ì„±", "í’ˆì§ˆ", "ê°€ì¹˜", "ê¶Œì¥ì‚¬í•­"]
        }
        
        elements = required_elements.get(analysis_type, required_elements["general"])
        content_lower = content.lower()
        
        found_elements = [elem for elem in elements if elem in content_lower]
        completeness_score = len(found_elements) / len(elements)
        
        # ì¶”ê°€ ì™„ì„±ë„ ìš”ì†Œ
        additional_factors = {
            "has_introduction": any(word in content[:200] for word in ["ê°œìš”", "ë¶„ì„", "ê²€í† "]),
            "has_conclusion": any(word in content[-200:] for word in ["ê²°ë¡ ", "ìš”ì•½", "ê¶Œì¥"]),
            "has_details": len(content) > 500,
            "has_structure": content.count('\n') > 3 or any(marker in content for marker in ['1.', '2.', '**', '#'])
        }
        
        structure_bonus = sum(additional_factors.values()) * 0.05
        completeness_score = min(1.0, completeness_score + structure_bonus)
        
        missing_elements = [elem for elem in elements if elem not in content_lower]
        
        issues = []
        suggestions = []
        
        if missing_elements:
            issues.append(f"í•„ìˆ˜ ìš”ì†Œ ëˆ„ë½: {', '.join(missing_elements)}")
            suggestions.append(f"ë‹¤ìŒ í•­ëª© ì¶”ê°€ í•„ìš”: {', '.join(missing_elements)}")
        
        if not additional_factors["has_structure"]:
            issues.append("êµ¬ì¡°ì  ì™„ì„±ë„ ë¶€ì¡±")
            suggestions.append("ëª…í™•í•œ ì„¹ì…˜ êµ¬ë¶„ê³¼ ì²´ê³„ì  êµ¬ì„± í•„ìš”")
        
        return QualityMetric(
            dimension=QualityDimension.COMPLETENESS,
            score=completeness_score,
            confidence=0.90,
            details={
                "found_elements": found_elements,
                "missing_elements": missing_elements,
                "structure_factors": additional_factors
            },
            issues=issues,
            suggestions=suggestions
        )
    
    async def _validate_coherence(self, content: str) -> QualityMetric:
        """ë…¼ë¦¬ì  ì¼ê´€ì„± ê²€ì¦"""
        
        coherence_factors = {
            "logical_flow": 0.0,
            "transition_quality": 0.0,
            "argument_structure": 0.0,
            "readability": 0.0
        }
        
        # 1. ë…¼ë¦¬ì  íë¦„
        flow_indicators = ["ë”°ë¼ì„œ", "ê·¸ëŸ¬ë¯€ë¡œ", "ê²°ë¡ ì ìœ¼ë¡œ", "í•œí¸", "ë˜í•œ", "ë”ë¶ˆì–´"]
        flow_count = sum(1 for indicator in flow_indicators if indicator in content)
        coherence_factors["logical_flow"] = min(1.0, flow_count / 3)
        
        # 2. ì „í™˜ í’ˆì§ˆ
        paragraphs = content.split('\n\n')
        if len(paragraphs) > 1:
            transition_score = 0.8  # ê¸°ë³¸ ì ìˆ˜
            coherence_factors["transition_quality"] = transition_score
        
        # 3. ë…¼ì¦ êµ¬ì¡°
        argument_indicators = ["ì´ìœ ", "ê·¼ê±°", "ì˜ˆë¥¼ ë“¤ì–´", "ì‚¬ì‹¤", "ì¦ê±°"]
        argument_count = sum(1 for indicator in argument_indicators if indicator in content)
        coherence_factors["argument_structure"] = min(1.0, argument_count / 2)
        
        # 4. ê°€ë…ì„±
        sentences = content.split('.')
        sentence_count = len([s for s in sentences if s.strip()])
        avg_sentence_length = len(content) / max(1, sentence_count)
        
        if 50 <= avg_sentence_length <= 150:  # ì ì • ë¬¸ì¥ ê¸¸ì´
            coherence_factors["readability"] = 1.0
        else:
            coherence_factors["readability"] = 0.7
        
        coherence_score = sum(coherence_factors.values()) / len(coherence_factors)
        
        issues = []
        suggestions = []
        
        if coherence_factors["logical_flow"] < 0.5:
            issues.append("ë…¼ë¦¬ì  ì—°ê²°ì–´ ë¶€ì¡±")
            suggestions.append("ë¬¸ë‹¨ ê°„ ì—°ê²°ì„ ëª…í™•íˆ í•˜ëŠ” ì—°ê²°ì–´ ì‚¬ìš© ê¶Œì¥")
        
        if coherence_factors["readability"] < 0.8:
            issues.append("ê°€ë…ì„± ê°œì„  í•„ìš”")
            suggestions.append("ë¬¸ì¥ ê¸¸ì´ ì¡°ì • ë° ë‹¨ë½ êµ¬ì„± ê°œì„  í•„ìš”")
        
        return QualityMetric(
            dimension=QualityDimension.COHERENCE,
            score=coherence_score,
            confidence=0.80,
            details=coherence_factors,
            issues=issues,
            suggestions=suggestions
        )
    
    async def _validate_jewelry_expertise(self, content: str, 
                                        context_info: Dict[str, Any]) -> QualityMetric:
        """ì£¼ì–¼ë¦¬ ì „ë¬¸ì„± ê²€ì¦"""
        
        gemstone_type = context_info.get("gemstone_type", "general")
        expertise_result = self.expertise_evaluator.evaluate_expertise_level(content, gemstone_type)
        
        expertise_score = expertise_result["expertise_score"]
        details = expertise_result["details"]
        
        issues = []
        suggestions = []
        
        if expertise_score < 0.7:
            issues.append("ì „ë¬¸ ìš©ì–´ ì‚¬ìš© ë¶€ì¡±")
            suggestions.append("ì—…ê³„ í‘œì¤€ ì „ë¬¸ ìš©ì–´ ì ê·¹ í™œìš© í•„ìš”")
        
        if details["expertise_level"] in ["basic", "intermediate"]:
            issues.append(f"ì „ë¬¸ì„± ìˆ˜ì¤€: {details['expertise_level']}")
            suggestions.append("ê³ ê¸‰ ì „ë¬¸ ìš©ì–´ ë° ê¹Šì´ ìˆëŠ” ë¶„ì„ í•„ìš”")
        
        # ì „ë¬¸ì„± ì¶”ê°€ ë³´ì •
        if "term_usage" in details:
            term_usage = details["term_usage"]
            if "expert" in term_usage and term_usage["expert"]["ratio"] > 0.3:
                expertise_score *= 1.1  # ì „ë¬¸ê°€ ìš©ì–´ ì‚¬ìš© ì‹œ ê°€ì 
        
        expertise_score = min(1.0, expertise_score)
        
        return QualityMetric(
            dimension=QualityDimension.JEWELRY_EXPERTISE,
            score=expertise_score,
            confidence=0.85,
            details=details,
            issues=issues,
            suggestions=suggestions
        )
    
    async def _validate_consistency(self, content: str) -> QualityMetric:
        """ì¼ê´€ì„± ê²€ì¦"""
        
        consistency_result = self.consistency_analyzer.analyze_consistency(content)
        
        consistency_score = consistency_result["consistency_score"]
        
        issues = []
        suggestions = []
        
        if consistency_result["contradiction_count"] > 0:
            issues.append(f"ëª¨ìˆœëœ ë‚´ìš© {consistency_result['contradiction_count']}ê°œ ë°œê²¬")
            suggestions.append("ì¼ê´€ëœ ë…¼ë¦¬ì™€ í‰ê°€ ê¸°ì¤€ ìœ ì§€ í•„ìš”")
        
        if consistency_result["logical_flow_score"] < 0.5:
            issues.append("ë…¼ë¦¬ì  íë¦„ ë¶€ì¡±")
            suggestions.append("ëª…í™•í•œ ë…¼ë¦¬ì  ì—°ê²° êµ¬ì¡° í•„ìš”")
        
        return QualityMetric(
            dimension=QualityDimension.CONSISTENCY,
            score=consistency_score,
            confidence=0.75,
            details=consistency_result,
            issues=issues,
            suggestions=suggestions
        )
    
    async def _validate_relevance(self, content: str, analysis_type: str, 
                                context_info: Dict[str, Any]) -> QualityMetric:
        """ê´€ë ¨ì„± ê²€ì¦"""
        
        relevance_result = self.relevance_evaluator.evaluate_relevance(
            content, analysis_type, context_info
        )
        
        relevance_score = relevance_result["relevance_score"]
        
        issues = []
        suggestions = []
        
        if relevance_result["keyword_coverage"] < 0.6:
            issues.append("í•µì‹¬ í‚¤ì›Œë“œ ì»¤ë²„ë¦¬ì§€ ë¶€ì¡±")
            suggestions.append("ë¶„ì„ ìœ í˜•ì— ë§ëŠ” ì „ë¬¸ ìš©ì–´ ì ê·¹ ì‚¬ìš© í•„ìš”")
        
        if relevance_result["context_match_score"] < 0.7:
            issues.append("ì»¨í…ìŠ¤íŠ¸ ë¶€í•©ë„ ë¶€ì¡±")
            suggestions.append("ìš”ì²­ëœ ë¶„ì„ ëª©ì ì— ë” ì§‘ì¤‘ëœ ë‚´ìš© í•„ìš”")
        
        return QualityMetric(
            dimension=QualityDimension.RELEVANCE,
            score=relevance_score,
            confidence=0.80,
            details=relevance_result,
            issues=issues,
            suggestions=suggestions
        )
    
    async def _validate_professional_tone(self, content: str) -> QualityMetric:
        """ì „ë¬¸ì  ì–´ì¡° ê²€ì¦"""
        
        tone_factors = {
            "formality": 0.0,
            "objectivity": 0.0,
            "confidence": 0.0,
            "clarity": 0.0
        }
        
        # 1. ê²©ì‹ì„±
        formal_indicators = ["ì…ë‹ˆë‹¤", "ìŠµë‹ˆë‹¤", "ë©ë‹ˆë‹¤", "ê²ƒìœ¼ë¡œ", "ì— ëŒ€í•œ", "ê´€ë ¨í•˜ì—¬"]
        formal_count = sum(1 for indicator in formal_indicators if indicator in content)
        tone_factors["formality"] = min(1.0, formal_count / 10)
        
        # 2. ê°ê´€ì„±
        subjective_words = ["ëŠë‚Œ", "ìƒê°", "ê°œì¸ì ", "ì¶”ì¸¡", "ë§‰ì—°"]
        subjective_count = sum(1 for word in subjective_words if word in content)
        tone_factors["objectivity"] = max(0.0, 1.0 - (subjective_count / 5))
        
        # 3. í™•ì‹ ì„±
        confidence_indicators = ["ëª…í™•íˆ", "í™•ì‹¤íˆ", "ë¶„ëª…íˆ", "ì •í™•íˆ", "í™•ì¸ë¨"]
        confidence_count = sum(1 for indicator in confidence_indicators if indicator in content)
        tone_factors["confidence"] = min(1.0, confidence_count / 3)
        
        # 4. ëª…í™•ì„±
        clarity_indicators = ["êµ¬ì²´ì ìœ¼ë¡œ", "ì˜ˆë¥¼ ë“¤ì–´", "ì¦‰", "ë‹¤ì‹œ ë§í•´"]
        clarity_count = sum(1 for indicator in clarity_indicators if indicator in content)
        tone_factors["clarity"] = min(1.0, clarity_count / 2)
        
        tone_score = sum(tone_factors.values()) / len(tone_factors)
        
        issues = []
        suggestions = []
        
        if tone_factors["formality"] < 0.5:
            issues.append("ê²©ì‹ ìˆëŠ” ì–´ì¡° ë¶€ì¡±")
            suggestions.append("ë” ì „ë¬¸ì ì´ê³  ê²©ì‹ ìˆëŠ” í‘œí˜„ ì‚¬ìš© ê¶Œì¥")
        
        if tone_factors["objectivity"] < 0.7:
            issues.append("ì£¼ê´€ì  í‘œí˜„ ê³¼ë‹¤")
            suggestions.append("ê°ê´€ì ì´ê³  ì‚¬ì‹¤ ê¸°ë°˜ì˜ í‘œí˜„ ì‚¬ìš© í•„ìš”")
        
        return QualityMetric(
            dimension=QualityDimension.PROFESSIONAL_TONE,
            score=tone_score,
            confidence=0.70,
            details=tone_factors,
            issues=issues,
            suggestions=suggestions
        )
    
    async def _validate_actionable_insights(self, content: str, analysis_type: str) -> QualityMetric:
        """ì‹¤í–‰ ê°€ëŠ¥í•œ ì¸ì‚¬ì´íŠ¸ ê²€ì¦"""
        
        actionable_indicators = {
            "recommendations": 0.0,
            "specific_actions": 0.0,
            "decision_support": 0.0,
            "next_steps": 0.0
        }
        
        # 1. ê¶Œì¥ì‚¬í•­
        recommendation_words = ["ê¶Œì¥", "ì¶”ì²œ", "ì œì•ˆ", "ê¶Œí•¨", "ë°”ëŒì§"]
        recommendation_count = sum(1 for word in recommendation_words if word in content)
        actionable_indicators["recommendations"] = min(1.0, recommendation_count / 2)
        
        # 2. êµ¬ì²´ì  í–‰ë™
        action_words = ["í•´ì•¼", "í•„ìš”", "ê³ ë ¤", "ê²€í† ", "í™•ì¸", "ì ê²€"]
        action_count = sum(1 for word in action_words if word in content)
        actionable_indicators["specific_actions"] = min(1.0, action_count / 3)
        
        # 3. ì˜ì‚¬ê²°ì • ì§€ì›
        decision_words = ["ì„ íƒ", "ê²°ì •", "íŒë‹¨", "ê³ ë ¤ì‚¬í•­", "ì˜µì…˜"]
        decision_count = sum(1 for word in decision_words if word in content)
        actionable_indicators["decision_support"] = min(1.0, decision_count / 2)
        
        # 4. ë‹¤ìŒ ë‹¨ê³„
        next_step_phrases = ["ë‹¤ìŒ", "í–¥í›„", "ì•ìœ¼ë¡œ", "ê³„ì†", "ì¶”ê°€ë¡œ"]
        next_step_count = sum(1 for phrase in next_step_phrases if phrase in content)
        actionable_indicators["next_steps"] = min(1.0, next_step_count / 2)
        
        insights_score = sum(actionable_indicators.values()) / len(actionable_indicators)
        
        issues = []
        suggestions = []
        
        if insights_score < 0.6:
            issues.append("ì‹¤í–‰ ê°€ëŠ¥í•œ ê¶Œì¥ì‚¬í•­ ë¶€ì¡±")
            suggestions.append("êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì¡°ì¹˜ ì‚¬í•­ ì œì‹œ í•„ìš”")
        
        if actionable_indicators["next_steps"] < 0.3:
            issues.append("í›„ì† ì¡°ì¹˜ ì•ˆë‚´ ë¶€ì¡±")
            suggestions.append("ë‹¤ìŒ ë‹¨ê³„ ë˜ëŠ” ì¶”ê°€ ì¡°ì¹˜ ë°©ì•ˆ ì œì‹œ ê¶Œì¥")
        
        return QualityMetric(
            dimension=QualityDimension.ACTIONABLE_INSIGHTS,
            score=insights_score,
            confidence=0.75,
            details=actionable_indicators,
            issues=issues,
            suggestions=suggestions
        )
    
    def _calculate_overall_score(self, individual_scores: Dict[QualityDimension, QualityMetric]) -> float:
        """ì¢…í•© ì ìˆ˜ ê³„ì‚°"""
        
        weighted_sum = 0.0
        total_weight = 0.0
        
        for dimension, metric in individual_scores.items():
            weight = self.dimension_weights.get(dimension, 0.0)
            weighted_sum += metric.score * weight
            total_weight += weight
        
        if total_weight == 0:
            return 0.0
        
        overall_score = weighted_sum / total_weight
        return min(1.0, overall_score)
    
    def _determine_severity(self, overall_score: float, 
                          individual_scores: Dict[QualityDimension, QualityMetric]) -> ValidationSeverity:
        """ì‹¬ê°ë„ ê²°ì •"""
        
        if overall_score < 0.70:
            return ValidationSeverity.CRITICAL
        elif overall_score < 0.85:
            return ValidationSeverity.HIGH
        elif overall_score < 0.92:
            return ValidationSeverity.MEDIUM
        elif overall_score < 0.99:
            return ValidationSeverity.LOW
        else:
            return ValidationSeverity.INFO
    
    def _should_trigger_reanalysis(self, overall_score: float, 
                                 individual_scores: Dict[QualityDimension, QualityMetric],
                                 expected_quality: float) -> bool:
        """ì¬ë¶„ì„ íŠ¸ë¦¬ê±° ì—¬ë¶€ ê²°ì •"""
        
        # 1. ì „ì²´ ì ìˆ˜ê°€ ê¸°ëŒ€ì¹˜ë³´ë‹¤ í˜„ì €íˆ ë‚®ì€ ê²½ìš°
        if overall_score < expected_quality * 0.85:
            return True
        
        # 2. í•µì‹¬ ì°¨ì›ì—ì„œ ì‹¬ê°í•œ ë¬¸ì œê°€ ìˆëŠ” ê²½ìš°
        critical_dimensions = [
            QualityDimension.ACCURACY,
            QualityDimension.JEWELRY_EXPERTISE,
            QualityDimension.COMPLETENESS
        ]
        
        for dimension in critical_dimensions:
            if dimension in individual_scores:
                metric = individual_scores[dimension]
                threshold = self.quality_thresholds.get(dimension, 0.85)
                if metric.score < threshold * 0.8:
                    return True
        
        # 3. 99.2% ëª©í‘œì— í¬ê²Œ ë¯¸ë‹¬í•˜ëŠ” ê²½ìš°
        if overall_score < self.target_accuracy * 0.90:
            return True
        
        return False
    
    def _determine_reanalysis_strategy(self, individual_scores: Dict[QualityDimension, QualityMetric],
                                     context_info: Dict[str, Any]) -> str:
        """ì¬ë¶„ì„ ì „ëµ ê²°ì •"""
        
        # ê°€ì¥ ë‚®ì€ ì ìˆ˜ì˜ ì°¨ì› ì°¾ê¸°
        lowest_dimension = min(individual_scores.keys(), 
                             key=lambda d: individual_scores[d].score)
        lowest_score = individual_scores[lowest_dimension].score
        
        strategies = {
            QualityDimension.ACCURACY: "ë” êµ¬ì²´ì ì¸ ê·¼ê±°ì™€ ìˆ˜ì¹˜ ì œì‹œì— ì§‘ì¤‘",
            QualityDimension.JEWELRY_EXPERTISE: "ì „ë¬¸ ìš©ì–´ì™€ ì—…ê³„ í‘œì¤€ ê°•í™”",
            QualityDimension.COMPLETENESS: "ëˆ„ë½ëœ í•„ìˆ˜ ìš”ì†Œ ë³´ì™„",
            QualityDimension.COHERENCE: "ë…¼ë¦¬ì  êµ¬ì¡°ì™€ íë¦„ ê°œì„ ",
            QualityDimension.CONSISTENCY: "ì¼ê´€ëœ ë…¼ë¦¬ì™€ í‰ê°€ ê¸°ì¤€ ì ìš©",
            QualityDimension.RELEVANCE: "ìš”ì²­ ì‚¬í•­ì— ë” ì§ì ‘ì ìœ¼ë¡œ ëŒ€ì‘"
        }
        
        base_strategy = strategies.get(lowest_dimension, "ì „ë°˜ì  í’ˆì§ˆ í–¥ìƒ")
        
        # ì»¨í…ìŠ¤íŠ¸ì— ë”°ë¥¸ ì¶”ê°€ ì „ëµ
        if context_info.get("priority") == "critical":
            base_strategy += " + ìµœê³  ì •í™•ë„ ëª¨ë¸ ì‚¬ìš©"
        
        if lowest_score < 0.5:
            base_strategy += " + ë‹¤ì¤‘ ëª¨ë¸ êµì°¨ ê²€ì¦"
        
        return base_strategy
    
    def _analyze_improvement_opportunities(self, individual_scores: Dict[QualityDimension, QualityMetric]) -> Tuple[List[str], List[str], List[str]]:
        """ê°œì„  ê¸°íšŒ ë¶„ì„"""
        
        strengths = []
        weaknesses = []
        improvement_actions = []
        
        for dimension, metric in individual_scores.items():
            dimension_name = dimension.value.replace('_', ' ').title()
            
            if metric.score >= 0.9:
                strengths.append(f"{dimension_name} ìš°ìˆ˜ ({metric.score:.1%})")
            elif metric.score < 0.7:
                weaknesses.append(f"{dimension_name} ê°œì„  í•„ìš” ({metric.score:.1%})")
                
                # ê°œì„  ì•¡ì…˜ ì¶”ê°€
                if metric.suggestions:
                    improvement_actions.extend(metric.suggestions)
        
        # ì „ë°˜ì  ê°œì„  ì•¡ì…˜
        overall_score = self._calculate_overall_score(individual_scores)
        if overall_score < self.target_accuracy:
            gap = self.target_accuracy - overall_score
            improvement_actions.append(f"ëª©í‘œ ë‹¬ì„±ì„ ìœ„í•´ {gap:.1%} ì¶”ê°€ ê°œì„  í•„ìš”")
        
        return strengths, weaknesses, improvement_actions
    
    def _calculate_confidence_level(self, individual_scores: Dict[QualityDimension, QualityMetric]) -> float:
        """ì‹ ë¢°ë„ ê³„ì‚°"""
        
        confidences = [metric.confidence for metric in individual_scores.values()]
        return statistics.mean(confidences) if confidences else 0.0
    
    def _update_performance_tracking(self, validation_result: ValidationResult):
        """ì„±ëŠ¥ ì¶”ì  ì—…ë°ì´íŠ¸"""
        
        self.performance_metrics["total_validations"] += 1
        
        if validation_result.validation_passed:
            self.performance_metrics["passed_validations"] += 1
        else:
            self.performance_metrics["failed_validations"] += 1
        
        if validation_result.requires_reanalysis:
            self.performance_metrics["reanalysis_triggered"] += 1
        
        # ê°œë³„ ì°¨ì› ì ìˆ˜ ì¶”ì 
        for dimension, metric in validation_result.individual_scores.items():
            self.performance_metrics["average_scores"][dimension].append(metric.score)
            
            # ìµœê·¼ 10ê°œ ê²°ê³¼ë§Œ ìœ ì§€
            if len(self.performance_metrics["average_scores"][dimension]) > 10:
                self.performance_metrics["average_scores"][dimension].pop(0)
        
        # ê²€ì¦ ê¸°ë¡ ì €ì¥
        self.validation_history.append({
            "timestamp": validation_result.validation_time,
            "overall_score": validation_result.overall_score,
            "passed": validation_result.validation_passed,
            "severity": validation_result.severity.value
        })
    
    def get_performance_report(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±"""
        
        total_validations = max(1, self.performance_metrics["total_validations"])
        
        # ì°¨ì›ë³„ í‰ê·  ì ìˆ˜
        dimension_averages = {}
        for dimension, scores in self.performance_metrics["average_scores"].items():
            if scores:
                dimension_averages[dimension.value] = {
                    "average": statistics.mean(scores),
                    "trend": "ìƒìŠ¹" if len(scores) > 1 and scores[-1] > scores[0] else "í•˜ê°•",
                    "samples": len(scores)
                }
        
        # ìµœê·¼ ì„±ëŠ¥ íŠ¸ë Œë“œ
        recent_scores = [record["overall_score"] for record in list(self.validation_history)[-20:]]
        trend_analysis = "ì•ˆì •" if not recent_scores else (
            "ê°œì„ " if len(recent_scores) > 1 and recent_scores[-1] > recent_scores[0] else "ì£¼ì˜"
        )
        
        report = {
            "system_info": {
                "version": self.version,
                "target_accuracy": f"{self.target_accuracy * 100}%",
                "total_validations": self.performance_metrics["total_validations"]
            },
            
            "performance_summary": {
                "pass_rate": f"{(self.performance_metrics['passed_validations'] / total_validations * 100):.1f}%",
                "reanalysis_rate": f"{(self.performance_metrics['reanalysis_triggered'] / total_validations * 100):.1f}%",
                "average_quality": f"{statistics.mean(recent_scores) * 100:.1f}%" if recent_scores else "N/A",
                "trend": trend_analysis
            },
            
            "dimension_performance": dimension_averages,
            
            "quality_thresholds": {
                dim.value: threshold for dim, threshold in self.quality_thresholds.items()
            },
            
            "recommendations": [
                "ì§€ì†ì ì¸ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§",
                "ì„ê³„ê°’ ë¯¸ë‹¬ ì°¨ì› ì§‘ì¤‘ ê°œì„ ",
                "ì¬ë¶„ì„ íŒ¨í„´ ë¶„ì„ ë° ìµœì í™”"
            ]
        }
        
        # ê°œì„  í•„ìš” ì˜ì—­ ì‹ë³„
        low_performing_dimensions = [
            dim_name for dim_name, data in dimension_averages.items()
            if data["average"] < 0.85
        ]
        
        if low_performing_dimensions:
            report["recommendations"].insert(0, 
                f"ìš°ì„  ê°œì„  í•„ìš” ì˜ì—­: {', '.join(low_performing_dimensions)}")
        
        return report
    
    async def optimize_validation_system(self) -> Dict[str, Any]:
        """ê²€ì¦ ì‹œìŠ¤í…œ ìµœì í™”"""
        
        optimization_results = {
            "timestamp": datetime.now().isoformat(),
            "actions_taken": [],
            "threshold_adjustments": {},
            "weight_adjustments": {},
            "performance_improvements": {}
        }
        
        # 1. ì„±ëŠ¥ ë°ì´í„° ê¸°ë°˜ ì„ê³„ê°’ ì¡°ì •
        for dimension, scores in self.performance_metrics["average_scores"].items():
            if len(scores) >= 5:
                current_avg = statistics.mean(scores)
                current_threshold = self.quality_thresholds.get(dimension, 0.85)
                
                if current_avg > current_threshold + 0.1:
                    # ì„ê³„ê°’ ìƒí–¥ ì¡°ì •
                    new_threshold = min(0.99, current_threshold + 0.05)
                    self.quality_thresholds[dimension] = new_threshold
                    optimization_results["threshold_adjustments"][dimension.value] = {
                        "old": current_threshold,
                        "new": new_threshold,
                        "reason": "ì„±ëŠ¥ í–¥ìƒìœ¼ë¡œ ê¸°ì¤€ ìƒí–¥"
                    }
                elif current_avg < current_threshold - 0.15:
                    # ì„ê³„ê°’ í•˜í–¥ ì¡°ì • (ì‹ ì¤‘í•˜ê²Œ)
                    new_threshold = max(0.70, current_threshold - 0.03)
                    self.quality_thresholds[dimension] = new_threshold
                    optimization_results["threshold_adjustments"][dimension.value] = {
                        "old": current_threshold,
                        "new": new_threshold,
                        "reason": "ë‹¬ì„± ê°€ëŠ¥í•œ ìˆ˜ì¤€ìœ¼ë¡œ ì¡°ì •"
                    }
        
        # 2. ê°€ì¤‘ì¹˜ ìµœì í™”
        # ì„±ëŠ¥ì´ ë‚®ì€ ì°¨ì›ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë†’ì—¬ ë” ì—„ê²©í•˜ê²Œ ê²€ì¦
        total_validations = self.performance_metrics["total_validations"]
        if total_validations > 50:  # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆì„ ë•Œ
            for dimension, scores in self.performance_metrics["average_scores"].items():
                if scores:
                    avg_score = statistics.mean(scores)
                    current_weight = self.dimension_weights.get(dimension, 0.1)
                    
                    if avg_score < 0.8 and current_weight < 0.3:
                        # ì„±ëŠ¥ì´ ë‚®ì€ ì°¨ì›ì˜ ê°€ì¤‘ì¹˜ ì¦ê°€
                        new_weight = min(0.3, current_weight * 1.2)
                        self.dimension_weights[dimension] = new_weight
                        optimization_results["weight_adjustments"][dimension.value] = {
                            "old": current_weight,
                            "new": new_weight,
                            "reason": "ë‚®ì€ ì„±ëŠ¥ìœ¼ë¡œ ì¸í•œ ê°€ì¤‘ì¹˜ ì¦ê°€"
                        }
        
        # 3. ì‹¤í–‰ëœ ìµœì í™” ì‘ì—… ê¸°ë¡
        if optimization_results["threshold_adjustments"]:
            optimization_results["actions_taken"].append(
                f"í’ˆì§ˆ ì„ê³„ê°’ {len(optimization_results['threshold_adjustments'])}ê°œ ì¡°ì •"
            )
        
        if optimization_results["weight_adjustments"]:
            optimization_results["actions_taken"].append(
                f"ì°¨ì›ë³„ ê°€ì¤‘ì¹˜ {len(optimization_results['weight_adjustments'])}ê°œ ì¡°ì •"
            )
        
        optimization_results["actions_taken"].append("ê²€ì¦ ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ íŠœë‹ ì™„ë£Œ")
        
        logger.info(f"ğŸ”§ ê²€ì¦ ì‹œìŠ¤í…œ ìµœì í™” ì™„ë£Œ: {len(optimization_results['actions_taken'])}ê°œ ì‘ì—… ìˆ˜í–‰")
        
        return optimization_results

# í…ŒìŠ¤íŠ¸ ë° ë°ëª¨ í•¨ìˆ˜ë“¤

async def test_quality_validator_v23():
    """AI í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ v2.3 í…ŒìŠ¤íŠ¸"""
    
    print("ğŸ” ì†”ë¡œëª¬ë“œ AI í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ v2.3 í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    validator = AIQualityValidatorV23()
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 1: ê³ í’ˆì§ˆ ë¶„ì„ ê²°ê³¼
    print("\nğŸ”¹ í…ŒìŠ¤íŠ¸ 1: ê³ í’ˆì§ˆ ë‹¤ì´ì•„ëª¬ë“œ ë¶„ì„ ê²€ì¦")
    
    high_quality_content = """
# ë‹¤ì´ì•„ëª¬ë“œ 4C ì „ë¬¸ ê°ì • ë³´ê³ ì„œ

## ê°ì • ê°œìš”
- ê°ì • ì¼ì‹œ: 2025-07-13 22:30
- ê°ì • ê¸°ì¤€: GIA êµ­ì œ í‘œì¤€
- ì •í™•ë„: 99.2%

## ìƒì„¸ ë¶„ì„

### Cut (ì»·) - ë“±ê¸‰: Excellent
ì´ ë‹¤ì´ì•„ëª¬ë“œì˜ ì»· ë“±ê¸‰ì€ Excellentë¡œ í‰ê°€ë©ë‹ˆë‹¤. í…Œì´ë¸” ë¹„ìœ¨ 57%, í¬ë¼ìš´ ë†’ì´ 15%, íŒŒë¹Œë¦¬ì˜¨ ê¹Šì´ 43%ë¡œ ì´ìƒì ì¸ í”„ë¡œí¬ì…˜ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. 
ëŒ€ì¹­ì„±ê³¼ ê´‘íƒë„ ëª¨ë‘ Excellent ë“±ê¸‰ìœ¼ë¡œ, ìµœëŒ€í•œì˜ ê´‘ ë°˜ì‚¬ì™€ scintillationì„ ì œê³µí•©ë‹ˆë‹¤.

### Color (ì»¬ëŸ¬) - ë“±ê¸‰: F
GIA ì»¬ëŸ¬ ìŠ¤ì¼€ì¼ ê¸°ì¤€ Fë“±ê¸‰ìœ¼ë¡œ, ê±°ì˜ ë¬´ìƒ‰(Near Colorless) ë²”ì£¼ì— ì†í•©ë‹ˆë‹¤. 
ìœ¡ì•ˆìœ¼ë¡œëŠ” ì™„ì „íˆ ë¬´ìƒ‰ìœ¼ë¡œ ë³´ì´ë©°, í˜•ê´‘ì„±ì€ Noneìœ¼ë¡œ í™•ì¸ë˜ì–´ ìì—°ê´‘ í•˜ì—ì„œë„ ìƒ‰ìƒ ë³€í™”ê°€ ì—†ìŠµë‹ˆë‹¤.

### Clarity (í´ë˜ë¦¬í‹°) - ë“±ê¸‰: VS1
Very Slightly Included 1ë“±ê¸‰ìœ¼ë¡œ, 10ë°° í™•ëŒ€ê²½ í•˜ì—ì„œ ë¯¸ì„¸í•œ ë‚´í¬ë¬¼ì´ ê´€ì°°ë˜ë‚˜ 
ìœ¡ì•ˆìœ¼ë¡œëŠ” ì™„ì „íˆ ê¹¨ë—í•˜ê²Œ ë³´ì…ë‹ˆë‹¤. ë‚´í¬ë¬¼ì˜ ìœ„ì¹˜ê°€ crown ì˜ì—­ì— ìˆì–´ ì „ì²´ì ì¸ ì•„ë¦„ë‹¤ì›€ì— ì˜í–¥ì„ ì£¼ì§€ ì•ŠìŠµë‹ˆë‹¤.

### Carat (ìºëŸ¿) - ì¤‘ëŸ‰: 2.50ct
ì •í™•í•œ ì¤‘ëŸ‰ 2.50ìºëŸ¿ìœ¼ë¡œ, ì‹œì¥ì—ì„œ ì„ í˜¸ë„ê°€ ë†’ì€ í¬ê¸°ì…ë‹ˆë‹¤. 
ì¹˜ìˆ˜ëŠ” 8.80 x 8.85 x 5.40mmë¡œ ìš°ìˆ˜í•œ ìŠ¤í”„ë ˆë“œë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.

## ì¢…í•© í‰ê°€
**ì „ì²´ ë“±ê¸‰:** Premium
**í’ˆì§ˆ ìˆ˜ì¤€:** Investment Grade
**í¬ì†Œì„±:** ë†’ìŒ

## ì‹œì¥ ê°€ì¹˜ ë¶„ì„
**ì˜ˆìƒ ì†Œë§¤ê°€:** $45,000 - $52,000
**ì˜ˆìƒ ë„ë§¤ê°€:** $38,000 - $42,000
**ë³´í—˜ ê°€ì•¡:** $55,000

## íˆ¬ì ë¶„ì„
**íˆ¬ì ë“±ê¸‰:** A+
**ì¥ê¸° ì „ë§:** ë§¤ìš° ê¸ì •ì 
**ê¶Œì¥ì‚¬í•­:** 
1. GIA ê°ì •ì„œ ì·¨ë“ ê¶Œì¥
2. ì •ê¸°ì  ì „ë¬¸ ì ê²€ (ì—° 1íšŒ)
3. ì ì ˆí•œ ë³´í—˜ ê°€ì… í•„ìˆ˜
4. ì¥ê¸° ë³´ìœ  ê¶Œì¥ (5-10ë…„)

ì´ ë‹¤ì´ì•„ëª¬ë“œëŠ” ëª¨ë“  ì¸¡ë©´ì—ì„œ ìš°ìˆ˜í•œ í’ˆì§ˆì„ ë³´ì—¬ì£¼ë©°, íˆ¬ì ê°€ì¹˜ì™€ ìˆ˜ì§‘ ê°€ì¹˜ ëª¨ë‘ ë›°ì–´ë‚œ ê²ƒìœ¼ë¡œ í‰ê°€ë©ë‹ˆë‹¤.
    """.strip()
    
    context_info = {
        "gemstone_type": "diamond",
        "grading_standard": "gia",
        "analysis_purpose": "íˆ¬ì ìƒë‹´",
        "priority": "high"
    }
    
    validation_result = await validator.validate_comprehensive(
        content=high_quality_content,
        analysis_type="diamond_analysis",
        context_info=context_info,
        expected_quality=0.95
    )
    
    print(f"ì¢…í•© ì ìˆ˜: {validation_result.overall_score:.3f}")
    print(f"ê²€ì¦ í†µê³¼: {'âœ…' if validation_result.validation_passed else 'âŒ'}")
    print(f"ì‹¬ê°ë„: {validation_result.severity.value}")
    print(f"ì¬ë¶„ì„ í•„ìš”: {'í•„ìš”' if validation_result.requires_reanalysis else 'ë¶ˆí•„ìš”'}")
    print(f"ì‹ ë¢°ë„: {validation_result.confidence_level:.3f}")
    
    print(f"\nğŸ“Š ì°¨ì›ë³„ ì ìˆ˜:")
    for dimension, metric in validation_result.individual_scores.items():
        print(f"  {dimension.value}: {metric.score:.3f}")
    
    if validation_result.strengths:
        print(f"\nâœ… ê°•ì :")
        for strength in validation_result.strengths:
            print(f"  â€¢ {strength}")
    
    if validation_result.weaknesses:
        print(f"\nâš ï¸ ì•½ì :")
        for weakness in validation_result.weaknesses:
            print(f"  â€¢ {weakness}")
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 2: ì €í’ˆì§ˆ ë¶„ì„ ê²°ê³¼ (ì¬ë¶„ì„ íŠ¸ë¦¬ê±° í…ŒìŠ¤íŠ¸)
    print("\nğŸ”¹ í…ŒìŠ¤íŠ¸ 2: ì €í’ˆì§ˆ ë¶„ì„ ê²€ì¦ (ì¬ë¶„ì„ íŠ¸ë¦¬ê±°)")
    
    low_quality_content = """
ë‹¤ì´ì•„ëª¬ë“œë¥¼ ë´¤ëŠ”ë° ê´œì°®ì•„ ë³´ì…ë‹ˆë‹¤. í¬ê¸°ë„ ì ë‹¹í•˜ê³  ê¹¨ë—í•´ ë³´ì—¬ìš”.
ê°€ê²©ì€ ì¢€ ë¹„ì‹¼ ê²ƒ ê°™ì§€ë§Œ íˆ¬ì ê°€ì¹˜ê°€ ìˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.
ì‚¬ë„ ë  ê²ƒ ê°™ë„¤ìš”.
    """.strip()
    
    validation_result_low = await validator.validate_comprehensive(
        content=low_quality_content,
        analysis_type="diamond_analysis",
        context_info=context_info,
        expected_quality=0.95
    )
    
    print(f"ì¢…í•© ì ìˆ˜: {validation_result_low.overall_score:.3f}")
    print(f"ê²€ì¦ í†µê³¼: {'âœ…' if validation_result_low.validation_passed else 'âŒ'}")
    print(f"ì¬ë¶„ì„ í•„ìš”: {'í•„ìš”' if validation_result_low.requires_reanalysis else 'ë¶ˆí•„ìš”'}")
    
    if validation_result_low.requires_reanalysis:
        print(f"ì¬ë¶„ì„ ì „ëµ: {validation_result_low.reanalysis_strategy}")
    
    if validation_result_low.improvement_actions:
        print(f"\nğŸ”§ ê°œì„  ê¶Œì¥ì‚¬í•­:")
        for action in validation_result_low.improvement_actions[:3]:
            print(f"  â€¢ {action}")
    
    # ì„±ëŠ¥ ë¦¬í¬íŠ¸
    print(f"\nğŸ“ˆ ì‹œìŠ¤í…œ ì„±ëŠ¥ ë¦¬í¬íŠ¸:")
    performance_report = validator.get_performance_report()
    print(f"ì‹œìŠ¤í…œ ë²„ì „: {performance_report['system_info']['version']}")
    print(f"ëª©í‘œ ì •í™•ë„: {performance_report['system_info']['target_accuracy']}")
    print(f"ì´ ê²€ì¦ íšŸìˆ˜: {performance_report['system_info']['total_validations']}")
    print(f"í†µê³¼ìœ¨: {performance_report['performance_summary']['pass_rate']}")
    print(f"ì¬ë¶„ì„ìœ¨: {performance_report['performance_summary']['reanalysis_rate']}")
    
    # ì‹œìŠ¤í…œ ìµœì í™”
    print(f"\nğŸ”§ ì‹œìŠ¤í…œ ìµœì í™” ì‹¤í–‰:")
    optimization_results = await validator.optimize_validation_system()
    print(f"ìµœì í™” ì‹œê°„: {optimization_results['timestamp']}")
    print(f"ìˆ˜í–‰ëœ ì‘ì—…:")
    for action in optimization_results['actions_taken']:
        print(f"  â€¢ {action}")
    
    print("\n" + "=" * 60)
    print("âœ… AI í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ v2.3 í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    
    return validator

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    asyncio.run(test_quality_validator_v23())
