"""
ğŸ” ì†”ë¡œëª¬ë“œ AI í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ v2.3
ì‹¤ì‹œê°„ í’ˆì§ˆ ê²€ì¦ + ìë™ ì¬ë¶„ì„ + 99.2% ì •í™•ë„ ë³´ì¥

ê°œë°œì: ì „ê·¼í˜ (ì†”ë¡œëª¬ë“œ ëŒ€í‘œ)
ëª©í‘œ: AI ì‘ë‹µ í’ˆì§ˆ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ë° ìë™ ê°œì„ 
"""

import asyncio
import time
import json
import statistics
from typing import Dict, List, Optional, Any, Tuple, Union
from dataclasses import dataclass, asdict
from enum import Enum
import logging
import re
from concurrent.futures import ThreadPoolExecutor

# ë‚´ë¶€ ëª¨ë“ˆ imports
try:
    from core.jewelry_specialized_prompts_v23 import JewelrySpecializedPrompts, AnalysisType, AIModelType
    from core.hybrid_llm_manager_v23 import HybridLLMManager, AIResponse
except ImportError as e:
    logging.warning(f"ëª¨ë“ˆ import ê²½ê³ : {e}")

logger = logging.getLogger(__name__)

class QualityMetric(Enum):
    """í’ˆì§ˆ í‰ê°€ ë©”íŠ¸ë¦­"""
    ACCURACY = "accuracy"
    COMPLETENESS = "completeness"
    JEWELRY_EXPERTISE = "jewelry_expertise"
    CONSISTENCY = "consistency"
    CLARITY = "clarity"
    ACTIONABILITY = "actionability"

class QualityLevel(Enum):
    """í’ˆì§ˆ ë“±ê¸‰"""
    EXCELLENT = "excellent"  # 95%+
    GOOD = "good"           # 85-94%
    FAIR = "fair"           # 70-84%
    POOR = "poor"           # <70%

@dataclass
class QualityScore:
    """í’ˆì§ˆ ì ìˆ˜ ë°ì´í„° í´ë˜ìŠ¤"""
    metric: QualityMetric
    score: float  # 0.0 - 1.0
    confidence: float  # 0.0 - 1.0
    details: Dict[str, Any]
    improvement_suggestions: List[str]

@dataclass
class QualityReport:
    """ì¢…í•© í’ˆì§ˆ ë³´ê³ ì„œ"""
    overall_score: float
    quality_level: QualityLevel
    metric_scores: Dict[QualityMetric, QualityScore]
    jewelry_expertise_score: float
    consistency_score: float
    needs_reanalysis: bool
    improvement_priority: List[str]
    timestamp: float

class JewelryExpertiseEvaluator:
    """ì£¼ì–¼ë¦¬ ì „ë¬¸ì„± í‰ê°€ê¸°"""
    
    def __init__(self):
        self.jewelry_keywords = self._initialize_jewelry_keywords()
        self.technical_terms = self._initialize_technical_terms()
        self.grading_standards = self._initialize_grading_standards()
        
    def _initialize_jewelry_keywords(self) -> Dict[str, List[str]]:
        """ì£¼ì–¼ë¦¬ ì „ë¬¸ í‚¤ì›Œë“œ ë°ì´í„°ë² ì´ìŠ¤"""
        return {
            "diamond_4c": [
                "ìºëŸ¿", "carat", "ì¤‘ëŸ‰", "ë¬´ê²Œ",
                "ì»·", "cut", "ì—°ë§ˆ", "ë¸Œë¦´ë¦¬ì–¸íŠ¸", "í”„ë¡œí¬ì…˜", "í…Œì´ë¸”", "í¬ë¼ìš´", "ê±°ë“¤", "íŒŒë¹Œë¦¬ì˜¨", "í˜ë¦¿",
                "ì»¬ëŸ¬", "color", "ë¬´ìƒ‰", "colorless", "near colorless", "faint yellow", "D", "E", "F", "G", "H", "I", "J",
                "í´ë˜ë¦¬í‹°", "clarity", "íˆ¬ëª…ë„", "ë‚´í¬ë¬¼", "inclusion", "ë¸”ë ˆë¯¸ì‰¬", "blemish", "FL", "IF", "VVS", "VS", "SI", "I1", "I2", "I3",
                "í˜ë”", "feather", "í´ë¼ìš°ë“œ", "cloud", "í¬ë¦¬ìŠ¤íƒˆ", "crystal", "í•€í¬ì¸íŠ¸", "pinpoint"
            ],
            "colored_stones": [
                "ë£¨ë¹„", "ruby", "ì‚¬íŒŒì´ì–´", "sapphire", "ì—ë©”ë„ë“œ", "emerald",
                "í”¼ì ¼ ë¸”ëŸ¬ë“œ", "pigeon blood", "ì½”ë¥¸í”Œë¼ì›Œ", "cornflower", "íŒŒë“œíŒŒë¼ì°¨", "padparadscha",
                "ë¯¸ì–€ë§ˆ", "myanmar", "ë²„ë§ˆ", "burma", "ìŠ¤ë¦¬ë‘ì¹´", "sri lanka", "ì„¸ì¼ë¡ ", "ceylon",
                "ì½œë¡¬ë¹„ì•„", "colombia", "ì ë¹„ì•„", "zambia", "ë¸Œë¼ì§ˆ", "brazil",
                "ê°€ì—´", "heated", "heat treatment", "ì˜¤ì¼ë§", "oiling", "ìˆ˜ì§€ì¶©ì „", "resin filling",
                "ì•„ìŠ¤í…Œë¦¬ì¦˜", "asterism", "ìƒ¤í† ì–€ì‹œ", "chatoyancy", "ì‹¤í¬", "silk", "ëŸ¬í‹¸", "rutile"
            ],
            "settings_design": [
                "í”„ë¡±", "prong", "ë² ì ¤", "bezel", "ì±„ë„", "channel", "íŒŒë² ", "pave", "ë§ˆì´í¬ë¡œíŒŒë² ", "micropave",
                "í…ì…˜", "tension", "ë¹„ë“œ", "bead", "ê·¸ë©", "grab", "ë°”", "bar",
                "ì†”ë¦¬í…Œì–´", "solitaire", "í—¤ì¼ë¡œ", "halo", "ì“°ë¦¬ìŠ¤í†¤", "three stone", "ì´í„°ë‹ˆí‹°", "eternity",
                "ì•„ë¥´ë°ì½”", "art deco", "ë¹…í† ë¦¬ì•ˆ", "victorian", "ì—ë“œì›Œë””ì•ˆ", "edwardian", "ì•„ë¥´ëˆ„ë³´", "art nouveau"
            ],
            "metals_materials": [
                "í”Œë˜í‹°ë‚˜", "platinum", "18K", "14K", "10K", "í™”ì´íŠ¸ê³¨ë“œ", "white gold", "ì˜ë¡œìš°ê³¨ë“œ", "yellow gold",
                "ë¡œì¦ˆê³¨ë“œ", "rose gold", "íŒ”ë¼ë“", "palladium", "ë¡œë“", "rhodium", "ì´ë¦¬ë“", "iridium",
                "ìŠ¤í„¸ë§ì‹¤ë²„", "sterling silver", "í‹°íƒ€ëŠ„", "titanium", "íƒ„íƒˆë¥¨", "tantalum"
            ],
            "certification": [
                "GIA", "AGS", "SSEF", "GÃ¼belin", "AGL", "GUILD", "Lotus", "AIGS",
                "ê°ì •ì„œ", "certificate", "ì¸ì¦ì„œ", "certification", "ê·¸ë ˆì´ë”©", "grading",
                "ë ˆì´ì €ì¸ìŠ¤í¬ë¦½ì…˜", "laser inscription", "í”Œë¡œíŒ…", "plotting", "ë‹¤ì´ì–´ê·¸ë¨", "diagram"
            ]
        }
    
    def _initialize_technical_terms(self) -> Dict[str, float]:
        """ê¸°ìˆ ì  ìš©ì–´ë³„ ì „ë¬¸ì„± ê°€ì¤‘ì¹˜"""
        return {
            # ë‹¤ì´ì•„ëª¬ë“œ ì „ë¬¸ ìš©ì–´ (ë†’ì€ ê°€ì¤‘ì¹˜)
            "ì•„ë‹¤ë§Œí‹´": 0.9, "adamantine": 0.9,
            "ë””ìŠ¤í¼ì…˜": 0.9, "dispersion": 0.9,
            "ë¸Œë¦´ë¦¬ì–¸ìŠ¤": 0.8, "brilliance": 0.8,
            "ì„¬ê´‘": 0.8, "fire": 0.8, "scintillation": 0.8,
            
            # ìœ ìƒ‰ë³´ì„ ì „ë¬¸ ìš©ì–´
            "ë‹¤ìƒ‰ì„±": 0.9, "pleochroism": 0.9,
            "êµ´ì ˆë¥ ": 0.8, "refractive index": 0.8,
            "ì´ì¤‘êµ´ì ˆ": 0.8, "birefringence": 0.8,
            "ê´‘íƒ": 0.7, "luster": 0.7,
            
            # ê°ì • ê¸°ìˆ  ìš©ì–´
            "ë¶„ê´‘ë¶„ì„": 0.9, "spectroscopy": 0.9,
            "í¬í† ë£¨ë¯¸ë„¤ì„¼ìŠ¤": 0.9, "photoluminescence": 0.9,
            "Xì„ í˜•ê´‘": 0.8, "x-ray fluorescence": 0.8,
            "ì ì™¸ì„ ë¶„ê´‘": 0.8, "infrared spectroscopy": 0.8,
            
            # ì¼ë°˜ ì£¼ì–¼ë¦¬ ìš©ì–´ (ì¤‘ê°„ ê°€ì¤‘ì¹˜)
            "ì„¸íŒ…": 0.6, "setting": 0.6,
            "ë§ˆìš´íŒ…": 0.6, "mounting": 0.6,
            "ë°´ë“œ": 0.5, "band": 0.5,
            "ìƒ¹í¬": 0.5, "shank": 0.5
        }
    
    def _initialize_grading_standards(self) -> Dict[str, Dict[str, float]]:
        """ë“±ê¸‰ í‘œì¤€ë³„ ì •í™•ì„± ì ìˆ˜"""
        return {
            "gia_color": {
                "D": 1.0, "E": 1.0, "F": 1.0, "G": 0.9, "H": 0.9, "I": 0.9, "J": 0.9,
                "K": 0.8, "L": 0.8, "M": 0.8, "N": 0.7, "O": 0.7, "P": 0.7,
                "Q": 0.6, "R": 0.6, "S": 0.5, "T": 0.5, "U": 0.5, "V": 0.4,
                "W": 0.4, "X": 0.4, "Y": 0.3, "Z": 0.3
            },
            "gia_clarity": {
                "FL": 1.0, "IF": 0.95, "VVS1": 0.9, "VVS2": 0.85,
                "VS1": 0.8, "VS2": 0.75, "SI1": 0.7, "SI2": 0.65,
                "I1": 0.5, "I2": 0.3, "I3": 0.1
            },
            "cut_grades": {
                "Excellent": 1.0, "Very Good": 0.8, "Good": 0.6, "Fair": 0.4, "Poor": 0.2,
                "ìµœìš°ìˆ˜": 1.0, "ìš°ìˆ˜": 0.8, "ì–‘í˜¸": 0.6, "ë³´í†µ": 0.4, "ë¶ˆëŸ‰": 0.2
            }
        }
    
    def evaluate_jewelry_expertise(self, content: str, analysis_type: AnalysisType) -> float:
        """ì£¼ì–¼ë¦¬ ì „ë¬¸ì„± ì ìˆ˜ í‰ê°€"""
        
        content_lower = content.lower()
        total_score = 0.0
        max_score = 0.0
        
        # ë¶„ì„ íƒ€ì…ë³„ ê´€ë ¨ í‚¤ì›Œë“œ ì„ íƒ
        relevant_categories = self._get_relevant_categories(analysis_type)
        
        for category in relevant_categories:
            if category in self.jewelry_keywords:
                category_score = 0.0
                category_max = len(self.jewelry_keywords[category])
                
                for keyword in self.jewelry_keywords[category]:
                    if keyword.lower() in content_lower:
                        # í‚¤ì›Œë“œ ë¹ˆë„ìˆ˜ ê³ ë ¤
                        frequency = content_lower.count(keyword.lower())
                        category_score += min(frequency * 0.1, 0.3)  # ìµœëŒ€ 0.3ì 
                
                total_score += min(category_score, 1.0)  # ì¹´í…Œê³ ë¦¬ë‹¹ ìµœëŒ€ 1ì 
                max_score += 1.0
        
        # ê¸°ìˆ ì  ìš©ì–´ ë³´ë„ˆìŠ¤
        technical_bonus = 0.0
        for term, weight in self.technical_terms.items():
            if term.lower() in content_lower:
                technical_bonus += weight * 0.1
        
        total_score += min(technical_bonus, 0.5)  # ìµœëŒ€ 0.5 ë³´ë„ˆìŠ¤
        max_score += 0.5
        
        # ë“±ê¸‰ í‘œì¤€ ì •í™•ì„± í™•ì¸
        grading_accuracy = self._check_grading_accuracy(content)
        total_score += grading_accuracy * 0.3
        max_score += 0.3
        
        # ì •ê·œí™”
        expertise_score = total_score / max_score if max_score > 0 else 0.0
        return min(expertise_score, 1.0)
    
    def _get_relevant_categories(self, analysis_type: AnalysisType) -> List[str]:
        """ë¶„ì„ íƒ€ì…ë³„ ê´€ë ¨ ì¹´í…Œê³ ë¦¬ ë°˜í™˜"""
        mapping = {
            AnalysisType.DIAMOND_4C: ["diamond_4c", "certification", "settings_design"],
            AnalysisType.COLORED_STONE: ["colored_stones", "certification", "metals_materials"],
            AnalysisType.JEWELRY_DESIGN: ["settings_design", "metals_materials", "colored_stones"],
            AnalysisType.BUSINESS_INSIGHT: ["certification", "diamond_4c", "colored_stones"],
            AnalysisType.CERTIFICATION: ["certification", "diamond_4c", "colored_stones"],
            AnalysisType.APPRAISAL: ["diamond_4c", "colored_stones", "certification"]
        }
        return mapping.get(analysis_type, ["diamond_4c", "colored_stones"])
    
    def _check_grading_accuracy(self, content: str) -> float:
        """ë“±ê¸‰ í‘œì¤€ ì •í™•ì„± ê²€ì‚¬"""
        accuracy_score = 0.0
        checks = 0
        
        # GIA ì»¬ëŸ¬ ë“±ê¸‰ í™•ì¸
        color_pattern = r'[D-Z]\s*(?:ì»¬ëŸ¬|color|ë“±ê¸‰)'
        color_matches = re.findall(color_pattern, content, re.IGNORECASE)
        if color_matches:
            checks += 1
            # ì‹¤ì œ GIA í‘œì¤€ì— ë§ëŠ”ì§€ í™•ì¸ (ê°„ë‹¨í•œ ì˜ˆì‹œ)
            valid_colors = ['D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']
            for match in color_matches:
                if any(color in match.upper() for color in valid_colors):
                    accuracy_score += 1.0
        
        # GIA í´ë˜ë¦¬í‹° ë“±ê¸‰ í™•ì¸  
        clarity_pattern = r'(?:FL|IF|VVS1|VVS2|VS1|VS2|SI1|SI2|I1|I2|I3)'
        clarity_matches = re.findall(clarity_pattern, content, re.IGNORECASE)
        if clarity_matches:
            checks += 1
            accuracy_score += 1.0  # íŒ¨í„´ì´ ë§ìœ¼ë©´ ì •í™•í•˜ë‹¤ê³  ê°€ì •
        
        return accuracy_score / checks if checks > 0 else 0.0

class ConsistencyChecker:
    """ì¼ê´€ì„± ê²€ì‚¬ê¸°"""
    
    def __init__(self):
        self.previous_analyses = []
        self.inconsistency_patterns = self._initialize_inconsistency_patterns()
    
    def _initialize_inconsistency_patterns(self) -> Dict[str, List[str]]:
        """ì¼ê´€ì„± ê²€ì‚¬ íŒ¨í„´"""
        return {
            "contradictory_grades": [
                ("excellent", "poor"), ("ìµœìš°ìˆ˜", "ë¶ˆëŸ‰"),
                ("high quality", "low quality"), ("ê³ í’ˆì§ˆ", "ì €í’ˆì§ˆ"),
                ("premium", "basic"), ("í”„ë¦¬ë¯¸ì—„", "ê¸°ë³¸")
            ],
            "price_inconsistencies": [
                ("expensive", "cheap"), ("ë¹„ì‹¸", "ì €ë ´"),
                ("valuable", "worthless"), ("ê°€ì¹˜ìˆ", "ê°€ì¹˜ì—†")
            ],
            "technical_contradictions": [
                ("flawless", "included"), ("ë¬´ê²°ì ", "ë‚´í¬ë¬¼"),
                ("colorless", "yellow"), ("ë¬´ìƒ‰", "ë…¸ë€ìƒ‰")
            ]
        }
    
    def check_internal_consistency(self, content: str) -> float:
        """ë‚´ë¶€ ì¼ê´€ì„± ê²€ì‚¬"""
        
        content_lower = content.lower()
        inconsistency_count = 0
        total_checks = 0
        
        for category, patterns in self.inconsistency_patterns.items():
            for positive, negative in patterns:
                total_checks += 1
                if positive.lower() in content_lower and negative.lower() in content_lower:
                    # ëª¨ìˆœì ì¸ í‘œí˜„ì´ ë™ì‹œì— ë‚˜íƒ€ë‚¨
                    inconsistency_count += 1
        
        # ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚° (ë†’ì„ìˆ˜ë¡ ì¼ê´€ì„±ì´ ì¢‹ìŒ)
        consistency_score = 1.0 - (inconsistency_count / total_checks) if total_checks > 0 else 1.0
        return max(consistency_score, 0.0)
    
    def check_cross_analysis_consistency(self, current_analysis: str, analysis_type: AnalysisType) -> float:
        """ê³¼ê±° ë¶„ì„ê³¼ì˜ ì¼ê´€ì„± ê²€ì‚¬"""
        
        if not self.previous_analyses:
            return 1.0  # ì²« ë¶„ì„ì´ë¯€ë¡œ ì™„ë²½í•œ ì¼ê´€ì„±
        
        # ìœ ì‚¬í•œ ë¶„ì„ íƒ€ì…ì˜ ê³¼ê±° ê²°ê³¼ì™€ ë¹„êµ
        similar_analyses = [
            analysis for analysis in self.previous_analyses 
            if analysis.get('type') == analysis_type
        ]
        
        if not similar_analyses:
            return 1.0
        
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ìœ ì‚¬ì„± ê²€ì‚¬
        current_keywords = set(current_analysis.lower().split())
        
        similarity_scores = []
        for past_analysis in similar_analyses[-3:]:  # ìµœê·¼ 3ê°œë§Œ ë¹„êµ
            past_keywords = set(past_analysis.get('content', '').lower().split())
            
            # Jaccard ìœ ì‚¬ë„ ê³„ì‚°
            intersection = len(current_keywords.intersection(past_keywords))
            union = len(current_keywords.union(past_keywords))
            
            similarity = intersection / union if union > 0 else 0.0
            similarity_scores.append(similarity)
        
        # í‰ê·  ìœ ì‚¬ë„ë¥¼ ì¼ê´€ì„± ì ìˆ˜ë¡œ ì‚¬ìš©
        return statistics.mean(similarity_scores) if similarity_scores else 1.0
    
    def add_analysis_record(self, content: str, analysis_type: AnalysisType):
        """ë¶„ì„ ê¸°ë¡ ì¶”ê°€"""
        record = {
            'content': content,
            'type': analysis_type,
            'timestamp': time.time()
        }
        
        self.previous_analyses.append(record)
        
        # ìµœëŒ€ 100ê°œê¹Œì§€ë§Œ ë³´ê´€
        if len(self.previous_analyses) > 100:
            self.previous_analyses = self.previous_analyses[-100:]

class AIQualityValidator:
    """AI í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ ë©”ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.jewelry_evaluator = JewelryExpertiseEvaluator()
        self.consistency_checker = ConsistencyChecker()
        self.quality_history = []
        self.reanalysis_threshold = 0.7  # 70% ë¯¸ë§Œ ì‹œ ì¬ë¶„ì„
        self.target_accuracy = 0.992  # 99.2% ëª©í‘œ
        
    async def validate_ai_response(self, 
                                 ai_response: AIResponse, 
                                 analysis_type: AnalysisType,
                                 original_request: str) -> QualityReport:
        """AI ì‘ë‹µ ì¢…í•© í’ˆì§ˆ ê²€ì¦"""
        
        start_time = time.time()
        
        # ê° ë©”íŠ¸ë¦­ë³„ ì ìˆ˜ ê³„ì‚°
        metric_scores = {}
        
        # 1. ì •í™•ì„± (Accuracy)
        accuracy_score = await self._evaluate_accuracy(ai_response, analysis_type)
        metric_scores[QualityMetric.ACCURACY] = accuracy_score
        
        # 2. ì™„ì„±ë„ (Completeness)
        completeness_score = self._evaluate_completeness(ai_response.content, analysis_type)
        metric_scores[QualityMetric.COMPLETENESS] = completeness_score
        
        # 3. ì£¼ì–¼ë¦¬ ì „ë¬¸ì„± (Jewelry Expertise)
        expertise_score = self.jewelry_evaluator.evaluate_jewelry_expertise(
            ai_response.content, analysis_type
        )
        jewelry_expertise_score = QualityScore(
            metric=QualityMetric.JEWELRY_EXPERTISE,
            score=expertise_score,
            confidence=0.9,
            details={"analysis_type": analysis_type.value},
            improvement_suggestions=self._get_expertise_suggestions(expertise_score)
        )
        metric_scores[QualityMetric.JEWELRY_EXPERTISE] = jewelry_expertise_score
        
        # 4. ì¼ê´€ì„± (Consistency)
        internal_consistency = self.consistency_checker.check_internal_consistency(ai_response.content)
        cross_consistency = self.consistency_checker.check_cross_analysis_consistency(
            ai_response.content, analysis_type
        )
        consistency_score = (internal_consistency + cross_consistency) / 2
        
        consistency_quality = QualityScore(
            metric=QualityMetric.CONSISTENCY,
            score=consistency_score,
            confidence=0.8,
            details={
                "internal_consistency": internal_consistency,
                "cross_consistency": cross_consistency
            },
            improvement_suggestions=self._get_consistency_suggestions(consistency_score)
        )
        metric_scores[QualityMetric.CONSISTENCY] = consistency_quality
        
        # 5. ëª…í™•ì„± (Clarity)
        clarity_score = self._evaluate_clarity(ai_response.content)
        metric_scores[QualityMetric.CLARITY] = clarity_score
        
        # 6. ì‹¤í–‰ê°€ëŠ¥ì„± (Actionability)
        actionability_score = self._evaluate_actionability(ai_response.content, analysis_type)
        metric_scores[QualityMetric.ACTIONABILITY] = actionability_score
        
        # ì¢…í•© ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘í‰ê· )
        weights = {
            QualityMetric.ACCURACY: 0.25,
            QualityMetric.JEWELRY_EXPERTISE: 0.25,
            QualityMetric.COMPLETENESS: 0.15,
            QualityMetric.CONSISTENCY: 0.15,
            QualityMetric.CLARITY: 0.1,
            QualityMetric.ACTIONABILITY: 0.1
        }
        
        overall_score = sum(
            metric_scores[metric].score * weight 
            for metric, weight in weights.items()
        )
        
        # í’ˆì§ˆ ë“±ê¸‰ ê²°ì •
        quality_level = self._determine_quality_level(overall_score)
        
        # ì¬ë¶„ì„ í•„ìš” ì—¬ë¶€ íŒë‹¨
        needs_reanalysis = overall_score < self.reanalysis_threshold
        
        # ê°œì„  ìš°ì„ ìˆœìœ„ ê²°ì •
        improvement_priority = self._determine_improvement_priority(metric_scores)
        
        # í’ˆì§ˆ ë³´ê³ ì„œ ìƒì„±
        quality_report = QualityReport(
            overall_score=overall_score,
            quality_level=quality_level,
            metric_scores=metric_scores,
            jewelry_expertise_score=expertise_score,
            consistency_score=consistency_score,
            needs_reanalysis=needs_reanalysis,
            improvement_priority=improvement_priority,
            timestamp=time.time()
        )
        
        # ê¸°ë¡ ì¶”ê°€
        self.quality_history.append(quality_report)
        self.consistency_checker.add_analysis_record(ai_response.content, analysis_type)
        
        processing_time = time.time() - start_time
        logger.info(f"ğŸ” í’ˆì§ˆ ê²€ì¦ ì™„ë£Œ: {overall_score:.3f} ({quality_level.value}) - {processing_time:.2f}ì´ˆ")
        
        return quality_report
    
    async def _evaluate_accuracy(self, ai_response: AIResponse, analysis_type: AnalysisType) -> QualityScore:
        """ì •í™•ì„± í‰ê°€"""
        
        # AI ëª¨ë¸ ìì²´ ì‹ ë¢°ë„
        model_confidence = ai_response.confidence
        
        # ê¸°ìˆ ì  ì •í™•ì„± í‰ê°€ (í‚¤ì›Œë“œ ê¸°ë°˜)
        technical_accuracy = self._check_technical_accuracy(ai_response.content, analysis_type)
        
        # êµ¬ì¡°ì  ì™„ì„±ë„
        structural_accuracy = self._check_structural_accuracy(ai_response.content, analysis_type)
        
        # ì¢…í•© ì •í™•ì„± ì ìˆ˜
        accuracy = (model_confidence * 0.4 + technical_accuracy * 0.4 + structural_accuracy * 0.2)
        
        return QualityScore(
            metric=QualityMetric.ACCURACY,
            score=accuracy,
            confidence=0.85,
            details={
                "model_confidence": model_confidence,
                "technical_accuracy": technical_accuracy,
                "structural_accuracy": structural_accuracy
            },
            improvement_suggestions=self._get_accuracy_suggestions(accuracy)
        )
    
    def _evaluate_completeness(self, content: str, analysis_type: AnalysisType) -> QualityScore:
        """ì™„ì„±ë„ í‰ê°€"""
        
        required_elements = self._get_required_elements(analysis_type)
        present_elements = []
        
        content_lower = content.lower()
        for element in required_elements:
            if any(keyword.lower() in content_lower for keyword in element['keywords']):
                present_elements.append(element['name'])
        
        completeness_ratio = len(present_elements) / len(required_elements)
        
        # ë‚´ìš© ê¸¸ì´ ë³´ë„ˆìŠ¤ (ë„ˆë¬´ ì§§ìœ¼ë©´ ê°ì )
        length_bonus = min(len(content) / 1000, 0.2)  # ìµœëŒ€ 0.2 ë³´ë„ˆìŠ¤
        
        completeness_score = min(completeness_ratio + length_bonus, 1.0)
        
        return QualityScore(
            metric=QualityMetric.COMPLETENESS,
            score=completeness_score,
            confidence=0.9,
            details={
                "required_elements": len(required_elements),
                "present_elements": len(present_elements),
                "missing_elements": [elem['name'] for elem in required_elements 
                                   if elem['name'] not in present_elements],
                "content_length": len(content)
            },
            improvement_suggestions=self._get_completeness_suggestions(completeness_score)
        )
    
    def _evaluate_clarity(self, content: str) -> QualityScore:
        """ëª…í™•ì„± í‰ê°€"""
        
        # ë¬¸ì¥ ê¸¸ì´ ë¶„ì„ (ë„ˆë¬´ ê¸¸ë©´ ê°ì )
        sentences = content.split('.')
        avg_sentence_length = statistics.mean([len(s.split()) for s in sentences if s.strip()])
        
        # ì´ìƒì ì¸ ë¬¸ì¥ ê¸¸ì´: 15-25 ë‹¨ì–´
        length_score = 1.0 if 15 <= avg_sentence_length <= 25 else max(0.5, 1.0 - abs(avg_sentence_length - 20) / 20)
        
        # êµ¬ì¡°í™” ì •ë„ (ì œëª©, ë²ˆí˜¸, ë¶ˆë › í¬ì¸íŠ¸ ë“±)
        structure_indicators = ['##', '###', '1.', '2.', '3.', 'â€¢', '-', '*']
        structure_score = min(sum(1 for indicator in structure_indicators if indicator in content) / 5, 1.0)
        
        # ì „ë¬¸ ìš©ì–´ì™€ ì¼ë°˜ ìš©ì–´ì˜ ê· í˜•
        total_words = len(content.split())
        technical_word_ratio = sum(1 for word in content.split() 
                                 if any(tech_term in word.lower() 
                                       for tech_term in self.jewelry_evaluator.technical_terms.keys())) / total_words
        
        # ì´ìƒì ì¸ ì „ë¬¸ ìš©ì–´ ë¹„ìœ¨: 5-15%
        term_balance_score = 1.0 if 0.05 <= technical_word_ratio <= 0.15 else max(0.3, 1.0 - abs(technical_word_ratio - 0.1) / 0.1)
        
        clarity_score = (length_score * 0.3 + structure_score * 0.4 + term_balance_score * 0.3)
        
        return QualityScore(
            metric=QualityMetric.CLARITY,
            score=clarity_score,
            confidence=0.8,
            details={
                "avg_sentence_length": avg_sentence_length,
                "structure_score": structure_score,
                "technical_word_ratio": technical_word_ratio
            },
            improvement_suggestions=self._get_clarity_suggestions(clarity_score)
        )
    
    def _evaluate_actionability(self, content: str, analysis_type: AnalysisType) -> QualityScore:
        """ì‹¤í–‰ê°€ëŠ¥ì„± í‰ê°€"""
        
        actionable_keywords = [
            "ì¶”ì²œ", "ê¶Œì¥", "ì œì•ˆ", "ì¡°ì–¸", "ê³ ë ¤", "í™•ì¸", "ê²€í† ", "í‰ê°€",
            "recommend", "suggest", "advise", "consider", "check", "verify"
        ]
        
        content_lower = content.lower()
        actionable_count = sum(1 for keyword in actionable_keywords if keyword in content_lower)
        
        # êµ¬ì²´ì ì¸ ìˆ«ìë‚˜ ë²”ìœ„ ì œì‹œ
        numeric_pattern = r'\$?[\d,]+\.?\d*\s*(?:ë‹¬ëŸ¬|ì›|USD|KRW|ìºëŸ¿|mm|%)'
        numeric_matches = len(re.findall(numeric_pattern, content, re.IGNORECASE))
        
        # êµ¬ì²´ì ì¸ ë“±ê¸‰ì´ë‚˜ í‰ê°€ ì œì‹œ
        grade_patterns = [
            r'[A-Z]\+?', r'[0-9]\.?[0-9]?ì ', r'[0-9]+%', 
            r'(?:ìš°ìˆ˜|ì–‘í˜¸|ë³´í†µ|ë¶ˆëŸ‰)', r'(?:excellent|good|fair|poor)'
        ]
        grade_matches = sum(len(re.findall(pattern, content, re.IGNORECASE)) for pattern in grade_patterns)
        
        actionability_score = min((actionable_count * 0.1 + numeric_matches * 0.05 + grade_matches * 0.05), 1.0)
        
        return QualityScore(
            metric=QualityMetric.ACTIONABILITY,
            score=actionability_score,
            confidence=0.75,
            details={
                "actionable_keywords": actionable_count,
                "numeric_references": numeric_matches,
                "grade_references": grade_matches
            },
            improvement_suggestions=self._get_actionability_suggestions(actionability_score)
        )
    
    def _check_technical_accuracy(self, content: str, analysis_type: AnalysisType) -> float:
        """ê¸°ìˆ ì  ì •í™•ì„± ê²€ì‚¬"""
        
        # ì˜ëª»ëœ ê¸°ìˆ  ì •ë³´ íŒ¨í„´ ê²€ì‚¬
        error_patterns = {
            "impossible_grades": [
                r'[A-C]\s*(?:ì»¬ëŸ¬|color)',  # A, B, C ì»¬ëŸ¬ëŠ” ì¡´ì¬í•˜ì§€ ì•ŠìŒ
                r'I[4-9]',  # I4 ì´ìƒ í´ë˜ë¦¬í‹°ëŠ” ì—†ìŒ
            ],
            "inconsistent_values": [
                r'[0-9]+\s*ìºëŸ¿.*?[0-9]+mm',  # ìºëŸ¿ê³¼ í¬ê¸°ê°€ ë¹„ë¡€í•˜ì§€ ì•ŠëŠ” ê²½ìš°
            ]
        }
        
        error_count = 0
        for category, patterns in error_patterns.items():
            for pattern in patterns:
                if re.search(pattern, content, re.IGNORECASE):
                    error_count += 1
        
        # ê¸°ìˆ ì  ì •í™•ì„± ì ìˆ˜ (ì—ëŸ¬ê°€ ì ì„ìˆ˜ë¡ ë†’ìŒ)
        return max(0.0, 1.0 - error_count * 0.2)
    
    def _check_structural_accuracy(self, content: str, analysis_type: AnalysisType) -> float:
        """êµ¬ì¡°ì  ì •í™•ì„± ê²€ì‚¬"""
        
        expected_sections = {
            AnalysisType.DIAMOND_4C: ["4C", "ìºëŸ¿", "ì»·", "ì»¬ëŸ¬", "í´ë˜ë¦¬í‹°"],
            AnalysisType.COLORED_STONE: ["ë³´ì„", "ìƒ‰ìƒ", "ì›ì‚°ì§€", "ì²˜ë¦¬"],
            AnalysisType.JEWELRY_DESIGN: ["ë””ìì¸", "ìŠ¤íƒ€ì¼", "ì†Œì¬", "ì œì‘"],
            AnalysisType.BUSINESS_INSIGHT: ["ì‹œì¥", "ê°€ê²©", "íŠ¸ë Œë“œ", "ì „ëµ"]
        }
        
        required_sections = expected_sections.get(analysis_type, [])
        content_lower = content.lower()
        
        present_sections = sum(1 for section in required_sections 
                             if section.lower() in content_lower)
        
        return present_sections / len(required_sections) if required_sections else 1.0
    
    def _get_required_elements(self, analysis_type: AnalysisType) -> List[Dict[str, Any]]:
        """ë¶„ì„ íƒ€ì…ë³„ í•„ìˆ˜ ìš”ì†Œ"""
        
        elements_map = {
            AnalysisType.DIAMOND_4C: [
                {"name": "ìºëŸ¿", "keywords": ["ìºëŸ¿", "carat", "ì¤‘ëŸ‰"]},
                {"name": "ì»·", "keywords": ["ì»·", "cut", "ì—°ë§ˆ"]},
                {"name": "ì»¬ëŸ¬", "keywords": ["ì»¬ëŸ¬", "color", "ìƒ‰ìƒ"]},
                {"name": "í´ë˜ë¦¬í‹°", "keywords": ["í´ë˜ë¦¬í‹°", "clarity", "íˆ¬ëª…ë„"]},
                {"name": "ê°€ê²©", "keywords": ["ê°€ê²©", "price", "ë¹„ìš©", "ë‹¬ëŸ¬", "ì›"]}
            ],
            AnalysisType.COLORED_STONE: [
                {"name": "ë³´ì„ì¢…ë¥˜", "keywords": ["ë£¨ë¹„", "ì‚¬íŒŒì´ì–´", "ì—ë©”ë„ë“œ", "ruby", "sapphire", "emerald"]},
                {"name": "ìƒ‰ìƒí‰ê°€", "keywords": ["ìƒ‰ìƒ", "color", "ì±„ë„", "ëª…ë„"]},
                {"name": "ì›ì‚°ì§€", "keywords": ["ì›ì‚°ì§€", "ë¯¸ì–€ë§ˆ", "ìŠ¤ë¦¬ë‘ì¹´", "ì½œë¡¬ë¹„ì•„", "origin"]},
                {"name": "ì²˜ë¦¬ì—¬ë¶€", "keywords": ["ì²˜ë¦¬", "ê°€ì—´", "ì˜¤ì¼ë§", "treatment", "heated"]},
                {"name": "í’ˆì§ˆë“±ê¸‰", "keywords": ["ë“±ê¸‰", "í’ˆì§ˆ", "AAA", "AA", "grade", "quality"]}
            ],
            AnalysisType.JEWELRY_DESIGN: [
                {"name": "ë””ìì¸ìŠ¤íƒ€ì¼", "keywords": ["ìŠ¤íƒ€ì¼", "ë””ìì¸", "ì•„ë¥´ë°ì½”", "ë¹…í† ë¦¬ì•ˆ", "style"]},
                {"name": "ì†Œì¬ë¶„ì„", "keywords": ["ê¸ˆì†", "í”Œë˜í‹°ë‚˜", "ê³¨ë“œ", "metal", "platinum", "gold"]},
                {"name": "ì œì‘ê¸°ë²•", "keywords": ["ì œì‘", "ì„¸íŒ…", "ê¸°ë²•", "craftsmanship", "setting"]},
                {"name": "ì°©ìš©ì„±", "keywords": ["ì°©ìš©", "í¸ì•ˆ", "ì‹¤ìš©", "wearability", "comfort"]}
            ]
        }
        
        return elements_map.get(analysis_type, [])
    
    def _determine_quality_level(self, score: float) -> QualityLevel:
        """ì ìˆ˜ì— ë”°ë¥¸ í’ˆì§ˆ ë“±ê¸‰ ê²°ì •"""
        if score >= 0.95:
            return QualityLevel.EXCELLENT
        elif score >= 0.85:
            return QualityLevel.GOOD
        elif score >= 0.70:
            return QualityLevel.FAIR
        else:
            return QualityLevel.POOR
    
    def _determine_improvement_priority(self, metric_scores: Dict[QualityMetric, QualityScore]) -> List[str]:
        """ê°œì„  ìš°ì„ ìˆœìœ„ ê²°ì •"""
        
        # ì ìˆ˜ê°€ ë‚®ì€ ë©”íŠ¸ë¦­ ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_metrics = sorted(
            metric_scores.items(), 
            key=lambda x: x[1].score
        )
        
        priority_list = []
        for metric, score_obj in sorted_metrics:
            if score_obj.score < 0.8:  # 80% ë¯¸ë§Œì¸ í•­ëª©ë“¤
                priority_list.extend(score_obj.improvement_suggestions)
        
        return priority_list[:5]  # ìƒìœ„ 5ê°œ ìš°ì„ ìˆœìœ„
    
    def _get_expertise_suggestions(self, score: float) -> List[str]:
        """ì „ë¬¸ì„± ê°œì„  ì œì•ˆ"""
        if score < 0.7:
            return [
                "ì£¼ì–¼ë¦¬ ì „ë¬¸ ìš©ì–´ ì‚¬ìš© ì¦ëŒ€",
                "GIA/SSEF ë“± êµ­ì œ í‘œì¤€ ì¤€ìˆ˜",
                "ê¸°ìˆ ì  ë¶„ì„ ê¹Šì´ í–¥ìƒ",
                "ê°ì • ê¸°ê´€ ì¸ì¦ ê¸°ì¤€ ë°˜ì˜"
            ]
        elif score < 0.85:
            return [
                "ê³ ê¸‰ ì „ë¬¸ ìš©ì–´ í™œìš©",
                "ì‹œì¥ ë¶„ì„ ì „ë¬¸ì„± ê°•í™”"
            ]
        else:
            return ["í˜„ì¬ ì „ë¬¸ì„± ìˆ˜ì¤€ ìœ ì§€"]
    
    def _get_consistency_suggestions(self, score: float) -> List[str]:
        """ì¼ê´€ì„± ê°œì„  ì œì•ˆ"""
        if score < 0.7:
            return [
                "ë‚´ë¶€ ëª¨ìˆœ í‘œí˜„ ì œê±°",
                "ê³¼ê±° ë¶„ì„ê³¼ì˜ ì¼ê´€ì„± í™•ë³´",
                "ë“±ê¸‰ ê¸°ì¤€ í†µì¼ì„± ìœ ì§€"
            ]
        else:
            return ["ì¼ê´€ì„± ìˆ˜ì¤€ ì–‘í˜¸"]
    
    def _get_accuracy_suggestions(self, score: float) -> List[str]:
        """ì •í™•ì„± ê°œì„  ì œì•ˆ"""
        if score < 0.8:
            return [
                "ê¸°ìˆ ì  ì •í™•ì„± ê²€í† ",
                "êµ¬ì¡°ì  ì™„ì„±ë„ ê°œì„ ",
                "ì°¸ì¡° í‘œì¤€ ì¬í™•ì¸"
            ]
        else:
            return ["ì •í™•ì„± ìˆ˜ì¤€ ì–‘í˜¸"]
    
    def _get_completeness_suggestions(self, score: float) -> List[str]:
        """ì™„ì„±ë„ ê°œì„  ì œì•ˆ"""
        if score < 0.8:
            return [
                "í•„ìˆ˜ ë¶„ì„ ìš”ì†Œ ì¶”ê°€",
                "ë‚´ìš© ìƒì„¸ë„ ì¦ëŒ€",
                "êµ¬ì¡°ì  ì™„ì„±ë„ í–¥ìƒ"
            ]
        else:
            return ["ì™„ì„±ë„ ìˆ˜ì¤€ ì–‘í˜¸"]
    
    def _get_clarity_suggestions(self, score: float) -> List[str]:
        """ëª…í™•ì„± ê°œì„  ì œì•ˆ"""
        if score < 0.8:
            return [
                "ë¬¸ì¥ ê¸¸ì´ ìµœì í™”",
                "êµ¬ì¡°í™” ê°œì„ ",
                "ì „ë¬¸ìš©ì–´ ì„¤ëª… ì¶”ê°€"
            ]
        else:
            return ["ëª…í™•ì„± ìˆ˜ì¤€ ì–‘í˜¸"]
    
    def _get_actionability_suggestions(self, score: float) -> List[str]:
        """ì‹¤í–‰ê°€ëŠ¥ì„± ê°œì„  ì œì•ˆ"""
        if score < 0.7:
            return [
                "êµ¬ì²´ì  ì¶”ì²œì‚¬í•­ ì¶”ê°€",
                "ìˆ˜ì¹˜ì  ê·¼ê±° ì œì‹œ",
                "ì‹¤í–‰ ê°€ëŠ¥í•œ ì¡°ì–¸ í¬í•¨"
            ]
        else:
            return ["ì‹¤í–‰ê°€ëŠ¥ì„± ìˆ˜ì¤€ ì–‘í˜¸"]
    
    async def auto_reanalysis_if_needed(self, 
                                      quality_report: QualityReport,
                                      hybrid_manager: 'HybridLLMManager',
                                      original_request: Any) -> Optional[AIResponse]:
        """í’ˆì§ˆì´ ë‚®ì„ ê²½ìš° ìë™ ì¬ë¶„ì„"""
        
        if not quality_report.needs_reanalysis:
            return None
        
        logger.info(f"ğŸ”„ ìë™ ì¬ë¶„ì„ ì‹œì‘ - í’ˆì§ˆ ì ìˆ˜: {quality_report.overall_score:.3f}")
        
        # ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ë¡œ ì¬ë¶„ì„
        enhanced_request = self._enhance_request_based_on_feedback(
            original_request, quality_report.improvement_priority
        )
        
        # ì¬ë¶„ì„ ì‹¤í–‰
        reanalysis_result = await hybrid_manager.hybrid_analyze(enhanced_request)
        
        if reanalysis_result['status'] == 'success':
            logger.info("âœ… ìë™ ì¬ë¶„ì„ ì™„ë£Œ")
            return reanalysis_result
        else:
            logger.error("âŒ ìë™ ì¬ë¶„ì„ ì‹¤íŒ¨")
            return None
    
    def _enhance_request_based_on_feedback(self, original_request: Any, priorities: List[str]) -> Any:
        """í”¼ë“œë°± ê¸°ë°˜ ìš”ì²­ ê°œì„ """
        
        # ì›ë³¸ ìš”ì²­ì— ê°œì„ ì‚¬í•­ ë°˜ì˜
        enhanced_request = original_request.copy() if hasattr(original_request, 'copy') else original_request
        
        # ê°œì„  ì§€ì‹œì‚¬í•­ ì¶”ê°€
        enhancement_note = f"\n\n[ê°œì„  ìš”êµ¬ì‚¬í•­: {', '.join(priorities[:3])}]"
        
        if hasattr(enhanced_request, 'text_content') and enhanced_request.text_content:
            enhanced_request.text_content += enhancement_note
        
        return enhanced_request
    
    def get_quality_analytics(self) -> Dict[str, Any]:
        """í’ˆì§ˆ ë¶„ì„ í†µê³„"""
        
        if not self.quality_history:
            return {"message": "í’ˆì§ˆ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}
        
        recent_reports = self.quality_history[-20:]  # ìµœê·¼ 20ê°œ
        
        avg_overall_score = statistics.mean([r.overall_score for r in recent_reports])
        
        quality_trend = []
        for i in range(1, len(recent_reports)):
            trend = recent_reports[i].overall_score - recent_reports[i-1].overall_score
            quality_trend.append(trend)
        
        avg_trend = statistics.mean(quality_trend) if quality_trend else 0.0
        
        # ë©”íŠ¸ë¦­ë³„ í‰ê·  ì ìˆ˜
        metric_averages = {}
        for metric in QualityMetric:
            scores = [r.metric_scores[metric].score for r in recent_reports 
                     if metric in r.metric_scores]
            metric_averages[metric.value] = statistics.mean(scores) if scores else 0.0
        
        # ì¬ë¶„ì„ ë¹„ìœ¨
        reanalysis_rate = sum(1 for r in recent_reports if r.needs_reanalysis) / len(recent_reports)
        
        # ëª©í‘œ ë‹¬ì„±ë¥ 
        target_achievement_rate = sum(1 for r in recent_reports if r.overall_score >= self.target_accuracy) / len(recent_reports)
        
        return {
            "ì´_ë¶„ì„_ìˆ˜": len(self.quality_history),
            "ìµœê·¼_í‰ê· _ì ìˆ˜": round(avg_overall_score, 3),
            "í’ˆì§ˆ_íŠ¸ë Œë“œ": "ìƒìŠ¹" if avg_trend > 0.01 else "í•˜ë½" if avg_trend < -0.01 else "ì•ˆì •",
            "ë©”íŠ¸ë¦­ë³„_í‰ê· ": {k: round(v, 3) for k, v in metric_averages.items()},
            "ì¬ë¶„ì„_ë¹„ìœ¨": f"{reanalysis_rate:.1%}",
            "ëª©í‘œ_ë‹¬ì„±ë¥ ": f"{target_achievement_rate:.1%}",
            "99.2%_ëª©í‘œ_ë‹¬ì„±": target_achievement_rate >= 0.992,
            "ìµœê·¼_í’ˆì§ˆ_ë“±ê¸‰": recent_reports[-1].quality_level.value if recent_reports else "N/A"
        }

# ë°ëª¨ ë° í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
async def demo_quality_validation():
    """í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ ë°ëª¨"""
    print("ğŸ” ì†”ë¡œëª¬ë“œ AI í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ v2.3 ë°ëª¨")
    print("=" * 60)
    
    validator = AIQualityValidator()
    
    # ëª¨ì˜ AI ì‘ë‹µ ìƒì„±
    from core.hybrid_llm_manager_v23 import AIResponse, AIModel
    
    mock_response = AIResponse(
        model=AIModel.GPT4V,
        content="""
        ## ë‹¤ì´ì•„ëª¬ë“œ 4C ì „ë¬¸ ë¶„ì„ ë³´ê³ ì„œ
        
        ### ê¸°ë³¸ ì •ë³´
        - **ì¤‘ëŸ‰**: 1.2 ìºëŸ¿
        - **í˜•íƒœ**: ë¼ìš´ë“œ ë¸Œë¦´ë¦¬ì–¸íŠ¸
        - **ì»¬ëŸ¬**: H ë“±ê¸‰ (Nearly Colorless)
        - **í´ë˜ë¦¬í‹°**: VS2 (Very Slightly Included)
        - **ì»·**: Very Good
        
        ### ìƒì„¸ ë¶„ì„
        ì´ ë‹¤ì´ì•„ëª¬ë“œëŠ” GIA í‘œì¤€ì— ë”°ë¼ ë¶„ì„í•œ ê²°ê³¼ ìš°ìˆ˜í•œ í’ˆì§ˆì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
        H ì»¬ëŸ¬ëŠ” ìœ¡ì•ˆìœ¼ë¡œ ê±°ì˜ ë¬´ìƒ‰ì— ê°€ê¹ê²Œ ë³´ì´ë©°, VS2 í´ë˜ë¦¬í‹°ëŠ” 
        10ë°° í™•ëŒ€ í•˜ì—ì„œë§Œ ë‚´í¬ë¬¼ì´ ê´€ì°°ë˜ëŠ” ìˆ˜ì¤€ì…ë‹ˆë‹¤.
        
        ### ì‹œì¥ ê°€ì¹˜
        - **ì˜ˆìƒ ê°€ê²©**: $5,000-6,000 USD
        - **íˆ¬ì ê°€ì¹˜**: ì¤‘ìƒê¸‰
        - **ì¶”ì²œë„**: ë†’ìŒ
        
        ### ì „ë¬¸ê°€ ì˜ê²¬
        ì „ì²´ì ìœ¼ë¡œ ê· í˜•ì¡íŒ í’ˆì§ˆì˜ ë‹¤ì´ì•„ëª¬ë“œë¡œ í‰ê°€ë©ë‹ˆë‹¤.
        """,
        confidence=0.92,
        processing_time=2.3,
        cost_estimate=0.024,
        jewelry_relevance=0.95,
        metadata={"tokens_used": 150}
    )
    
    print("ğŸ“Š AI ì‘ë‹µ ë‚´ìš©:")
    print(mock_response.content[:300] + "...")
    print()
    
    # í’ˆì§ˆ ê²€ì¦ ì‹¤í–‰
    quality_report = await validator.validate_ai_response(
        mock_response, 
        AnalysisType.DIAMOND_4C,
        "1.2ìºëŸ¿ ë¼ìš´ë“œ ë‹¤ì´ì•„ëª¬ë“œ ë¶„ì„ ìš”ì²­"
    )
    
    print("ğŸ¯ í’ˆì§ˆ ê²€ì¦ ê²°ê³¼:")
    print(f"   ì „ì²´ ì ìˆ˜: {quality_report.overall_score:.3f}")
    print(f"   í’ˆì§ˆ ë“±ê¸‰: {quality_report.quality_level.value}")
    print(f"   ì£¼ì–¼ë¦¬ ì „ë¬¸ì„±: {quality_report.jewelry_expertise_score:.3f}")
    print(f"   ì¼ê´€ì„± ì ìˆ˜: {quality_report.consistency_score:.3f}")
    print(f"   ì¬ë¶„ì„ í•„ìš”: {'ì˜ˆ' if quality_report.needs_reanalysis else 'ì•„ë‹ˆì˜¤'}")
    print()
    
    print("ğŸ“ˆ ë©”íŠ¸ë¦­ë³„ ìƒì„¸ ì ìˆ˜:")
    for metric, score_obj in quality_report.metric_scores.items():
        print(f"   {metric.value}: {score_obj.score:.3f} (ì‹ ë¢°ë„: {score_obj.confidence:.2f})")
    print()
    
    print("ğŸ¯ ê°œì„  ìš°ì„ ìˆœìœ„:")
    for i, priority in enumerate(quality_report.improvement_priority[:3], 1):
        print(f"   {i}. {priority}")
    print()
    
    # ì—¬ëŸ¬ ë¶„ì„ í›„ í†µê³„
    print("ğŸ“Š í’ˆì§ˆ ë¶„ì„ í†µê³„:")
    analytics = validator.get_quality_analytics()
    for key, value in analytics.items():
        print(f"   {key}: {value}")

if __name__ == "__main__":
    asyncio.run(demo_quality_validation())
