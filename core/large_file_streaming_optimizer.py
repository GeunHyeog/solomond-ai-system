#!/usr/bin/env python3
"""
ëŒ€ìš©ëŸ‰ íŒŒì¼ ìŠ¤íŠ¸ë¦¬ë° ìµœì í™” ì‹œìŠ¤í…œ v2.6
ë©”ëª¨ë¦¬ íš¨ìœ¨ì  íŒŒì¼ ì²˜ë¦¬ ë° ìŠ¤íŠ¸ë¦¬ë° êµ¬í˜„
"""

import os
import io
import gc
import time
import hashlib
import threading
import mmap
from typing import Iterator, BinaryIO, Dict, List, Any, Optional, Tuple, Union
from dataclasses import dataclass, field
from pathlib import Path
import tempfile
import shutil
from datetime import datetime
import logging
from contextlib import contextmanager
import asyncio
import aiofiles
import psutil

@dataclass
class StreamingConfig:
    """ìŠ¤íŠ¸ë¦¬ë° ì„¤ì •"""
    chunk_size_mb: float = 8.0  # 8MB ì²­í¬
    max_memory_usage_mb: float = 256.0  # ìµœëŒ€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
    enable_memory_mapping: bool = True  # ë©”ëª¨ë¦¬ ë§¤í•‘ í™œì„±í™”
    enable_compression: bool = False  # ì••ì¶• í™œì„±í™” (CPU vs ë©”ëª¨ë¦¬ íŠ¸ë ˆì´ë“œì˜¤í”„)
    temp_dir: Optional[str] = None  # ì„ì‹œ ë””ë ‰í† ë¦¬
    buffer_count: int = 3  # ë²„í¼ ê°œìˆ˜ (íŠ¸ë¦¬í”Œ ë²„í¼ë§)
    enable_async: bool = True  # ë¹„ë™ê¸° ì²˜ë¦¬
    progress_callback: Optional[callable] = None  # ì§„í–‰ë¥  ì½œë°±

@dataclass
class StreamingStats:
    """ìŠ¤íŠ¸ë¦¬ë° í†µê³„"""
    total_size_mb: float = 0.0
    processed_size_mb: float = 0.0
    chunks_processed: int = 0
    processing_time_seconds: float = 0.0
    peak_memory_usage_mb: float = 0.0
    average_chunk_time_ms: float = 0.0
    throughput_mbps: float = 0.0
    memory_efficiency: float = 1.0  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
    compression_ratio: float = 1.0  # ì••ì¶•ë¥  (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)

@dataclass
class FileChunk:
    """íŒŒì¼ ì²­í¬"""
    chunk_id: int
    data: bytes
    size_bytes: int
    offset: int
    is_compressed: bool = False
    checksum: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)

class LargeFileStreamingOptimizer:
    """ëŒ€ìš©ëŸ‰ íŒŒì¼ ìŠ¤íŠ¸ë¦¬ë° ìµœì í™” ì‹œìŠ¤í…œ"""
    
    def __init__(self, config: Optional[StreamingConfig] = None):
        self.config = config or StreamingConfig()
        self.stats = StreamingStats()
        self.is_streaming = False
        self.cancel_requested = False
        self.lock = threading.RLock()
        self.logger = self._setup_logging()
        
        # ì²­í¬ í¬ê¸°ë¥¼ ë°”ì´íŠ¸ë¡œ ë³€í™˜
        self.chunk_size_bytes = int(self.config.chunk_size_mb * 1024 * 1024)
        self.max_memory_bytes = int(self.config.max_memory_usage_mb * 1024 * 1024)
        
        # ì„ì‹œ ë””ë ‰í† ë¦¬ ì„¤ì •
        if self.config.temp_dir:
            self.temp_dir = Path(self.config.temp_dir)
        else:
            self.temp_dir = Path(tempfile.gettempdir()) / "solomond_streaming"
        
        self.temp_dir.mkdir(exist_ok=True, parents=True)
        
        # ë²„í¼ í’€
        self.buffer_pool = []
        self._init_buffer_pool()
        
        # ì••ì¶• ëª¨ë“ˆ (ì˜µì…˜)
        self.compressor = None
        if self.config.enable_compression:
            try:
                import lz4.frame
                self.compressor = lz4.frame
                self.logger.info("âœ… LZ4 ì••ì¶• í™œì„±í™”")
            except ImportError:
                try:
                    import gzip
                    self.compressor = gzip
                    self.logger.info("âœ… GZIP ì••ì¶• í™œì„±í™”")
                except ImportError:
                    self.logger.warning("âš ï¸ ì••ì¶• ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
    
    def _setup_logging(self) -> logging.Logger:
        """ë¡œê¹… ì„¤ì •"""
        logger = logging.getLogger(f'{__name__}.{self.__class__.__name__}')
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s | %(name)s | %(levelname)s | %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            logger.setLevel(logging.INFO)
        return logger
    
    def _init_buffer_pool(self) -> None:
        """ë²„í¼ í’€ ì´ˆê¸°í™”"""
        try:
            for i in range(self.config.buffer_count):
                buffer = bytearray(self.chunk_size_bytes)
                self.buffer_pool.append(buffer)
            
            self.logger.debug(f"âœ… ë²„í¼ í’€ ì´ˆê¸°í™”: {self.config.buffer_count}ê°œ Ã— {self.config.chunk_size_mb}MB")
        except Exception as e:
            self.logger.error(f"âŒ ë²„í¼ í’€ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def _get_buffer(self) -> bytearray:
        """ë²„í¼ ê°€ì ¸ì˜¤ê¸°"""
        with self.lock:
            if self.buffer_pool:
                return self.buffer_pool.pop()
            else:
                # í’€ì´ ë¹„ì–´ìˆìœ¼ë©´ ìƒˆë¡œ ìƒì„±
                return bytearray(self.chunk_size_bytes)
    
    def _return_buffer(self, buffer: bytearray) -> None:
        """ë²„í¼ ë°˜í™˜"""
        with self.lock:
            if len(self.buffer_pool) < self.config.buffer_count:
                # ë²„í¼ ì´ˆê¸°í™”
                buffer[:] = b'\x00' * len(buffer)
                self.buffer_pool.append(buffer)
    
    def _calculate_checksum(self, data: bytes) -> str:
        """ì²´í¬ì„¬ ê³„ì‚°"""
        return hashlib.md5(data).hexdigest()
    
    def _compress_data(self, data: bytes) -> Tuple[bytes, float]:
        """ë°ì´í„° ì••ì¶•"""
        if not self.compressor:
            return data, 1.0
        
        try:
            if hasattr(self.compressor, 'compress'):
                # LZ4 ë˜ëŠ” gzip
                if self.compressor.__name__ == 'lz4.frame':
                    compressed = self.compressor.compress(data)
                else:
                    compressed = self.compressor.compress(data)
            else:
                compressed = data
            
            compression_ratio = len(compressed) / len(data) if len(data) > 0 else 1.0
            return compressed, compression_ratio
        
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì••ì¶• ì‹¤íŒ¨: {e}")
            return data, 1.0
    
    def _monitor_memory_usage(self) -> float:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§"""
        try:
            process = psutil.Process()
            memory_info = process.memory_info()
            memory_mb = memory_info.rss / (1024 * 1024)
            
            # í”¼í¬ ë©”ëª¨ë¦¬ ì—…ë°ì´íŠ¸
            if memory_mb > self.stats.peak_memory_usage_mb:
                self.stats.peak_memory_usage_mb = memory_mb
            
            return memory_mb
        except Exception:
            return 0.0
    
    @contextmanager
    def _memory_mapped_file(self, file_path: Union[str, Path]):
        """ë©”ëª¨ë¦¬ ë§¤í•‘ëœ íŒŒì¼ ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ì"""
        file_path = Path(file_path)
        
        if not self.config.enable_memory_mapping or file_path.stat().st_size > self.max_memory_bytes:
            # ë©”ëª¨ë¦¬ ë§¤í•‘ ë¹„í™œì„±í™” ë˜ëŠ” íŒŒì¼ì´ ë„ˆë¬´ í° ê²½ìš° ì¼ë°˜ íŒŒì¼ ì—´ê¸°
            with open(file_path, 'rb') as f:
                yield f
        else:
            # ë©”ëª¨ë¦¬ ë§¤í•‘ ì‚¬ìš©
            with open(file_path, 'rb') as f:
                with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mmapped_file:
                    yield mmapped_file
    
    def stream_file_chunks(self, file_path: Union[str, Path]) -> Iterator[FileChunk]:
        """íŒŒì¼ì„ ì²­í¬ ë‹¨ìœ„ë¡œ ìŠ¤íŠ¸ë¦¬ë°"""
        file_path = Path(file_path)
        
        if not file_path.exists():
            raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        
        # í†µê³„ ì´ˆê¸°í™”
        self.stats = StreamingStats()
        self.stats.total_size_mb = file_path.stat().st_size / (1024 * 1024)
        self.is_streaming = True
        self.cancel_requested = False
        
        start_time = time.time()
        chunk_id = 0
        
        try:
            with self._memory_mapped_file(file_path) as file_obj:
                self.logger.info(f"ğŸš€ íŒŒì¼ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘: {file_path.name} ({self.stats.total_size_mb:.1f}MB)")
                
                while not self.cancel_requested:
                    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬
                    current_memory = self._monitor_memory_usage()
                    if current_memory > self.config.max_memory_usage_mb:
                        self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì´ˆê³¼: {current_memory:.1f}MB")
                        # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
                        gc.collect()
                    
                    # ë²„í¼ ê°€ì ¸ì˜¤ê¸°
                    buffer = self._get_buffer()
                    
                    try:
                        # ì²­í¬ ì½ê¸°
                        chunk_start_time = time.time()
                        data = file_obj.read(self.chunk_size_bytes)
                        
                        if not data:
                            break  # íŒŒì¼ ë
                        
                        # ì‹¤ì œ ë°ì´í„° í¬ê¸°ì— ë§ê²Œ ë²„í¼ ì¡°ì •
                        if len(data) < len(buffer):
                            buffer = buffer[:len(data)]
                        
                        buffer[:len(data)] = data
                        actual_data = bytes(buffer[:len(data)])
                        
                        # ì••ì¶• (ì˜µì…˜)
                        compressed_data, compression_ratio = actual_data, 1.0
                        if self.config.enable_compression and len(actual_data) > 1024:  # 1KB ì´ìƒë§Œ ì••ì¶•
                            compressed_data, compression_ratio = self._compress_data(actual_data)
                        
                        # ì²´í¬ì„¬ ê³„ì‚°
                        checksum = self._calculate_checksum(actual_data)
                        
                        # ì²­í¬ ê°ì²´ ìƒì„±
                        chunk = FileChunk(
                            chunk_id=chunk_id,
                            data=compressed_data,
                            size_bytes=len(actual_data),
                            offset=chunk_id * self.chunk_size_bytes,
                            is_compressed=compression_ratio < 1.0,
                            checksum=checksum,
                            metadata={
                                'compression_ratio': compression_ratio,
                                'processing_time_ms': (time.time() - chunk_start_time) * 1000
                            }
                        )
                        
                        # í†µê³„ ì—…ë°ì´íŠ¸
                        self.stats.chunks_processed += 1
                        self.stats.processed_size_mb += len(actual_data) / (1024 * 1024)
                        
                        chunk_time_ms = (time.time() - chunk_start_time) * 1000
                        self.stats.average_chunk_time_ms = (
                            (self.stats.average_chunk_time_ms * (chunk_id) + chunk_time_ms) / (chunk_id + 1)
                        )
                        
                        # ì§„í–‰ë¥  ì½œë°±
                        if self.config.progress_callback:
                            progress = (self.stats.processed_size_mb / self.stats.total_size_mb) * 100
                            self.config.progress_callback(progress, chunk)
                        
                        yield chunk
                        
                        chunk_id += 1
                    
                    finally:
                        # ë²„í¼ ë°˜í™˜
                        self._return_buffer(buffer)
        
        finally:
            self.is_streaming = False
            
            # ìµœì¢… í†µê³„ ê³„ì‚°
            self.stats.processing_time_seconds = time.time() - start_time
            if self.stats.processing_time_seconds > 0:
                self.stats.throughput_mbps = self.stats.processed_size_mb / self.stats.processing_time_seconds
            
            self.stats.memory_efficiency = self.stats.peak_memory_usage_mb / self.stats.total_size_mb if self.stats.total_size_mb > 0 else 1.0
            
            self.logger.info(f"âœ… íŒŒì¼ ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ: {self.stats.chunks_processed}ê°œ ì²­í¬, {self.stats.throughput_mbps:.1f}MB/s")
    
    async def async_stream_file_chunks(self, file_path: Union[str, Path]) -> Iterator[FileChunk]:
        """ë¹„ë™ê¸° íŒŒì¼ ìŠ¤íŠ¸ë¦¬ë°"""
        if not self.config.enable_async:
            # ë™ê¸° ë²„ì „ìœ¼ë¡œ í´ë°±
            for chunk in self.stream_file_chunks(file_path):
                yield chunk
            return
        
        file_path = Path(file_path)
        
        if not file_path.exists():
            raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        
        # í†µê³„ ì´ˆê¸°í™”
        self.stats = StreamingStats()
        self.stats.total_size_mb = file_path.stat().st_size / (1024 * 1024)
        self.is_streaming = True
        self.cancel_requested = False
        
        start_time = time.time()
        chunk_id = 0
        
        try:
            async with aiofiles.open(file_path, 'rb') as file_obj:
                self.logger.info(f"ğŸš€ ë¹„ë™ê¸° íŒŒì¼ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘: {file_path.name} ({self.stats.total_size_mb:.1f}MB)")
                
                while not self.cancel_requested:
                    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬
                    current_memory = self._monitor_memory_usage()
                    if current_memory > self.config.max_memory_usage_mb:
                        self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì´ˆê³¼: {current_memory:.1f}MB")
                        gc.collect()
                        await asyncio.sleep(0.01)  # ì§§ì€ ëŒ€ê¸°
                    
                    # ì²­í¬ ì½ê¸°
                    chunk_start_time = time.time()
                    data = await file_obj.read(self.chunk_size_bytes)
                    
                    if not data:
                        break  # íŒŒì¼ ë
                    
                    # ì••ì¶• (ì˜µì…˜)
                    compressed_data, compression_ratio = data, 1.0
                    if self.config.enable_compression and len(data) > 1024:
                        compressed_data, compression_ratio = self._compress_data(data)
                    
                    # ì²´í¬ì„¬ ê³„ì‚°
                    checksum = self._calculate_checksum(data)
                    
                    # ì²­í¬ ê°ì²´ ìƒì„±
                    chunk = FileChunk(
                        chunk_id=chunk_id,
                        data=compressed_data,
                        size_bytes=len(data),
                        offset=chunk_id * self.chunk_size_bytes,
                        is_compressed=compression_ratio < 1.0,
                        checksum=checksum,
                        metadata={
                            'compression_ratio': compression_ratio,
                            'processing_time_ms': (time.time() - chunk_start_time) * 1000
                        }
                    )
                    
                    # í†µê³„ ì—…ë°ì´íŠ¸
                    self.stats.chunks_processed += 1
                    self.stats.processed_size_mb += len(data) / (1024 * 1024)
                    
                    # ì§„í–‰ë¥  ì½œë°±
                    if self.config.progress_callback:
                        progress = (self.stats.processed_size_mb / self.stats.total_size_mb) * 100
                        self.config.progress_callback(progress, chunk)
                    
                    yield chunk
                    
                    chunk_id += 1
                    
                    # ë‹¤ë¥¸ íƒœìŠ¤í¬ì—ê²Œ ì œì–´ê¶Œ ì–‘ë³´
                    await asyncio.sleep(0)
        
        finally:
            self.is_streaming = False
            
            # ìµœì¢… í†µê³„ ê³„ì‚°
            self.stats.processing_time_seconds = time.time() - start_time
            if self.stats.processing_time_seconds > 0:
                self.stats.throughput_mbps = self.stats.processed_size_mb / self.stats.processing_time_seconds
            
            self.stats.memory_efficiency = self.stats.peak_memory_usage_mb / self.stats.total_size_mb if self.stats.total_size_mb > 0 else 1.0
            
            self.logger.info(f"âœ… ë¹„ë™ê¸° íŒŒì¼ ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ: {self.stats.chunks_processed}ê°œ ì²­í¬, {self.stats.throughput_mbps:.1f}MB/s")
    
    def process_file_streaming(self, file_path: Union[str, Path], processor_func: callable) -> Any:
        """ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ íŒŒì¼ ì²˜ë¦¬"""
        results = []
        
        try:
            for chunk in self.stream_file_chunks(file_path):
                if self.cancel_requested:
                    break
                
                # ì²­í¬ ì²˜ë¦¬
                try:
                    result = processor_func(chunk)
                    if result is not None:
                        results.append(result)
                except Exception as e:
                    self.logger.error(f"âŒ ì²­í¬ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    continue
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬ (ì£¼ê¸°ì )
                if chunk.chunk_id % 10 == 0:
                    gc.collect()
            
            return results
        
        except Exception as e:
            self.logger.error(f"âŒ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise
    
    def cancel_streaming(self) -> None:
        """ìŠ¤íŠ¸ë¦¬ë° ì·¨ì†Œ"""
        self.cancel_requested = True
        self.logger.info("ğŸ›‘ ìŠ¤íŠ¸ë¦¬ë° ì·¨ì†Œ ìš”ì²­")
    
    def get_streaming_stats(self) -> Dict[str, Any]:
        """ìŠ¤íŠ¸ë¦¬ë° í†µê³„ ë°˜í™˜"""
        return {
            'is_streaming': self.is_streaming,
            'total_size_mb': self.stats.total_size_mb,
            'processed_size_mb': self.stats.processed_size_mb,
            'progress_percent': (self.stats.processed_size_mb / self.stats.total_size_mb * 100) if self.stats.total_size_mb > 0 else 0,
            'chunks_processed': self.stats.chunks_processed,
            'processing_time_seconds': self.stats.processing_time_seconds,
            'peak_memory_usage_mb': self.stats.peak_memory_usage_mb,
            'average_chunk_time_ms': self.stats.average_chunk_time_ms,
            'throughput_mbps': self.stats.throughput_mbps,
            'memory_efficiency': self.stats.memory_efficiency,
            'compression_ratio': self.stats.compression_ratio,
            'estimated_remaining_time_seconds': self._estimate_remaining_time(),
            'memory_usage_trend': self._get_memory_trend()
        }
    
    def _estimate_remaining_time(self) -> float:
        """ë‚¨ì€ ì‹œê°„ ì˜ˆì¸¡"""
        if not self.is_streaming or self.stats.throughput_mbps <= 0:
            return 0.0
        
        remaining_mb = self.stats.total_size_mb - self.stats.processed_size_mb
        return remaining_mb / self.stats.throughput_mbps if self.stats.throughput_mbps > 0 else 0.0
    
    def _get_memory_trend(self) -> str:
        """ë©”ëª¨ë¦¬ ì‚¬ìš© ì¶”ì„¸"""
        current_memory = self._monitor_memory_usage()
        if current_memory > self.config.max_memory_usage_mb * 0.9:
            return "critical"
        elif current_memory > self.config.max_memory_usage_mb * 0.7:
            return "high"
        elif current_memory > self.config.max_memory_usage_mb * 0.5:
            return "medium"
        else:
            return "low"
    
    def optimize_for_file_type(self, file_path: Union[str, Path]) -> None:
        """íŒŒì¼ íƒ€ì…ì— ë”°ë¥¸ ìµœì í™”"""
        file_path = Path(file_path)
        file_size_mb = file_path.stat().st_size / (1024 * 1024)
        file_ext = file_path.suffix.lower()
        
        # íŒŒì¼ íƒ€ì…ë³„ ìµœì í™”
        if file_ext in ['.mp4', '.avi', '.mov', '.mkv']:
            # ë¹„ë””ì˜¤ íŒŒì¼: í° ì²­í¬, ì••ì¶• ë¹„í™œì„±í™”
            self.config.chunk_size_mb = min(32.0, file_size_mb / 10)
            self.config.enable_compression = False
            self.config.buffer_count = 4
            
        elif file_ext in ['.mp3', '.wav', '.m4a', '.flac']:
            # ì˜¤ë””ì˜¤ íŒŒì¼: ì¤‘ê°„ ì²­í¬, ì••ì¶• ì„ íƒì 
            self.config.chunk_size_mb = min(16.0, file_size_mb / 20)
            self.config.enable_compression = file_size_mb > 100  # 100MB ì´ìƒë§Œ ì••ì¶•
            self.config.buffer_count = 3
            
        elif file_ext in ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']:
            # ì´ë¯¸ì§€ íŒŒì¼: ì‘ì€ ì²­í¬, ì••ì¶• ë¹„í™œì„±í™” (ì´ë¯¸ ì••ì¶•ë¨)
            self.config.chunk_size_mb = min(4.0, file_size_mb / 5)
            self.config.enable_compression = False
            self.config.buffer_count = 2
            
        elif file_ext in ['.txt', '.log', '.csv', '.json']:
            # í…ìŠ¤íŠ¸ íŒŒì¼: ì‘ì€ ì²­í¬, ì••ì¶• í™œì„±í™”
            self.config.chunk_size_mb = min(2.0, file_size_mb / 50)
            self.config.enable_compression = True
            self.config.buffer_count = 2
        
        else:
            # ê¸°ë³¸ ì„¤ì •
            self.config.chunk_size_mb = min(8.0, file_size_mb / 20)
            self.config.enable_compression = file_size_mb > 50
            self.config.buffer_count = 3
        
        # ì²­í¬ í¬ê¸° ì¬ê³„ì‚°
        self.chunk_size_bytes = int(self.config.chunk_size_mb * 1024 * 1024)
        
        # ë²„í¼ í’€ ì¬ì´ˆê¸°í™”
        self.buffer_pool.clear()
        self._init_buffer_pool()
        
        self.logger.info(f"ğŸ“Š íŒŒì¼ íƒ€ì… ìµœì í™” ì™„ë£Œ: {file_ext} -> ì²­í¬ {self.config.chunk_size_mb}MB, ì••ì¶• {'ON' if self.config.enable_compression else 'OFF'}")
    
    def cleanup(self) -> None:
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        self.cancel_streaming()
        
        # ë²„í¼ í’€ ì •ë¦¬
        with self.lock:
            self.buffer_pool.clear()
        
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        try:
            if self.temp_dir.exists():
                for temp_file in self.temp_dir.glob("*"):
                    temp_file.unlink()
                
                # ë¹ˆ ë””ë ‰í† ë¦¬ë©´ ì œê±°
                try:
                    self.temp_dir.rmdir()
                except OSError:
                    pass  # ë””ë ‰í† ë¦¬ê°€ ë¹„ì–´ìˆì§€ ì•ŠìŒ
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨: {e}")
        
        # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        gc.collect()
        
        self.logger.info("ğŸ§¹ ìŠ¤íŠ¸ë¦¬ë° ìµœì í™” ì‹œìŠ¤í…œ ì •ë¦¬ ì™„ë£Œ")

# ì „ì—­ ìŠ¤íŠ¸ë¦¬ë° ìµœì í™” ì‹œìŠ¤í…œ
_global_streaming_optimizer = None
_global_lock = threading.Lock()

def get_global_streaming_optimizer(config: Optional[StreamingConfig] = None) -> LargeFileStreamingOptimizer:
    """ì „ì—­ ìŠ¤íŠ¸ë¦¬ë° ìµœì í™” ì‹œìŠ¤í…œ ê°€ì ¸ì˜¤ê¸°"""
    global _global_streaming_optimizer
    
    with _global_lock:
        if _global_streaming_optimizer is None:
            _global_streaming_optimizer = LargeFileStreamingOptimizer(config)
        return _global_streaming_optimizer

# í¸ì˜ í•¨ìˆ˜ë“¤
def stream_large_file(file_path: Union[str, Path], 
                     chunk_size_mb: float = 8.0,
                     max_memory_mb: float = 256.0,
                     enable_compression: bool = False,
                     progress_callback: Optional[callable] = None) -> Iterator[FileChunk]:
    """ëŒ€ìš©ëŸ‰ íŒŒì¼ ìŠ¤íŠ¸ë¦¬ë° (í¸ì˜ í•¨ìˆ˜)"""
    config = StreamingConfig(
        chunk_size_mb=chunk_size_mb,
        max_memory_usage_mb=max_memory_mb,
        enable_compression=enable_compression,
        progress_callback=progress_callback
    )
    
    optimizer = LargeFileStreamingOptimizer(config)
    optimizer.optimize_for_file_type(file_path)
    
    for chunk in optimizer.stream_file_chunks(file_path):
        yield chunk

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ìš© í° íŒŒì¼ ìƒì„±
    test_file = Path("test_large_file.bin")
    test_size_mb = 100  # 100MB í…ŒìŠ¤íŠ¸ íŒŒì¼
    
    if not test_file.exists():
        print(f"ğŸ”§ í…ŒìŠ¤íŠ¸ íŒŒì¼ ìƒì„± ì¤‘: {test_size_mb}MB")
        with open(test_file, 'wb') as f:
            chunk_size = 1024 * 1024  # 1MB ì²­í¬
            for i in range(test_size_mb):
                data = os.urandom(chunk_size)
                f.write(data)
        print("âœ… í…ŒìŠ¤íŠ¸ íŒŒì¼ ìƒì„± ì™„ë£Œ")
    
    # ì§„í–‰ë¥  ì½œë°± í•¨ìˆ˜
    def progress_callback(progress: float, chunk: FileChunk):
        print(f"ğŸ“Š ì§„í–‰ë¥ : {progress:.1f}% - ì²­í¬ {chunk.chunk_id} ({chunk.size_bytes/1024/1024:.1f}MB)")
    
    # ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸
    print("\nğŸš€ ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    config = StreamingConfig(
        chunk_size_mb=8.0,
        max_memory_usage_mb=64.0,
        enable_compression=True,
        progress_callback=progress_callback
    )
    
    optimizer = LargeFileStreamingOptimizer(config)
    optimizer.optimize_for_file_type(test_file)
    
    total_chunks = 0
    total_size = 0
    
    start_time = time.time()
    
    for chunk in optimizer.stream_file_chunks(test_file):
        total_chunks += 1
        total_size += chunk.size_bytes
        
        # ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜
        time.sleep(0.01)  # 10ms ì²˜ë¦¬ ì‹œê°„
    
    end_time = time.time()
    
    # ê²°ê³¼ ì¶œë ¥
    stats = optimizer.get_streaming_stats()
    print(f"\nğŸ“Š ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ:")
    print(f"  ì´ ì²­í¬: {total_chunks}ê°œ")
    print(f"  ì´ í¬ê¸°: {total_size/1024/1024:.1f}MB")
    print(f"  ì²˜ë¦¬ ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")
    print(f"  ì²˜ë¦¬ëŸ‰: {stats['throughput_mbps']:.1f}MB/s")
    print(f"  í”¼í¬ ë©”ëª¨ë¦¬: {stats['peak_memory_usage_mb']:.1f}MB")
    print(f"  ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±: {stats['memory_efficiency']:.2f}")
    
    # ì •ë¦¬
    optimizer.cleanup()
    
    # í…ŒìŠ¤íŠ¸ íŒŒì¼ ì‚­ì œ
    if test_file.exists():
        test_file.unlink()
        print("ğŸ§¹ í…ŒìŠ¤íŠ¸ íŒŒì¼ ì‚­ì œ ì™„ë£Œ")
    
    print("âœ… ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")