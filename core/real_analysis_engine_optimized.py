#!/usr/bin/env python3
"""
ìµœì í™”ëœ ì‹¤ì œ ë¶„ì„ ì—”ì§„
ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€, ëª¨ë¸ ìºì‹±, ì²­í¬ ì²˜ë¦¬ í†µí•©
"""

import os
import time
import logging
import gc
from typing import Dict, List, Any, Optional, Union
from pathlib import Path
import json
from datetime import datetime

# ìµœì í™” ì‹œìŠ¤í…œë“¤ import
from .memory_cleanup_manager import get_global_memory_manager, register_temp_file, emergency_cleanup
from .optimized_model_loader import get_optimized_model_loader, load_whisper, load_easyocr, load_transformers
from .chunk_processor_optimized import get_global_chunk_processor, ChunkType, process_audio_chunked, process_image_chunked

# ê¸°ì¡´ importë“¤
from utils.logger import get_logger

class OptimizedRealAnalysisEngine:
    """ìµœì í™”ëœ ì‹¤ì œ ë¶„ì„ ì—”ì§„"""
    
    def __init__(self):
        self.logger = get_logger(__name__)
        
        # ìµœì í™” ì‹œìŠ¤í…œë“¤ ì´ˆê¸°í™”
        self.memory_manager = get_global_memory_manager()
        self.model_loader = get_optimized_model_loader()
        self.chunk_processor = get_global_chunk_processor()
        
        # ì§„í–‰ ìƒí™© ì½œë°± ë“±ë¡
        self.chunk_processor.add_progress_callback(self._chunk_progress_callback)
        
        # ë¶„ì„ í†µê³„
        self.analysis_stats = {
            "total_files": 0,
            "successful_analyses": 0,
            "failed_analyses": 0,
            "total_processing_time": 0,
            "memory_peaks": [],
            "chunk_processing_times": []
        }
        
        self.logger.info("âœ… ìµœì í™”ëœ ë¶„ì„ ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _chunk_progress_callback(self, current: int, total: int, message: str = "") -> None:
        """ì²­í¬ ì²˜ë¦¬ ì§„í–‰ ìƒí™© ì½œë°±"""
        progress = (current / total) * 100 if total > 0 else 0
        self.logger.info(f"ì²­í¬ ì§„í–‰: {current}/{total} ({progress:.1f}%) - {message}")
    
    def analyze_audio_file_optimized(self, file_path: str, language: str = "korean") -> Dict[str, Any]:
        """ìµœì í™”ëœ ì˜¤ë””ì˜¤ íŒŒì¼ ë¶„ì„"""
        start_time = time.time()
        self.logger.info(f"ğŸµ ìµœì í™”ëœ ì˜¤ë””ì˜¤ ë¶„ì„ ì‹œì‘: {file_path}")
        
        try:
            # ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
            before_memory = self.memory_manager.get_memory_usage()
            
            # ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬ë¥¼ ìœ„í•œ í”„ë¡œì„¸ì„œ í•¨ìˆ˜
            def process_audio_chunk(chunk_file: str, chunk_info) -> Dict[str, Any]:
                """ì˜¤ë””ì˜¤ ì²­í¬ ì²˜ë¦¬ í•¨ìˆ˜"""
                try:
                    # Whisper ëª¨ë¸ ë¡œë”© (ìºì‹±ë¨)
                    model = load_whisper(model_name="base", device="cpu")
                    
                    # ìŒì„± í…ìŠ¤íŠ¸ ë³€í™˜
                    result = model.transcribe(chunk_file, language=language)
                    
                    return {
                        "text": result.get("text", ""),
                        "segments": result.get("segments", []),
                        "chunk_index": chunk_info.index,
                        "start_time": chunk_info.start_time,
                        "end_time": chunk_info.end_time
                    }
                except Exception as e:
                    self.logger.error(f"ì˜¤ë””ì˜¤ ì²­í¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    return {"text": "", "error": str(e)}
            
            # ê²°ê³¼ ë³‘í•© í•¨ìˆ˜
            def merge_audio_results(chunk_results: List[Dict]) -> Dict[str, Any]:
                """ì˜¤ë””ì˜¤ ë¶„ì„ ê²°ê³¼ ë³‘í•©"""
                full_text = ""
                all_segments = []
                
                # ì²­í¬ ìˆœì„œëŒ€ë¡œ ì •ë ¬
                sorted_results = sorted(chunk_results, key=lambda x: x.get("chunk_index", 0))
                
                for result in sorted_results:
                    if result.get("text"):
                        full_text += " " + result["text"]
                    if result.get("segments"):
                        all_segments.extend(result["segments"])
                
                return {
                    "text": full_text.strip(),
                    "segments": all_segments,
                    "total_chunks": len(sorted_results)
                }
            
            # ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬ ì‹¤í–‰
            result = process_audio_chunked(
                file_path=file_path,
                processor_func=process_audio_chunk,
                merger_func=merge_audio_results
            )
            
            # í›„ì²˜ë¦¬ ë° í†µê³„
            processing_time = time.time() - start_time
            after_memory = self.memory_manager.get_memory_usage()
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self.analysis_stats["total_files"] += 1
            self.analysis_stats["total_processing_time"] += processing_time
            self.analysis_stats["memory_peaks"].append(after_memory["rss_mb"])
            
            if result["success"]:
                self.analysis_stats["successful_analyses"] += 1
                self.logger.info(
                    f"âœ… ì˜¤ë””ì˜¤ ë¶„ì„ ì™„ë£Œ: {file_path} "
                    f"({processing_time:.2f}s, "
                    f"{result['successful_chunks']}/{result['total_chunks']} ì²­í¬, "
                    f"ë©”ëª¨ë¦¬: {before_memory['rss_mb']:.1f}â†’{after_memory['rss_mb']:.1f}MB)"
                )
            else:
                self.analysis_stats["failed_analyses"] += 1
                self.logger.error(f"âŒ ì˜¤ë””ì˜¤ ë¶„ì„ ì‹¤íŒ¨: {file_path}")
            
            return {
                "success": result["success"],
                "file_path": file_path,
                "file_type": "audio",
                "text": result["result"]["text"] if result["result"] else "",
                "segments": result["result"]["segments"] if result["result"] else [],
                "processing_time": processing_time,
                "chunk_stats": {
                    "total_chunks": result["total_chunks"],
                    "successful_chunks": result["successful_chunks"],
                    "success_rate": result["success_rate"]
                },
                "memory_usage": {
                    "before_mb": before_memory["rss_mb"],
                    "after_mb": after_memory["rss_mb"],
                    "peak_mb": max(before_memory["rss_mb"], after_memory["rss_mb"])
                },
                "errors": result.get("errors", [])
            }
            
        except Exception as e:
            processing_time = time.time() - start_time
            self.analysis_stats["failed_analyses"] += 1
            
            self.logger.error(f"âŒ ì˜¤ë””ì˜¤ ë¶„ì„ ì „ì²´ ì‹¤íŒ¨: {file_path} - {e}")
            
            return {
                "success": False,
                "file_path": file_path,
                "file_type": "audio",
                "error": str(e),
                "processing_time": processing_time
            }
    
    def analyze_image_file_optimized(self, file_path: str, languages: List[str] = ['ko', 'en']) -> Dict[str, Any]:
        """ìµœì í™”ëœ ì´ë¯¸ì§€ íŒŒì¼ ë¶„ì„"""
        start_time = time.time()
        self.logger.info(f"ğŸ–¼ï¸ ìµœì í™”ëœ ì´ë¯¸ì§€ ë¶„ì„ ì‹œì‘: {file_path}")
        
        try:
            # ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
            before_memory = self.memory_manager.get_memory_usage()
            
            # ì´ë¯¸ì§€ëŠ” ë³´í†µ ë‹¨ì¼ íŒŒì¼ë¡œ ì²˜ë¦¬í•˜ì§€ë§Œ, ëŒ€ìš©ëŸ‰ì¸ ê²½ìš° ì²­í¬ ì²˜ë¦¬
            file_size_mb = os.path.getsize(file_path) / (1024 * 1024)
            
            if file_size_mb > 50:  # 50MB ì´ìƒì¸ ê²½ìš° ì²­í¬ ì²˜ë¦¬
                return self._analyze_large_image_chunked(file_path, languages)
            else:
                return self._analyze_single_image(file_path, languages)
                
        except Exception as e:
            processing_time = time.time() - start_time
            self.analysis_stats["failed_analyses"] += 1
            
            self.logger.error(f"âŒ ì´ë¯¸ì§€ ë¶„ì„ ì „ì²´ ì‹¤íŒ¨: {file_path} - {e}")
            
            return {
                "success": False,
                "file_path": file_path,
                "file_type": "image",
                "error": str(e),
                "processing_time": processing_time
            }
    
    def _analyze_single_image(self, file_path: str, languages: List[str]) -> Dict[str, Any]:
        """ë‹¨ì¼ ì´ë¯¸ì§€ ë¶„ì„"""
        start_time = time.time()
        
        try:
            # EasyOCR ëª¨ë¸ ë¡œë”© (ìºì‹±ë¨)
            reader = load_easyocr(lang_list=languages, gpu=False)
            
            # OCR ì‹¤í–‰
            results = reader.readtext(file_path)
            
            # ê²°ê³¼ ì •ë¦¬
            extracted_texts = []
            confidence_scores = []
            
            for (bbox, text, confidence) in results:
                if confidence > 0.3:  # 30% ì´ìƒ ì‹ ë¢°ë„ë§Œ í¬í•¨
                    extracted_texts.append(text)
                    confidence_scores.append(confidence)
            
            full_text = " ".join(extracted_texts)
            avg_confidence = sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0
            
            processing_time = time.time() - start_time
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self.analysis_stats["total_files"] += 1
            self.analysis_stats["successful_analyses"] += 1
            self.analysis_stats["total_processing_time"] += processing_time
            
            self.logger.info(f"âœ… ì´ë¯¸ì§€ ë¶„ì„ ì™„ë£Œ: {file_path} ({processing_time:.2f}s)")
            
            return {
                "success": True,
                "file_path": file_path,
                "file_type": "image",
                "text": full_text,
                "ocr_results": results,
                "text_blocks": extracted_texts,
                "confidence_scores": confidence_scores,
                "average_confidence": avg_confidence,
                "processing_time": processing_time
            }
            
        except Exception as e:
            processing_time = time.time() - start_time
            self.logger.error(f"âŒ ë‹¨ì¼ ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨: {file_path} - {e}")
            
            return {
                "success": False,
                "file_path": file_path,
                "file_type": "image",
                "error": str(e),
                "processing_time": processing_time
            }
    
    def _analyze_large_image_chunked(self, file_path: str, languages: List[str]) -> Dict[str, Any]:
        """ëŒ€ìš©ëŸ‰ ì´ë¯¸ì§€ ì²­í¬ ë¶„ì„"""
        start_time = time.time()
        
        try:
            # ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬ë¥¼ ìœ„í•œ í”„ë¡œì„¸ì„œ í•¨ìˆ˜
            def process_image_chunk(chunk_file: str, chunk_info) -> Dict[str, Any]:
                """ì´ë¯¸ì§€ ì²­í¬ ì²˜ë¦¬ í•¨ìˆ˜"""
                try:
                    # EasyOCR ëª¨ë¸ ë¡œë”© (ìºì‹±ë¨)
                    reader = load_easyocr(lang_list=languages, gpu=False)
                    
                    # OCR ì‹¤í–‰
                    results = reader.readtext(chunk_file)
                    
                    # ê²°ê³¼ ì •ë¦¬
                    extracted_texts = []
                    confidence_scores = []
                    
                    for (bbox, text, confidence) in results:
                        if confidence > 0.3:
                            extracted_texts.append(text)
                            confidence_scores.append(confidence)
                    
                    return {
                        "texts": extracted_texts,
                        "confidences": confidence_scores,
                        "raw_results": results,
                        "chunk_index": chunk_info.index
                    }
                except Exception as e:
                    self.logger.error(f"ì´ë¯¸ì§€ ì²­í¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    return {"texts": [], "error": str(e)}
            
            # ê²°ê³¼ ë³‘í•© í•¨ìˆ˜
            def merge_image_results(chunk_results: List[Dict]) -> Dict[str, Any]:
                """ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼ ë³‘í•©"""
                all_texts = []
                all_confidences = []
                all_raw_results = []
                
                # ì²­í¬ ìˆœì„œëŒ€ë¡œ ì •ë ¬
                sorted_results = sorted(chunk_results, key=lambda x: x.get("chunk_index", 0))
                
                for result in sorted_results:
                    if result.get("texts"):
                        all_texts.extend(result["texts"])
                    if result.get("confidences"):
                        all_confidences.extend(result["confidences"])
                    if result.get("raw_results"):
                        all_raw_results.extend(result["raw_results"])
                
                full_text = " ".join(all_texts)
                avg_confidence = sum(all_confidences) / len(all_confidences) if all_confidences else 0
                
                return {
                    "text": full_text,
                    "text_blocks": all_texts,
                    "confidence_scores": all_confidences,
                    "average_confidence": avg_confidence,
                    "ocr_results": all_raw_results,
                    "total_chunks": len(sorted_results)
                }
            
            # ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬ ì‹¤í–‰
            result = process_image_chunked(
                file_path=file_path,
                processor_func=process_image_chunk,
                merger_func=merge_image_results
            )
            
            processing_time = time.time() - start_time
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self.analysis_stats["total_files"] += 1
            self.analysis_stats["total_processing_time"] += processing_time
            
            if result["success"]:
                self.analysis_stats["successful_analyses"] += 1
                self.logger.info(f"âœ… ëŒ€ìš©ëŸ‰ ì´ë¯¸ì§€ ë¶„ì„ ì™„ë£Œ: {file_path} ({processing_time:.2f}s)")
            else:
                self.analysis_stats["failed_analyses"] += 1
                
            return {
                "success": result["success"],
                "file_path": file_path,
                "file_type": "image",
                "text": result["result"]["text"] if result["result"] else "",
                "text_blocks": result["result"]["text_blocks"] if result["result"] else [],
                "confidence_scores": result["result"]["confidence_scores"] if result["result"] else [],
                "average_confidence": result["result"]["average_confidence"] if result["result"] else 0,
                "ocr_results": result["result"]["ocr_results"] if result["result"] else [],
                "processing_time": processing_time,
                "chunk_stats": {
                    "total_chunks": result["total_chunks"],
                    "successful_chunks": result["successful_chunks"],
                    "success_rate": result["success_rate"]
                },
                "errors": result.get("errors", [])
            }
            
        except Exception as e:
            processing_time = time.time() - start_time
            self.analysis_stats["failed_analyses"] += 1
            
            self.logger.error(f"âŒ ëŒ€ìš©ëŸ‰ ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨: {file_path} - {e}")
            
            return {
                "success": False,
                "file_path": file_path,
                "file_type": "image",
                "error": str(e),
                "processing_time": processing_time
            }
    
    def create_comprehensive_summary(self, analysis_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ì¢…í•© ìš”ì•½ ìƒì„±"""
        start_time = time.time()
        
        try:
            # ëª¨ë“  í…ìŠ¤íŠ¸ ê²°í•©
            all_texts = []
            successful_files = []
            failed_files = []
            
            for result in analysis_results:
                if result.get("success", False):
                    successful_files.append(result["file_path"])
                    if result.get("text"):
                        all_texts.append(result["text"])
                else:
                    failed_files.append(result["file_path"])
            
            combined_text = " ".join(all_texts)
            
            if not combined_text.strip():
                return {
                    "success": False,
                    "error": "ë¶„ì„ëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.",
                    "summary": "",
                    "key_points": [],
                    "file_stats": {
                        "total_files": len(analysis_results),
                        "successful_files": len(successful_files),
                        "failed_files": len(failed_files)
                    }
                }
            
            # Transformers ëª¨ë¸ë¡œ ìš”ì•½ ìƒì„±
            try:
                summarizer = load_transformers("facebook/bart-large-cnn", "summarization")
                
                # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸´ ê²½ìš° ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
                max_length = 1024
                if len(combined_text) > max_length:
                    chunks = [combined_text[i:i+max_length] for i in range(0, len(combined_text), max_length)]
                    chunk_summaries = []
                    
                    for chunk in chunks:
                        if len(chunk.strip()) > 50:  # ìµœì†Œ ê¸¸ì´ ì²´í¬
                            try:
                                summary = summarizer(chunk, max_length=150, min_length=30, do_sample=False)
                                chunk_summaries.append(summary[0]['summary_text'])
                            except:
                                continue
                    
                    # ì²­í¬ ìš”ì•½ë“¤ì„ ë‹¤ì‹œ ìš”ì•½
                    if chunk_summaries:
                        final_text = " ".join(chunk_summaries)
                        if len(final_text) > max_length:
                            final_text = final_text[:max_length]
                        
                        final_summary = summarizer(final_text, max_length=200, min_length=50, do_sample=False)
                        summary_text = final_summary[0]['summary_text']
                    else:
                        summary_text = "ìš”ì•½ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
                else:
                    summary = summarizer(combined_text, max_length=200, min_length=50, do_sample=False)
                    summary_text = summary[0]['summary_text']
                
            except Exception as e:
                self.logger.warning(f"Transformers ìš”ì•½ ì‹¤íŒ¨, ê¸°ë³¸ ìš”ì•½ ì‚¬ìš©: {e}")
                # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ìš”ì•½
                sentences = combined_text.split('.')
                summary_text = '. '.join(sentences[:3]) + '.' if len(sentences) > 3 else combined_text
            
            # í‚¤ í¬ì¸íŠ¸ ì¶”ì¶œ (ê°„ë‹¨í•œ ë°©ì‹)
            sentences = [s.strip() for s in combined_text.split('.') if len(s.strip()) > 10]
            key_points = sentences[:5]  # ìƒìœ„ 5ê°œ ë¬¸ì¥
            
            processing_time = time.time() - start_time
            
            self.logger.info(f"âœ… ì¢…í•© ìš”ì•½ ìƒì„± ì™„ë£Œ ({processing_time:.2f}s)")
            
            return {
                "success": True,
                "summary": summary_text,
                "key_points": key_points,
                "full_text": combined_text,
                "processing_time": processing_time,
                "file_stats": {
                    "total_files": len(analysis_results),
                    "successful_files": len(successful_files),
                    "failed_files": len(failed_files),
                    "success_rate": len(successful_files) / len(analysis_results) if analysis_results else 0
                },
                "successful_files": successful_files,
                "failed_files": failed_files
            }
            
        except Exception as e:
            processing_time = time.time() - start_time
            self.logger.error(f"âŒ ì¢…í•© ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
            
            return {
                "success": False,
                "error": str(e),
                "processing_time": processing_time,
                "summary": "",
                "key_points": []
            }
    
    def get_system_status(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ"""
        memory_status = self.memory_manager.get_status()
        model_cache_info = self.model_loader.get_cache_info()
        
        return {
            "analysis_stats": self.analysis_stats,
            "memory_status": memory_status,
            "model_cache_info": model_cache_info,
            "optimization_features": {
                "memory_management": True,
                "model_caching": True,
                "chunk_processing": True,
                "auto_cleanup": memory_status["cleanup_running"]
            }
        }
    
    def cleanup_system(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ì •ë¦¬"""
        self.logger.info("ğŸ§¹ ì‹œìŠ¤í…œ ì •ë¦¬ ì‹œì‘")
        
        # ë©”ëª¨ë¦¬ ì‘ê¸‰ ì •ë¦¬
        memory_result = emergency_cleanup()
        
        # ëª¨ë¸ ìºì‹œ ì •ë¦¬
        model_cleared = self.model_loader.clear_cache()
        
        # ì²­í¬ í”„ë¡œì„¸ì„œëŠ” ìë™ìœ¼ë¡œ ì •ë¦¬ë¨ (ì„ì‹œ íŒŒì¼ë“¤)
        
        result = {
            "memory_cleanup": memory_result,
            "model_cache_cleared": model_cleared,
            "cleanup_time": datetime.now().isoformat()
        }
        
        self.logger.info(f"âœ… ì‹œìŠ¤í…œ ì •ë¦¬ ì™„ë£Œ: {result}")
        return result

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
global_optimized_analysis_engine = OptimizedRealAnalysisEngine()

def get_optimized_analysis_engine() -> OptimizedRealAnalysisEngine:
    """ìµœì í™”ëœ ë¶„ì„ ì—”ì§„ ë°˜í™˜"""
    return global_optimized_analysis_engine

# í¸ì˜ í•¨ìˆ˜ë“¤
def analyze_file_optimized(file_path: str, file_type: str = "auto") -> Dict[str, Any]:
    """íŒŒì¼ ë¶„ì„ (ìµœì í™”) - í¸ì˜ í•¨ìˆ˜"""
    engine = get_optimized_analysis_engine()
    
    if file_type == "auto":
        # íŒŒì¼ í™•ì¥ìë¡œ íƒ€ì… ê²°ì •
        ext = Path(file_path).suffix.lower()
        if ext in ['.wav', '.mp3', '.m4a', '.flac', '.ogg']:
            file_type = "audio"
        elif ext in ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp']:
            file_type = "image"
        else:
            return {"success": False, "error": f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {ext}"}
    
    if file_type == "audio":
        return engine.analyze_audio_file_optimized(file_path)
    elif file_type == "image":
        return engine.analyze_image_file_optimized(file_path)
    else:
        return {"success": False, "error": f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ íƒ€ì…: {file_type}"}