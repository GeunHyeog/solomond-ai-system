"""
ğŸ§  Smart Content Merger v2.1
ì§€ëŠ¥í˜• ë‚´ìš© ë³‘í•© ë° í˜„ì¥ ìµœì í™” ëª¨ë“ˆ

ì£¼ìš” ê¸°ëŠ¥:
- ë‹¤ì¤‘ íŒŒì¼ ì§€ëŠ¥í˜• ë³‘í•©
- ì¤‘ë³µ ë‚´ìš© ìë™ ê°ì§€ ë° ì œê±°
- ì‹œê°„ìˆœ ë‚´ìš© ì •ë ¬ ë° ì—°ê²°
- ì£¼ì–¼ë¦¬ ì „ë¬¸ìš©ì–´ í†µí•© ê´€ë¦¬
- í˜„ì¥ ì‹¤ì‹œê°„ ë‚´ìš© í†µí•©
"""

import numpy as np
from typing import Dict, List, Tuple, Optional, Union, Any
from datetime import datetime, timedelta
import re
import logging
from collections import defaultdict, Counter
from difflib import SequenceMatcher
import warnings
warnings.filterwarnings("ignore")

class ContentItem:
    """ë‚´ìš© ì•„ì´í…œ í´ë˜ìŠ¤"""
    def __init__(self, 
                 content: str, 
                 source_type: str, 
                 timestamp: float = None,
                 metadata: Dict = None):
        self.content = content
        self.source_type = source_type  # 'audio', 'image', 'document'
        self.timestamp = timestamp or datetime.now().timestamp()
        self.metadata = metadata or {}
        self.processed_content = None
        self.importance_score = 0.0
        self.similarity_groups = []
        self.merged_with = []

class SmartContentMerger:
    """ì§€ëŠ¥í˜• ë‚´ìš© ë³‘í•©ê¸°"""
    
    def __init__(self, jewelry_mode: bool = True):
        self.logger = logging.getLogger(__name__)
        self.jewelry_mode = jewelry_mode
        
        # ë³‘í•© ì„¤ì •
        self.merge_config = {
            'similarity_threshold': 0.7,        # ìœ ì‚¬ë„ ì„ê³„ê°’
            'time_window_seconds': 30.0,        # ì‹œê°„ ìœˆë„ìš° (ì´ˆ)
            'min_content_length': 10,           # ìµœì†Œ ë‚´ìš© ê¸¸ì´
            'duplicate_threshold': 0.9,         # ì¤‘ë³µ ì„ê³„ê°’
            'importance_weight_audio': 0.4,     # ìŒì„± ê°€ì¤‘ì¹˜
            'importance_weight_image': 0.4,     # ì´ë¯¸ì§€ ê°€ì¤‘ì¹˜
            'importance_weight_document': 0.2,  # ë¬¸ì„œ ê°€ì¤‘ì¹˜
        }
        
        # ì£¼ì–¼ë¦¬ ì „ë¬¸ìš©ì–´ ë§¤í•‘
        self.jewelry_terms = {
            'quality_terms': [
                'diamond', 'ë‹¤ì´ì•„ëª¬ë“œ', 'carat', 'ìºëŸ¿', 'cut', 'ì»·',
                'clarity', 'íˆ¬ëª…ë„', 'color', 'ìƒ‰ìƒ', 'certificate', 'ê°ì •ì„œ'
            ],
            'material_terms': [
                'gold', 'ê¸ˆ', 'silver', 'ì€', 'platinum', 'ë°±ê¸ˆ',
                'gemstone', 'ë³´ì„', 'ruby', 'ë£¨ë¹„', 'emerald', 'ì—ë©”ë„ë“œ'
            ],
            'product_terms': [
                'ring', 'ë°˜ì§€', 'necklace', 'ëª©ê±¸ì´', 'earring', 'ê·€ê±¸ì´',
                'bracelet', 'íŒ”ì°Œ', 'pendant', 'íœë˜íŠ¸', 'brooch', 'ë¸Œë¡œì¹˜'
            ],
            'business_terms': [
                'price', 'ê°€ê²©', 'appraisal', 'í‰ê°€', 'collection', 'ì»¬ë ‰ì…˜',
                'exhibition', 'ì „ì‹œíšŒ', 'auction', 'ê²½ë§¤', 'investment', 'íˆ¬ì'
            ]
        }
        
        # ì¤‘ë³µ ì œê±° íŒ¨í„´
        self.duplicate_patterns = [
            r'(\w+)\s+\1\s+\1+',               # ë‹¨ì–´ ë°˜ë³µ
            r'(.{10,})\s*\1',                  # ê¸´ êµ¬ë¬¸ ë°˜ë³µ
            r'(\S+\s+\S+)\s+\1',               # ë‹¨ì–´ ìŒ ë°˜ë³µ
        ]
        
        # ë‚´ìš© ë¶„ë¥˜ í‚¤ì›Œë“œ
        self.content_categories = {
            'product_description': ['íŠ¹ì§•', 'ë””ìì¸', 'ì†Œì¬', 'í¬ê¸°', 'feature', 'design', 'material', 'size'],
            'quality_assessment': ['í’ˆì§ˆ', 'ë“±ê¸‰', 'ìƒíƒœ', 'quality', 'grade', 'condition'],
            'pricing_information': ['ê°€ê²©', 'ë¹„ìš©', 'ê²¬ì ', 'price', 'cost', 'estimate'],
            'technical_specs': ['ì‚¬ì–‘', 'ê·œê²©', 'ì¹˜ìˆ˜', 'specification', 'dimension'],
            'market_information': ['ì‹œì¥', 'íŠ¸ë Œë“œ', 'ê²½ìŸ', 'market', 'trend', 'competition']
        }

    def merge_multiple_contents(self, 
                              content_items: List[ContentItem],
                              merge_strategy: str = 'comprehensive') -> Dict:
        """
        ë‹¤ì¤‘ ë‚´ìš© ì§€ëŠ¥í˜• ë³‘í•©
        
        Args:
            content_items: ë³‘í•©í•  ë‚´ìš© ì•„ì´í…œë“¤
            merge_strategy: ë³‘í•© ì „ëµ ('comprehensive', 'temporal', 'importance_based')
            
        Returns:
            Dict: ë³‘í•© ê²°ê³¼
        """
        try:
            self.logger.info(f"ë‚´ìš© ë³‘í•© ì‹œì‘: {len(content_items)}ê°œ ì•„ì´í…œ, ì „ëµ: {merge_strategy}")
            
            # 1. ì „ì²˜ë¦¬
            processed_items = self._preprocess_content_items(content_items)
            
            # 2. ì¤‘ë³µ ë‚´ìš© ê°ì§€ ë° ì œê±°
            deduplicated_items = self._remove_duplicates(processed_items)
            
            # 3. ìœ ì‚¬ ë‚´ìš© ê·¸ë£¹í•‘
            similarity_groups = self._group_similar_content(deduplicated_items)
            
            # 4. ì‹œê°„ìˆœ ì •ë ¬
            temporal_sorted = self._sort_by_temporal_logic(similarity_groups)
            
            # 5. ë‚´ìš© ì¤‘ìš”ë„ ê³„ì‚°
            importance_scored = self._calculate_importance_scores(temporal_sorted)
            
            # 6. ë³‘í•© ì „ëµ ì ìš©
            merged_content = self._apply_merge_strategy(importance_scored, merge_strategy)
            
            # 7. ìµœì¢… ì •ë¦¬ ë° êµ¬ì¡°í™”
            final_result = self._finalize_merged_content(merged_content)
            
            # 8. ë³‘í•© ë©”íƒ€ë°ì´í„° ìƒì„±
            merge_metadata = self._generate_merge_metadata(content_items, final_result)
            
            return {
                'merged_content': final_result,
                'metadata': merge_metadata,
                'original_count': len(content_items),
                'final_count': len(final_result.get('sections', [])),
                'merge_strategy': merge_strategy,
                'quality_score': self._calculate_merge_quality_score(final_result),
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            self.logger.error(f"ë‚´ìš© ë³‘í•© ì˜¤ë¥˜: {str(e)}")
            return {
                'error': str(e),
                'merged_content': {},
                'metadata': {}
            }

    def merge_realtime_content(self, 
                             existing_content: Dict,
                             new_content: ContentItem) -> Dict:
        """
        ì‹¤ì‹œê°„ ë‚´ìš© ë³‘í•© (ê¸°ì¡´ ë‚´ìš©ì— ìƒˆ ë‚´ìš© ì¶”ê°€)
        
        Args:
            existing_content: ê¸°ì¡´ ë³‘í•©ëœ ë‚´ìš©
            new_content: ìƒˆë¡œ ì¶”ê°€í•  ë‚´ìš©
            
        Returns:
            Dict: ì—…ë°ì´íŠ¸ëœ ë³‘í•© ê²°ê³¼
        """
        try:
            # ìƒˆ ë‚´ìš© ì „ì²˜ë¦¬
            processed_new = self._preprocess_single_content(new_content)
            
            # ê¸°ì¡´ ë‚´ìš©ê³¼ ìœ ì‚¬ë„ ê²€ì‚¬
            similarity_results = self._check_similarity_with_existing(
                existing_content, processed_new
            )
            
            # ë³‘í•© ê²°ì •
            if similarity_results['should_merge']:
                # ê¸°ì¡´ ì„¹ì…˜ì— ë³‘í•©
                updated_content = self._merge_with_existing_section(
                    existing_content, processed_new, similarity_results
                )
            else:
                # ìƒˆ ì„¹ì…˜ìœ¼ë¡œ ì¶”ê°€
                updated_content = self._add_as_new_section(
                    existing_content, processed_new
                )
            
            # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            self._update_realtime_metadata(updated_content, new_content)
            
            return updated_content
            
        except Exception as e:
            self.logger.error(f"ì‹¤ì‹œê°„ ë‚´ìš© ë³‘í•© ì˜¤ë¥˜: {str(e)}")
            return existing_content

    def extract_key_insights(self, merged_content: Dict) -> Dict:
        """ë³‘í•©ëœ ë‚´ìš©ì—ì„œ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ"""
        try:
            insights = {
                'key_topics': [],
                'important_facts': [],
                'jewelry_specific_info': {},
                'action_items': [],
                'summary': '',
                'confidence_score': 0.0
            }
            
            if not merged_content.get('sections'):
                return insights
            
            # ì „ì²´ í…ìŠ¤íŠ¸ ê²°í•©
            all_text = ' '.join([
                section.get('content', '') 
                for section in merged_content.get('sections', [])
            ])
            
            # ì£¼ìš” í† í”½ ì¶”ì¶œ
            insights['key_topics'] = self._extract_key_topics(all_text)
            
            # ì¤‘ìš”í•œ ì‚¬ì‹¤ ì¶”ì¶œ
            insights['important_facts'] = self._extract_important_facts(all_text)
            
            # ì£¼ì–¼ë¦¬ íŠ¹í™” ì •ë³´ ì¶”ì¶œ
            if self.jewelry_mode:
                insights['jewelry_specific_info'] = self._extract_jewelry_info(all_text)
            
            # ì•¡ì…˜ ì•„ì´í…œ ì¶”ì¶œ
            insights['action_items'] = self._extract_action_items(all_text)
            
            # ìš”ì•½ ìƒì„±
            insights['summary'] = self._generate_smart_summary(merged_content)
            
            # ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
            insights['confidence_score'] = self._calculate_insight_confidence(
                merged_content, insights
            )
            
            return insights
            
        except Exception as e:
            self.logger.error(f"ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
            return {
                'key_topics': [],
                'important_facts': [],
                'jewelry_specific_info': {},
                'action_items': [],
                'summary': 'ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.',
                'confidence_score': 0.0
            }

    # === ì „ì²˜ë¦¬ ë©”ì„œë“œë“¤ ===
    
    def _preprocess_content_items(self, items: List[ContentItem]) -> List[ContentItem]:
        """ë‚´ìš© ì•„ì´í…œë“¤ ì „ì²˜ë¦¬"""
        processed_items = []
        
        for item in items:
            # ë‚´ìš© ì •ë¦¬
            cleaned_content = self._clean_content(item.content)
            
            # ìµœì†Œ ê¸¸ì´ ì²´í¬
            if len(cleaned_content) < self.merge_config['min_content_length']:
                continue
            
            # ì²˜ë¦¬ëœ ë‚´ìš© ì €ì¥
            item.processed_content = cleaned_content
            
            # ê¸°ë³¸ ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚°
            item.importance_score = self._calculate_basic_importance(item)
            
            processed_items.append(item)
        
        return processed_items
    
    def _preprocess_single_content(self, item: ContentItem) -> ContentItem:
        """ë‹¨ì¼ ë‚´ìš© ì•„ì´í…œ ì „ì²˜ë¦¬"""
        item.processed_content = self._clean_content(item.content)
        item.importance_score = self._calculate_basic_importance(item)
        return item
    
    def _clean_content(self, content: str) -> str:
        """ë‚´ìš© ì •ë¦¬"""
        if not content:
            return ""
        
        # ì¤‘ë³µ ì œê±°
        for pattern in self.duplicate_patterns:
            content = re.sub(pattern, r'\1', content, flags=re.IGNORECASE)
        
        # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        content = re.sub(r'\s+', ' ', content)
        
        # ì•ë’¤ ê³µë°± ì œê±°
        content = content.strip()
        
        return content
    
    def _calculate_basic_importance(self, item: ContentItem) -> float:
        """ê¸°ë³¸ ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚°"""
        score = 0.0
        content = item.processed_content or item.content
        
        # ì†ŒìŠ¤ íƒ€ì…ë³„ ê¸°ë³¸ ê°€ì¤‘ì¹˜
        source_weights = {
            'audio': self.merge_config['importance_weight_audio'],
            'image': self.merge_config['importance_weight_image'],
            'document': self.merge_config['importance_weight_document']
        }
        score += source_weights.get(item.source_type, 0.2)
        
        # ë‚´ìš© ê¸¸ì´ ë³´ë„ˆìŠ¤
        length_bonus = min(0.3, len(content) / 1000)
        score += length_bonus
        
        # ì£¼ì–¼ë¦¬ ìš©ì–´ ë³´ë„ˆìŠ¤
        if self.jewelry_mode:
            jewelry_bonus = self._calculate_jewelry_term_bonus(content)
            score += jewelry_bonus
        
        # ìˆ«ì/ë°ì´í„° í¬í•¨ ë³´ë„ˆìŠ¤
        if re.search(r'\d+(?:\.\d+)?', content):
            score += 0.1
        
        return min(1.0, score)
    
    def _calculate_jewelry_term_bonus(self, content: str) -> float:
        """ì£¼ì–¼ë¦¬ ìš©ì–´ ë³´ë„ˆìŠ¤ ê³„ì‚°"""
        bonus = 0.0
        content_lower = content.lower()
        
        for category, terms in self.jewelry_terms.items():
            found_terms = sum(1 for term in terms if term.lower() in content_lower)
            bonus += found_terms * 0.02  # ìš©ì–´ë‹¹ 2% ë³´ë„ˆìŠ¤
        
        return min(0.2, bonus)  # ìµœëŒ€ 20% ë³´ë„ˆìŠ¤
    
    # === ì¤‘ë³µ ì œê±° ë©”ì„œë“œë“¤ ===
    
    def _remove_duplicates(self, items: List[ContentItem]) -> List[ContentItem]:
        """ì¤‘ë³µ ë‚´ìš© ì œê±°"""
        unique_items = []
        seen_contents = []
        
        for item in items:
            content = item.processed_content
            is_duplicate = False
            
            for seen_content in seen_contents:
                similarity = self._calculate_text_similarity(content, seen_content)
                if similarity >= self.merge_config['duplicate_threshold']:
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                unique_items.append(item)
                seen_contents.append(content)
            else:
                self.logger.debug(f"ì¤‘ë³µ ë‚´ìš© ì œê±°: {content[:50]}...")
        
        return unique_items
    
    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        if not text1 or not text2:
            return 0.0
        
        # ê¸°ë³¸ ë¬¸ìì—´ ìœ ì‚¬ë„
        basic_similarity = SequenceMatcher(None, text1.lower(), text2.lower()).ratio()
        
        # ë‹¨ì–´ ê¸°ë°˜ ìœ ì‚¬ë„
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())
        
        if not words1 and not words2:
            return 1.0
        elif not words1 or not words2:
            return 0.0
        
        intersection = len(words1.intersection(words2))
        union = len(words1.union(words2))
        word_similarity = intersection / union if union > 0 else 0.0
        
        # ê°€ì¤‘ í‰ê· 
        return basic_similarity * 0.6 + word_similarity * 0.4
    
    # === ìœ ì‚¬ ë‚´ìš© ê·¸ë£¹í•‘ ë©”ì„œë“œë“¤ ===
    
    def _group_similar_content(self, items: List[ContentItem]) -> List[List[ContentItem]]:
        """ìœ ì‚¬í•œ ë‚´ìš©ë¼ë¦¬ ê·¸ë£¹í•‘"""
        groups = []
        ungrouped_items = items.copy()
        
        while ungrouped_items:
            current_item = ungrouped_items.pop(0)
            current_group = [current_item]
            
            # ìœ ì‚¬í•œ ì•„ì´í…œ ì°¾ê¸°
            remaining_items = []
            for item in ungrouped_items:
                similarity = self._calculate_text_similarity(
                    current_item.processed_content,
                    item.processed_content
                )
                
                if similarity >= self.merge_config['similarity_threshold']:
                    current_group.append(item)
                    item.similarity_groups.append(len(groups))
                else:
                    remaining_items.append(item)
            
            groups.append(current_group)
            ungrouped_items = remaining_items
        
        return groups
    
    # === ì‹œê°„ìˆœ ì •ë ¬ ë©”ì„œë“œë“¤ ===
    
    def _sort_by_temporal_logic(self, groups: List[List[ContentItem]]) -> List[List[ContentItem]]:
        """ì‹œê°„ ë…¼ë¦¬ì— ë”°ë¥¸ ì •ë ¬"""
        # ê° ê·¸ë£¹ ë‚´ì—ì„œ ì‹œê°„ìˆœ ì •ë ¬
        for group in groups:
            group.sort(key=lambda x: x.timestamp)
        
        # ê·¸ë£¹ë“¤ì„ ëŒ€í‘œ ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬
        groups.sort(key=lambda group: min(item.timestamp for item in group))
        
        return groups
    
    # === ì¤‘ìš”ë„ ê³„ì‚° ë©”ì„œë“œë“¤ ===
    
    def _calculate_importance_scores(self, groups: List[List[ContentItem]]) -> List[List[ContentItem]]:
        """ê·¸ë£¹ë³„ ì¤‘ìš”ë„ ì ìˆ˜ ì¬ê³„ì‚°"""
        for group in groups:
            # ê·¸ë£¹ ë‚´ ìƒí˜¸ ì°¸ì¡° ë³´ë„ˆìŠ¤
            if len(group) > 1:
                for item in group:
                    item.importance_score += 0.1 * (len(group) - 1)
            
            # ì‹œê°„ì  ì—°ì†ì„± ë³´ë„ˆìŠ¤
            if len(group) > 1:
                for i in range(1, len(group)):
                    time_diff = group[i].timestamp - group[i-1].timestamp
                    if time_diff <= self.merge_config['time_window_seconds']:
                        group[i].importance_score += 0.05
        
        return groups
    
    # === ë³‘í•© ì „ëµ ì ìš© ë©”ì„œë“œë“¤ ===
    
    def _apply_merge_strategy(self, groups: List[List[ContentItem]], strategy: str) -> Dict:
        """ë³‘í•© ì „ëµ ì ìš©"""
        if strategy == 'comprehensive':
            return self._comprehensive_merge(groups)
        elif strategy == 'temporal':
            return self._temporal_merge(groups)
        elif strategy == 'importance_based':
            return self._importance_based_merge(groups)
        else:
            return self._comprehensive_merge(groups)
    
    def _comprehensive_merge(self, groups: List[List[ContentItem]]) -> Dict:
        """ì¢…í•©ì  ë³‘í•©"""
        merged_sections = []
        
        for i, group in enumerate(groups):
            if not group:
                continue
            
            # ê·¸ë£¹ì˜ ëŒ€í‘œ ë‚´ìš© ê²°ì •
            primary_item = max(group, key=lambda x: x.importance_score)
            
            # ê·¸ë£¹ ë‚´ìš© í†µí•©
            merged_content = self._merge_group_content(group, primary_item)
            
            # ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
            category = self._classify_content_category(merged_content)
            
            section = {
                'section_id': f"section_{i+1}",
                'category': category,
                'content': merged_content,
                'source_types': list(set(item.source_type for item in group)),
                'timestamp_range': {
                    'start': min(item.timestamp for item in group),
                    'end': max(item.timestamp for item in group)
                },
                'importance_score': max(item.importance_score for item in group),
                'item_count': len(group)
            }
            
            merged_sections.append(section)
        
        return {
            'sections': merged_sections,
            'merge_type': 'comprehensive',
            'total_sections': len(merged_sections)
        }
    
    def _temporal_merge(self, groups: List[List[ContentItem]]) -> Dict:
        """ì‹œê°„ ê¸°ë°˜ ë³‘í•©"""
        # ëª¨ë“  ì•„ì´í…œì„ ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬
        all_items = []
        for group in groups:
            all_items.extend(group)
        
        all_items.sort(key=lambda x: x.timestamp)
        
        # ì‹œê°„ ìœˆë„ìš° ê¸°ë°˜ ì„¹ì…˜ ìƒì„±
        sections = []
        current_section_items = []
        section_start_time = None
        
        for item in all_items:
            if not current_section_items:
                current_section_items = [item]
                section_start_time = item.timestamp
            else:
                time_diff = item.timestamp - section_start_time
                if time_diff <= self.merge_config['time_window_seconds'] * 2:
                    current_section_items.append(item)
                else:
                    # í˜„ì¬ ì„¹ì…˜ ì™„ë£Œ
                    if current_section_items:
                        section = self._create_temporal_section(current_section_items)
                        sections.append(section)
                    
                    # ìƒˆ ì„¹ì…˜ ì‹œì‘
                    current_section_items = [item]
                    section_start_time = item.timestamp
        
        # ë§ˆì§€ë§‰ ì„¹ì…˜ ì¶”ê°€
        if current_section_items:
            section = self._create_temporal_section(current_section_items)
            sections.append(section)
        
        return {
            'sections': sections,
            'merge_type': 'temporal',
            'total_sections': len(sections)
        }
    
    def _importance_based_merge(self, groups: List[List[ContentItem]]) -> Dict:
        """ì¤‘ìš”ë„ ê¸°ë°˜ ë³‘í•©"""
        # ëª¨ë“  ì•„ì´í…œì„ ì¤‘ìš”ë„ìˆœìœ¼ë¡œ ì •ë ¬
        all_items = []
        for group in groups:
            all_items.extend(group)
        
        all_items.sort(key=lambda x: x.importance_score, reverse=True)
        
        # ì¤‘ìš”ë„ë³„ ì„¹ì…˜ ìƒì„±
        high_importance = [item for item in all_items if item.importance_score >= 0.7]
        medium_importance = [item for item in all_items if 0.4 <= item.importance_score < 0.7]
        low_importance = [item for item in all_items if item.importance_score < 0.4]
        
        sections = []
        
        if high_importance:
            sections.append({
                'section_id': 'high_importance',
                'category': 'critical_information',
                'content': self._merge_items_content(high_importance),
                'importance_level': 'high',
                'item_count': len(high_importance)
            })
        
        if medium_importance:
            sections.append({
                'section_id': 'medium_importance',
                'category': 'supporting_information',
                'content': self._merge_items_content(medium_importance),
                'importance_level': 'medium',
                'item_count': len(medium_importance)
            })
        
        if low_importance:
            sections.append({
                'section_id': 'low_importance',
                'category': 'additional_information',
                'content': self._merge_items_content(low_importance),
                'importance_level': 'low',
                'item_count': len(low_importance)
            })
        
        return {
            'sections': sections,
            'merge_type': 'importance_based',
            'total_sections': len(sections)
        }
    
    def _merge_group_content(self, group: List[ContentItem], primary_item: ContentItem) -> str:
        """ê·¸ë£¹ ë‚´ìš© ë³‘í•©"""
        if len(group) == 1:
            return group[0].processed_content
        
        # ì£¼ìš” ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë³´ì¡° ë‚´ìš© ì¶”ê°€
        merged_content = primary_item.processed_content
        
        for item in group:
            if item == primary_item:
                continue
            
            # ìƒˆë¡œìš´ ì •ë³´ê°€ ìˆëŠ”ì§€ í™•ì¸
            similarity = self._calculate_text_similarity(
                merged_content, item.processed_content
            )
            
            if similarity < 0.8:  # ì¶©ë¶„íˆ ë‹¤ë¥¸ ë‚´ìš©ì´ë©´ ì¶”ê°€
                merged_content += f" {item.processed_content}"
        
        return self._clean_content(merged_content)
    
    def _merge_items_content(self, items: List[ContentItem]) -> str:
        """ì•„ì´í…œë“¤ì˜ ë‚´ìš© ë³‘í•©"""
        if not items:
            return ""
        
        if len(items) == 1:
            return items[0].processed_content
        
        # ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_items = sorted(items, key=lambda x: x.importance_score, reverse=True)
        
        merged_content = sorted_items[0].processed_content
        
        for item in sorted_items[1:]:
            # ìœ ì‚¬ë„ ì²´í¬
            similarity = self._calculate_text_similarity(merged_content, item.processed_content)
            if similarity < 0.8:
                merged_content += f" {item.processed_content}"
        
        return self._clean_content(merged_content)
    
    def _create_temporal_section(self, items: List[ContentItem]) -> Dict:
        """ì‹œê°„ ê¸°ë°˜ ì„¹ì…˜ ìƒì„±"""
        if not items:
            return {}
        
        merged_content = self._merge_items_content(items)
        category = self._classify_content_category(merged_content)
        
        return {
            'section_id': f"temporal_{int(items[0].timestamp)}",
            'category': category,
            'content': merged_content,
            'source_types': list(set(item.source_type for item in items)),
            'timestamp_range': {
                'start': min(item.timestamp for item in items),
                'end': max(item.timestamp for item in items)
            },
            'item_count': len(items)
        }
    
    def _classify_content_category(self, content: str) -> str:
        """ë‚´ìš© ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
        content_lower = content.lower()
        
        category_scores = {}
        for category, keywords in self.content_categories.items():
            score = sum(1 for keyword in keywords if keyword.lower() in content_lower)
            if score > 0:
                category_scores[category] = score
        
        if category_scores:
            return max(category_scores, key=category_scores.get)
        else:
            return 'general_information'
    
    # === ìµœì¢… ì •ë¦¬ ë©”ì„œë“œë“¤ ===
    
    def _finalize_merged_content(self, merged_content: Dict) -> Dict:
        """ë³‘í•©ëœ ë‚´ìš© ìµœì¢… ì •ë¦¬"""
        # ì„¹ì…˜ ì¬ì •ë ¬ (ì¤‘ìš”ë„ ë° ì‹œê°„ìˆœ)
        sections = merged_content.get('sections', [])
        
        # ê° ì„¹ì…˜ì˜ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        for section in sections:
            section['quality_score'] = self._calculate_section_quality(section)
        
        # ì¤‘ìš”ë„ì™€ í’ˆì§ˆì„ ê³ ë ¤í•œ ì •ë ¬
        sections.sort(key=lambda x: (
            x.get('importance_score', 0) * 0.6 + 
            x.get('quality_score', 0) * 0.4
        ), reverse=True)
        
        # ì„¹ì…˜ ID ì¬í• ë‹¹
        for i, section in enumerate(sections):
            section['section_id'] = f"final_section_{i+1}"
            section['order'] = i + 1
        
        merged_content['sections'] = sections
        merged_content['final_section_count'] = len(sections)
        
        return merged_content
    
    def _calculate_section_quality(self, section: Dict) -> float:
        """ì„¹ì…˜ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        content = section.get('content', '')
        
        if not content:
            return 0.0
        
        quality_score = 0.0
        
        # ë‚´ìš© ê¸¸ì´ ì ìˆ˜
        length_score = min(1.0, len(content) / 500)
        quality_score += length_score * 0.3
        
        # ì•„ì´í…œ ìˆ˜ ì ìˆ˜
        item_count = section.get('item_count', 1)
        count_score = min(1.0, item_count / 5)
        quality_score += count_score * 0.2
        
        # ì†ŒìŠ¤ ë‹¤ì–‘ì„± ì ìˆ˜
        source_types = section.get('source_types', [])
        diversity_score = len(source_types) / 3  # ìµœëŒ€ 3ê°œ ì†ŒìŠ¤
        quality_score += diversity_score * 0.2
        
        # ì£¼ì–¼ë¦¬ ìš©ì–´ ì ìˆ˜ (ì£¼ì–¼ë¦¬ ëª¨ë“œì¸ ê²½ìš°)
        if self.jewelry_mode:
            jewelry_score = self._calculate_jewelry_term_bonus(content)
            quality_score += jewelry_score * 0.3
        else:
            quality_score += 0.3  # ê¸°ë³¸ ì ìˆ˜
        
        return min(1.0, quality_score)
    
    # === ë©”íƒ€ë°ì´í„° ìƒì„± ë©”ì„œë“œë“¤ ===
    
    def _generate_merge_metadata(self, original_items: List[ContentItem], final_result: Dict) -> Dict:
        """ë³‘í•© ë©”íƒ€ë°ì´í„° ìƒì„±"""
        metadata = {
            'merge_timestamp': datetime.now().isoformat(),
            'original_items': {
                'total_count': len(original_items),
                'audio_count': len([item for item in original_items if item.source_type == 'audio']),
                'image_count': len([item for item in original_items if item.source_type == 'image']),
                'document_count': len([item for item in original_items if item.source_type == 'document'])
            },
            'final_sections': {
                'total_count': len(final_result.get('sections', [])),
                'average_quality': np.mean([
                    section.get('quality_score', 0) 
                    for section in final_result.get('sections', [])
                ]) if final_result.get('sections') else 0
            },
            'compression_ratio': len(final_result.get('sections', [])) / len(original_items) if original_items else 0,
            'time_span': {
                'start': min(item.timestamp for item in original_items) if original_items else 0,
                'end': max(item.timestamp for item in original_items) if original_items else 0
            },
            'processing_statistics': {
                'duplicates_removed': len(original_items) - len(final_result.get('sections', [])),
                'categories_identified': len(set(
                    section.get('category', 'unknown') 
                    for section in final_result.get('sections', [])
                ))
            }
        }
        
        return metadata
    
    def _calculate_merge_quality_score(self, final_result: Dict) -> float:
        """ë³‘í•© í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        sections = final_result.get('sections', [])
        
        if not sections:
            return 0.0
        
        # ì„¹ì…˜ë³„ í’ˆì§ˆ ì ìˆ˜ í‰ê· 
        quality_scores = [section.get('quality_score', 0) for section in sections]
        average_quality = np.mean(quality_scores) if quality_scores else 0
        
        # ì¶”ê°€ í’ˆì§ˆ ìš”ì†Œë“¤
        category_diversity = len(set(section.get('category', 'unknown') for section in sections))
        diversity_bonus = min(0.2, category_diversity / 5)  # ìµœëŒ€ 20% ë³´ë„ˆìŠ¤
        
        total_score = average_quality + diversity_bonus
        
        return min(1.0, total_score)
    
    # === ì‹¤ì‹œê°„ ë³‘í•© ë©”ì„œë“œë“¤ ===
    
    def _check_similarity_with_existing(self, existing_content: Dict, new_item: ContentItem) -> Dict:
        """ê¸°ì¡´ ë‚´ìš©ê³¼ì˜ ìœ ì‚¬ë„ ê²€ì‚¬"""
        sections = existing_content.get('sections', [])
        
        if not sections:
            return {'should_merge': False, 'best_match': None, 'similarity': 0.0}
        
        best_similarity = 0.0
        best_match_section = None
        
        for section in sections:
            similarity = self._calculate_text_similarity(
                section.get('content', ''),
                new_item.processed_content
            )
            
            if similarity > best_similarity:
                best_similarity = similarity
                best_match_section = section
        
        should_merge = best_similarity >= self.merge_config['similarity_threshold']
        
        return {
            'should_merge': should_merge,
            'best_match': best_match_section,
            'similarity': best_similarity
        }
    
    def _merge_with_existing_section(self, existing_content: Dict, new_item: ContentItem, similarity_results: Dict) -> Dict:
        """ê¸°ì¡´ ì„¹ì…˜ì— ë³‘í•©"""
        best_match = similarity_results['best_match']
        
        if not best_match:
            return existing_content
        
        # ê¸°ì¡´ ë‚´ìš©ê³¼ ìƒˆ ë‚´ìš© ë³‘í•©
        existing_text = best_match.get('content', '')
        new_text = new_item.processed_content
        
        # ìœ ì‚¬ë„ê°€ ë†’ì§€ë§Œ ì™„ì „íˆ ê°™ì§€ ì•Šë‹¤ë©´ ì¶”ê°€ ì •ë³´ë¡œ ê°„ì£¼
        if similarity_results['similarity'] < 0.95:
            merged_text = f"{existing_text} {new_text}"
            best_match['content'] = self._clean_content(merged_text)
        
        # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
        if 'source_types' not in best_match:
            best_match['source_types'] = []
        
        if new_item.source_type not in best_match['source_types']:
            best_match['source_types'].append(new_item.source_type)
        
        # ì‹œê°„ ë²”ìœ„ ì—…ë°ì´íŠ¸
        if 'timestamp_range' not in best_match:
            best_match['timestamp_range'] = {'start': new_item.timestamp, 'end': new_item.timestamp}
        else:
            best_match['timestamp_range']['end'] = max(
                best_match['timestamp_range']['end'], 
                new_item.timestamp
            )
        
        return existing_content
    
    def _add_as_new_section(self, existing_content: Dict, new_item: ContentItem) -> Dict:
        """ìƒˆ ì„¹ì…˜ìœ¼ë¡œ ì¶”ê°€"""
        if 'sections' not in existing_content:
            existing_content['sections'] = []
        
        new_section = {
            'section_id': f"realtime_section_{len(existing_content['sections']) + 1}",
            'category': self._classify_content_category(new_item.processed_content),
            'content': new_item.processed_content,
            'source_types': [new_item.source_type],
            'timestamp_range': {
                'start': new_item.timestamp,
                'end': new_item.timestamp
            },
            'importance_score': new_item.importance_score,
            'item_count': 1,
            'quality_score': self._calculate_basic_importance(new_item)
        }
        
        existing_content['sections'].append(new_section)
        
        return existing_content
    
    def _update_realtime_metadata(self, content: Dict, new_item: ContentItem) -> None:
        """ì‹¤ì‹œê°„ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸"""
        if 'metadata' not in content:
            content['metadata'] = {}
        
        metadata = content['metadata']
        
        # ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œê°„
        metadata['last_update'] = datetime.now().isoformat()
        
        # ì´ ì•„ì´í…œ ìˆ˜
        metadata['total_items'] = metadata.get('total_items', 0) + 1
        
        # ì†ŒìŠ¤ë³„ ì¹´ìš´íŠ¸
        source_key = f"{new_item.source_type}_count"
        metadata[source_key] = metadata.get(source_key, 0) + 1
    
    # === ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ ë©”ì„œë“œë“¤ ===
    
    def _extract_key_topics(self, text: str) -> List[str]:
        """ì£¼ìš” í† í”½ ì¶”ì¶œ"""
        topics = []
        
        # ì£¼ì–¼ë¦¬ ê´€ë ¨ í† í”½ ì¶”ì¶œ
        if self.jewelry_mode:
            for category, terms in self.jewelry_terms.items():
                found_terms = [term for term in terms if term.lower() in text.lower()]
                if found_terms:
                    topics.extend(found_terms[:3])  # ì¹´í…Œê³ ë¦¬ë³„ ìµœëŒ€ 3ê°œ
        
        # ì¼ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ (ë¹ˆë„ ê¸°ë°˜)
        words = re.findall(r'\b\w{4,}\b', text.lower())
        word_freq = Counter(words)
        common_words = [word for word, count in word_freq.most_common(5) if count > 1]
        topics.extend(common_words)
        
        return list(set(topics))[:10]  # ìµœëŒ€ 10ê°œ í† í”½
    
    def _extract_important_facts(self, text: str) -> List[str]:
        """ì¤‘ìš”í•œ ì‚¬ì‹¤ ì¶”ì¶œ"""
        facts = []
        
        # ìˆ«ìê°€ í¬í•¨ëœ ë¬¸ì¥ (ê°€ê²©, ì‚¬ì–‘ ë“±)
        number_sentences = re.findall(r'[^.!?]*\d+[^.!?]*[.!?]', text)
        facts.extend(number_sentences[:3])
        
        # ëŒ€ë¬¸ìë¡œ ì‹œì‘í•˜ëŠ” ì¤‘ìš” ë¬¸ì¥
        important_sentences = re.findall(r'[A-Z][^.!?]{20,}[.!?]', text)
        facts.extend(important_sentences[:3])
        
        return facts[:5]  # ìµœëŒ€ 5ê°œ ì‚¬ì‹¤
    
    def _extract_jewelry_info(self, text: str) -> Dict:
        """ì£¼ì–¼ë¦¬ íŠ¹í™” ì •ë³´ ì¶”ì¶œ"""
        jewelry_info = {
            'materials': [],
            'quality_info': [],
            'products': [],
            'prices': [],
            'certifications': []
        }
        
        text_lower = text.lower()
        
        # ì¬ë£Œ ì •ë³´
        for term in self.jewelry_terms['material_terms']:
            if term.lower() in text_lower:
                jewelry_info['materials'].append(term)
        
        # í’ˆì§ˆ ì •ë³´
        for term in self.jewelry_terms['quality_terms']:
            if term.lower() in text_lower:
                jewelry_info['quality_info'].append(term)
        
        # ì œí’ˆ ì •ë³´
        for term in self.jewelry_terms['product_terms']:
            if term.lower() in text_lower:
                jewelry_info['products'].append(term)
        
        # ê°€ê²© ì •ë³´ ì¶”ì¶œ
        price_patterns = [
            r'\$[\d,]+(?:\.\d{2})?',
            r'â‚©[\d,]+',
            r'\d+\s*ì›',
            r'\d+\s*ë‹¬ëŸ¬'
        ]
        
        for pattern in price_patterns:
            matches = re.findall(pattern, text)
            jewelry_info['prices'].extend(matches)
        
        # ì¸ì¦ì„œ ì •ë³´
        cert_keywords = ['GIA', 'AGS', 'certificate', 'certification', 'ê°ì •ì„œ', 'ì¸ì¦ì„œ']
        for keyword in cert_keywords:
            if keyword.lower() in text_lower:
                jewelry_info['certifications'].append(keyword)
        
        return jewelry_info
    
    def _extract_action_items(self, text: str) -> List[str]:
        """ì•¡ì…˜ ì•„ì´í…œ ì¶”ì¶œ"""
        action_items = []
        
        # ëª…ë ¹í˜• ë™ì‚¬ë¡œ ì‹œì‘í•˜ëŠ” ë¬¸ì¥
        action_patterns = [
            r'[Cc]heck [^.!?]*[.!?]',
            r'[Cc]onfirm [^.!?]*[.!?]',
            r'[Cc]ontact [^.!?]*[.!?]',
            r'[Ss]end [^.!?]*[.!?]',
            r'í™•ì¸[^.!?]*[.!?]',
            r'ì—°ë½[^.!?]*[.!?]',
            r'ì¤€ë¹„[^.!?]*[.!?]'
        ]
        
        for pattern in action_patterns:
            matches = re.findall(pattern, text)
            action_items.extend(matches)
        
        # ì§ˆë¬¸ í˜•íƒœ (ë‹µë³€ì´ í•„ìš”í•œ ê²ƒë“¤)
        question_patterns = [
            r'[^.!?]*\?',
            r'[^.!?]*ì¸ê°€\?',
            r'[^.!?]*ì…ë‹ˆê¹Œ\?'
        ]
        
        for pattern in question_patterns:
            matches = re.findall(pattern, text)
            action_items.extend(matches[:2])  # ìµœëŒ€ 2ê°œ ì§ˆë¬¸
        
        return action_items[:5]  # ìµœëŒ€ 5ê°œ ì•¡ì…˜ ì•„ì´í…œ
    
    def _generate_smart_summary(self, merged_content: Dict) -> str:
        """ìŠ¤ë§ˆíŠ¸ ìš”ì•½ ìƒì„±"""
        sections = merged_content.get('sections', [])
        
        if not sections:
            return "ë³‘í•©ëœ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."
        
        # ê°€ì¥ ì¤‘ìš”í•œ ì„¹ì…˜ë“¤ì˜ ë‚´ìš© ì¶”ì¶œ
        important_sections = sorted(
            sections, 
            key=lambda x: x.get('importance_score', 0), 
            reverse=True
        )[:3]
        
        summary_parts = []
        
        for i, section in enumerate(important_sections):
            content = section.get('content', '')
            
            # ì²« ë²ˆì§¸ ë¬¸ì¥ ë˜ëŠ” ì²˜ìŒ 100ì ì¶”ì¶œ
            first_sentence = re.split(r'[.!?]', content)[0]
            if len(first_sentence) > 100:
                first_sentence = first_sentence[:100] + "..."
            elif len(first_sentence) < len(content):
                first_sentence += "."
            
            summary_parts.append(f"{i+1}. {first_sentence}")
        
        return " ".join(summary_parts)
    
    def _calculate_insight_confidence(self, merged_content: Dict, insights: Dict) -> float:
        """ì¸ì‚¬ì´íŠ¸ ì‹ ë¢°ë„ ê³„ì‚°"""
        confidence = 0.0
        
        # ì„¹ì…˜ ìˆ˜ ê¸°ë°˜ ì‹ ë¢°ë„
        section_count = len(merged_content.get('sections', []))
        confidence += min(0.3, section_count / 10)
        
        # ì†ŒìŠ¤ ë‹¤ì–‘ì„± ê¸°ë°˜ ì‹ ë¢°ë„
        all_source_types = set()
        for section in merged_content.get('sections', []):
            all_source_types.update(section.get('source_types', []))
        
        source_diversity = len(all_source_types)
        confidence += min(0.3, source_diversity / 3)
        
        # ì¶”ì¶œëœ ì •ë³´ í’ë¶€ì„± ê¸°ë°˜ ì‹ ë¢°ë„
        info_richness = (
            len(insights.get('key_topics', [])) +
            len(insights.get('important_facts', [])) +
            len(insights.get('action_items', []))
        )
        confidence += min(0.4, info_richness / 15)
        
        return min(1.0, confidence)


# ì‚¬ìš© ì˜ˆì œ
if __name__ == "__main__":
    merger = SmartContentMerger(jewelry_mode=True)
    
    print("ğŸ§  Smart Content Merger v2.1 - í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 50)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    test_items = [
        ContentItem(
            content="ë‹¤ì´ì•„ëª¬ë“œ ë°˜ì§€ì˜ í’ˆì§ˆì€ 4Cë¡œ í‰ê°€ë©ë‹ˆë‹¤. ìºëŸ¿, ì»·, íˆ¬ëª…ë„, ìƒ‰ìƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.",
            source_type="audio",
            timestamp=1625097600,
            metadata={"confidence": 85.0}
        ),
        ContentItem(
            content="Diamond ring quality assessment: 4C evaluation - Carat, Cut, Clarity, Color",
            source_type="image",
            timestamp=1625097605,
            metadata={"ocr_confidence": 92.0}
        ),
        ContentItem(
            content="GIA ê°ì •ì„œì— ë”°ë¥´ë©´ ì´ ë‹¤ì´ì•„ëª¬ë“œëŠ” 1.5ìºëŸ¿, VVS1 ë“±ê¸‰ì…ë‹ˆë‹¤.",
            source_type="document",
            timestamp=1625097610,
            metadata={"document_type": "certificate"}
        )
    ]
    
    # ë³‘í•© ì‹¤í–‰
    result = merger.merge_multiple_contents(test_items, merge_strategy='comprehensive')
    
    print(f"ë³‘í•© ê²°ê³¼:")
    print(f"- ì›ë³¸ ì•„ì´í…œ ìˆ˜: {result['original_count']}")
    print(f"- ìµœì¢… ì„¹ì…˜ ìˆ˜: {result['final_count']}")
    print(f"- ë³‘í•© í’ˆì§ˆ ì ìˆ˜: {result['quality_score']:.3f}")
    
    # ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ
    insights = merger.extract_key_insights(result['merged_content'])
    print(f"- ì¶”ì¶œëœ í‚¤ì›Œë“œ: {len(insights['key_topics'])}ê°œ")
    print(f"- ì‹ ë¢°ë„: {insights['confidence_score']:.3f}")
    
    print("ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ âœ…")
