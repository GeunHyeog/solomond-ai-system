#!/usr/bin/env python3
"""
ìŠ¤ë§ˆíŠ¸ AI ëª¨ë¸ ë¡œë” v2.6
ë©”ëª¨ë¦¬ ê´€ë¦¬ ê¸°ëŠ¥ì´ í†µí•©ëœ AI ëª¨ë¸ ë¡œë”
"""

import os
import time
import threading
from typing import Dict, List, Any, Optional, Union, Tuple
from pathlib import Path
import logging
from dataclasses import dataclass
from contextlib import contextmanager

from .ai_model_memory_manager import (
    get_global_ai_memory_manager, 
    ModelType, 
    ModelPriority,
    AIModelMemoryManager
)

@dataclass
class ModelLoadResult:
    """ëª¨ë¸ ë¡œë“œ ê²°ê³¼"""
    success: bool
    model: Optional[Any] = None
    model_id: str = ""
    load_time_seconds: float = 0.0
    memory_usage_mb: float = 0.0
    error_message: Optional[str] = None

class SmartAIModelLoader:
    """ìŠ¤ë§ˆíŠ¸ AI ëª¨ë¸ ë¡œë”"""
    
    def __init__(self, memory_manager: Optional[AIModelMemoryManager] = None):
        self.memory_manager = memory_manager or get_global_ai_memory_manager()
        self.logger = self._setup_logging()
        
        # ë¡œë“œëœ ëª¨ë¸ ìºì‹œ
        self.model_cache: Dict[str, Any] = {}
        self.load_lock = threading.RLock()
        
        # ëª¨ë¸ë³„ ê¸°ë³¸ ì„¤ì •
        self.model_configs = {
            ModelType.WHISPER: {
                'default_model': 'base',
                'device_preference': 'cpu',  # GPU ë©”ëª¨ë¦¬ ì ˆì•½
                'priority': ModelPriority.HIGH
            },
            ModelType.EASYOCR: {
                'default_languages': ['ko', 'en'],
                'device_preference': 'cpu',
                'priority': ModelPriority.MEDIUM
            },
            ModelType.TRANSFORMERS: {
                'default_model': 'bert-base-multilingual-cased',
                'device_preference': 'cpu',
                'priority': ModelPriority.MEDIUM
            }
        }
        
        self.logger.info("ğŸ¤– ìŠ¤ë§ˆíŠ¸ AI ëª¨ë¸ ë¡œë” ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _setup_logging(self) -> logging.Logger:
        """ë¡œê¹… ì„¤ì •"""
        logger = logging.getLogger(f'{__name__}.{self.__class__.__name__}')
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s | %(name)s | %(levelname)s | %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            logger.setLevel(logging.INFO)
        return logger
    
    def load_whisper_model(self, model_size: str = "base", device: str = "auto") -> ModelLoadResult:
        """Whisper ëª¨ë¸ ë¡œë“œ"""
        model_id = f"whisper_{model_size}"
        
        with self.load_lock:
            # ì´ë¯¸ ë¡œë“œëœ ëª¨ë¸ í™•ì¸
            cached_model = self.memory_manager.access_model(model_id)
            if cached_model is not None:
                return ModelLoadResult(
                    success=True,
                    model=cached_model,
                    model_id=model_id,
                    load_time_seconds=0.0
                )
            
            # ë©”ëª¨ë¦¬ ì••ë°• ìƒí™© ì²´í¬
            profile = self.memory_manager.get_memory_profile()
            if profile.memory_pressure_level in ['high', 'critical']:
                self.logger.warning("âš ï¸ ë©”ëª¨ë¦¬ ì••ë°•ìœ¼ë¡œ ì¸í•œ ì‚¬ì „ ìµœì í™” ì‹¤í–‰")
                self.memory_manager.optimize_memory()
            
            start_time = time.time()
            
            try:
                # ì¥ì¹˜ ê²°ì •
                if device == "auto":
                    device = "cpu"  # GPU ë©”ëª¨ë¦¬ ì ˆì•½ ìš°ì„ 
                
                # Whisper ëª¨ë¸ ë¡œë“œ
                import whisper
                
                self.logger.info(f"ğŸ¤ Whisper ëª¨ë¸ ë¡œë“œ ì¤‘: {model_size} (ì¥ì¹˜: {device})")
                
                # CPU ëª¨ë“œ ê°•ì œ ì„¤ì •
                os.environ['CUDA_VISIBLE_DEVICES'] = ''
                
                model = whisper.load_model(model_size, device=device)
                
                load_time = time.time() - start_time
                
                # ë©”ëª¨ë¦¬ ê´€ë¦¬ìì— ë“±ë¡
                success = self.memory_manager.register_model(
                    model_id=model_id,
                    model_type=ModelType.WHISPER,
                    model_obj=model,
                    priority=ModelPriority.HIGH,
                    device=device,
                    metadata={
                        'model_size': model_size,
                        'load_time': load_time,
                        'framework': 'whisper'
                    }
                )
                
                if success:
                    self.logger.info(f"âœ… Whisper ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_size} ({load_time:.2f}ì´ˆ)")
                    
                    return ModelLoadResult(
                        success=True,
                        model=model,
                        model_id=model_id,
                        load_time_seconds=load_time,
                        memory_usage_mb=self._get_model_memory(model_id)
                    )
                else:
                    return ModelLoadResult(
                        success=False,
                        error_message="ë©”ëª¨ë¦¬ ê´€ë¦¬ì ë“±ë¡ ì‹¤íŒ¨"
                    )
                
            except Exception as e:
                error_msg = f"Whisper ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}"
                self.logger.error(f"âŒ {error_msg}")
                
                return ModelLoadResult(
                    success=False,
                    error_message=error_msg
                )
    
    def load_easyocr_model(self, languages: List[str] = None, device: str = "auto") -> ModelLoadResult:
        """EasyOCR ëª¨ë¸ ë¡œë“œ"""
        if languages is None:
            languages = ['ko', 'en']
        
        model_id = f"easyocr_{'_'.join(languages)}"
        
        with self.load_lock:
            # ì´ë¯¸ ë¡œë“œëœ ëª¨ë¸ í™•ì¸
            cached_model = self.memory_manager.access_model(model_id)
            if cached_model is not None:
                return ModelLoadResult(
                    success=True,
                    model=cached_model,
                    model_id=model_id,
                    load_time_seconds=0.0
                )
            
            start_time = time.time()
            
            try:
                # ì¥ì¹˜ ê²°ì •
                if device == "auto":
                    device = "cpu"  # GPU ë©”ëª¨ë¦¬ ì ˆì•½
                
                # EasyOCR ëª¨ë¸ ë¡œë“œ
                import easyocr
                
                self.logger.info(f"ğŸ–¼ï¸ EasyOCR ëª¨ë¸ ë¡œë“œ ì¤‘: {languages} (ì¥ì¹˜: {device})")
                
                # GPU ì‚¬ìš© ì—¬ë¶€ ê²°ì •
                gpu = False if device == "cpu" else True
                
                reader = easyocr.Reader(languages, gpu=gpu)
                
                load_time = time.time() - start_time
                
                # ë©”ëª¨ë¦¬ ê´€ë¦¬ìì— ë“±ë¡
                success = self.memory_manager.register_model(
                    model_id=model_id,
                    model_type=ModelType.EASYOCR,
                    model_obj=reader,
                    priority=ModelPriority.MEDIUM,
                    device=device,
                    metadata={
                        'languages': languages,
                        'load_time': load_time,
                        'framework': 'easyocr'
                    }
                )
                
                if success:
                    self.logger.info(f"âœ… EasyOCR ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {languages} ({load_time:.2f}ì´ˆ)")
                    
                    return ModelLoadResult(
                        success=True,
                        model=reader,
                        model_id=model_id,
                        load_time_seconds=load_time,
                        memory_usage_mb=self._get_model_memory(model_id)
                    )
                else:
                    return ModelLoadResult(
                        success=False,
                        error_message="ë©”ëª¨ë¦¬ ê´€ë¦¬ì ë“±ë¡ ì‹¤íŒ¨"
                    )
                
            except Exception as e:
                error_msg = f"EasyOCR ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}"
                self.logger.error(f"âŒ {error_msg}")
                
                return ModelLoadResult(
                    success=False,
                    error_message=error_msg
                )
    
    def load_transformers_model(self, model_name: str, task: str = "summarization", device: str = "auto") -> ModelLoadResult:
        """Transformers ëª¨ë¸ ë¡œë“œ"""
        model_id = f"transformers_{model_name.replace('/', '_')}_{task}"
        
        with self.load_lock:
            # ì´ë¯¸ ë¡œë“œëœ ëª¨ë¸ í™•ì¸
            cached_model = self.memory_manager.access_model(model_id)
            if cached_model is not None:
                return ModelLoadResult(
                    success=True,
                    model=cached_model,
                    model_id=model_id,
                    load_time_seconds=0.0
                )
            
            start_time = time.time()
            
            try:
                # ì¥ì¹˜ ê²°ì •
                if device == "auto":
                    device = "cpu"  # GPU ë©”ëª¨ë¦¬ ì ˆì•½
                
                # Transformers ëª¨ë¸ ë¡œë“œ
                from transformers import pipeline
                
                self.logger.info(f"ğŸ¤– Transformers ëª¨ë¸ ë¡œë“œ ì¤‘: {model_name} ({task}, ì¥ì¹˜: {device})")
                
                # CPU ëª¨ë“œ ê°•ì œ ì„¤ì •
                import torch
                device_id = -1 if device == "cpu" else 0
                
                model = pipeline(
                    task=task,
                    model=model_name,
                    device=device_id,
                    torch_dtype=torch.float32  # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ float32 ì‚¬ìš©
                )
                
                load_time = time.time() - start_time
                
                # ë©”ëª¨ë¦¬ ê´€ë¦¬ìì— ë“±ë¡
                success = self.memory_manager.register_model(
                    model_id=model_id,
                    model_type=ModelType.TRANSFORMERS,
                    model_obj=model,
                    priority=ModelPriority.MEDIUM,
                    device=device,
                    metadata={
                        'model_name': model_name,
                        'task': task,
                        'load_time': load_time,
                        'framework': 'transformers'
                    }
                )
                
                if success:
                    self.logger.info(f"âœ… Transformers ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_name} ({load_time:.2f}ì´ˆ)")
                    
                    return ModelLoadResult(
                        success=True,
                        model=model,
                        model_id=model_id,
                        load_time_seconds=load_time,
                        memory_usage_mb=self._get_model_memory(model_id)
                    )
                else:
                    return ModelLoadResult(
                        success=False,
                        error_message="ë©”ëª¨ë¦¬ ê´€ë¦¬ì ë“±ë¡ ì‹¤íŒ¨"
                    )
                
            except Exception as e:
                error_msg = f"Transformers ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}"
                self.logger.error(f"âŒ {error_msg}")
                
                return ModelLoadResult(
                    success=False,
                    error_message=error_msg
                )
    
    def get_model(self, model_id: str) -> Optional[Any]:
        """ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°"""
        return self.memory_manager.access_model(model_id)
    
    def unload_model(self, model_id: str) -> bool:
        """ëª¨ë¸ ì–¸ë¡œë“œ"""
        success = self.memory_manager.unregister_model(model_id)
        if success:
            self.logger.info(f"ğŸ“¤ ëª¨ë¸ ì–¸ë¡œë“œ: {model_id}")
        return success
    
    def optimize_memory(self, target_mb: Optional[float] = None) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìµœì í™”"""
        return self.memory_manager.optimize_memory(target_mb)
    
    def get_memory_status(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìƒíƒœ ë°˜í™˜"""
        profile = self.memory_manager.get_memory_profile()
        stats = self.memory_manager.get_model_stats()
        
        return {
            'memory_profile': {
                'total_memory_mb': profile.total_memory_mb,
                'available_memory_mb': profile.available_memory_mb,
                'ai_models_memory_mb': profile.ai_models_memory_mb,
                'memory_pressure_level': profile.memory_pressure_level
            },
            'model_stats': stats,
            'loaded_models': self.memory_manager.get_model_details()
        }
    
    def _get_model_memory(self, model_id: str) -> float:
        """ëª¨ë¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°€ì ¸ì˜¤ê¸°"""
        model_details = self.memory_manager.get_model_details()
        for detail in model_details:
            if detail['model_id'] == model_id:
                return detail['memory_usage_mb']
        return 0.0
    
    @contextmanager
    def smart_model_context(self, model_type: str, **kwargs):
        """ìŠ¤ë§ˆíŠ¸ ëª¨ë¸ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""
        model_result = None
        
        try:
            # ëª¨ë¸ íƒ€ì…ë³„ ë¡œë“œ
            if model_type.lower() == "whisper":
                model_result = self.load_whisper_model(**kwargs)
            elif model_type.lower() == "easyocr":
                model_result = self.load_easyocr_model(**kwargs)
            elif model_type.lower() == "transformers":
                model_result = self.load_transformers_model(**kwargs)
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {model_type}")
            
            if not model_result.success:
                raise RuntimeError(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model_result.error_message}")
            
            yield model_result.model
            
        finally:
            # ì»¨í…ìŠ¤íŠ¸ ì¢…ë£Œ ì‹œ ë©”ëª¨ë¦¬ ìµœì í™” (ì„ íƒì )
            if model_result and model_result.success:
                # ë‚®ì€ ìš°ì„ ìˆœìœ„ ëª¨ë¸ë“¤ì€ ìë™ ì–¸ë¡œë“œë  ìˆ˜ ìˆìŒ
                pass
    
    def preload_models(self, model_configs: List[Dict[str, Any]]) -> Dict[str, ModelLoadResult]:
        """ëª¨ë¸ë“¤ ì‚¬ì „ ë¡œë“œ"""
        results = {}
        
        self.logger.info(f"ğŸš€ ëª¨ë¸ ì‚¬ì „ ë¡œë“œ ì‹œì‘: {len(model_configs)}ê°œ ëª¨ë¸")
        
        for config in model_configs:
            model_type = config.get('type')
            model_id = config.get('id', f"{model_type}_{len(results)}")
            
            try:
                if model_type == "whisper":
                    result = self.load_whisper_model(
                        model_size=config.get('model_size', 'base'),
                        device=config.get('device', 'auto')
                    )
                elif model_type == "easyocr":
                    result = self.load_easyocr_model(
                        languages=config.get('languages', ['ko', 'en']),
                        device=config.get('device', 'auto')
                    )
                elif model_type == "transformers":
                    result = self.load_transformers_model(
                        model_name=config.get('model_name', 'bert-base-multilingual-cased'),
                        task=config.get('task', 'summarization'),
                        device=config.get('device', 'auto')
                    )
                else:
                    result = ModelLoadResult(
                        success=False,
                        error_message=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {model_type}"
                    )
                
                results[model_id] = result
                
                if result.success:
                    self.logger.info(f"âœ… ì‚¬ì „ ë¡œë“œ ì„±ê³µ: {model_id}")
                else:
                    self.logger.error(f"âŒ ì‚¬ì „ ë¡œë“œ ì‹¤íŒ¨: {model_id} - {result.error_message}")
                
            except Exception as e:
                results[model_id] = ModelLoadResult(
                    success=False,
                    error_message=str(e)
                )
                self.logger.error(f"âŒ ì‚¬ì „ ë¡œë“œ ì˜¤ë¥˜: {model_id} - {e}")
        
        success_count = sum(1 for r in results.values() if r.success)
        self.logger.info(f"ğŸ‰ ëª¨ë¸ ì‚¬ì „ ë¡œë“œ ì™„ë£Œ: {success_count}/{len(model_configs)} ì„±ê³µ")
        
        return results
    
    def get_model_recommendations(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì‚¬ìš© ê¶Œì¥ì‚¬í•­"""
        profile = self.memory_manager.get_memory_profile()
        
        recommendations = {
            'memory_status': profile.memory_pressure_level,
            'recommendations': []
        }
        
        if profile.memory_pressure_level == 'critical':
            recommendations['recommendations'].extend([
                "ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì‹¬ê°í•©ë‹ˆë‹¤. ì¦‰ì‹œ ì¼ë¶€ ëª¨ë¸ì„ ì–¸ë¡œë“œí•˜ì„¸ìš”.",
                "Whisper ëª¨ë¸ì„ 'tiny' ë˜ëŠ” 'base'ë¡œ ë³€ê²½í•˜ì„¸ìš”.",
                "ë¶ˆí•„ìš”í•œ Transformers ëª¨ë¸ì„ ì œê±°í•˜ì„¸ìš”."
            ])
        elif profile.memory_pressure_level == 'high':
            recommendations['recommendations'].extend([
                "ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ìŠµë‹ˆë‹¤. ëª¨ë¸ ìµœì í™”ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.",
                "ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ë“¤ì„ í™•ì¸í•˜ì—¬ ì–¸ë¡œë“œí•˜ì„¸ìš”.",
                "ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš©ì„ ê³ ë ¤í•˜ì„¸ìš”."
            ])
        elif profile.memory_pressure_level == 'medium':
            recommendations['recommendations'].extend([
                "ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë³´í†µ ìˆ˜ì¤€ì…ë‹ˆë‹¤.",
                "í•„ìš”ì— ë”°ë¼ ë” í° ëª¨ë¸ ì‚¬ìš©ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.",
                "ì£¼ê¸°ì ì¸ ë©”ëª¨ë¦¬ ìµœì í™”ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤."
            ])
        else:
            recommendations['recommendations'].extend([
                "ë©”ëª¨ë¦¬ ìƒíƒœê°€ ì–‘í˜¸í•©ë‹ˆë‹¤.",
                "ê³ ì„±ëŠ¥ ëª¨ë¸ ì‚¬ìš©ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.",
                "ì¶”ê°€ ëª¨ë¸ ë¡œë“œê°€ ì•ˆì „í•©ë‹ˆë‹¤."
            ])
        
        return recommendations
    
    def cleanup(self) -> None:
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        self.logger.info("ğŸ§¹ ìŠ¤ë§ˆíŠ¸ AI ëª¨ë¸ ë¡œë” ì •ë¦¬")
        # ë©”ëª¨ë¦¬ ê´€ë¦¬ìëŠ” ì „ì—­ì´ë¯€ë¡œ ì—¬ê¸°ì„œ ì •ë¦¬í•˜ì§€ ì•ŠìŒ

# ì „ì—­ ìŠ¤ë§ˆíŠ¸ ëª¨ë¸ ë¡œë”
_global_smart_loader = None
_global_loader_lock = threading.Lock()

def get_global_smart_loader() -> SmartAIModelLoader:
    """ì „ì—­ ìŠ¤ë§ˆíŠ¸ ëª¨ë¸ ë¡œë” ê°€ì ¸ì˜¤ê¸°"""
    global _global_smart_loader
    
    with _global_loader_lock:
        if _global_smart_loader is None:
            _global_smart_loader = SmartAIModelLoader()
        return _global_smart_loader

# í¸ì˜ í•¨ìˆ˜ë“¤
def smart_load_whisper(model_size: str = "base", device: str = "auto") -> ModelLoadResult:
    """Whisper ëª¨ë¸ ìŠ¤ë§ˆíŠ¸ ë¡œë“œ"""
    loader = get_global_smart_loader()
    return loader.load_whisper_model(model_size, device)

def smart_load_easyocr(languages: List[str] = None, device: str = "auto") -> ModelLoadResult:
    """EasyOCR ëª¨ë¸ ìŠ¤ë§ˆíŠ¸ ë¡œë“œ"""
    loader = get_global_smart_loader()
    return loader.load_easyocr_model(languages, device)

def smart_load_transformers(model_name: str, task: str = "summarization", device: str = "auto") -> ModelLoadResult:
    """Transformers ëª¨ë¸ ìŠ¤ë§ˆíŠ¸ ë¡œë“œ"""
    loader = get_global_smart_loader()
    return loader.load_transformers_model(model_name, task, device)

def get_ai_memory_status() -> Dict[str, Any]:
    """AI ëª¨ë¸ ë©”ëª¨ë¦¬ ìƒíƒœ ì¡°íšŒ"""
    loader = get_global_smart_loader()
    return loader.get_memory_status()

def optimize_ai_models_memory(target_mb: Optional[float] = None) -> Dict[str, Any]:
    """AI ëª¨ë¸ ë©”ëª¨ë¦¬ ìµœì í™”"""
    loader = get_global_smart_loader()
    return loader.optimize_memory(target_mb)

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ìŠ¤ë§ˆíŠ¸ AI ëª¨ë¸ ë¡œë” í…ŒìŠ¤íŠ¸
    loader = SmartAIModelLoader()
    
    # Whisper ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸
    print("ğŸ¤ Whisper ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸")
    whisper_result = loader.load_whisper_model("base")
    
    if whisper_result.success:
        print(f"âœ… Whisper ë¡œë“œ ì„±ê³µ: {whisper_result.load_time_seconds:.2f}ì´ˆ, {whisper_result.memory_usage_mb:.1f}MB")
    else:
        print(f"âŒ Whisper ë¡œë“œ ì‹¤íŒ¨: {whisper_result.error_message}")
    
    # EasyOCR ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸
    print("\nğŸ–¼ï¸ EasyOCR ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸")
    easyocr_result = loader.load_easyocr_model(['ko', 'en'])
    
    if easyocr_result.success:
        print(f"âœ… EasyOCR ë¡œë“œ ì„±ê³µ: {easyocr_result.load_time_seconds:.2f}ì´ˆ, {easyocr_result.memory_usage_mb:.1f}MB")
    else:
        print(f"âŒ EasyOCR ë¡œë“œ ì‹¤íŒ¨: {easyocr_result.error_message}")
    
    # ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
    print("\nğŸ“Š ë©”ëª¨ë¦¬ ìƒíƒœ:")
    status = loader.get_memory_status()
    memory_profile = status['memory_profile']
    print(f"  AI ëª¨ë¸ ë©”ëª¨ë¦¬: {memory_profile['ai_models_memory_mb']:.1f}MB")
    print(f"  ë©”ëª¨ë¦¬ ì••ë°• ìˆ˜ì¤€: {memory_profile['memory_pressure_level']}")
    
    # ëª¨ë¸ ê¶Œì¥ì‚¬í•­
    print("\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
    recommendations = loader.get_model_recommendations()
    for rec in recommendations['recommendations']:
        print(f"  - {rec}")
    
    # ì •ë¦¬
    loader.cleanup()
    print("\nâœ… ìŠ¤ë§ˆíŠ¸ AI ëª¨ë¸ ë¡œë” í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")