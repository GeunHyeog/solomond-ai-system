#!/usr/bin/env python3
"""
ê°„ë‹¨í•œ Flask ê¸°ë°˜ AI ë¶„ì„ ì›¹ì•±
Streamlit ë¬¸ì œë¥¼ ìš°íšŒí•˜ì—¬ AI ê¸°ëŠ¥ êµ¬í˜„
"""

from flask import Flask, request, render_template_string, jsonify
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

try:
    from shared.ollama_interface import OllamaInterface
    ollama = OllamaInterface()
    AI_AVAILABLE = True
except Exception as e:
    AI_AVAILABLE = False
    print(f"Ollama not available: {e}")

app = Flask(__name__)

HTML_TEMPLATE = """
<!DOCTYPE html>
<html>
<head>
    <title>ğŸ¤– AI ì»¨í¼ëŸ°ìŠ¤ ë¶„ì„ê¸°</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 40px; }
        .container { max-width: 800px; margin: 0 auto; }
        textarea { width: 100%; height: 200px; }
        button { background: #4CAF50; color: white; padding: 10px 20px; border: none; border-radius: 5px; cursor: pointer; }
        .result { background: #f9f9f9; padding: 20px; margin-top: 20px; border-radius: 5px; }
        .status { color: #666; font-style: italic; }
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸ¤– AI ì»¨í¼ëŸ°ìŠ¤ ë¶„ì„ê¸°</h1>
        
        {% if ai_available %}
            <p class="status">âœ… AI ì—°ê²°ë¨ ({{ model_count }}ê°œ ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥)</p>
        {% else %}
            <p class="status">âŒ AI ì—°ê²° ì‹¤íŒ¨</p>
        {% endif %}
        
        <form method="post">
            <h3>ğŸ“ íšŒì˜ ë‚´ìš© ì…ë ¥:</h3>
            <textarea name="content" placeholder="í™”ì1: ì•ˆë…•í•˜ì„¸ìš”. ì˜¤ëŠ˜ íšŒì˜ë¥¼ ì‹œì‘í•˜ê² ìŠµë‹ˆë‹¤.
í™”ì2: ë„¤, í”„ë¡œì íŠ¸ ì§„í–‰ ìƒí™©ì„ ì ê²€í•´ë³´ê² ìŠµë‹ˆë‹¤.">{{ content or '' }}</textarea>
            
            <br><br>
            <button type="submit">ğŸ” AI ë¶„ì„ ì‹¤í–‰</button>
        </form>
        
        {% if analysis %}
        <div class="result">
            <h3>ğŸ¯ AI ë¶„ì„ ê²°ê³¼:</h3>
            
            <h4>ğŸ” ì „ì²´ ìƒí™© ë¶„ì„:</h4>
            <p>{{ analysis.situation_analysis }}</p>
            
            <h4>ğŸ­ í™”ìë³„ ì˜ë¯¸ ë¶„ì„:</h4>
            {% for speaker, meaning in analysis.speaker_meanings.items() %}
                <p><strong>{{ speaker }}:</strong> {{ meaning }}</p>
            {% endfor %}
            
            <h4>ğŸ“‹ íšŒì˜ ë§¥ë½ ë¶„ì„:</h4>
            <p>{{ analysis.context_analysis }}</p>
            
            <h4>ğŸ¯ AI ì¢…í•© ê²°ë¡ :</h4>
            <p><strong>{{ analysis.conclusion }}</strong></p>
        </div>
        {% endif %}
    </div>
</body>
</html>
"""

@app.route('/', methods=['GET', 'POST'])
def index():
    content = ""
    analysis = None
    
    if request.method == 'POST':
        content = request.form.get('content', '')
        
        if content and AI_AVAILABLE:
            # í™”ì êµ¬ë¶„
            speaker_contents = {}
            lines = content.split('\n')
            
            for line in lines:
                if 'í™”ì' in line and ':' in line:
                    if 'í™”ì1' in line:
                        speaker_id = 'í™”ì_1'
                    elif 'í™”ì2' in line:
                        speaker_id = 'í™”ì_2'
                    else:
                        continue
                    
                    if speaker_id not in speaker_contents:
                        speaker_contents[speaker_id] = []
                    
                    speaker_text = line.split(':', 1)[-1].strip()
                    if speaker_text:
                        speaker_contents[speaker_id].append(speaker_text)
            
            # AI ë¶„ì„ ì‹¤í–‰
            try:
                model = "qwen2.5:7b"
                
                # ì „ì²´ ìƒí™© ë¶„ì„
                situation_prompt = f"""ë‹¤ìŒ íšŒì˜ ë‚´ìš©ì„ ë¶„ì„í•´ì£¼ì„¸ìš”:

{content}

ë¶„ì„ ê²°ê³¼ (ê°„ë‹¨íˆ):
1. íšŒì˜ ì£¼ì œ:
2. ì£¼ìš” ì´ìŠˆ:
3. ë¶„ìœ„ê¸°:"""
                
                situation_analysis = ollama.generate_response(situation_prompt, model=model)
                
                # í™”ìë³„ ì˜ë¯¸ ë¶„ì„
                speaker_meanings = {}
                for speaker_id, contents in speaker_contents.items():
                    if contents:
                        combined_speech = " ".join(contents)
                        speaker_prompt = f"{speaker_id} ë°œì–¸ ì˜ë¯¸ ë¶„ì„: {combined_speech}"
                        speaker_meanings[speaker_id] = ollama.generate_response(speaker_prompt, model=model)[:200] + "..."
                
                # íšŒì˜ ë§¥ë½ ë¶„ì„
                context_prompt = f"íšŒì˜ ê²°ë¡ ê³¼ ë‹¤ìŒ ë‹¨ê³„: {content[:500]}"
                context_analysis = ollama.generate_response(context_prompt, model=model)
                
                # ì¢…í•© ê²°ë¡ 
                conclusion_prompt = f"í•œ ë¬¸ì¥ìœ¼ë¡œ ì´ íšŒì˜ë¥¼ ìš”ì•½í•˜ë©´: {content[:300]}"
                conclusion = ollama.generate_response(conclusion_prompt, model=model)
                
                analysis = {
                    'situation_analysis': situation_analysis[:500] + "...",
                    'speaker_meanings': speaker_meanings,
                    'context_analysis': context_analysis[:300] + "...",
                    'conclusion': conclusion[:200] + "..."
                }
                
            except Exception as e:
                analysis = {'error': str(e)}
    
    return render_template_string(HTML_TEMPLATE, 
                                content=content, 
                                analysis=analysis,
                                ai_available=AI_AVAILABLE,
                                model_count=len(ollama.available_models) if AI_AVAILABLE else 0)

if __name__ == '__main__':
    print("AI ë¶„ì„ ì›¹ì•± ì‹œì‘...")
    print("ì ‘ì† ì£¼ì†Œ: http://localhost:8555")
    app.run(host='127.0.0.1', port=8555, debug=False)