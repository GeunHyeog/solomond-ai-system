#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ”§ Serena ìë™ ìˆ˜ì • ë„êµ¬
SOLOMOND AI ì‹œìŠ¤í…œì˜ ì¼ë°˜ì ì¸ ë¬¸ì œë“¤ì„ ìë™ìœ¼ë¡œ ìˆ˜ì •

ì£¼ìš” ê¸°ëŠ¥:
1. ThreadPool ì—ëŸ¬ ìë™ ìˆ˜ì •
2. ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€ ì½”ë“œ ì¶”ê°€
3. Streamlit ì„±ëŠ¥ ìµœì í™”
4. GPU/CPU ë©”ëª¨ë¦¬ ê´€ë¦¬ ê°œì„ 
5. ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”
"""

import re
import ast
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
import logging
from dataclasses import dataclass

@dataclass
class FixRule:
    """ìˆ˜ì • ê·œì¹™"""
    name: str
    pattern: str
    replacement: str
    description: str
    priority: int  # 1=highest, 5=lowest
    file_types: List[str]

class SerenaAutoFixer:
    """Serena ìë™ ìˆ˜ì • ë„êµ¬"""
    
    def __init__(self):
        self.logger = self._setup_logger()
        self.fix_rules = self._initialize_fix_rules()
        self.backup_suffix = ".serena_backup"
        
    def _setup_logger(self) -> logging.Logger:
        logger = logging.getLogger("SerenaAutoFixer")
        logger.setLevel(logging.INFO)
        handler = logging.FileHandler("serena_auto_fixer.log")
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        return logger
    
    def _initialize_fix_rules(self) -> List[FixRule]:
        """ìˆ˜ì • ê·œì¹™ ì´ˆê¸°í™”"""
        return [
            # ThreadPool ë¬¸ì œ ìˆ˜ì •
            FixRule(
                name="threadpool_context_manager",
                pattern=r'(\s*)(executor\s*=\s*ThreadPoolExecutor\([^)]*\))\s*\n((?:(?!\nexecutor\.shutdown).*\n)*)',
                replacement=r'\1with ThreadPoolExecutor(\3) as executor:\n\1    # ThreadPool ì‘ì—…ë“¤\n\1    pass  # ì‹¤ì œ ì‘ì—… ì½”ë“œë¥¼ ì—¬ê¸°ì— ë°°ì¹˜\n',
                description="ThreadPoolExecutorë¥¼ with ë¬¸ìœ¼ë¡œ ê°ì‹¸ì„œ ìë™ ì •ë¦¬",
                priority=1,
                file_types=[".py"]
            ),
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì¶”ê°€
            FixRule(
                name="gpu_memory_cleanup",
                pattern=r'(import torch.*?\n)',
                replacement=r'\1import gc\n\ndef cleanup_gpu_memory():\n    """GPU ë©”ëª¨ë¦¬ ì •ë¦¬"""\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        gc.collect()\n\n',
                description="GPU ë©”ëª¨ë¦¬ ì •ë¦¬ í•¨ìˆ˜ ì¶”ê°€",
                priority=2,
                file_types=[".py"]
            ),
            
            # Streamlit ìºì‹œ ìµœì í™”
            FixRule(
                name="streamlit_cache_optimization",
                pattern=r'@st\.cache\b',
                replacement='@st.cache_data',
                description="ì˜¤ë˜ëœ st.cacheë¥¼ st.cache_dataë¡œ ì—…ë°ì´íŠ¸",
                priority=2,
                file_types=[".py"]
            ),
            
            # ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€
            FixRule(
                name="large_object_cleanup",
                pattern=r'(\s*)(.*(?:large_data|big_array|huge_list|massive_dict)\s*=\s*.*)\n',
                replacement=r'\1\2\n\1# ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€: ì‚¬ìš© í›„ ì •ë¦¬\n\1# del large_data; gc.collect()  # í•„ìš”ì‹œ ì£¼ì„ í•´ì œ\n',
                description="ëŒ€ìš©ëŸ‰ ê°ì²´ì— ì •ë¦¬ ì½”ë©˜íŠ¸ ì¶”ê°€",
                priority=3,
                file_types=[".py"]
            ),
            
            # ì˜ˆì™¸ ì²˜ë¦¬ ê°œì„ 
            FixRule(
                name="empty_except_handler",
                pattern=r'(\s*except.*?:\s*\n\s*)pass\s*\n',
                replacement=r'\1logging.warning("Exception occurred but was ignored")\n\1pass\n',
                description="ë¹ˆ except ë¸”ë¡ì— ë¡œê¹… ì¶”ê°€",
                priority=2,
                file_types=[".py"]
            ),
            
            # íŒŒì¼ ì²˜ë¦¬ with ë¬¸ ì‚¬ìš©
            FixRule(
                name="file_context_manager",
                pattern=r'(\s*)f\s*=\s*open\(([^)]+)\)\s*\n',
                replacement=r'\1with open(\2) as f:\n\1    # íŒŒì¼ ì‘ì—…ì„ ì—¬ê¸°ì—ì„œ ìˆ˜í–‰\n\1    pass\n',
                description="íŒŒì¼ ì—´ê¸°ë¥¼ with ë¬¸ìœ¼ë¡œ ë³€ê²½",
                priority=2,
                file_types=[".py"]
            )
        ]
    
    def analyze_file(self, file_path: str) -> Dict[str, List[Dict]]:
        """íŒŒì¼ ë¶„ì„í•˜ì—¬ ì ìš© ê°€ëŠ¥í•œ ìˆ˜ì •ì‚¬í•­ ì°¾ê¸°"""
        fixes_needed = {}
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            for rule in self.fix_rules:
                if any(file_path.endswith(ft) for ft in rule.file_types):
                    matches = list(re.finditer(rule.pattern, content, re.MULTILINE | re.DOTALL))
                    
                    if matches:
                        fixes_needed[rule.name] = [
                            {
                                'rule': rule,
                                'match': match,
                                'line_number': content[:match.start()].count('\n') + 1,
                                'matched_text': match.group()
                            }
                            for match in matches
                        ]
                        
        except Exception as e:
            self.logger.error(f"íŒŒì¼ ë¶„ì„ ì‹¤íŒ¨ {file_path}: {e}")
            
        return fixes_needed
    
    def create_backup(self, file_path: str) -> str:
        """íŒŒì¼ ë°±ì—… ìƒì„±"""
        backup_path = file_path + self.backup_suffix
        
        try:
            with open(file_path, 'r', encoding='utf-8') as original:
                content = original.read()
            
            with open(backup_path, 'w', encoding='utf-8') as backup:
                backup.write(content)
                
            self.logger.info(f"ë°±ì—… ìƒì„±: {backup_path}")
            return backup_path
            
        except Exception as e:
            self.logger.error(f"ë°±ì—… ìƒì„± ì‹¤íŒ¨ {file_path}: {e}")
            raise
    
    def apply_fix(self, file_path: str, rule_name: str) -> bool:
        """íŠ¹ì • ìˆ˜ì • ê·œì¹™ ì ìš©"""
        rule = next((r for r in self.fix_rules if r.name == rule_name), None)
        if not rule:
            self.logger.error(f"ì•Œ ìˆ˜ ì—†ëŠ” ê·œì¹™: {rule_name}")
            return False
        
        try:
            # ë°±ì—… ìƒì„±
            self.create_backup(file_path)
            
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # íŒ¨í„´ ë§¤ì¹­ ë° êµì²´
            original_content = content
            content = re.sub(rule.pattern, rule.replacement, content, flags=re.MULTILINE | re.DOTALL)
            
            if content != original_content:
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(content)
                
                self.logger.info(f"ìˆ˜ì • ì ìš© ì™„ë£Œ: {file_path} - {rule.name}")
                return True
            else:
                self.logger.info(f"ìˆ˜ì • ì‚¬í•­ ì—†ìŒ: {file_path} - {rule.name}")
                return False
                
        except Exception as e:
            self.logger.error(f"ìˆ˜ì • ì ìš© ì‹¤íŒ¨ {file_path} - {rule_name}: {e}")
            return False
    
    def apply_all_fixes(self, file_path: str, priority_threshold: int = 3) -> Dict[str, bool]:
        """ìš°ì„ ìˆœìœ„ê°€ ì„ê³„ê°’ ì´í•˜ì¸ ëª¨ë“  ìˆ˜ì •ì‚¬í•­ ì ìš©"""
        fixes_needed = self.analyze_file(file_path)
        results = {}
        
        # ìš°ì„ ìˆœìœ„ ìˆœìœ¼ë¡œ ì •ë ¬
        applicable_rules = [
            rule for rule in self.fix_rules 
            if rule.priority <= priority_threshold and rule.name in fixes_needed
        ]
        applicable_rules.sort(key=lambda r: r.priority)
        
        for rule in applicable_rules:
            result = self.apply_fix(file_path, rule.name)
            results[rule.name] = result
            
        return results
    
    def fix_threadpool_specifically(self, file_path: str) -> bool:
        """ThreadPool ë¬¸ì œ íŠ¹ë³„ ìˆ˜ì •"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ë°±ì—… ìƒì„±
            self.create_backup(file_path)
            
            # ThreadPoolExecutor íŒ¨í„´ë“¤ ì°¾ê¸°
            patterns_and_fixes = [
                # íŒ¨í„´ 1: ê¸°ë³¸ ThreadPoolExecutor ì‚¬ìš©
                (
                    r'(\s*)(executor\s*=\s*ThreadPoolExecutor\([^)]*\))\s*\n(.*?)(\n\s*executor\.shutdown\(\))',
                    r'\1with ThreadPoolExecutor() as executor:\n\3'
                ),
                
                # íŒ¨í„´ 2: submit ì‚¬ìš©í•˜ëŠ” ê²½ìš°
                (
                    r'(\s*)(executor\s*=\s*ThreadPoolExecutor\([^)]*\))\s*\n((?:.*?executor\.submit.*?\n)*)(.*?)(\n\s*(?:executor\.shutdown\(\)|del\s+executor))',
                    r'\1with ThreadPoolExecutor() as executor:\n\3\4'
                ),
                
                # íŒ¨í„´ 3: map ì‚¬ìš©í•˜ëŠ” ê²½ìš°
                (
                    r'(\s*)(executor\s*=\s*ThreadPoolExecutor\([^)]*\))\s*\n((?:.*?executor\.map.*?\n)*)(.*?)(\n\s*(?:executor\.shutdown\(\)|del\s+executor))',
                    r'\1with ThreadPoolExecutor() as executor:\n\3\4'
                )
            ]
            
            modified = False
            for pattern, replacement in patterns_and_fixes:
                new_content = re.sub(pattern, replacement, content, flags=re.MULTILINE | re.DOTALL)
                if new_content != content:
                    content = new_content
                    modified = True
            
            # ThreadPoolExecutor import í™•ì¸ ë° ì¶”ê°€
            if 'ThreadPoolExecutor' in content and 'from concurrent.futures import ThreadPoolExecutor' not in content:
                if 'import concurrent.futures' not in content:
                    # import ì¶”ê°€
                    import_line = 'from concurrent.futures import ThreadPoolExecutor\n'
                    
                    # ë‹¤ë¥¸ import ë¬¸ ë’¤ì— ì¶”ê°€
                    import_match = re.search(r'((?:^import .*?\n|^from .*? import .*?\n)+)', content, re.MULTILINE)
                    if import_match:
                        content = content[:import_match.end()] + import_line + content[import_match.end():]
                    else:
                        content = import_line + content
                    modified = True
            
            if modified:
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(content)
                
                self.logger.info(f"ThreadPool ìˆ˜ì • ì™„ë£Œ: {file_path}")
                return True
            else:
                self.logger.info(f"ThreadPool ìˆ˜ì • ì‚¬í•­ ì—†ìŒ: {file_path}")
                return False
                
        except Exception as e:
            self.logger.error(f"ThreadPool ìˆ˜ì • ì‹¤íŒ¨ {file_path}: {e}")
            return False
    
    def generate_fix_report(self, directory: str) -> Dict[str, Any]:
        """ë””ë ‰í† ë¦¬ ì „ì²´ ìˆ˜ì • ë³´ê³ ì„œ ìƒì„±"""
        directory = Path(directory)
        report = {
            'timestamp': str(datetime.now()),
            'total_files': 0,
            'files_with_issues': 0,
            'total_issues': 0,
            'issues_by_type': {},
            'files_analyzed': []
        }
        
        for py_file in directory.glob("**/*.py"):
            if any(skip in str(py_file) for skip in ['venv', '__pycache__', '.git']):
                continue
            
            report['total_files'] += 1
            fixes_needed = self.analyze_file(str(py_file))
            
            if fixes_needed:
                report['files_with_issues'] += 1
                
                file_report = {
                    'file': str(py_file),
                    'issues': {}
                }
                
                for rule_name, matches in fixes_needed.items():
                    issue_count = len(matches)
                    report['total_issues'] += issue_count
                    
                    if rule_name not in report['issues_by_type']:
                        report['issues_by_type'][rule_name] = 0
                    report['issues_by_type'][rule_name] += issue_count
                    
                    file_report['issues'][rule_name] = {
                        'count': issue_count,
                        'locations': [match['line_number'] for match in matches]
                    }
                
                report['files_analyzed'].append(file_report)
        
        return report

def fix_solomond_threadpool_issues():
    """SOLOMOND AI ì‹œìŠ¤í…œì˜ ThreadPool ì´ìŠˆ ì¼ê´„ ìˆ˜ì •"""
    fixer = SerenaAutoFixer()
    
    # ì£¼ìš” íŒŒì¼ë“¤ ìˆ˜ì •
    important_files = [
        'conference_analysis_COMPLETE_WORKING.py',
        'hybrid_compute_manager.py',
        'core/multimodal_pipeline.py',
        'core/batch_processing_engine.py'
    ]
    
    results = {}
    
    for file_name in important_files:
        file_path = Path(file_name)
        if file_path.exists():
            print(f"ğŸ”§ {file_name} ThreadPool ì´ìŠˆ ìˆ˜ì • ì¤‘...")
            success = fixer.fix_threadpool_specifically(str(file_path))
            results[file_name] = success
            
            if success:
                print(f"âœ… {file_name} ìˆ˜ì • ì™„ë£Œ")
            else:
                print(f"â„¹ï¸  {file_name} ìˆ˜ì • ì‚¬í•­ ì—†ìŒ")
        else:
            print(f"âŒ {file_name} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            results[file_name] = False
    
    return results

if __name__ == "__main__":
    from datetime import datetime
    
    print("ğŸ§  Serena ìë™ ìˆ˜ì • ë„êµ¬ ì‹œì‘")
    print("ğŸ¯ SOLOMOND AI ThreadPool ì´ìŠˆ ìˆ˜ì • ì¤‘...")
    
    results = fix_solomond_threadpool_issues()
    
    print("\nğŸ“Š ìˆ˜ì • ê²°ê³¼:")
    for file_name, success in results.items():
        status = "âœ… ì„±ê³µ" if success else "âŒ ì‹¤íŒ¨/ë¶ˆí•„ìš”"
        print(f"  {file_name}: {status}")
    
    print(f"\nğŸ’¾ ë¡œê·¸ íŒŒì¼: serena_auto_fixer.log")
    print("ğŸ‰ Serena ìë™ ìˆ˜ì • ì™„ë£Œ!")