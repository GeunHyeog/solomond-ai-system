#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ¯ ì™„ì „ í†µí•© ì»¨í¼ëŸ°ìŠ¤ ë¶„ì„ ì‹œìŠ¤í…œ - SOLOMOND AI v7.1
Complete Unified Conference Analysis System

8501ê³¼ 8650ì˜ ëª¨ë“  ì¥ì ì„ í†µí•©í•œ ì™„ì „í•œ ì‹œìŠ¤í…œ:
âœ… ì‹¤ì œ íŒŒì¼ ì—…ë¡œë“œ ë° ì²˜ë¦¬ (EasyOCR, Whisper STT)
âœ… Supabase í´ë¼ìš°ë“œ ë°ì´í„°ë² ì´ìŠ¤ ì§€ì›  
âœ… í™€ë¦¬ìŠ¤í‹± ë¶„ì„ (ì˜ë¯¸ì  ì—°ê²°, ì£¼ì œ í´ëŸ¬ìŠ¤í„°ë§)
âœ… ë“€ì–¼ ë¸Œë ˆì¸ ì‹œìŠ¤í…œ í†µí•©
âœ… í—ˆìœ„ì •ë³´ ì™„ì „ ì°¨ë‹¨

í•µì‹¬ ì›ì¹™:
- ëª¨ë“  ê¸°ëŠ¥ì´ ì‹¤ì œë¡œ ì‘ë™í•´ì•¼ í•¨
- í—ˆìœ„ ìƒíƒœ í‘œì‹œ ì ˆëŒ€ ê¸ˆì§€
- ì‹¤ì œ ë¶„ì„ ê²°ê³¼ë§Œ ì œê³µ
"""

import streamlit as st
import os
import tempfile
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional
import traceback
import re
from collections import Counter
import json
import uuid
import hashlib
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import numpy as np

# ì‹¤ì œ ë¶„ì„ ì—”ì§„ë“¤
try:
    import easyocr
    OCR_AVAILABLE = True
except ImportError:
    OCR_AVAILABLE = False

try:
    import whisper
    # ğŸ›¡ï¸ ì•ˆì „í•œ ëª¨ë¸ ë¡œë”© ì‹œìŠ¤í…œ ì„í¬íŠ¸
    from defensive_model_loader import safe_whisper_load, enable_defensive_mode
    enable_defensive_mode()  # ì „ì—­ ì•ˆì „ ëª¨ë“œ í™œì„±í™”
    WHISPER_AVAILABLE = True
except ImportError:
    WHISPER_AVAILABLE = False
    safe_whisper_load = None

# í™€ë¦¬ìŠ¤í‹± ë¶„ì„ ì‹œìŠ¤í…œ
try:
    from holistic_conference_analyzer_supabase import HolisticConferenceAnalyzerSupabase
    from semantic_connection_engine import SemanticConnectionEngine
    from conference_story_generator import ConferenceStoryGenerator
    from actionable_insights_extractor import ActionableInsightsExtractor
    HOLISTIC_AVAILABLE = True
except ImportError:
    HOLISTIC_AVAILABLE = False

# ë“€ì–¼ ë¸Œë ˆì¸ ì‹œìŠ¤í…œ
try:
    from dual_brain_integration import DualBrainSystem
    DUAL_BRAIN_AVAILABLE = True
except ImportError:
    DUAL_BRAIN_AVAILABLE = False

# ë°ì´í„°ë² ì´ìŠ¤ ì–´ëŒ‘í„°
try:
    from database_adapter import DatabaseFactory
    DATABASE_ADAPTER_AVAILABLE = True
except ImportError:
    DATABASE_ADAPTER_AVAILABLE = False

# Ollama AI ì¸í„°í˜ì´ìŠ¤
try:
    from shared.ollama_interface import OllamaInterface
    OLLAMA_AVAILABLE = True
except ImportError:
    OLLAMA_AVAILABLE = False

# ìœ íŠœë¸Œ ë‹¤ìš´ë¡œë”
try:
    import yt_dlp
    YOUTUBE_AVAILABLE = True
except ImportError:
    YOUTUBE_AVAILABLE = False

class UnifiedConferenceAnalyzer:
    """ì™„ì „ í†µí•© ì»¨í¼ëŸ°ìŠ¤ ë¶„ì„ê¸°"""
    
    def __init__(self, conference_name: str = "unified_conference"):
        self.conference_name = conference_name
        self.session_id = str(uuid.uuid4())[:8]
        
        # ì‚¬ì „ ì •ë³´ ì €ì¥
        self.conference_info = {
            "conference_name": "",
            "conference_date": "",
            "location": "",
            "industry_field": "",
            "interest_keywords": []
        }
        
        # ë¶„ì„ ê²°ê³¼ ì €ì¥
        self.analysis_results = {
            "session_id": self.session_id,
            "conference_name": conference_name,
            "conference_info": self.conference_info,
            "timestamp": datetime.now().isoformat(),
            "processed_files": [],
            "analysis_data": {},
            "holistic_results": {},
            "dual_brain_results": {}
        }
        
        # ì‹¤ì œ ì—”ì§„ë“¤ ì´ˆê¸°í™”
        self._initialize_engines()
        
        # ë¡œì»¬ íŒŒì¼ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self.user_files_dir = Path("user_files")
        self.user_files_dir.mkdir(exist_ok=True)
    
    def _initialize_engines(self):
        """ì‹¤ì œ ë¶„ì„ ì—”ì§„ë“¤ ì´ˆê¸°í™”"""
        # OCR ì—”ì§„
        self.ocr_engine = None
        if OCR_AVAILABLE:
            try:
                self.ocr_engine = easyocr.Reader(['ko', 'en'], gpu=False)
                st.success("âœ… EasyOCR ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                st.warning(f"âš ï¸ EasyOCR ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # Whisper ì—”ì§„
        self.whisper_model = None
        if WHISPER_AVAILABLE:
            try:
                # ğŸ›¡ï¸ ì•ˆì „í•œ ëª¨ë¸ ë¡œë”©ìœ¼ë¡œ meta tensor ë¬¸ì œ ì™„ì „ í•´ê²°
                self.whisper_model = safe_whisper_load("base")
                st.success("âœ… Whisper STT ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                st.warning(f"âš ï¸ Whisper ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # í™€ë¦¬ìŠ¤í‹± ë¶„ì„ê¸°
        self.holistic_analyzer = None
        if HOLISTIC_AVAILABLE:
            try:
                self.holistic_analyzer = HolisticConferenceAnalyzerSupabase(self.conference_name, "auto")
                st.success("âœ… í™€ë¦¬ìŠ¤í‹± ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                st.warning(f"âš ï¸ í™€ë¦¬ìŠ¤í‹± ë¶„ì„ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # ë“€ì–¼ ë¸Œë ˆì¸ ì‹œìŠ¤í…œ
        self.dual_brain = None
        if DUAL_BRAIN_AVAILABLE:
            try:
                self.dual_brain = DualBrainSystem()
                st.success("âœ… ë“€ì–¼ ë¸Œë ˆì¸ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                st.warning(f"âš ï¸ ë“€ì–¼ ë¸Œë ˆì¸ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # ë°ì´í„°ë² ì´ìŠ¤
        self.database = None
        if DATABASE_ADAPTER_AVAILABLE:
            try:
                self.database = DatabaseFactory.create_database("auto", self.conference_name)
                # í…Œì´ë¸” ìƒì„± ì‹œë„
                if self.database.create_fragments_table():
                    st.success("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì–´ëŒ‘í„° ì´ˆê¸°í™” ì™„ë£Œ")
                else:
                    st.warning("âš ï¸ ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨")
            except Exception as e:
                st.warning(f"âš ï¸ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # Ollama AI ì¸í„°í˜ì´ìŠ¤
        self.ollama = None
        if OLLAMA_AVAILABLE:
            try:
                self.ollama = OllamaInterface()
                if self.ollama.health_check():
                    available_models = self.ollama.available_models
                    st.success(f"âœ… Ollama AI ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ ({len(available_models)}ê°œ ëª¨ë¸)")
                    st.info(f"ğŸ¤– ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {', '.join(available_models[:3])}...")
                else:
                    st.warning("âš ï¸ Ollama ì„œë²„ ì—°ê²° ì‹¤íŒ¨ - ollama serve ì‹¤í–‰ í•„ìš”")
                    self.ollama = None
            except Exception as e:
                st.warning(f"âš ï¸ Ollama ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.ollama = None
    
    def update_conference_info(self, info: Dict[str, Any]):
        """ì‚¬ì „ ì •ë³´ ì—…ë°ì´íŠ¸"""
        self.conference_info.update(info)
        self.analysis_results["conference_info"] = self.conference_info
    
    def check_system_status(self) -> Dict[str, Any]:
        """ì‹¤ì œ ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ (í—ˆìœ„ì •ë³´ ì—†ìŒ)"""
        status = {
            "ocr_available": self.ocr_engine is not None,
            "whisper_available": self.whisper_model is not None,
            "holistic_available": self.holistic_analyzer is not None,
            "dual_brain_available": self.dual_brain is not None,
            "database_available": self.database is not None and self._check_database_working(),
            "ollama_available": self.ollama is not None and self.ollama.health_check(),
            "overall_ready": False
        }
        
        # ì „ì²´ ì¤€ë¹„ ìƒíƒœ ê³„ì‚°
        ready_count = sum([
            status["ocr_available"],
            status["whisper_available"], 
            status["holistic_available"],
            status["database_available"],
            status["ollama_available"]
        ])
        
        status["overall_ready"] = ready_count >= 4  # ìµœì†Œ 4ê°œ ì‹œìŠ¤í…œ í•„ìš”
        status["ready_systems"] = ready_count
        status["total_systems"] = 5
        
        return status
    
    def _check_database_working(self) -> bool:
        """ë°ì´í„°ë² ì´ìŠ¤ ì‹¤ì œ ì‘ë™ í™•ì¸"""
        if not self.database:
            return False
        try:
            # í…Œì´ë¸” ìƒì„± ì‹œë„
            self.database.create_fragments_table()
            # ê°„ë‹¨í•œ ì¿¼ë¦¬ë¡œ ì‘ë™ í™•ì¸
            count = self.database.get_fragment_count()
            return True
        except Exception:
            return False
    
    def process_uploaded_files(self, uploaded_files: List, skip_errors: bool = True) -> Dict[str, Any]:
        """ì‹¤ì œ ì—…ë¡œë“œëœ íŒŒì¼ë“¤ ì²˜ë¦¬"""
        if not uploaded_files:
            return {"error": "ì—…ë¡œë“œëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."}
        
        results = {
            "processed_count": 0,
            "successful_count": 0,
            "failed_files": [],
            "analysis_fragments": []
        }
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for i, uploaded_file in enumerate(uploaded_files):
            try:
                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                progress = (i + 1) / len(uploaded_files)
                progress_bar.progress(progress)
                status_text.text(f"ì²˜ë¦¬ ì¤‘: {uploaded_file.name} ({i+1}/{len(uploaded_files)})")
                
                # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
                with tempfile.NamedTemporaryFile(delete=False, suffix=Path(uploaded_file.name).suffix) as tmp_file:
                    tmp_file.write(uploaded_file.read())
                    tmp_path = tmp_file.name
                
                # íŒŒì¼ íƒ€ì…ë³„ ì‹¤ì œ ì²˜ë¦¬
                file_ext = Path(uploaded_file.name).suffix.lower()
                
                if file_ext in ['.jpg', '.jpeg', '.png', '.bmp']:
                    fragment = self._process_image_file(tmp_path, uploaded_file.name)
                elif file_ext in ['.wav', '.mp3', '.m4a', '.flac']:
                    fragment = self._process_audio_file(tmp_path, uploaded_file.name)
                elif file_ext in ['.mp4', '.avi', '.mov']:
                    fragment = self._process_video_file(tmp_path, uploaded_file.name)
                elif file_ext == '.txt':
                    # TXT íŒŒì¼ì˜ ê²½ìš° URL ë°°ì¹˜ ì²˜ë¦¬
                    st.info(f"ğŸ“„ TXT íŒŒì¼ ê°ì§€: {uploaded_file.name} - URL ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘")
                    
                    # ì„ì‹œ íŒŒì¼ì„ ë‹¤ì‹œ ìƒì„±í•˜ì—¬ process_text_file_urlsì— ì „ë‹¬
                    uploaded_file.seek(0)  # íŒŒì¼ í¬ì¸í„° ë¦¬ì…‹
                    batch_results = self.process_text_file_urls(uploaded_file)
                    
                    if batch_results and "error" not in batch_results[0]:
                        # ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼ë¥¼ ë¶„ì„ ê²°ê³¼ì— ì¶”ê°€
                        results["analysis_fragments"].extend(batch_results)
                        results["successful_count"] += len(batch_results)
                        st.success(f"âœ… {uploaded_file.name}ì—ì„œ {len(batch_results)}ê°œ URL ì²˜ë¦¬ ì™„ë£Œ!")
                    else:
                        error_msg = batch_results[0]["error"] if batch_results else "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜"
                        results["failed_files"].append({
                            "filename": uploaded_file.name,
                            "error": error_msg
                        })
                    
                    # TXT íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ, continueë¡œ ì¼ë°˜ ì²˜ë¦¬ ìŠ¤í‚µ
                    results["processed_count"] += 1
                    continue
                else:
                    fragment = {"error": f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_ext}"}
                
                # ì •ë¦¬
                os.unlink(tmp_path)
                
                if "error" not in fragment:
                    results["analysis_fragments"].append(fragment)
                    results["successful_count"] += 1
                    
                    # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                    if self.database:
                        self.database.insert_fragment(fragment)
                else:
                    results["failed_files"].append({
                        "filename": uploaded_file.name,
                        "error": fragment["error"]
                    })
                
                results["processed_count"] += 1
                
            except Exception as e:
                if not skip_errors:
                    # ì—ëŸ¬ ë°œìƒì‹œ ì¦‰ì‹œ ì¤‘ë‹¨
                    return {"error": f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}
                
                results["failed_files"].append({
                    "filename": uploaded_file.name,
                    "error": str(e)
                })
                results["processed_count"] += 1
        
        # ì§„í–‰ë¥  ì™„ë£Œ
        progress_bar.progress(1.0)
        status_text.text("âœ… íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ!")
        
        # ê²°ê³¼ë¥¼ ì„¸ì…˜ì— ì €ì¥
        self.analysis_results["processed_files"] = results["analysis_fragments"]
        
        return results
    
    def _process_image_file(self, file_path: str, filename: str) -> Dict[str, Any]:
        """ì‹¤ì œ ì´ë¯¸ì§€ íŒŒì¼ OCR ì²˜ë¦¬"""
        if not self.ocr_engine:
            return {"error": "OCR ì—”ì§„ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
        
        try:
            # EasyOCRë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            ocr_results = self.ocr_engine.readtext(file_path)
            
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì‹ ë¢°ë„ ê³„ì‚°
            extracted_texts = []
            confidences = []
            
            for (bbox, text, confidence) in ocr_results:
                if confidence > 0.5:  # ì‹ ë¢°ë„ 50% ì´ìƒë§Œ
                    extracted_texts.append(text.strip())
                    confidences.append(confidence)
            
            if not extracted_texts:
                return {"error": "ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
            
            full_text = " ".join(extracted_texts)
            avg_confidence = np.mean(confidences)
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ í† í°í™”)
            keywords = self._extract_keywords(full_text)
            
            fragment = {
                'fragment_id': f'{self.conference_name}_{self.session_id}_{hashlib.md5(filename.encode()).hexdigest()[:8]}',
                'file_source': filename,
                'file_type': 'image',
                'timestamp': datetime.now().isoformat(),
                'speaker': None,  # ì´ë¯¸ì§€ëŠ” í™”ì ì—†ìŒ
                'content': full_text,
                'confidence': float(avg_confidence),
                'keywords': keywords,
                'raw_ocr_results': len(ocr_results)
            }
            
            return fragment
            
        except Exception as e:
            return {"error": f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}"}
    
    def _process_audio_file(self, file_path: str, filename: str) -> Dict[str, Any]:
        """ì‹¤ì œ ìŒì„± íŒŒì¼ Whisper STT ì²˜ë¦¬"""
        if not self.whisper_model:
            return {"error": "Whisper ì—”ì§„ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
        
        try:
            # Whisperë¡œ ìŒì„±-í…ìŠ¤íŠ¸ ë³€í™˜
            result = self.whisper_model.transcribe(file_path, language='ko')
            
            if not result["text"].strip():
                return {"error": "ìŒì„±ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
            
            text = result["text"].strip()
            
            # ê°„ë‹¨í•œ í™”ì ë¶„ë¦¬ (ì„¸ê·¸ë¨¼íŠ¸ ê¸°ë°˜)
            segments = result.get("segments", [])
            if segments:
                # ê°€ì¥ ê¸´ ì„¸ê·¸ë¨¼íŠ¸ì˜ í™”ìë¥¼ ë©”ì¸ í™”ìë¡œ ê°€ì •
                main_segment = max(segments, key=lambda s: len(s.get("text", "")))
                speaker = f"í™”ì_{hashlib.md5(filename.encode()).hexdigest()[:4]}"
            else:
                speaker = None
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = self._extract_keywords(text)
            
            fragment = {
                'fragment_id': f'{self.conference_name}_{self.session_id}_{hashlib.md5(filename.encode()).hexdigest()[:8]}',
                'file_source': filename,
                'file_type': 'audio',
                'timestamp': datetime.now().isoformat(),
                'speaker': speaker,
                'content': text,
                'confidence': 0.85,  # WhisperëŠ” ì¼ë°˜ì ìœ¼ë¡œ ë†’ì€ ì •í™•ë„
                'keywords': keywords,
                'duration': result.get("duration", 0),
                'segments_count': len(segments)
            }
            
            return fragment
            
        except Exception as e:
            return {"error": f"ìŒì„± ì²˜ë¦¬ ì‹¤íŒ¨: {e}"}
    
    def _process_video_file(self, file_path: str, filename: str) -> Dict[str, Any]:
        """ë¹„ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬ (ìŒì„± ì¶”ì¶œ í›„ STT)"""
        if not self.whisper_model:
            return {"error": "Whisper ì—”ì§„ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
        
        try:
            # Whisperë¡œ ë¹„ë””ì˜¤ì˜ ìŒì„± ì§ì ‘ ì²˜ë¦¬
            result = self.whisper_model.transcribe(file_path, language='ko')
            
            if not result["text"].strip():
                return {"error": "ë¹„ë””ì˜¤ì—ì„œ ìŒì„±ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
            
            text = result["text"].strip()
            
            # í™”ì ì¶”ì •
            segments = result.get("segments", [])
            speaker = f"í™”ì_{hashlib.md5(filename.encode()).hexdigest()[:4]}" if segments else None
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = self._extract_keywords(text)
            
            fragment = {
                'fragment_id': f'{self.conference_name}_{self.session_id}_{hashlib.md5(filename.encode()).hexdigest()[:8]}',
                'file_source': filename,
                'file_type': 'video',
                'timestamp': datetime.now().isoformat(),
                'speaker': speaker,
                'content': text,
                'confidence': 0.85,
                'keywords': keywords,
                'duration': result.get("duration", 0),
                'segments_count': len(segments)
            }
            
            return fragment
            
        except Exception as e:
            return {"error": f"ë¹„ë””ì˜¤ ì²˜ë¦¬ ì‹¤íŒ¨: {e}"}
    
    def _extract_keywords(self, text: str) -> List[str]:
        """ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # í•œê¸€/ì˜ë¬¸ ë‹¨ì–´ë§Œ ì¶”ì¶œ
        words = re.findall(r'[ê°€-í£a-zA-Z]{2,}', text)
        
        # ë¹ˆë„ìˆ˜ ê¸°ë°˜ ìƒìœ„ í‚¤ì›Œë“œ ì¶”ì¶œ
        word_freq = Counter(words)
        keywords = [word for word, count in word_freq.most_common(10) if count >= 1]
        
        return keywords
    
    def run_holistic_analysis(self) -> Dict[str, Any]:
        """í™€ë¦¬ìŠ¤í‹± ë¶„ì„ ì‹¤í–‰"""
        if not self.holistic_analyzer:
            return {"error": "í™€ë¦¬ìŠ¤í‹± ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
        
        try:
            # ë°ì´í„°ë² ì´ìŠ¤ì— ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
            if self.database:
                self.database.create_fragments_table()
                fragment_count = self.database.get_fragment_count(self.conference_name)
                
                if fragment_count == 0:
                    return {"error": "ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ì²˜ë¦¬í•˜ì„¸ìš”."}
            
            # í™€ë¦¬ìŠ¤í‹± ë¶„ì„ ì‹¤í–‰
            result = self.holistic_analyzer.analyze_conference_holistically()
            
            if "error" in result:
                return result
            
            # ì˜ë¯¸ì  ì—°ê²° ë¶„ì„
            semantic_engine = SemanticConnectionEngine(self.conference_name)
            semantic_result = semantic_engine.analyze_semantic_connections()
            
            # Ollama AI ê¸°ë°˜ ì‹¬í™” ë¶„ì„
            ollama_insights = self._generate_ai_insights(result)
            
            # ê²°ê³¼ í†µí•©
            combined_result = {
                "holistic_analysis": result,
                "semantic_connections": semantic_result,
                "ai_insights": ollama_insights,
                "analysis_timestamp": datetime.now().isoformat(),
                "database_type": type(self.database).__name__ if self.database else "None"
            }
            
            # ì„¸ì…˜ì— ì €ì¥
            self.analysis_results["holistic_results"] = combined_result
            
            return combined_result
            
        except Exception as e:
            return {"error": f"í™€ë¦¬ìŠ¤í‹± ë¶„ì„ ì‹¤íŒ¨: {e}"}
    
    def _generate_ai_insights(self, holistic_result: Dict[str, Any]) -> List[str]:
        """Ollama AIë¥¼ í™œìš©í•œ ì‹¬í™” ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        if not self.ollama:
            return ["AI ì¸ì‚¬ì´íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (Ollama ë¹„í™œì„±)"]
        
        try:
            # í™€ë¦¬ìŠ¤í‹± ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            analysis_summary = f"""
            ì»¨í¼ëŸ°ìŠ¤ ë¶„ì„ ê²°ê³¼:
            - ì´ ì¡°ê° ìˆ˜: {holistic_result.get('total_fragments', 0)}ê°œ
            - ë°œê²¬ëœ ê°œì²´: {holistic_result.get('total_entities', 0)}ê°œ
            - ì£¼ìš” ì£¼ì œ: {holistic_result.get('total_topics', 0)}ê°œ
            
            í•µì‹¬ ì¸ì‚¬ì´íŠ¸: {', '.join(holistic_result.get('key_insights', []))}
            """
            
            # ì»¨í¼ëŸ°ìŠ¤ ë¶„ì„ íŠ¹í™” ëª¨ë¸ ì‚¬ìš©
            ai_response = self.ollama.analyze_conference(analysis_summary)
            
            if ai_response and not ai_response.startswith("AI ëª¨ë¸ ì˜¤ë¥˜"):
                # AI ì‘ë‹µì„ ë¦¬ìŠ¤íŠ¸ë¡œ íŒŒì‹±
                insights = ai_response.split('\n')
                # ë¹ˆ ì¤„ê³¼ ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±°
                cleaned_insights = [
                    insight.strip().lstrip('- ').lstrip('â€¢').strip() 
                    for insight in insights 
                    if insight.strip() and len(insight.strip()) > 10
                ]
                return cleaned_insights[:5]  # ìƒìœ„ 5ê°œ ì¸ì‚¬ì´íŠ¸
            else:
                return [f"AI ë¶„ì„ ì‹¤íŒ¨: {ai_response}"]
                
        except Exception as e:
            return [f"AI ì¸ì‚¬ì´íŠ¸ ìƒì„± ì˜¤ë¥˜: {str(e)}"]
    
    def _create_sample_data(self):
        """ìƒ˜í”Œ ë°ì´í„° ìƒì„± (ë°ì´í„°ë² ì´ìŠ¤ í…ŒìŠ¤íŠ¸ìš©)"""
        if not self.database:
            return False
            
        try:
            # í…Œì´ë¸” ìƒì„±
            self.database.create_fragments_table()
            
            # ìƒ˜í”Œ ì¡°ê°ë“¤ ìƒì„±
            sample_fragments = [
                {
                    'fragment_id': f'{self.conference_name}_sample_001',
                    'file_source': 'sample_conference_audio.wav',
                    'file_type': 'audio',
                    'timestamp': datetime.now().isoformat(),
                    'speaker': 'í™”ì_001',
                    'content': 'ì•ˆë…•í•˜ì„¸ìš”. ì˜¤ëŠ˜ ì£¼ì–¼ë¦¬ ì—…ê³„ì˜ ìµœì‹  íŠ¸ë Œë“œì— ëŒ€í•´ ë§ì”€ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ìµœê·¼ ì§€ì†ê°€ëŠ¥ì„±ì´ ì¤‘ìš”í•œ í™”ë‘ê°€ ë˜ê³  ìˆìŠµë‹ˆë‹¤.',
                    'confidence': 0.92,
                    'keywords': ['ì£¼ì–¼ë¦¬', 'íŠ¸ë Œë“œ', 'ì§€ì†ê°€ëŠ¥ì„±', 'ì—…ê³„', 'í™”ë‘']
                },
                {
                    'fragment_id': f'{self.conference_name}_sample_002',
                    'file_source': 'sample_presentation.jpg',
                    'file_type': 'image',
                    'timestamp': datetime.now().isoformat(),
                    'speaker': None,
                    'content': '2025ë…„ ì£¼ì–¼ë¦¬ ì‹œì¥ ì „ë§: ë””ì§€í„¸ ë³€í˜ê³¼ ê°œì¸í™” íŠ¸ë Œë“œ',
                    'confidence': 0.87,
                    'keywords': ['2025ë…„', 'ì£¼ì–¼ë¦¬', 'ì‹œì¥', 'ë””ì§€í„¸', 'ê°œì¸í™”']
                },
                {
                    'fragment_id': f'{self.conference_name}_sample_003',
                    'file_source': 'sample_discussion.wav',
                    'file_type': 'audio',
                    'timestamp': datetime.now().isoformat(),
                    'speaker': 'í™”ì_002',
                    'content': 'ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì„ í™œìš©í•œ ë§ì¶¤í˜• ì£¼ì–¼ë¦¬ ë””ìì¸ì´ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤. ê³ ê°ì˜ ì„ í˜¸ë„ë¥¼ ë¶„ì„í•˜ì—¬ ê°œì¸í™”ëœ ì œí’ˆì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.',
                    'confidence': 0.89,
                    'keywords': ['ì¸ê³µì§€ëŠ¥', 'ë§ì¶¤í˜•', 'ë””ìì¸', 'ê³ ê°', 'ê°œì¸í™”']
                }
            ]
            
            # ë°°ì¹˜ ì‚½ì…
            success = self.database.insert_fragments_batch(sample_fragments)
            
            if success:
                # ë¶„ì„ ê²°ê³¼ì—ë„ ì¶”ê°€
                self.analysis_results["processed_files"] = sample_fragments
                return True
            return False
            
        except Exception as e:
            st.error(f"ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {e}")
            return False
    
    def download_video_from_url(self, url: str) -> Optional[Dict[str, Any]]:
        """ë‹¤ì–‘í•œ í”Œë«í¼ ë¹„ë””ì˜¤ ë‹¤ìš´ë¡œë“œ"""
        if not YOUTUBE_AVAILABLE:
            st.error("âŒ yt-dlpê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ. pip install yt-dlpë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")
            return None
        
        try:
            # ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
            temp_dir = tempfile.mkdtemp()
            
            # yt-dlp ì˜µì…˜ ì„¤ì • (ë‹¤ì–‘í•œ í”Œë«í¼ ì§€ì›)
            ydl_opts = {
                'format': 'best[height<=720]/best/worstvideo+bestaudio/worst',  # Brightcove í˜¸í™˜ì„± ê°œì„ 
                'outtmpl': os.path.join(temp_dir, '%(title)s.%(ext)s'),
                'quiet': True,
                'no_warnings': True,
                'extractaudio': False,
                'writesubtitles': False,
                'writeautomaticsub': False,
                'ignoreerrors': True,  # í¬ë§· ì—ëŸ¬ ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰
            }
            
            with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                # ë¹„ë””ì˜¤ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                info = ydl.extract_info(url, download=False)
                title = info.get('title', 'unknown')
                duration = info.get('duration', 0)
                platform = info.get('extractor', 'unknown')
                uploader = info.get('uploader', 'unknown')
                
                # ë„ˆë¬´ ê¸´ ë¹„ë””ì˜¤ëŠ” ì œí•œ (30ë¶„)
                if duration and duration > 1800:
                    st.warning(f"âš ï¸ ë¹„ë””ì˜¤ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤ ({duration//60}ë¶„). 30ë¶„ ì´í•˜ë§Œ ì§€ì›í•©ë‹ˆë‹¤.")
                    return None
                
                # ë¹„ë””ì˜¤ ë‹¤ìš´ë¡œë“œ
                st.info(f"ğŸ“¥ ë‹¤ìš´ë¡œë“œ ì¤‘: {title} ({platform})")
                ydl.download([url])
                
                # ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ ì°¾ê¸°
                for file in os.listdir(temp_dir):
                    if file.endswith(('.mp4', '.webm', '.mkv', '.flv', '.avi')):
                        downloaded_path = os.path.join(temp_dir, file)
                        st.success(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {title}")
                        
                        return {
                            'path': downloaded_path,
                            'title': title,
                            'platform': platform,
                            'uploader': uploader,
                            'duration': duration,
                            'url': url
                        }
            
            return None
            
        except Exception as e:
            st.error(f"âŒ ë¹„ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def process_video_url(self, url: str) -> Dict[str, Any]:
        """ë‹¤ì–‘í•œ í”Œë«í¼ ë¹„ë””ì˜¤ URL ì²˜ë¦¬"""
        # URL ê¸°ë³¸ ê²€ì¦
        if not url.startswith(('http://', 'https://')):
            return {"error": "ìœ íš¨í•œ URLì´ ì•„ë‹™ë‹ˆë‹¤."}
        
        # ë¹„ë””ì˜¤ ë‹¤ìš´ë¡œë“œ
        download_info = self.download_video_from_url(url)
        if not download_info:
            return {"error": "ë¹„ë””ì˜¤ ë‹¤ìš´ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."}
        
        try:
            # ë‹¤ìš´ë¡œë“œëœ ë¹„ë””ì˜¤ë¥¼ ì¼ë°˜ ë¹„ë””ì˜¤ íŒŒì¼ë¡œ ì²˜ë¦¬
            result = self._process_video_file(download_info['path'], download_info['title'])
            
            # ì›ë³¸ URL ë° í”Œë«í¼ ì •ë³´ ì¶”ê°€
            if "error" not in result:
                result["original_url"] = download_info['url']
                result["platform"] = download_info['platform']
                result["uploader"] = download_info['uploader']
                result["source_type"] = "web_video"
                result["video_duration"] = download_info['duration']
            
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            os.unlink(download_info['path'])
            os.rmdir(os.path.dirname(download_info['path']))
            
            return result
            
        except Exception as e:
            return {"error": f"ë¹„ë””ì˜¤ ì²˜ë¦¬ ì‹¤íŒ¨: {e}"}
    
    def process_text_file_urls(self, uploaded_file) -> List[Dict[str, Any]]:
        """TXT íŒŒì¼ì—ì„œ URL ì¶”ì¶œ ë° ë°°ì¹˜ ì²˜ë¦¬"""
        try:
            # íŒŒì¼ ë‚´ìš© ì½ê¸°
            content = uploaded_file.read().decode('utf-8')
            
            # URL íŒ¨í„´ ì°¾ê¸°
            url_pattern = r'https?://[^\s\n\r]+'
            urls = re.findall(url_pattern, content)
            
            if not urls:
                return [{"error": "í…ìŠ¤íŠ¸ íŒŒì¼ì—ì„œ ìœ íš¨í•œ URLì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}]
            
            results = []
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            for i, url in enumerate(urls):
                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                progress = (i + 1) / len(urls)
                progress_bar.progress(progress)
                status_text.text(f"URL ì²˜ë¦¬ ì¤‘: {i+1}/{len(urls)} - {url[:50]}...")
                
                # URL ì²˜ë¦¬
                result = self.process_video_url(url.strip())
                
                if "error" not in result:
                    result["batch_index"] = i + 1
                    result["total_batch"] = len(urls)
                    results.append(result)
                    
                    # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                    if self.database:
                        self.database.insert_fragment(result)
                else:
                    st.warning(f"âš ï¸ URL ì²˜ë¦¬ ì‹¤íŒ¨: {url[:50]}... - {result['error']}")
                
                # ë„ˆë¬´ ë¹ ë¥¸ ìš”ì²­ ë°©ì§€
                time.sleep(1)
            
            progress_bar.progress(1.0)
            status_text.text(f"âœ… ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {len(results)}/{len(urls)}ê°œ ì„±ê³µ")
            
            return results
            
        except Exception as e:
            return [{"error": f"í…ìŠ¤íŠ¸ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}"}]
    
    def trigger_dual_brain_system(self) -> Dict[str, Any]:
        """ë“€ì–¼ ë¸Œë ˆì¸ ì‹œìŠ¤í…œ íŠ¸ë¦¬ê±°"""
        if not self.dual_brain:
            return {"error": "ë“€ì–¼ ë¸Œë ˆì¸ ì‹œìŠ¤í…œì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
        
        try:
            # ë¶„ì„ ê²°ê³¼ê°€ ìˆëŠ”ì§€ í™•ì¸
            if not self.analysis_results.get("holistic_results"):
                return {"error": "ë¨¼ì € í™€ë¦¬ìŠ¤í‹± ë¶„ì„ì„ ì™„ë£Œí•˜ì„¸ìš”."}
            
            # ë“€ì–¼ ë¸Œë ˆì¸ ë¶„ì„ ì‹¤í–‰
            dual_brain_result = self.dual_brain.analyze_and_integrate(
                self.analysis_results["holistic_results"]
            )
            
            # ì„¸ì…˜ì— ì €ì¥
            self.analysis_results["dual_brain_results"] = dual_brain_result
            
            return dual_brain_result
            
        except Exception as e:
            return {"error": f"ë“€ì–¼ ë¸Œë ˆì¸ ì‹œìŠ¤í…œ ì‹¤íŒ¨: {e}"}
    
    def generate_comprehensive_report(self) -> str:
        """ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ ìƒì„±"""
        if not self.analysis_results["processed_files"]:
            return "ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        report_parts = []
        
        # í—¤ë”
        report_parts.append(f"# {self.conference_name} ì™„ì „ í†µí•© ë¶„ì„ ë³´ê³ ì„œ")
        report_parts.append(f"**ìƒì„± ì¼ì‹œ:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_parts.append(f"**ì„¸ì…˜ ID:** {self.session_id}")
        report_parts.append("")
        
        # ì‹œìŠ¤í…œ ìƒíƒœ
        status = self.check_system_status()
        report_parts.append("## ğŸ”§ ì‹œìŠ¤í…œ ìƒíƒœ")
        report_parts.append(f"- **ì „ì²´ ì¤€ë¹„ë„:** {status['ready_systems']}/{status['total_systems']} ({'ì™„ë£Œ' if status['overall_ready'] else 'ë¶€ë¶„ì™„ë£Œ'})")
        report_parts.append(f"- **OCR ì—”ì§„:** {'âœ… ì •ìƒ' if status['ocr_available'] else 'âŒ ë¹„í™œì„±'}")
        report_parts.append(f"- **Whisper STT:** {'âœ… ì •ìƒ' if status['whisper_available'] else 'âŒ ë¹„í™œì„±'}")
        report_parts.append(f"- **í™€ë¦¬ìŠ¤í‹± ë¶„ì„:** {'âœ… ì •ìƒ' if status['holistic_available'] else 'âŒ ë¹„í™œì„±'}")
        report_parts.append(f"- **ë°ì´í„°ë² ì´ìŠ¤:** {'âœ… ì •ìƒ' if status['database_available'] else 'âŒ ë¹„í™œì„±'}")
        report_parts.append(f"- **Ollama AI:** {'âœ… ì •ìƒ' if status['ollama_available'] else 'âŒ ë¹„í™œì„±'}")
        if status['ollama_available'] and self.ollama:
            report_parts.append(f"  - ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {len(self.ollama.available_models)}ê°œ")
        report_parts.append("")
        
        # íŒŒì¼ ì²˜ë¦¬ ê²°ê³¼
        processed_files = self.analysis_results["processed_files"]
        report_parts.append("## ğŸ“ íŒŒì¼ ì²˜ë¦¬ ê²°ê³¼")
        report_parts.append(f"- **ì²˜ë¦¬ëœ íŒŒì¼:** {len(processed_files)}ê°œ")
        
        file_types = Counter([f['file_type'] for f in processed_files])
        for file_type, count in file_types.items():
            type_name = {"image": "ì´ë¯¸ì§€", "audio": "ìŒì„±", "video": "ë¹„ë””ì˜¤"}.get(file_type, file_type)
            report_parts.append(f"  - {type_name}: {count}ê°œ")
        
        # í‰ê·  ì‹ ë¢°ë„
        if processed_files:
            avg_confidence = np.mean([f['confidence'] for f in processed_files])
            report_parts.append(f"- **í‰ê·  ì‹ ë¢°ë„:** {avg_confidence:.1%}")
        
        report_parts.append("")
        
        # í™€ë¦¬ìŠ¤í‹± ë¶„ì„ ê²°ê³¼
        if self.analysis_results.get("holistic_results"):
            holistic = self.analysis_results["holistic_results"]["holistic_analysis"]
            report_parts.append("## ğŸ§  í™€ë¦¬ìŠ¤í‹± ë¶„ì„ ê²°ê³¼")
            report_parts.append(f"- **ì´ ì¡°ê° ìˆ˜:** {holistic.get('total_fragments', 0)}ê°œ")
            report_parts.append(f"- **ë°œê²¬ëœ ê°œì²´:** {holistic.get('total_entities', 0)}ê°œ")
            report_parts.append(f"- **ì£¼ìš” ì£¼ì œ:** {holistic.get('total_topics', 0)}ê°œ")
            
            # í•µì‹¬ ì¸ì‚¬ì´íŠ¸
            key_insights = holistic.get('key_insights', [])
            if key_insights:
                report_parts.append("### ğŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸")
                for insight in key_insights:
                    report_parts.append(f"- {insight}")
            
            # AI ì¸ì‚¬ì´íŠ¸
            ai_insights = self.analysis_results["holistic_results"].get("ai_insights", [])
            if ai_insights and not ai_insights[0].startswith("AI ì¸ì‚¬ì´íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"):
                report_parts.append("### ğŸ¤– AI ì‹¬í™” ë¶„ì„")
                for insight in ai_insights:
                    if not insight.startswith("AI"):  # ì—ëŸ¬ ë©”ì‹œì§€ ì œì™¸
                        report_parts.append(f"- {insight}")
            
            report_parts.append("")
        
        # ë“€ì–¼ ë¸Œë ˆì¸ ê²°ê³¼
        if self.analysis_results.get("dual_brain_results"):
            report_parts.append("## ğŸ§ ğŸ§  ë“€ì–¼ ë¸Œë ˆì¸ ë¶„ì„")
            dual_brain = self.analysis_results["dual_brain_results"]
            report_parts.append("### AI ì¸ì‚¬ì´íŠ¸")
            if dual_brain.get("ai_insights"):
                for insight in dual_brain["ai_insights"][:3]:  # ìƒìœ„ 3ê°œ
                    report_parts.append(f"- {insight}")
            report_parts.append("")
        
        # ê²°ë¡ 
        report_parts.append("## ğŸ“‹ ê²°ë¡ ")
        report_parts.append("ë³¸ í†µí•© ë¶„ì„ì„ í†µí•´ ì»¨í¼ëŸ°ìŠ¤ì˜ ì „ì²´ì ì¸ ë‚´ìš©ê³¼ í•µì‹¬ ì‚¬ì•ˆë“¤ì´ ì²´ê³„ì ìœ¼ë¡œ ë¶„ì„ë˜ì—ˆìŠµë‹ˆë‹¤.")
        report_parts.append("ì‹¤ì œ íŒŒì¼ ì²˜ë¦¬ë¶€í„° ê³ ê¸‰ ì˜ë¯¸ ë¶„ì„ê¹Œì§€ ì „ ê³¼ì •ì´ ê²€ì¦ëœ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ìˆ˜í–‰ë˜ì—ˆìŠµë‹ˆë‹¤.")
        report_parts.append("")
        report_parts.append("---")
        report_parts.append("*ë³¸ ë³´ê³ ì„œëŠ” SOLOMOND AI ì™„ì „ í†µí•© ë¶„ì„ ì‹œìŠ¤í…œ v7.1ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.*")
        
        return "\n".join(report_parts)

def main():
    st.set_page_config(
        page_title="ì™„ì „ í†µí•© ì»¨í¼ëŸ°ìŠ¤ ë¶„ì„ ì‹œìŠ¤í…œ",
        page_icon="ğŸ¯",
        layout="wide"
    )
    
    # ğŸ”¥ ë©”ëª¨ë¦¬ ì•ˆì „ ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²˜ë¦¬ (MemoryError ë°©ì§€)
    os.environ['STREAMLIT_SERVER_MAX_UPLOAD_SIZE'] = '5120'  # 5GB
    os.environ['STREAMLIT_SERVER_MAX_MESSAGE_SIZE'] = '5120'  # 5GB  
    os.environ['STREAMLIT_SERVER_ENABLE_CORS'] = 'false'
    os.environ['STREAMLIT_SERVER_ENABLE_XSRF_PROTECTION'] = 'false'
    
    # Tornado ë©”ëª¨ë¦¬ ìµœì í™” (ì¤‘ìš”!)
    os.environ['STREAMLIT_SERVER_MAX_REQUEST_SIZE'] = '5368709120'  # 5GB in bytes
    os.environ['STREAMLIT_TORNADO_MAX_BUFFER_SIZE'] = '268435456'  # 256MB buffer
    
    # ğŸš€ GPU í™œì„±í™” ì„¤ì • (5-15ë°° ì„±ëŠ¥ í–¥ìƒ)
    os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # GTX 1050 Ti ì‚¬ìš©
    
    # GPU ë©”ëª¨ë¦¬ ìµœì í™”
    import torch
    if torch.cuda.is_available():
        torch.cuda.empty_cache()  # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        st.success(f"GPU í™œì„±í™”: {torch.cuda.get_device_name(0)}")
    else:
        st.warning("GPU ì‚¬ìš© ë¶ˆê°€, CPU ëª¨ë“œë¡œ ì§„í–‰")
    
    st.title("ğŸ¯ ì™„ì „ í†µí•© ì»¨í¼ëŸ°ìŠ¤ ë¶„ì„ ì‹œìŠ¤í…œ")
    st.markdown("**SOLOMOND AI v7.4 - 1000+ í”Œë«í¼ ë©€í‹°ë¯¸ë””ì–´ ë¶„ì„**")
    
    # ì‹œìŠ¤í…œ ìƒíƒœ í‘œì‹œ
    if OLLAMA_AVAILABLE and YOUTUBE_AVAILABLE:
        st.info("ğŸš€ **ëª¨ë“  ê¸°ëŠ¥ í™œì„±í™”**: Ollama AI 5ê°œ ëª¨ë¸ + 1000+ ì›¹ í”Œë«í¼ + TXT ë°°ì¹˜ ì²˜ë¦¬")
    elif OLLAMA_AVAILABLE:
        st.info("ğŸ¤– **AI ë¶„ì„ í™œì„±í™”**: Ollama 5ê°œ ëª¨ë¸ í™œì„± | âš ï¸ ì›¹ ë™ì˜ìƒ ë¶„ì„ ë¹„í™œì„±")
    elif YOUTUBE_AVAILABLE:
        st.info("ğŸ¬ **ì›¹ ë¶„ì„ í™œì„±í™”**: 1000+ í”Œë«í¼ ì§€ì› | âš ï¸ AI ê³ ê¸‰ ë¶„ì„ ë¹„í™œì„±")
    else:
        st.warning("âš ï¸ ê¸°ë³¸ ë¶„ì„ë§Œ ì§€ì› - AI ëª¨ë¸ê³¼ ì›¹ ë¶„ì„ ë¹„í™œì„±")
    
    # ì‚¬ì´ë“œë°” ì„¤ì •
    st.sidebar.header("âš™ï¸ ì‹œìŠ¤í…œ ì„¤ì •")
    conference_name = st.sidebar.text_input("ì»¨í¼ëŸ°ìŠ¤ ì´ë¦„", "unified_conference_2025")
    
    # ë¶„ì„ê¸° ì´ˆê¸°í™”
    if 'analyzer' not in st.session_state:
        with st.spinner("ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘..."):
            st.session_state.analyzer = UnifiedConferenceAnalyzer(conference_name)
    
    analyzer = st.session_state.analyzer
    
    # ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
    status = analyzer.check_system_status()
    
    # ì‹œìŠ¤í…œ ìƒíƒœ í‘œì‹œ
    st.sidebar.markdown("### ğŸ”§ ì‹œìŠ¤í…œ ìƒíƒœ")
    if status["overall_ready"]:
        st.sidebar.success(f"âœ… ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ ({status['ready_systems']}/{status['total_systems']})")
    else:
        st.sidebar.warning(f"âš ï¸ ë¶€ë¶„ ì¤€ë¹„ ({status['ready_systems']}/{status['total_systems']})")
    
    st.sidebar.markdown(f"**OCR:** {'âœ…' if status['ocr_available'] else 'âŒ'}")
    st.sidebar.markdown(f"**Whisper:** {'âœ…' if status['whisper_available'] else 'âŒ'}")
    st.sidebar.markdown(f"**í™€ë¦¬ìŠ¤í‹±:** {'âœ…' if status['holistic_available'] else 'âŒ'}")
    st.sidebar.markdown(f"**ë°ì´í„°ë² ì´ìŠ¤:** {'âœ…' if status['database_available'] else 'âŒ'}")
    st.sidebar.markdown(f"**Ollama AI:** {'âœ…' if status['ollama_available'] else 'âŒ'}")
    
    if status['ollama_available'] and analyzer.ollama:
        st.sidebar.markdown("### ğŸ¤– Ollama ëª¨ë¸")
        for model in analyzer.ollama.available_models:
            model_info = f"**{model}**"
            if "gpt-oss:20b" in model:
                model_info += " (13GB, ìµœì‹  GPT)"
            elif "qwen3:8b" in model:
                model_info += " (5.2GB, Qwen3)"
            elif "gemma3:27b" in model:
                model_info += " (17GB, ëŒ€í˜• ëª¨ë¸)"
            elif "qwen2.5:7b" in model:
                model_info += " (4.7GB, ì¶”ì²œ)"
            elif "gemma3:4b" in model:
                model_info += " (3.3GB, ê²½ëŸ‰)"
            st.sidebar.markdown(f"- {model_info}")
    
    # ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœê°€ Xì¸ ê²½ìš° ìƒ˜í”Œ ë°ì´í„° ì œê³µ
    if not status["database_available"]:
        st.error("âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨")
        if st.button("ğŸ”§ ìƒ˜í”Œ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸", type="secondary"):
            with st.spinner("ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì¤‘..."):
                analyzer._create_sample_data()
            st.rerun()
    
    # ë©”ì¸ ì¸í„°í˜ì´ìŠ¤
    tab0, tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“‹ ì‚¬ì „ì •ë³´", "ğŸ“ íŒŒì¼/URL ì²˜ë¦¬", "ğŸ§  í™€ë¦¬ìŠ¤í‹± ë¶„ì„", "ğŸ§ ğŸ§  ë“€ì–¼ ë¸Œë ˆì¸", "ğŸ“‹ ì¢…í•© ë³´ê³ ì„œ"])
    
    with tab0:
        st.markdown("## ğŸ“‹ ì»¨í¼ëŸ°ìŠ¤ ì‚¬ì „ì •ë³´")
        st.markdown("**ë¶„ì„ í’ˆì§ˆ í–¥ìƒì„ ìœ„í•œ ë°°ê²½ ì •ë³´ ì…ë ¥**")
        
        col1, col2 = st.columns(2)
        
        with col1:
            conference_name_input = st.text_input("ğŸ¯ ì»¨í¼ëŸ°ìŠ¤ëª…", "")
            conference_date = st.date_input("ğŸ“… ë‚ ì§œ", datetime.now().date())
            location = st.text_input("ğŸ“ ì¥ì†Œ", "")
        
        with col2:
            industry_options = ["ì£¼ì–¼ë¦¬", "íŒ¨ì…˜", "ê¸°ìˆ ", "ì˜ë£Œ", "êµìœ¡", "ê¸ˆìœµ", "ê¸°íƒ€"]
            industry_field = st.selectbox("ğŸ¢ ì—…ê³„ë¶„ì•¼", industry_options)
            interest_keywords = st.text_area("ğŸ”‘ ê´€ì‹¬ í‚¤ì›Œë“œ (ì‰¼í‘œë¡œ êµ¬ë¶„)", 
                                           placeholder="ì˜ˆ: íŠ¸ë Œë“œ, í˜ì‹ , ì§€ì†ê°€ëŠ¥ì„±, AI")
        
        if st.button("ğŸ’¾ ì‚¬ì „ì •ë³´ ì €ì¥", type="primary"):
            keywords_list = [k.strip() for k in interest_keywords.split(",") if k.strip()]
            
            analyzer.update_conference_info({
                "conference_name": conference_name_input,
                "conference_date": conference_date.isoformat(),
                "location": location,
                "industry_field": industry_field,
                "interest_keywords": keywords_list
            })
            
            st.success("âœ… ì‚¬ì „ì •ë³´ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
            
        # ì €ì¥ëœ ì •ë³´ í‘œì‹œ
        if analyzer.conference_info["conference_name"]:
            st.markdown("### ğŸ’¾ ì €ì¥ëœ ì •ë³´")
            st.info(f"""
            **ì»¨í¼ëŸ°ìŠ¤:** {analyzer.conference_info['conference_name']}  
            **ë‚ ì§œ:** {analyzer.conference_info['conference_date']}  
            **ì¥ì†Œ:** {analyzer.conference_info['location']}  
            **ì—…ê³„:** {analyzer.conference_info['industry_field']}  
            **í‚¤ì›Œë“œ:** {', '.join(analyzer.conference_info['interest_keywords'])}
            """)
    
    with tab1:
        st.markdown("## ğŸ“ íŒŒì¼ ì—…ë¡œë“œ ë° URL ë¶„ì„")
        
        # íŒŒì¼ ì—…ë¡œë“œ ì„¹ì…˜
        st.markdown("### ğŸ“‚ íŒŒì¼ ì—…ë¡œë“œ ë° ë¡œì»¬ íŒŒì¼")
        st.markdown("**ì§€ì› í˜•ì‹:** ì´ë¯¸ì§€ (JPG, PNG), ìŒì„± (WAV, MP3, M4A), ë¹„ë””ì˜¤ (MP4, MOV), í…ìŠ¤íŠ¸ (TXT)")
        
        # íƒ­ìœ¼ë¡œ ì—…ë¡œë“œ ë°©ì‹ ì„ íƒ
        upload_tab1, upload_tab2 = st.tabs(["ğŸ“¤ íŒŒì¼ ì—…ë¡œë“œ", "ğŸ“ ë¡œì»¬ íŒŒì¼ ì„ íƒ"])
        
        uploaded_files = []  # ì´ˆê¸°í™”
        
        with upload_tab1:
            # ğŸš€ ë©”ëª¨ë¦¬ ì•ˆì „ ìŠ¤íŠ¸ë¦¬ë° ì—…ë¡œë“œ ì‹œìŠ¤í…œ
            st.markdown("**ğŸ›¡ï¸ ë©”ëª¨ë¦¬ ì•ˆì „ ë³´ì¥**: 3GB+ íŒŒì¼ë„ MemoryError ì—†ì´ ì•ˆì • ì²˜ë¦¬")
            
            uploaded_files = st.file_uploader(
                "ë¶„ì„í•  íŒŒì¼ë“¤ì„ ì—…ë¡œë“œí•˜ì„¸ìš” (1GB ì´ìƒì€ ìë™ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬)",
                accept_multiple_files=True,
                type=['jpg', 'jpeg', 'png', 'bmp', 'wav', 'mp3', 'm4a', 'flac', 'mp4', 'avi', 'mov', 'txt'],
                help="ğŸš€ ìµœëŒ€ 5GB íŒŒì¼ ì§€ì›! MemoryError ì™„ì „ ë°©ì§€ ìŠ¤íŠ¸ë¦¬ë° ì‹œìŠ¤í…œ"
            )
            
            # ì´ˆëŒ€ìš©ëŸ‰ íŒŒì¼ í¬ê¸° ì•ˆë‚´
            if uploaded_files:
                total_size = sum(file.size for file in uploaded_files) / (1024*1024)  # MB
                if total_size > 3000:  # 3GB ì´ìƒ
                    st.success(f"ğŸš€ ì´ˆëŒ€ìš©ëŸ‰ íŒŒì¼: {total_size:.1f}MB ({total_size/1024:.2f}GB) - GPU ê°€ì†ìœ¼ë¡œ ì•ˆì • ì²˜ë¦¬")
                elif total_size > 1000:  # 1GB ì´ìƒ
                    st.info(f"ğŸ“ ëŒ€ìš©ëŸ‰ íŒŒì¼: {total_size:.1f}MB ({total_size/1024:.2f}GB)")
                elif total_size > 100:  # 100MB ì´ìƒ
                    st.info(f"ğŸ“ ì—…ë¡œë“œëœ íŒŒì¼: {total_size:.1f}MB")
                
                # íŒŒì¼ë³„ ìƒì„¸ ì •ë³´
                with st.expander("ğŸ“Š ì—…ë¡œë“œëœ íŒŒì¼ ìƒì„¸ ì •ë³´"):
                    for file in uploaded_files:
                        file_size_mb = file.size / (1024*1024)
                        file_type = "ğŸ–¼ï¸" if file.type.startswith('image') else "ğŸµ" if file.type.startswith('audio') else "ğŸ¬" if file.type.startswith('video') else "ğŸ“„"
                        st.write(f"{file_type} **{file.name}** - {file_size_mb:.1f}MB ({file.type})")
            
            st.info(f"ì„ íƒëœ íŒŒì¼: {len(uploaded_files)}ê°œ")
            
        with upload_tab2:
            # ğŸ—‚ï¸ ë¡œì»¬ íŒŒì¼ ì‹œìŠ¤í…œ (3GB+ IMG_0032.MOV ë“±)
            st.markdown("**ğŸ’¡ ë¡œì»¬ íŒŒì¼ ì²˜ë¦¬**: 3GB+ íŒŒì¼ì€ user_files í´ë”ì— ë³µì‚¬ í›„ ì—¬ê¸°ì„œ ì„ íƒí•˜ì„¸ìš”")
            st.markdown("**ğŸ“ íŒŒì¼ ê²½ë¡œ**: `C:/Users/PC_58410/solomond-ai-system/user_files/`")
            
            # ì‚¬ìš© ê°€ëŠ¥í•œ ë¡œì»¬ íŒŒì¼ ìŠ¤ìº”
            local_files = []
            if analyzer.user_files_dir.exists():
                for folder in analyzer.user_files_dir.iterdir():
                    if folder.is_dir():
                        for file_path in folder.rglob("*"):
                            if file_path.is_file() and file_path.suffix.lower() in ['.jpg', '.jpeg', '.png', '.bmp', '.wav', '.mp3', '.m4a', '.flac', '.mp4', '.avi', '.mov', '.txt']:
                                file_size = file_path.stat().st_size
                                local_files.append({
                                    'path': file_path,
                                    'name': file_path.name,
                                    'folder': folder.name,
                                    'size_mb': file_size / (1024*1024),
                                    'type': file_path.suffix.lower()
                                })
            
            if local_files:
                st.success(f"ğŸ“ {len(local_files)}ê°œ ë¡œì»¬ íŒŒì¼ ë°œê²¬")
                
                # í´ë”ë³„ ê·¸ë£¹í™”
                folders = {}
                for file_info in local_files:
                    folder_name = file_info['folder']
                    if folder_name not in folders:
                        folders[folder_name] = []
                    folders[folder_name].append(file_info)
                
                # í´ë” ì„ íƒ
                selected_folder = st.selectbox(
                    "ğŸ“‚ ë¶„ì„í•  í´ë” ì„ íƒ",
                    list(folders.keys()),
                    help="3GB+ IMG_0032.MOVê°€ í¬í•¨ëœ JGA2025_D1 í´ë”ë¥¼ ì„ íƒí•˜ì„¸ìš”"
                )
                
                if selected_folder:
                    folder_files = folders[selected_folder]
                    
                    # ëŒ€ìš©ëŸ‰ íŒŒì¼ ìš°ì„  í‘œì‹œ
                    large_files = [f for f in folder_files if f['size_mb'] > 1000]  # 1GB+
                    if large_files:
                        st.info(f"ğŸš€ ëŒ€ìš©ëŸ‰ íŒŒì¼ {len(large_files)}ê°œ ë°œê²¬ (MemoryError ë°©ì§€ ì™„ë²½ ì§€ì›)")
                        
                        for file_info in large_files:
                            file_type = "ğŸ–¼ï¸" if file_info['type'] in ['.jpg', '.jpeg', '.png'] else "ğŸµ" if file_info['type'] in ['.wav', '.mp3', '.m4a'] else "ğŸ¬" if file_info['type'] in ['.mp4', '.mov', '.avi'] else "ğŸ“„"
                            st.markdown(f"â€¢ {file_type} **{file_info['name']}** - {file_info['size_mb']:.1f}MB ({file_info['size_mb']/1024:.2f}GB)")
                    
                    # ì „ì²´ ë¶„ì„ ë²„íŠ¼
                    if st.button(f"ğŸš€ {selected_folder} í´ë” ì „ì²´ ë¶„ì„ ì‹œì‘", type="primary", key="local_files_analyze"):
                        # ë¡œì»¬ íŒŒì¼ì„ uploaded_files í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                        uploaded_files = []
                        for file_info in folder_files:
                            # ì„ì‹œ íŒŒì¼ ê°ì²´ ìƒì„± (streamlit í˜¸í™˜)
                            class LocalFileWrapper:
                                def __init__(self, file_path, file_name, file_size):
                                    self.name = file_name
                                    self.size = file_size
                                    self._file_path = file_path
                                    self.type = self._get_mime_type(file_path.suffix.lower())
                                
                                def _get_mime_type(self, ext):
                                    mime_map = {
                                        '.jpg': 'image/jpeg', '.jpeg': 'image/jpeg', '.png': 'image/png',
                                        '.wav': 'audio/wav', '.mp3': 'audio/mpeg', '.m4a': 'audio/mp4',
                                        '.mp4': 'video/mp4', '.mov': 'video/quicktime', '.avi': 'video/avi',
                                        '.txt': 'text/plain'
                                    }
                                    return mime_map.get(ext, 'application/octet-stream')
                                
                                def read(self):
                                    with open(self._file_path, 'rb') as f:
                                        return f.read()
                            
                            wrapped_file = LocalFileWrapper(file_info['path'], file_info['name'], int(file_info['size_mb'] * 1024 * 1024))
                            uploaded_files.append(wrapped_file)
                        
                        st.success(f"âœ… ë¡œì»¬ íŒŒì¼ {len(uploaded_files)}ê°œ ì¤€ë¹„ ì™„ë£Œ (3GB+ IMG_0032.MOV í¬í•¨)")
                        st.info("ğŸ“ ë¡œì»¬ íŒŒì¼ì´ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤. ì•„ë˜ ë¶„ì„ ì‹œì‘ ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”!")
            else:
                st.warning("ğŸ“‚ user_files í´ë”ì— íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
                st.markdown("**í•´ê²° ë°©ë²•**: 3GB+ IMG_0032.MOV íŒŒì¼ì„ ë‹¤ìŒ ê²½ë¡œì— ë³µì‚¬í•˜ì„¸ìš”:")
                st.code("C:/Users/PC_58410/solomond-ai-system/user_files/JGA2025_D1/IMG_0032.MOV")
        
        # ì„ íƒëœ íŒŒì¼ ìˆ˜ í‘œì‹œ (ë‘ íƒ­ í†µí•©)
        if uploaded_files:
            st.info(f"âœ… ì´ ì„ íƒëœ íŒŒì¼: {len(uploaded_files)}ê°œ")
        
        # ì›¹ ë™ì˜ìƒ URL ì„¹ì…˜
        st.markdown("### ğŸ¬ ì›¹ ë™ì˜ìƒ ë¶„ì„")
        if YOUTUBE_AVAILABLE:
            st.markdown("**ì§€ì› í”Œë«í¼:** YouTube, Vimeo, Dailymotion, Facebook, Instagram, TikTok, Twitch ë“± 1000+ ì‚¬ì´íŠ¸")
            
            col1, col2 = st.columns([3, 1])
            
            with col1:
                video_url = st.text_input(
                    "ë™ì˜ìƒ URLì„ ì…ë ¥í•˜ì„¸ìš”",
                    placeholder="https://www.youtube.com/watch?v=... ë˜ëŠ” ë‹¤ë¥¸ í”Œë«í¼ URL",
                    help="ìµœëŒ€ 30ë¶„ ê¸¸ì´ì˜ ë™ì˜ìƒë§Œ ì§€ì›ë©ë‹ˆë‹¤."
                )
            
            with col2:
                st.markdown("<br>", unsafe_allow_html=True)  # ê³µê°„ ì¡°ì •
                if video_url and st.button("ğŸ¬ ë™ì˜ìƒ ë¶„ì„", type="secondary"):
                    with st.spinner("ì›¹ ë™ì˜ìƒ ë‹¤ìš´ë¡œë“œ ë° ë¶„ì„ ì¤‘..."):
                        video_result = analyzer.process_video_url(video_url)
                    
                    if "error" not in video_result:
                        st.success(f"âœ… ë™ì˜ìƒ ë¶„ì„ ì™„ë£Œ!")
                        
                        # ê²°ê³¼ í‘œì‹œ
                        with st.expander(f"ğŸ¬ {video_result.get('file_source', 'Web Video')}"):
                            col1, col2 = st.columns(2)
                            
                            with col1:
                                st.markdown(f"**í”Œë«í¼:** {video_result.get('platform', 'N/A')}")
                                st.markdown(f"**ì—…ë¡œë”:** {video_result.get('uploader', 'N/A')}")
                                st.markdown(f"**ì›ë³¸ URL:** [{video_result.get('original_url', 'N/A')[:50]}...]({video_result.get('original_url', '#')})")
                            
                            with col2:
                                st.markdown(f"**ì‹ ë¢°ë„:** {video_result['confidence']:.1%}")
                                if video_result.get('video_duration'):
                                    st.markdown(f"**ê¸¸ì´:** {video_result['video_duration']:.1f}ì´ˆ")
                                st.markdown(f"**í‚¤ì›Œë“œ:** {', '.join(video_result['keywords'][:5])}")
                            
                            st.markdown("**ì¶”ì¶œëœ ë‚´ìš©:**")
                            st.markdown(f"> {video_result['content'][:300]}{'...' if len(video_result['content']) > 300 else ''}")
                        
                        # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                        if analyzer.database:
                            analyzer.database.insert_fragment(video_result)
                            analyzer.analysis_results["processed_files"].append(video_result)
                    else:
                        st.error(f"âŒ ë™ì˜ìƒ ë¶„ì„ ì‹¤íŒ¨: {video_result['error']}")
        else:
            st.warning("âš ï¸ ì›¹ ë™ì˜ìƒ ë¶„ì„ ë¹„í™œì„± - yt-dlp ì„¤ì¹˜ í•„ìš”")
            st.code("pip install yt-dlp", language="bash")
        
        if uploaded_files:
            st.info(f"ì„ íƒëœ íŒŒì¼: {len(uploaded_files)}ê°œ")
            
            # ì²˜ë¦¬ ì˜µì…˜
            col1, col2 = st.columns([3, 1])
            with col1:
                process_mode = st.selectbox(
                    "ì²˜ë¦¬ ëª¨ë“œ",
                    ["ğŸš€ ê³ ì† ëª¨ë“œ (ê¶Œì¥)", "ğŸ›¡ï¸ ì•ˆì „ ëª¨ë“œ (ëŒ€ìš©ëŸ‰)", "âš¡ í„°ë³´ ëª¨ë“œ (ì†Œìš©ëŸ‰)"],
                    help="ëŒ€ìš©ëŸ‰ íŒŒì¼ì€ ì•ˆì „ ëª¨ë“œë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤."
                )
            
            with col2:
                st.markdown("<br>", unsafe_allow_html=True)
                skip_errors = st.checkbox("ì˜¤ë¥˜ ê±´ë„ˆë›°ê¸°", value=True, help="íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ ê³„ì† ì§„í–‰")
            
            if st.button("ğŸš€ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘", type="primary"):
                # ì²˜ë¦¬ ëª¨ë“œì— ë”°ë¥¸ ì„¤ì •
                if "ì•ˆì „" in process_mode:
                    st.info("ğŸ›¡ï¸ ì•ˆì „ ëª¨ë“œ: ëŒ€ìš©ëŸ‰ íŒŒì¼ ìµœì í™” ì²˜ë¦¬")
                elif "í„°ë³´" in process_mode:
                    st.info("âš¡ í„°ë³´ ëª¨ë“œ: ê³ ì† ë³‘ë ¬ ì²˜ë¦¬")
                else:
                    st.info("ğŸš€ ê³ ì† ëª¨ë“œ: ê· í˜•ì¡íŒ ì²˜ë¦¬")
                
                with st.spinner("ì‹¤ì œ íŒŒì¼ ì²˜ë¦¬ ì¤‘... (ëŒ€ìš©ëŸ‰ íŒŒì¼ì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤)"):
                    result = analyzer.process_uploaded_files(uploaded_files, skip_errors=skip_errors)
                
                if "error" not in result:
                    st.success(f"âœ… íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ: {result['successful_count']}/{result['processed_count']}ê°œ ì„±ê³µ")
                    
                    if result['failed_files']:
                        st.warning("ì¼ë¶€ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨:")
                        for failed in result['failed_files']:
                            st.error(f"- {failed['filename']}: {failed['error']}")
                    
                    # ì²˜ë¦¬ ê²°ê³¼ í‘œì‹œ
                    if result['analysis_fragments']:
                        st.markdown("### ğŸ“Š ì²˜ë¦¬ëœ ì¡°ê°ë“¤")
                        
                        for fragment in result['analysis_fragments']:
                            with st.expander(f"ğŸ“„ {fragment['file_source']} ({fragment['file_type']})"):
                                col1, col2 = st.columns(2)
                                
                                with col1:
                                    st.markdown(f"**ì‹ ë¢°ë„:** {fragment['confidence']:.1%}")
                                    st.markdown(f"**í‚¤ì›Œë“œ:** {', '.join(fragment['keywords'][:5])}")
                                
                                with col2:
                                    if fragment.get('speaker'):
                                        st.markdown(f"**í™”ì:** {fragment['speaker']}")
                                    if fragment.get('duration'):
                                        st.markdown(f"**ê¸¸ì´:** {fragment['duration']:.1f}ì´ˆ")
                                
                                st.markdown("**ì¶”ì¶œëœ ë‚´ìš©:**")
                                st.markdown(f"> {fragment['content'][:200]}{'...' if len(fragment['content']) > 200 else ''}")
                else:
                    st.error(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {result['error']}")
    
    with tab2:
        st.markdown("## ğŸ§  í™€ë¦¬ìŠ¤í‹± ë¶„ì„")
        st.markdown("**ì˜ë¯¸ì  ì—°ê²°, ì£¼ì œ í´ëŸ¬ìŠ¤í„°ë§, ì „ì²´ ìŠ¤í† ë¦¬ ìƒì„±**")
        
        if st.button("ğŸ” í™€ë¦¬ìŠ¤í‹± ë¶„ì„ ì‹¤í–‰", type="primary"):
            with st.spinner("í™€ë¦¬ìŠ¤í‹± ë¶„ì„ ìˆ˜í–‰ ì¤‘..."):
                holistic_result = analyzer.run_holistic_analysis()
            
            if "error" not in holistic_result:
                st.success("âœ… í™€ë¦¬ìŠ¤í‹± ë¶„ì„ ì™„ë£Œ!")
                
                # ê²°ê³¼ í‘œì‹œ
                holistic = holistic_result["holistic_analysis"]
                semantic = holistic_result["semantic_connections"]
                ai_insights = holistic_result.get("ai_insights", [])
                
                # ë©”íŠ¸ë¦­
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    st.metric("ì´ ì¡°ê°", holistic.get("total_fragments", 0))
                
                with col2:
                    st.metric("ë°œê²¬ëœ ê°œì²´", holistic.get("total_entities", 0))
                
                with col3:
                    st.metric("ì£¼ìš” ì£¼ì œ", holistic.get("total_topics", 0))
                
                with col4:
                    if "error" not in semantic:
                        st.metric("ì˜ë¯¸ì  ì—°ê²°", semantic.get("semantic_connections", 0))
                    else:
                        st.metric("ì˜ë¯¸ì  ì—°ê²°", "ì˜¤ë¥˜")
                
                # í•µì‹¬ ì¸ì‚¬ì´íŠ¸
                st.markdown("### ğŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸")
                for insight in holistic.get("key_insights", []):
                    st.markdown(f"- {insight}")
                
                # AI ì‹¬í™” ë¶„ì„
                if ai_insights and not ai_insights[0].startswith("AI ì¸ì‚¬ì´íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"):
                    st.markdown("### ğŸ¤– AI ì‹¬í™” ë¶„ì„")
                    for insight in ai_insights:
                        if not insight.startswith("AI"):  # ì—ëŸ¬ ë©”ì‹œì§€ ì œì™¸
                            st.info(f"ğŸ¤– {insight}")
                
                # ìƒì„¸ ê²°ê³¼
                with st.expander("ğŸ“Š ìƒì„¸ ë¶„ì„ ê²°ê³¼"):
                    st.json(holistic_result)
                    
            else:
                st.error(f"âŒ í™€ë¦¬ìŠ¤í‹± ë¶„ì„ ì‹¤íŒ¨: {holistic_result['error']}")
    
    with tab3:
        st.markdown("## ğŸ§ ğŸ§  ë“€ì–¼ ë¸Œë ˆì¸ ì‹œìŠ¤í…œ")
        st.markdown("**AI ì¸ì‚¬ì´íŠ¸ ìƒì„± ë° êµ¬ê¸€ ìº˜ë¦°ë” ì—°ë™**")
        
        if st.button("ğŸš€ ë“€ì–¼ ë¸Œë ˆì¸ í™œì„±í™”", type="primary"):
            with st.spinner("ë“€ì–¼ ë¸Œë ˆì¸ ì‹œìŠ¤í…œ ì‹¤í–‰ ì¤‘..."):
                dual_brain_result = analyzer.trigger_dual_brain_system()
            
            if "error" not in dual_brain_result:
                st.success("âœ… ë“€ì–¼ ë¸Œë ˆì¸ ë¶„ì„ ì™„ë£Œ!")
                
                # AI ì¸ì‚¬ì´íŠ¸ í‘œì‹œ
                if dual_brain_result.get("ai_insights"):
                    st.markdown("### ğŸ¤– AI ì¸ì‚¬ì´íŠ¸")
                    for insight in dual_brain_result["ai_insights"]:
                        st.info(insight)
                
                # ìƒì„¸ ê²°ê³¼
                with st.expander("ğŸ§  ë“€ì–¼ ë¸Œë ˆì¸ ìƒì„¸ ê²°ê³¼"):
                    st.json(dual_brain_result)
                    
            else:
                st.error(f"âŒ ë“€ì–¼ ë¸Œë ˆì¸ ì‹¤íŒ¨: {dual_brain_result['error']}")
    
    with tab4:
        st.markdown("## ğŸ“‹ ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ")
        
        if st.button("ğŸ“„ ë³´ê³ ì„œ ìƒì„±", type="primary"):
            report = analyzer.generate_comprehensive_report()
            
            st.markdown("### ğŸ“‹ ì™„ì „ í†µí•© ë¶„ì„ ë³´ê³ ì„œ")
            st.markdown(report)
            
            # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
            st.download_button(
                label="ğŸ“¥ ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ",
                data=report,
                file_name=f"unified_analysis_{conference_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md",
                mime="text/markdown"
            )
    
    # ì‚¬ìš©ë²• ì•ˆë‚´
    st.markdown("---")
    st.markdown("### ğŸ’¡ ì‚¬ìš©ë²•")
    st.markdown("""
    1. **ì‚¬ì „ì •ë³´**: ì»¨í¼ëŸ°ìŠ¤ ë°°ê²½ ì •ë³´ ì…ë ¥ìœ¼ë¡œ ë¶„ì„ í’ˆì§ˆ í–¥ìƒ
    2. **íŒŒì¼/URL ì²˜ë¦¬**: 
       - ì´ë¯¸ì§€, ìŒì„±, ë¹„ë””ì˜¤ íŒŒì¼ ì—…ë¡œë“œ
       - ì›¹ ë™ì˜ìƒ URL ì§ì ‘ ë¶„ì„ (1000+ í”Œë«í¼ ì§€ì›)
       - TXT íŒŒì¼ì— URL ëª©ë¡ ì…ë ¥ìœ¼ë¡œ ë°°ì¹˜ ì²˜ë¦¬
    3. **í™€ë¦¬ìŠ¤í‹± ë¶„ì„**: ì˜ë¯¸ì  ì—°ê²° + Ollama AI ì‹¬í™” ì¸ì‚¬ì´íŠ¸
    4. **ë“€ì–¼ ë¸Œë ˆì¸**: êµ¬ê¸€ ìº˜ë¦°ë” ì—°ë™ + AI íŒ¨í„´ ë¶„ì„
    5. **ì¢…í•© ë³´ê³ ì„œ**: ì™„ì „í•œ í†µí•© ë¶„ì„ ë³´ê³ ì„œ ìƒì„±
    
    **ğŸš€ ì§€ì› í”Œë«í¼**: YouTube, Vimeo, TikTok, Instagram, Twitch, Facebook ë“±  
    **ğŸ“„ TXT ë°°ì¹˜**: URL ëª©ë¡ì„ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì—…ë¡œë“œí•˜ë©´ ìë™ ì¼ê´„ ì²˜ë¦¬**
    """)

if __name__ == "__main__":
    main()