#!/usr/bin/env python3
"""
Ollama ìë™ ì„¤ì¹˜ ë° ì„¤ì • ìŠ¤í¬ë¦½íŠ¸
ì†”ë¡œëª¬ë“œ AI v2.4 í†µí•©ìš©
"""

import os
import sys
import subprocess
import platform
import time
import requests
import json
from pathlib import Path
from typing import Dict, List, Any
import asyncio
import aiohttp

class OllamaAutoSetup:
    """Ollama ìë™ ì„¤ì¹˜ ë° ì„¤ì •"""
    
    def __init__(self):
        self.platform = platform.system().lower()
        self.arch = platform.machine().lower()
        
        # í•„ìš”í•œ ëª¨ë¸ë“¤
        self.required_models = [
            "llama3.2:3b",          # ê²½ëŸ‰ í•œêµ­ì–´ ëª¨ë¸ (4GB)
            "mistral:7b",           # ê°ì • ë¶„ì„ (4.1GB)
            "codellama:7b"          # êµ¬ì¡°í™”ëœ ì¶œë ¥ (3.8GB)
        ]
        
        # ì„ íƒ ëª¨ë¸ë“¤ (ê³ ì„±ëŠ¥)
        self.optional_models = [
            "llama3.1:8b",          # ê³ ì„±ëŠ¥ í•œêµ­ì–´ (4.7GB)
            "qwen2.5:7b"            # ë‹¤êµ­ì–´ ì§€ì› (4.4GB)
        ]
        
        print("Ollama ìë™ ì„¤ì¹˜ ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
        
    def check_system_requirements(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ í™•ì¸"""
        
        requirements = {
            "platform_supported": True,
            "memory_gb": 0,
            "disk_space_gb": 0,
            "recommendations": [],
            "warnings": []
        }
        
        try:
            # ë©”ëª¨ë¦¬ í™•ì¸
            if self.platform == "windows":
                import psutil
                memory_bytes = psutil.virtual_memory().total
                requirements["memory_gb"] = memory_bytes / (1024**3)
            
            # ë””ìŠ¤í¬ ê³µê°„ í™•ì¸
            disk_usage = os.statvfs('.') if hasattr(os, 'statvfs') else None
            if disk_usage:
                requirements["disk_space_gb"] = (disk_usage.f_bavail * disk_usage.f_frsize) / (1024**3)
            
            # ê¶Œì¥ì‚¬í•­ ìƒì„±
            if requirements["memory_gb"] < 8:
                requirements["warnings"].append("ë©”ëª¨ë¦¬ 8GB ë¯¸ë§Œ: ì„±ëŠ¥ ì €í•˜ ê°€ëŠ¥")
                requirements["recommendations"].append("ê²½ëŸ‰ ëª¨ë¸ë§Œ ì„¤ì¹˜ ê¶Œì¥")
            elif requirements["memory_gb"] >= 16:
                requirements["recommendations"].append("ê³ ì„±ëŠ¥ ëª¨ë¸ ì„¤ì¹˜ ê°€ëŠ¥")
            
            if requirements["disk_space_gb"] < 20:
                requirements["warnings"].append("ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡±: 20GB ì´ìƒ ê¶Œì¥")
                
        except Exception as e:
            requirements["warnings"].append(f"ì‹œìŠ¤í…œ ì •ë³´ í™•ì¸ ì‹¤íŒ¨: {str(e)}")
            
        return requirements
    
    def install_ollama(self) -> bool:
        """Ollama ì„¤ì¹˜"""
        
        print("Ollama ì„¤ì¹˜ ì‹œì‘...")
        
        try:
            if self.platform == "windows":
                return self._install_windows()
            elif self.platform == "linux":
                return self._install_linux()
            elif self.platform == "darwin":
                return self._install_macos()
            else:
                print(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” í”Œë«í¼: {self.platform}")
                return False
                
        except Exception as e:
            print(f"âŒ ì„¤ì¹˜ ì‹¤íŒ¨: {str(e)}")
            return False
    
    def _install_windows(self) -> bool:
        """Windowsì— Ollama ì„¤ì¹˜"""
        
        try:
            # winget ì‚¬ìš© ì‹œë„
            result = subprocess.run(
                ["winget", "install", "ollama"],
                capture_output=True,
                text=True,
                timeout=300
            )
            
            if result.returncode == 0:
                print("âœ… wingetìœ¼ë¡œ ì„¤ì¹˜ ì™„ë£Œ")
                return True
            else:
                print("âš ï¸ winget ì„¤ì¹˜ ì‹¤íŒ¨, ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ ì•ˆë‚´")
                self._manual_install_guide()
                return False
                
        except subprocess.TimeoutExpired:
            print("âš ï¸ ì„¤ì¹˜ ì‹œê°„ ì´ˆê³¼")
            return False
        except FileNotFoundError:
            print("âš ï¸ wingetì„ ì°¾ì„ ìˆ˜ ì—†ìŒ, ìˆ˜ë™ ì„¤ì¹˜ í•„ìš”")
            self._manual_install_guide()
            return False
    
    def _install_linux(self) -> bool:
        """Linuxì— Ollama ì„¤ì¹˜"""
        
        try:
            # ê³µì‹ ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš©
            result = subprocess.run(
                ["curl", "-fsSL", "https://ollama.ai/install.sh"],
                capture_output=True,
                text=True,
                timeout=30
            )
            
            if result.returncode == 0:
                # ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
                install_result = subprocess.run(
                    ["sh", "-c", result.stdout],
                    capture_output=True,
                    text=True,
                    timeout=300
                )
                
                if install_result.returncode == 0:
                    print("âœ… Linux ì„¤ì¹˜ ì™„ë£Œ")
                    return True
                    
        except Exception as e:
            print(f"âŒ Linux ì„¤ì¹˜ ì‹¤íŒ¨: {str(e)}")
            
        return False
    
    def _install_macos(self) -> bool:
        """macOSì— Ollama ì„¤ì¹˜"""
        
        try:
            # Homebrew ì‚¬ìš© ì‹œë„
            result = subprocess.run(
                ["brew", "install", "ollama"],
                capture_output=True,
                text=True,
                timeout=300
            )
            
            if result.returncode == 0:
                print("âœ… Homebrewë¡œ ì„¤ì¹˜ ì™„ë£Œ")
                return True
                
        except Exception as e:
            print(f"âŒ macOS ì„¤ì¹˜ ì‹¤íŒ¨: {str(e)}")
            
        return False
    
    def _manual_install_guide(self):
        """ìˆ˜ë™ ì„¤ì¹˜ ê°€ì´ë“œ"""
        
        print("""
        ğŸ“‹ ìˆ˜ë™ ì„¤ì¹˜ ê°€ì´ë“œ:
        
        1. https://ollama.ai/download ë°©ë¬¸
        2. ìš´ì˜ì²´ì œì— ë§ëŠ” ì„¤ì¹˜íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        3. ì„¤ì¹˜íŒŒì¼ ì‹¤í–‰
        4. ì„¤ì¹˜ ì™„ë£Œ í›„ 'ollama serve' ëª…ë ¹ì–´ ì‹¤í–‰
        5. ë‹¤ì‹œ ì´ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
        """)
    
    def check_ollama_status(self) -> bool:
        """Ollama ì„œë²„ ìƒíƒœ í™•ì¸"""
        
        try:
            result = subprocess.run(
                ["ollama", "--version"],
                capture_output=True,
                text=True,
                timeout=10
            )
            
            if result.returncode == 0:
                print(f"âœ… Ollama ì„¤ì¹˜ë¨: {result.stdout.strip()}")
                return True
            else:
                print("âŒ Ollama ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
                return False
                
        except FileNotFoundError:
            print("âŒ Ollama ëª…ë ¹ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return False
        except Exception as e:
            print(f"âŒ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {str(e)}")
            return False
    
    def start_ollama_server(self) -> bool:
        """Ollama ì„œë²„ ì‹œì‘"""
        
        print("ğŸš€ Ollama ì„œë²„ ì‹œì‘...")
        
        try:
            # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì„œë²„ ì‹œì‘
            if self.platform == "windows":
                subprocess.Popen(
                    ["ollama", "serve"],
                    creationflags=subprocess.CREATE_NEW_CONSOLE
                )
            else:
                subprocess.Popen(
                    ["ollama", "serve"],
                    stdout=subprocess.DEVNULL,
                    stderr=subprocess.DEVNULL
                )
            
            # ì„œë²„ ì‹œì‘ ëŒ€ê¸°
            for i in range(30):  # 30ì´ˆ ëŒ€ê¸°
                time.sleep(1)
                if self._check_server_running():
                    print("âœ… Ollama ì„œë²„ ì‹œì‘ë¨")
                    return True
                print(f"â³ ì„œë²„ ì‹œì‘ ëŒ€ê¸°... ({i+1}/30)")
            
            print("âŒ ì„œë²„ ì‹œì‘ ì‹œê°„ ì´ˆê³¼")
            return False
            
        except Exception as e:
            print(f"âŒ ì„œë²„ ì‹œì‘ ì‹¤íŒ¨: {str(e)}")
            return False
    
    def _check_server_running(self) -> bool:
        """ì„œë²„ ì‹¤í–‰ ìƒíƒœ í™•ì¸"""
        
        try:
            response = requests.get("http://localhost:11434/api/tags", timeout=2)
            return response.status_code == 200
        except:
            return False
    
    async def install_models(self, install_optional: bool = False) -> Dict[str, bool]:
        """ëª¨ë¸ ì„¤ì¹˜"""
        
        print("ğŸ§  AI ëª¨ë¸ ì„¤ì¹˜ ì‹œì‘...")
        
        models_to_install = self.required_models.copy()
        if install_optional:
            models_to_install.extend(self.optional_models)
        
        results = {}
        
        for model in models_to_install:
            print(f"ğŸ“¥ {model} ì„¤ì¹˜ ì¤‘...")
            success = await self._install_single_model(model)
            results[model] = success
            
            if success:
                print(f"âœ… {model} ì„¤ì¹˜ ì™„ë£Œ")
            else:
                print(f"âŒ {model} ì„¤ì¹˜ ì‹¤íŒ¨")
        
        return results
    
    async def _install_single_model(self, model_name: str) -> bool:
        """ê°œë³„ ëª¨ë¸ ì„¤ì¹˜"""
        
        try:
            process = await asyncio.create_subprocess_exec(
                "ollama", "pull", model_name,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            stdout, stderr = await process.communicate()
            
            if process.returncode == 0:
                return True
            else:
                print(f"âš ï¸ {model_name} ì„¤ì¹˜ ì˜¤ë¥˜: {stderr.decode()}")
                return False
                
        except Exception as e:
            print(f"âŒ {model_name} ì„¤ì¹˜ ì˜ˆì™¸: {str(e)}")
            return False
    
    async def test_integration(self) -> Dict[str, Any]:
        """ì†”ë¡œëª¬ë“œ AI í†µí•© í…ŒìŠ¤íŠ¸"""
        
        print("ğŸ§ª ì†”ë¡œëª¬ë“œ AI í†µí•© í…ŒìŠ¤íŠ¸...")
        
        try:
            from core.ollama_integration_engine import OllamaIntegrationEngine
            
            engine = OllamaIntegrationEngine()
            status = await engine.check_ollama_availability()
            
            if status["server_available"]:
                # ê°„ë‹¨í•œ í•œêµ­ì–´ í…ŒìŠ¤íŠ¸
                test_result = await engine.analyze_korean_conversation(
                    "ê³ ê°: ì•ˆë…•í•˜ì„¸ìš”. ìƒë‹´ì‚¬: ë„¤, ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"
                )
                
                return {
                    "integration_success": True,
                    "server_status": status,
                    "test_result": test_result
                }
            else:
                return {
                    "integration_success": False,
                    "server_status": status
                }
                
        except Exception as e:
            return {
                "integration_success": False,
                "error": str(e)
            }
    
    def generate_config(self) -> str:
        """ì„¤ì • íŒŒì¼ ìƒì„±"""
        
        config = {
            "ollama": {
                "base_url": "http://localhost:11434",
                "models": {
                    "korean_chat": "llama3.2:3b",
                    "emotion_analysis": "mistral:7b",
                    "structured_output": "codellama:7b"
                },
                "timeout": 60,
                "max_retries": 3
            },
            "integration": {
                "fallback_enabled": True,
                "cache_responses": True,
                "log_level": "INFO"
            }
        }
        
        config_path = "config/ollama_config.json"
        os.makedirs("config", exist_ok=True)
        
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“‹ ì„¤ì • íŒŒì¼ ìƒì„±: {config_path}")
        return config_path

async def main():
    """ë©”ì¸ ì„¤ì¹˜ í”„ë¡œì„¸ìŠ¤"""
    
    setup = OllamaAutoSetup()
    
    print("=" * 60)
    print("ğŸ¦™ Ollama + ì†”ë¡œëª¬ë“œ AI í†µí•© ì„¤ì¹˜")
    print("=" * 60)
    
    # 1. ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ í™•ì¸
    print("\n1ï¸âƒ£ ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ í™•ì¸")
    requirements = setup.check_system_requirements()
    
    print(f"ğŸ’» ë©”ëª¨ë¦¬: {requirements['memory_gb']:.1f}GB")
    print(f"ğŸ’¾ ë””ìŠ¤í¬: {requirements['disk_space_gb']:.1f}GB")
    
    if requirements["warnings"]:
        print("âš ï¸ ê²½ê³ ì‚¬í•­:")
        for warning in requirements["warnings"]:
            print(f"  â€¢ {warning}")
    
    # 2. Ollama ì„¤ì¹˜
    print("\n2ï¸âƒ£ Ollama ì„¤ì¹˜")
    if not setup.check_ollama_status():
        install_success = setup.install_ollama()
        if not install_success:
            print("âŒ ì„¤ì¹˜ ì‹¤íŒ¨. ìˆ˜ë™ ì„¤ì¹˜ í›„ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.")
            return
    
    # 3. ì„œë²„ ì‹œì‘
    print("\n3ï¸âƒ£ Ollama ì„œë²„ ì‹œì‘")
    if not setup._check_server_running():
        server_success = setup.start_ollama_server()
        if not server_success:
            print("âŒ ì„œë²„ ì‹œì‘ ì‹¤íŒ¨")
            return
    
    # 4. ëª¨ë¸ ì„¤ì¹˜
    print("\n4ï¸âƒ£ AI ëª¨ë¸ ì„¤ì¹˜")
    install_optional = input("ê³ ì„±ëŠ¥ ëª¨ë¸ë„ ì„¤ì¹˜í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").lower() == 'y'
    
    model_results = await setup.install_models(install_optional)
    
    successful_models = [model for model, success in model_results.items() if success]
    print(f"âœ… ì„¤ì¹˜ ì™„ë£Œ: {len(successful_models)}/{len(model_results)} ëª¨ë¸")
    
    # 5. í†µí•© í…ŒìŠ¤íŠ¸
    print("\n5ï¸âƒ£ ì†”ë¡œëª¬ë“œ AI í†µí•© í…ŒìŠ¤íŠ¸")
    test_result = await setup.test_integration()
    
    if test_result["integration_success"]:
        print("âœ… í†µí•© í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
        print("ğŸ‰ Ollama + ì†”ë¡œëª¬ë“œ AI í†µí•© ì™„ë£Œ!")
    else:
        print("âŒ í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
        print(f"ì˜¤ë¥˜: {test_result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
    
    # 6. ì„¤ì • íŒŒì¼ ìƒì„±
    print("\n6ï¸âƒ£ ì„¤ì • íŒŒì¼ ìƒì„±")
    config_path = setup.generate_config()
    
    print("\n" + "=" * 60)
    print("ğŸ¯ ì„¤ì¹˜ ì™„ë£Œ!")
    print("=" * 60)
    print("ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì†”ë¡œëª¬ë“œ AIë¥¼ ì‹œì‘í•˜ì„¸ìš”:")
    print("python -m streamlit run jewelry_stt_ui_v23_real.py --server.port 8503")
    print("=" * 60)

if __name__ == "__main__":
    asyncio.run(main())