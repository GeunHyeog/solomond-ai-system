#!/usr/bin/env python3
"""
ğŸ•·ï¸ Module 2: ì›¹ í¬ë¡¤ëŸ¬ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤
- Streamlit UIì˜ ì›¹ í¬ë¡¤ë§ ê¸°ëŠ¥ì„ FastAPIë¡œ ë³€í™˜
- ìŠ¤ë§ˆíŠ¸ ë©”ëª¨ë¦¬ ë§¤ë‹ˆì €ì™€ ì™„ì „ í†µí•©
- í¬íŠ¸ 8002ì—ì„œ ì‹¤í–‰
"""

import logging
import asyncio
import uuid
from typing import List, Dict, Any, Optional, Union
from datetime import datetime
from urllib.parse import urlparse

from fastapi import FastAPI, HTTPException, BackgroundTasks, Query
from fastapi.responses import JSONResponse
from pydantic import BaseModel, HttpUrl
import uvicorn
import httpx
from bs4 import BeautifulSoup
import requests

# ìµœì í™”ëœ ì»´í¬ë„ŒíŠ¸ë“¤ import
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))
from core.smart_memory_manager import get_memory_stats
from config.security_config import get_system_config

logger = logging.getLogger(__name__)

# Pydantic ëª¨ë¸ë“¤
class CrawlRequest(BaseModel):
    """í¬ë¡¤ë§ ìš”ì²­ ëª¨ë¸"""
    urls: List[str]
    max_depth: int = 1
    max_pages: int = 10
    include_images: bool = True
    include_links: bool = True
    custom_headers: Optional[Dict[str, str]] = None

class CrawlResult(BaseModel):
    """í¬ë¡¤ë§ ê²°ê³¼ ëª¨ë¸"""
    session_id: str
    status: str
    results: Dict[str, Any]
    processing_time: float
    timestamp: datetime

class WebCrawlerService:
    """ì›¹ í¬ë¡¤ëŸ¬ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.active_sessions: Dict[str, Dict] = {}
        self.crawl_cache: Dict[str, Any] = {}
        self.user_agent = "SolomondAI-WebCrawler/4.0"
        
    async def crawl_websites(self, urls: List[str], max_depth: int = 1, 
                           max_pages: int = 10, include_images: bool = True,
                           include_links: bool = True, 
                           custom_headers: Optional[Dict[str, str]] = None) -> Dict[str, Any]:
        """ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì‹¤í–‰"""
        session_id = str(uuid.uuid4())
        start_time = asyncio.get_event_loop().time()
        
        try:
            # ì„¸ì…˜ ì •ë³´ ì €ì¥
            self.active_sessions[session_id] = {
                'start_time': start_time,
                'status': 'processing',
                'urls_count': len(urls),
                'max_depth': max_depth,
                'max_pages': max_pages
            }
            
            results = {
                'session_id': session_id,
                'crawled_pages': [],
                'total_pages': 0,
                'total_images': 0,
                'total_links': 0,
                'errors': [],
                'summary': {}
            }
            
            # ê° URL í¬ë¡¤ë§
            for url in urls[:max_pages]:  # max_pages ì œí•œ ì ìš©
                try:
                    page_result = await self._crawl_single_page(
                        url, include_images, include_links, custom_headers
                    )
                    results['crawled_pages'].append(page_result)
                    
                    # í†µê³„ ì—…ë°ì´íŠ¸
                    results['total_pages'] += 1
                    if page_result.get('images'):
                        results['total_images'] += len(page_result['images'])
                    if page_result.get('links'):
                        results['total_links'] += len(page_result['links'])
                        
                except Exception as e:
                    error_info = {'url': url, 'error': str(e)}
                    results['errors'].append(error_info)
                    logger.error(f"URL í¬ë¡¤ë§ ì‹¤íŒ¨ {url}: {e}")
            
            # ê¹Šì´ë³„ í¬ë¡¤ë§ (depth > 1ì¸ ê²½ìš°)
            if max_depth > 1 and results['crawled_pages']:
                await self._crawl_deeper(
                    results, max_depth, max_pages, 
                    include_images, include_links, custom_headers
                )
            
            # ìš”ì•½ ìƒì„±
            results['summary'] = await self._generate_crawl_summary(results)
            
            processing_time = asyncio.get_event_loop().time() - start_time
            
            # ì„¸ì…˜ ì™„ë£Œ
            self.active_sessions[session_id]['status'] = 'completed'
            self.active_sessions[session_id]['processing_time'] = processing_time
            
            return {
                'session_id': session_id,
                'status': 'success',
                'results': results,
                'processing_time': processing_time,
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"í¬ë¡¤ë§ ì‹¤íŒ¨ {session_id}: {e}")
            
            if session_id in self.active_sessions:
                self.active_sessions[session_id]['status'] = 'failed'
                self.active_sessions[session_id]['error'] = str(e)
            
            raise HTTPException(status_code=500, detail=f"í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}")
    
    async def _crawl_single_page(self, url: str, include_images: bool, 
                               include_links: bool, 
                               custom_headers: Optional[Dict[str, str]] = None) -> Dict[str, Any]:
        """ë‹¨ì¼ í˜ì´ì§€ í¬ë¡¤ë§"""
        headers = {
            'User-Agent': self.user_agent,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        }
        
        if custom_headers:
            headers.update(custom_headers)
        
        try:
            # ìºì‹œ í™•ì¸
            cache_key = f"{url}_{hash(str(headers))}"
            if cache_key in self.crawl_cache:
                logger.info(f"ìºì‹œì—ì„œ ê°€ì ¸ì˜´: {url}")
                return self.crawl_cache[cache_key]
            
            # HTTP ìš”ì²­
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.get(url, headers=headers, follow_redirects=True)
                response.raise_for_status()
            
            # HTML íŒŒì‹±
            soup = BeautifulSoup(response.text, 'html.parser')
            
            page_result = {
                'url': url,
                'title': self._extract_title(soup),
                'status_code': response.status_code,
                'content_length': len(response.text),
                'text_content': self._extract_text_content(soup),
                'meta_description': self._extract_meta_description(soup),
                'crawled_at': datetime.now().isoformat()
            }
            
            # ì´ë¯¸ì§€ ì¶”ì¶œ
            if include_images:
                page_result['images'] = self._extract_images(soup, url)
            
            # ë§í¬ ì¶”ì¶œ
            if include_links:
                page_result['links'] = self._extract_links(soup, url)
            
            # ìºì‹œ ì €ì¥
            self.crawl_cache[cache_key] = page_result
            
            return page_result
            
        except httpx.TimeoutException:
            raise Exception(f"ìš”ì²­ ì‹œê°„ ì´ˆê³¼: {url}")
        except httpx.HTTPStatusError as e:
            raise Exception(f"HTTP ì˜¤ë¥˜ {e.response.status_code}: {url}")
        except Exception as e:
            raise Exception(f"í˜ì´ì§€ í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}")
    
    def _extract_title(self, soup: BeautifulSoup) -> str:
        """í˜ì´ì§€ ì œëª© ì¶”ì¶œ"""
        title_tag = soup.find('title')
        return title_tag.get_text(strip=True) if title_tag else "ì œëª© ì—†ìŒ"
    
    def _extract_text_content(self, soup: BeautifulSoup) -> str:
        """í…ìŠ¤íŠ¸ ì½˜í…ì¸  ì¶”ì¶œ"""
        # ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼ íƒœê·¸ ì œê±°
        for script in soup(["script", "style"]):
            script.decompose()
        
        text = soup.get_text()
        # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ ì¹˜í™˜, ì¤„ë°”ê¿ˆ ì •ë¦¬
        import re
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text[:1000]  # 1000ìë¡œ ì œí•œ
    
    def _extract_meta_description(self, soup: BeautifulSoup) -> Optional[str]:
        """ë©”íƒ€ ì„¤ëª… ì¶”ì¶œ"""
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc:
            return meta_desc.get('content', '').strip()
        return None
    
    def _extract_images(self, soup: BeautifulSoup, base_url: str) -> List[Dict[str, str]]:
        """ì´ë¯¸ì§€ URL ì¶”ì¶œ"""
        images = []
        img_tags = soup.find_all('img', src=True)
        
        for img in img_tags[:20]:  # ìµœëŒ€ 20ê°œë¡œ ì œí•œ
            src = img.get('src')
            if src:
                # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                from urllib.parse import urljoin
                full_url = urljoin(base_url, src)
                
                images.append({
                    'src': full_url,
                    'alt': img.get('alt', ''),
                    'title': img.get('title', '')
                })
        
        return images
    
    def _extract_links(self, soup: BeautifulSoup, base_url: str) -> List[Dict[str, str]]:
        """ë§í¬ URL ì¶”ì¶œ"""
        links = []
        a_tags = soup.find_all('a', href=True)
        
        for a in a_tags[:50]:  # ìµœëŒ€ 50ê°œë¡œ ì œí•œ
            href = a.get('href')
            if href:
                from urllib.parse import urljoin
                full_url = urljoin(base_url, href)
                
                # ì™¸ë¶€ ë§í¬ë§Œ ìˆ˜ì§‘ (http/httpsë¡œ ì‹œì‘í•˜ëŠ” ê²ƒ)
                if full_url.startswith(('http://', 'https://')):
                    links.append({
                        'url': full_url,
                        'text': a.get_text(strip=True)[:100],  # ë§í¬ í…ìŠ¤íŠ¸ 100ì ì œí•œ
                        'title': a.get('title', '')
                    })
        
        return links
    
    async def _crawl_deeper(self, results: Dict[str, Any], max_depth: int, 
                          max_pages: int, include_images: bool, include_links: bool,
                          custom_headers: Optional[Dict[str, str]] = None):
        """ê¹Šì´ë³„ í¬ë¡¤ë§ (í˜„ì¬ëŠ” ê¸°ë³¸ êµ¬í˜„)"""
        # ì¶”í›„ êµ¬í˜„: ì²« ë²ˆì§¸ ë ˆë²¨ì—ì„œ ì°¾ì€ ë§í¬ë“¤ì„ ì´ìš©í•´ ë” ê¹Šì´ í¬ë¡¤ë§
        logger.info(f"ê¹Šì´ {max_depth} í¬ë¡¤ë§ ê¸°ëŠ¥ ê°œë°œ ì˜ˆì •")
    
    async def _generate_crawl_summary(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½ ìƒì„±"""
        return {
            'total_pages_crawled': results['total_pages'],
            'total_images_found': results['total_images'],
            'total_links_found': results['total_links'],
            'total_errors': len(results['errors']),
            'success_rate': f"{((results['total_pages'] / (results['total_pages'] + len(results['errors']))) * 100) if (results['total_pages'] + len(results['errors'])) > 0 else 0:.1f}%",
            'recommendations': self._generate_recommendations(results)
        }
    
    def _generate_recommendations(self, results: Dict[str, Any]) -> List[str]:
        """ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        if results['total_errors'] > 0:
            recommendations.append("ì¼ë¶€ í˜ì´ì§€ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë„¤íŠ¸ì›Œí¬ ìƒíƒœë¥¼ í™•ì¸í•´ë³´ì„¸ìš”.")
        
        if results['total_images'] > 100:
            recommendations.append("ì´ë¯¸ì§€ê°€ ë§ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ê¸°ëŠ¥ì„ ê³ ë ¤í•´ë³´ì„¸ìš”.")
        
        if results['total_links'] > 200:
            recommendations.append("ë§ì€ ë§í¬ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. ë” ê¹Šì€ í¬ë¡¤ë§ì„ ê³ ë ¤í•´ë³´ì„¸ìš”.")
        
        recommendations.append("í¬ë¡¤ë§ ê²°ê³¼ë¥¼ CSV/JSONìœ¼ë¡œ ë‚´ë³´ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        return recommendations

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="Module 2: ì›¹ í¬ë¡¤ëŸ¬ ì„œë¹„ìŠ¤",
    description="ì§€ëŠ¥í˜• ì›¹ í¬ë¡¤ë§ ë° ë°ì´í„° ìˆ˜ì§‘",
    version="4.0.0"
)

# ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
service = WebCrawlerService()

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ì²´í¬"""
    memory_stats = get_memory_stats()
    
    return {
        "status": "healthy",
        "service": "module2_crawler",
        "version": "4.0.0",
        "memory": {
            "memory_percent": memory_stats.get('memory_info', {}).get('percent', 0)
        },
        "cache": {
            "cached_pages": len(service.crawl_cache),
            "active_sessions": len(service.active_sessions)
        },
        "timestamp": datetime.now().isoformat()
    }

@app.post("/crawl", response_model=CrawlResult)
async def crawl_websites(
    background_tasks: BackgroundTasks,
    urls: List[str] = Query(..., description="í¬ë¡¤ë§í•  URL ëª©ë¡"),
    max_depth: int = Query(1, description="ìµœëŒ€ í¬ë¡¤ë§ ê¹Šì´"),
    max_pages: int = Query(10, description="ìµœëŒ€ í˜ì´ì§€ ìˆ˜"),
    include_images: bool = Query(True, description="ì´ë¯¸ì§€ í¬í•¨ ì—¬ë¶€"),
    include_links: bool = Query(True, description="ë§í¬ í¬í•¨ ì—¬ë¶€")
):
    """ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§ API"""
    if not urls:
        raise HTTPException(status_code=400, detail="í¬ë¡¤ë§í•  URLì´ ì—†ìŠµë‹ˆë‹¤")
    
    # URL ìœ íš¨ì„± ê²€ì‚¬
    for url in urls:
        try:
            parsed = urlparse(url)
            if not parsed.scheme or not parsed.netloc:
                raise HTTPException(status_code=400, detail=f"ìœ íš¨í•˜ì§€ ì•Šì€ URL: {url}")
        except Exception:
            raise HTTPException(status_code=400, detail=f"URL íŒŒì‹± ì˜¤ë¥˜: {url}")
    
    # í¬ë¡¤ë§ ì‹¤í–‰
    result = await service.crawl_websites(
        urls, max_depth, max_pages, include_images, include_links
    )
    
    return result

@app.get("/sessions/{session_id}")
async def get_session_info(session_id: str):
    """ì„¸ì…˜ ì •ë³´ ì¡°íšŒ"""
    if session_id not in service.active_sessions:
        raise HTTPException(status_code=404, detail="ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    return service.active_sessions[session_id]

@app.get("/sessions")
async def list_sessions():
    """ì „ì²´ ì„¸ì…˜ ëª©ë¡"""
    return {
        "total_sessions": len(service.active_sessions),
        "sessions": service.active_sessions
    }

@app.delete("/cache")
async def clear_cache():
    """í¬ë¡¤ë§ ìºì‹œ ì§€ìš°ê¸°"""
    cache_size = len(service.crawl_cache)
    service.crawl_cache.clear()
    
    return {
        "message": f"{cache_size}ê°œì˜ ìºì‹œ í•­ëª©ì„ ì‚­ì œí–ˆìŠµë‹ˆë‹¤",
        "timestamp": datetime.now().isoformat()
    }

@app.get("/stats")
async def get_service_stats():
    """ì„œë¹„ìŠ¤ í†µê³„"""
    memory_stats = get_memory_stats()
    
    return {
        "service_info": {
            "name": "module2_crawler",
            "version": "4.0.0",
            "uptime": "ì‹¤í–‰ ì¤‘"
        },
        "memory": memory_stats,
        "sessions": {
            "total": len(service.active_sessions),
            "active": len([s for s in service.active_sessions.values() 
                          if s['status'] == 'processing']),
            "completed": len([s for s in service.active_sessions.values() 
                             if s['status'] == 'completed'])
        },
        "cache": {
            "cached_pages": len(service.crawl_cache),
            "cache_hit_rate": "ì¶”í›„ êµ¬í˜„"
        }
    }

if __name__ == "__main__":
    logger.info("ğŸ•·ï¸ Module 2 ì›¹ í¬ë¡¤ëŸ¬ ì„œë¹„ìŠ¤ ì‹œì‘: http://localhost:8002")
    
    uvicorn.run(
        "module2_service:app",
        host="0.0.0.0", 
        port=8002,
        reload=True,
        log_level="info"
    )