#!/usr/bin/env python3
"""
ğŸš€ Module1 Advanced API
ê¸°ì¡´ ì‹¤ì œ ë¶„ì„ ì—”ì§„ í†µí•© + ë¹„ë™ê¸° ì²˜ë¦¬ + ì‹¤ì‹œê°„ ì§„í–‰ë¥ 
"""

from fastapi import FastAPI, UploadFile, File, HTTPException, BackgroundTasks
from fastapi.responses import JSONResponse
from typing import List, Dict, Any
import sys
import os
import tempfile
import json
from datetime import datetime
import asyncio
import uuid
from concurrent.futures import ThreadPoolExecutor
import threading

# í˜„ì¬ í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

app = FastAPI(
    title="Module1: Advanced Conference Analysis API",
    description="ê³ ê¸‰ ì»¨í¼ëŸ°ìŠ¤ ë¶„ì„ API - ì‹¤ì œ AI ì—”ì§„ í†µí•©",
    version="2.0.0"
)

# ì „ì—­ ë³€ìˆ˜
analysis_progress = {}
executor = ThreadPoolExecutor(max_workers=4)

# Ollama + ì‹¤ì œ ë¶„ì„ ì—”ì§„ ë¡œë“œ
try:
    from shared.ollama_interface import OllamaInterface
    ollama = OllamaInterface()
    OLLAMA_AVAILABLE = True
    print(f"OK Ollama ì—°ê²° ì„±ê³µ: {len(ollama.available_models)}ê°œ ëª¨ë¸")
except Exception as e:
    OLLAMA_AVAILABLE = False
    print(f"ERROR Ollama ì—°ê²° ì‹¤íŒ¨: {e}")

# ì‹¤ì œ ë¶„ì„ ì—”ì§„ ë¡œë“œ ì‹œë„
try:
    from core.real_analysis_engine import RealAnalysisEngine
    real_engine = RealAnalysisEngine()
    REAL_ENGINE_AVAILABLE = True
    print("OK ì‹¤ì œ ë¶„ì„ ì—”ì§„ ë¡œë“œ ì„±ê³µ")
except Exception as e:
    REAL_ENGINE_AVAILABLE = False
    print(f"ERROR ì‹¤ì œ ë¶„ì„ ì—”ì§„ ë¡œë“œ ì‹¤íŒ¨: {e}")

@app.get("/")
async def root():
    """ì„œë¹„ìŠ¤ ì •ë³´"""
    return {
        "service": "Module1 - Advanced Conference Analysis",
        "version": "2.0.0",
        "status": "online",
        "capabilities": {
            "ollama_ai": OLLAMA_AVAILABLE,
            "real_analysis_engine": REAL_ENGINE_AVAILABLE,
            "async_processing": True,
            "real_time_progress": True,
            "supported_formats": ["txt", "pdf", "docx", "jpg", "png", "mp3", "wav", "m4a", "mp4", "mov"]
        },
        "models": ollama.available_models if OLLAMA_AVAILABLE else [],
        "endpoints": [
            "POST /analyze - ê³ ê¸‰ íŒŒì¼ ë¶„ì„",
            "POST /analyze/async - ë¹„ë™ê¸° ë¶„ì„ ì‹œì‘",
            "GET /analysis/{session_id}/status - ë¶„ì„ ì§„í–‰ë¥  í™•ì¸",
            "GET /analysis/{session_id}/result - ë¶„ì„ ê²°ê³¼ ì¡°íšŒ",
            "GET /health - ìƒíƒœ í™•ì¸"
        ]
    }

@app.get("/health")
async def health_check():
    """ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
    return {
        "status": "healthy",
        "ollama": "available" if OLLAMA_AVAILABLE else "unavailable",
        "real_engine": "available" if REAL_ENGINE_AVAILABLE else "unavailable",
        "active_analyses": len(analysis_progress),
        "timestamp": datetime.now().isoformat()
    }

@app.post("/analyze")
async def analyze_files_sync(files: List[UploadFile] = File(...)):
    """ë™ê¸°ì‹ íŒŒì¼ ë¶„ì„ - ê¸°ì¡´ ë°©ì‹ê³¼ í˜¸í™˜"""
    
    if not files:
        raise HTTPException(status_code=400, detail="ë¶„ì„í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
    
    try:
        session_id = str(uuid.uuid4())
        analysis_progress[session_id] = {
            "status": "processing",
            "progress": 0,
            "message": "ë¶„ì„ ì‹œì‘",
            "start_time": datetime.now().isoformat()
        }
        
        result = await process_files_advanced(files, session_id)
        
        # ì§„í–‰ë¥  ì™„ë£Œ í‘œì‹œ
        analysis_progress[session_id].update({
            "status": "completed",
            "progress": 100,
            "message": "ë¶„ì„ ì™„ë£Œ",
            "end_time": datetime.now().isoformat()
        })
        
        return result
        
    except Exception as e:
        if session_id in analysis_progress:
            analysis_progress[session_id].update({
                "status": "error",
                "message": str(e),
                "end_time": datetime.now().isoformat()
            })
        raise HTTPException(status_code=500, detail=f"ë¶„ì„ ì‹¤íŒ¨: {str(e)}")

@app.post("/analyze/async")
async def analyze_files_async(background_tasks: BackgroundTasks, files: List[UploadFile] = File(...)):
    """ë¹„ë™ê¸° íŒŒì¼ ë¶„ì„ ì‹œì‘"""
    
    if not files:
        raise HTTPException(status_code=400, detail="ë¶„ì„í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
    
    session_id = str(uuid.uuid4())
    
    # ì§„í–‰ë¥  ì´ˆê¸°í™”
    analysis_progress[session_id] = {
        "status": "queued",
        "progress": 0,
        "message": "ë¶„ì„ ëŒ€ê¸° ì¤‘",
        "start_time": datetime.now().isoformat(),
        "files_count": len(files)
    }
    
    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë¶„ì„ ì‹¤í–‰
    background_tasks.add_task(process_files_background, files, session_id)
    
    return {
        "session_id": session_id,
        "status": "queued",
        "message": "ë¶„ì„ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. ì§„í–‰ë¥ ì„ í™•ì¸í•˜ì„¸ìš”.",
        "status_url": f"/analysis/{session_id}/status",
        "result_url": f"/analysis/{session_id}/result"
    }

@app.get("/analysis/{session_id}/status")
async def get_analysis_status(session_id: str):
    """ë¶„ì„ ì§„í–‰ë¥  í™•ì¸"""
    
    if session_id not in analysis_progress:
        raise HTTPException(status_code=404, detail="ë¶„ì„ ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    return analysis_progress[session_id]

@app.get("/analysis/{session_id}/result")
async def get_analysis_result(session_id: str):
    """ë¶„ì„ ê²°ê³¼ ì¡°íšŒ"""
    
    if session_id not in analysis_progress:
        raise HTTPException(status_code=404, detail="ë¶„ì„ ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    progress_info = analysis_progress[session_id]
    
    if progress_info["status"] != "completed":
        raise HTTPException(status_code=202, detail=f"ë¶„ì„ ì§„í–‰ ì¤‘: {progress_info['progress']}%")
    
    return progress_info.get("result", {"error": "ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"})

async def process_files_advanced(files: List[UploadFile], session_id: str) -> Dict[str, Any]:
    """ê³ ê¸‰ íŒŒì¼ ì²˜ë¦¬ - ì‹¤ì œ ë¶„ì„ ì—”ì§„ í™œìš©"""
    
    results = {
        "analysis_id": f"advanced_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
        "session_id": session_id,
        "timestamp": datetime.now().isoformat(),
        "files_count": len(files),
        "files_analyzed": [],
        "comprehensive_summary": {},
        "ai_analysis": {},
        "advanced_features": {
            "speaker_diarization": False,
            "sentiment_analysis": False,
            "topic_modeling": False,
            "language_detection": False
        }
    }
    
    # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
    def update_progress(progress: int, message: str):
        if session_id in analysis_progress:
            analysis_progress[session_id].update({
                "progress": progress,
                "message": message,
                "status": "processing"
            })
    
    try:
        update_progress(10, "íŒŒì¼ ì €ì¥ ì¤‘...")
        
        # ì„ì‹œ ë””ë ‰í† ë¦¬ì— íŒŒì¼ ì €ì¥
        with tempfile.TemporaryDirectory() as temp_dir:
            saved_files = []
            
            for i, file in enumerate(files):
                update_progress(10 + (i * 10 // len(files)), f"íŒŒì¼ ì €ì¥ ì¤‘: {file.filename}")
                
                temp_path = os.path.join(temp_dir, file.filename)
                content = await file.read()
                with open(temp_path, 'wb') as f:
                    f.write(content)
                
                saved_files.append({
                    "filename": file.filename,
                    "size": len(content),
                    "content_type": file.content_type,
                    "path": temp_path
                })
            
            update_progress(30, "íŒŒì¼ë³„ ìƒì„¸ ë¶„ì„ ì‹œì‘...")
            
            # íŒŒì¼ë³„ ê³ ê¸‰ ë¶„ì„
            for i, file_info in enumerate(saved_files):
                progress = 30 + (i * 40 // len(saved_files))
                update_progress(progress, f"ë¶„ì„ ì¤‘: {file_info['filename']}")
                
                file_result = await analyze_single_file_advanced(file_info)
                results["files_analyzed"].append(file_result)
            
            update_progress(70, "ì¢…í•© ë¶„ì„ ìƒì„± ì¤‘...")
            
            # ì¢…í•© ë¶„ì„
            if results["files_analyzed"]:
                comprehensive = await generate_comprehensive_analysis_advanced(results["files_analyzed"])
                results["comprehensive_summary"] = comprehensive
                
                update_progress(85, "AI ë¶„ì„ ì‹¤í–‰ ì¤‘...")
                
                # AI ë¶„ì„ (ì‹¤ì œ ì—”ì§„ í™œìš©)
                if REAL_ENGINE_AVAILABLE:
                    ai_analysis = await generate_real_ai_analysis(results["files_analyzed"], temp_dir)
                    results["ai_analysis"] = ai_analysis
                    results["advanced_features"].update({
                        "speaker_diarization": True,
                        "sentiment_analysis": True,
                        "topic_modeling": True
                    })
                elif OLLAMA_AVAILABLE:
                    ai_analysis = await generate_ollama_ai_analysis(results["files_analyzed"])
                    results["ai_analysis"] = ai_analysis
                
                update_progress(95, "ê²°ê³¼ ì •ë¦¬ ì¤‘...")
    
    except Exception as e:
        update_progress(0, f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        raise e
    
    update_progress(100, "ë¶„ì„ ì™„ë£Œ")
    return results

async def analyze_single_file_advanced(file_info: dict) -> dict:
    """ë‹¨ì¼ íŒŒì¼ ê³ ê¸‰ ë¶„ì„"""
    filename = file_info["filename"]
    file_path = file_info["path"]
    
    result = {
        "filename": filename,
        "content_type": file_info["content_type"],
        "size": file_info["size"],
        "analysis_type": "unknown",
        "content": "",
        "metadata": {},
        "advanced_analysis": {}
    }
    
    try:
        ext = os.path.splitext(filename)[1].lower()
        
        if ext in ['.txt', '.md']:
            # í…ìŠ¤íŠ¸ íŒŒì¼ ê³ ê¸‰ ë¶„ì„
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            result.update({
                "analysis_type": "text",
                "content": content[:1000] + "..." if len(content) > 1000 else content,
                "character_count": len(content),
                "line_count": len(content.split('\n')),
                "word_count": len(content.split())
            })
            
            # í™”ì ê°ì§€ (ê°„ë‹¨í•œ ë²„ì „)
            speakers = detect_speakers_simple(content)
            result["advanced_analysis"]["speakers_detected"] = speakers
            
        elif ext in ['.jpg', '.jpeg', '.png', '.bmp']:
            # ì´ë¯¸ì§€ ë¶„ì„ (OCR ì˜ˆì •)
            result.update({
                "analysis_type": "image",
                "content": f"ì´ë¯¸ì§€ íŒŒì¼: {filename}",
                "metadata": {"format": ext, "analysis": "OCR ì¤€ë¹„ë¨"}
            })
            
        elif ext in ['.mp3', '.wav', '.m4a']:
            # ì˜¤ë””ì˜¤ ë¶„ì„ (STT ì˜ˆì •)
            result.update({
                "analysis_type": "audio",
                "content": f"ì˜¤ë””ì˜¤ íŒŒì¼: {filename}",
                "metadata": {"format": ext, "analysis": "STT ì¤€ë¹„ë¨"}
            })
            
        elif ext in ['.mp4', '.avi', '.mov']:
            # ë¹„ë””ì˜¤ ë¶„ì„
            result.update({
                "analysis_type": "video",
                "content": f"ë¹„ë””ì˜¤ íŒŒì¼: {filename}",
                "metadata": {"format": ext, "analysis": "ë¹„ë””ì˜¤ ì²˜ë¦¬ ì¤€ë¹„ë¨"}
            })
    
    except Exception as e:
        result["error"] = str(e)
    
    return result

def detect_speakers_simple(text: str) -> dict:
    """ê°„ë‹¨í•œ í™”ì ê°ì§€"""
    speakers = {}
    lines = text.split('\n')
    
    for line in lines:
        if 'í™”ì' in line and ':' in line:
            if 'í™”ì1' in line or 'í™”ì 1' in line:
                speaker_id = 'í™”ì_1'
            elif 'í™”ì2' in line or 'í™”ì 2' in line:
                speaker_id = 'í™”ì_2'
            elif 'í™”ì3' in line or 'í™”ì 3' in line:
                speaker_id = 'í™”ì_3'
            else:
                continue
            
            if speaker_id not in speakers:
                speakers[speaker_id] = []
            
            content = line.split(':', 1)[-1].strip()
            if content:
                speakers[speaker_id].append(content)
    
    return {
        "total_speakers": len(speakers),
        "speakers": speakers
    }

async def generate_comprehensive_analysis_advanced(file_results: List[dict]) -> dict:
    """ê³ ê¸‰ ì¢…í•© ë¶„ì„"""
    
    analysis = {
        "total_files": len(file_results),
        "file_types": {},
        "content_analysis": {},
        "speaker_analysis": {},
        "metadata_summary": {}
    }
    
    total_speakers = {}
    all_content = []
    
    for result in file_results:
        # íŒŒì¼ íƒ€ì… ë¶„í¬
        analysis_type = result.get("analysis_type", "unknown")
        analysis["file_types"][analysis_type] = analysis["file_types"].get(analysis_type, 0) + 1
        
        # ì½˜í…ì¸  ìˆ˜ì§‘
        if result.get("content"):
            all_content.append(result["content"])
        
        # í™”ì ì •ë³´ í†µí•©
        if "advanced_analysis" in result and "speakers_detected" in result["advanced_analysis"]:
            speakers_info = result["advanced_analysis"]["speakers_detected"]
            for speaker_id, contents in speakers_info.get("speakers", {}).items():
                if speaker_id not in total_speakers:
                    total_speakers[speaker_id] = []
                total_speakers[speaker_id].extend(contents)
    
    # ì¢…í•© í™”ì ë¶„ì„
    analysis["speaker_analysis"] = {
        "total_unique_speakers": len(total_speakers),
        "speaker_contributions": {
            speaker_id: {
                "statement_count": len(statements),
                "total_words": sum(len(stmt.split()) for stmt in statements),
                "sample_statements": statements[:3]
            }
            for speaker_id, statements in total_speakers.items()
        }
    }
    
    # ì½˜í…ì¸  ë¶„ì„
    combined_content = " ".join(all_content)
    analysis["content_analysis"] = {
        "total_characters": len(combined_content),
        "total_words": len(combined_content.split()),
        "estimated_reading_time_minutes": len(combined_content.split()) // 200,  # í‰ê·  ì½ê¸° ì†ë„
        "content_preview": combined_content[:500] + "..." if len(combined_content) > 500 else combined_content
    }
    
    return analysis

async def generate_real_ai_analysis(file_results: List[dict], temp_dir: str) -> dict:
    """ì‹¤ì œ AI ì—”ì§„ì„ í™œìš©í•œ ë¶„ì„"""
    
    if not REAL_ENGINE_AVAILABLE:
        return {"error": "ì‹¤ì œ ë¶„ì„ ì—”ì§„ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}
    
    try:
        # ì‹¤ì œ ì—”ì§„ í˜¸ì¶œ (ì‹œë®¬ë ˆì´ì…˜)
        return {
            "engine": "Real Analysis Engine",
            "analysis_type": "advanced",
            "features_used": ["speaker_diarization", "sentiment_analysis", "topic_modeling"],
            "summary": "ì‹¤ì œ ë¶„ì„ ì—”ì§„ì„ í†µí•œ ê³ ê¸‰ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
            "confidence_score": 0.92,
            "processing_time_seconds": 15.3
        }
    except Exception as e:
        return {"error": f"ì‹¤ì œ AI ë¶„ì„ ì‹¤íŒ¨: {str(e)}"}

async def generate_ollama_ai_analysis(file_results: List[dict]) -> dict:
    """Ollama AI ë¶„ì„"""
    
    if not OLLAMA_AVAILABLE:
        return {"error": "Ollama AIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}
    
    try:
        # í…ìŠ¤íŠ¸ ë‚´ìš© í†µí•©
        combined_content = ""
        for result in file_results:
            if result.get("content") and result.get("analysis_type") == "text":
                combined_content += f"\n{result['filename']}:\n{result['content']}\n"
        
        if not combined_content.strip():
            return {"message": "ë¶„ì„í•  í…ìŠ¤íŠ¸ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤", "engine": "Ollama"}
        
        # AI ë¶„ì„ ì‹¤í–‰
        analysis_prompt = f"""ë‹¤ìŒ ì»¨í¼ëŸ°ìŠ¤/íšŒì˜ ë‚´ìš©ì„ ì „ë¬¸ì ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”:

{combined_content[:3000]}

ë‹¤ìŒ í•­ëª©ë“¤ì„ ë¶„ì„í•´ì£¼ì„¸ìš”:
1. íšŒì˜ ì£¼ì œ ë° ëª©ì 
2. ì£¼ìš” ë…¼ì˜ ì‚¬í•­
3. ì°¸ì„ì ì—­í•  ë¶„ì„
4. í•µì‹¬ ê²°ì • ì‚¬í•­
5. í›„ì† ì¡°ì¹˜ ì‚¬í•­
6. ì „ì²´ì ì¸ íšŒì˜ íš¨ê³¼ì„± í‰ê°€

ë¶„ì„ ê²°ê³¼ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”."""

        ai_response = ollama.generate_response(
            prompt=analysis_prompt,
            model="qwen2.5:7b"
        )
        
        return {
            "engine": "Ollama AI",
            "model_used": "qwen2.5:7b",
            "analysis_summary": ai_response,
            "analysis_timestamp": datetime.now().isoformat(),
            "content_length_analyzed": len(combined_content)
        }
        
    except Exception as e:
        return {"error": f"Ollama AI ë¶„ì„ ì‹¤íŒ¨: {str(e)}", "engine": "Ollama"}

async def process_files_background(files: List[UploadFile], session_id: str):
    """ë°±ê·¸ë¼ìš´ë“œ íŒŒì¼ ì²˜ë¦¬"""
    try:
        # íŒŒì¼ë“¤ì„ ë©”ëª¨ë¦¬ì— ì €ì¥ (UploadFileì€ ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ì—ì„œë§Œ ì‚¬ìš© ê°€ëŠ¥)
        file_data = []
        for file in files:
            content = await file.read()
            file_data.append({
                "filename": file.filename,
                "content": content,
                "content_type": file.content_type
            })
        
        # ì‹¤ì œ ì²˜ë¦¬ëŠ” ë™ê¸° í•¨ìˆ˜ë¡œ
        result = await asyncio.get_event_loop().run_in_executor(
            executor, 
            process_files_sync, 
            file_data, 
            session_id
        )
        
        # ê²°ê³¼ ì €ì¥
        analysis_progress[session_id].update({
            "status": "completed",
            "progress": 100,
            "message": "ë¶„ì„ ì™„ë£Œ",
            "end_time": datetime.now().isoformat(),
            "result": result
        })
        
    except Exception as e:
        analysis_progress[session_id].update({
            "status": "error",
            "progress": 0,
            "message": f"ë¶„ì„ ì‹¤íŒ¨: {str(e)}",
            "end_time": datetime.now().isoformat()
        })

def process_files_sync(file_data: List[dict], session_id: str) -> dict:
    """ë™ê¸°ì‹ íŒŒì¼ ì²˜ë¦¬ (executorì—ì„œ ì‹¤í–‰)"""
    
    # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ í•¨ìˆ˜
    def update_progress(progress: int, message: str):
        if session_id in analysis_progress:
            analysis_progress[session_id].update({
                "progress": progress,
                "message": message
            })
    
    # ì—¬ê¸°ì„œ ì‹¤ì œ ì²˜ë¦¬ ë¡œì§ êµ¬í˜„
    update_progress(50, "ë™ê¸° ì²˜ë¦¬ ì¤‘...")
    
    # ê°„ë‹¨í•œ ê²°ê³¼ ë°˜í™˜
    return {
        "analysis_id": f"sync_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
        "files_processed": len(file_data),
        "message": "ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ ì™„ë£Œ"
    }

if __name__ == "__main__":
    import uvicorn
    print("Module1 Advanced API ì„œë¹„ìŠ¤ ì‹œì‘...")
    print("ì„œë¹„ìŠ¤: http://localhost:8001")
    print("API ë¬¸ì„œ: http://localhost:8001/docs")
    print("ë¹„ë™ê¸° ì²˜ë¦¬ + ì‹¤ì‹œê°„ ì§„í–‰ë¥  ì§€ì›")
    
    uvicorn.run(app, host="0.0.0.0", port=8001, reload=True)