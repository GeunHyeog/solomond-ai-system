#!/usr/bin/env python3
"""
2025ë…„ ìµœì‹  ê³ ì„±ëŠ¥ ëª¨ë¸ ì„¤ì¹˜
"""

import subprocess
import os
import time
import requests

def find_ollama():
    """Ollama ì‹¤í–‰íŒŒì¼ ì°¾ê¸°"""
    
    paths = [
        r"C:\Users\{}\AppData\Local\Programs\Ollama\ollama.exe".format(os.getenv("USERNAME")),
        r"C:\Program Files\Ollama\ollama.exe",
        r"C:\Program Files (x86)\Ollama\ollama.exe"
    ]
    
    for path in paths:
        if os.path.exists(path):
            return path
    
    return "ollama"  # PATHì— ìˆë‹¤ê³  ê°€ì •

def check_available_models():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ìµœì‹  ëª¨ë¸ í™•ì¸"""
    
    print("=== 2025ë…„ ìµœì‹  ëª¨ë¸ í™•ì¸ ===")
    
    # ì¶”ì²œ ëª¨ë¸ë“¤ (ì„±ëŠ¥ ìˆœ)
    latest_models = [
        ("qwen2.5:14b", "Qwen2.5 14B - í•œêµ­ì–´ íŠ¹í™” ìµœì‹  ëª¨ë¸ (ê°•ë ¥ ì¶”ì²œ!)"),
        ("qwen2.5:7b", "Qwen2.5 7B - í•œêµ­ì–´ íŠ¹í™” ê²½ëŸ‰ ë²„ì „"),
        ("llama3.3:8b", "Llama 3.3 8B - Meta ìµœì‹  ëª¨ë¸"),
        ("llama3.2:8b", "Llama 3.2 8B - Meta ì•ˆì • ë²„ì „"),  
        ("mistral-nemo:12b", "Mistral-Nemo 12B - íš¨ìœ¨ì„± ìµœê°•"),
        ("phi3.5:3.8b", "Phi 3.5 - Microsoft ê²½ëŸ‰ ê³ ì„±ëŠ¥")
    ]
    
    print("ì¶”ì²œ ìµœì‹  ëª¨ë¸:")
    for i, (model_id, description) in enumerate(latest_models, 1):
        print(f"  {i}. {description}")
    
    return latest_models

def install_recommended_models(ollama_path):
    """ì¶”ì²œ ëª¨ë¸ ì„¤ì¹˜"""
    
    print("\n=== ìµœì‹  ëª¨ë¸ ìë™ ì„¤ì¹˜ ===")
    
    # 1ìˆœìœ„ ëª¨ë¸ë“¤ (í•œêµ­ì–´ ì£¼ì–¼ë¦¬ì— ìµœì )
    priority_models = [
        ("qwen2.5:7b", "Qwen2.5 7B - í•œêµ­ì–´ ìµœì í™” (1ìˆœìœ„)"),
        ("llama3.2:8b", "Llama 3.2 8B - ë²”ìš© ê³ ì„±ëŠ¥ (2ìˆœìœ„)")
    ]
    
    installed = []
    
    for model_id, description in priority_models:
        print(f"\n{description} ì„¤ì¹˜ ì¤‘...")
        print(f"ëª¨ë¸: {model_id}")
        print("ë‹¤ìš´ë¡œë“œ ì§„í–‰ ì¤‘... (5-10ë¶„ ì†Œìš”)")
        
        try:
            # ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© í‘œì‹œ
            process = subprocess.Popen(
                [ollama_path, "pull", model_id],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                universal_newlines=True
            )
            
            # ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§
            dots = 0
            while process.poll() is None:
                print("." * (dots % 10 + 1), end="\r")
                dots += 1
                time.sleep(1)
            
            if process.returncode == 0:
                print(f"\nâœ“ {model_id} ì„¤ì¹˜ ì™„ë£Œ!")
                installed.append(model_id)
            else:
                error = process.stderr.read()
                print(f"\nâœ— {model_id} ì„¤ì¹˜ ì‹¤íŒ¨: {error}")
                
        except Exception as e:
            print(f"\nâœ— ì„¤ì¹˜ ì˜¤ë¥˜: {str(e)}")
    
    return installed

def performance_benchmark(models):
    """ìµœì‹  ëª¨ë¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
    
    print(f"\n=== ìµœì‹  ëª¨ë¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ===")
    
    # í•œêµ­ì–´ ì£¼ì–¼ë¦¬ ì „ë¬¸ í…ŒìŠ¤íŠ¸
    test_prompt = """í•œêµ­ì–´ë¡œ ì „ë¬¸ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:

ê³ ê° ìƒí™©: "25ì‚´ ì§ì¥ì¸ì´ ê²°í˜¼ ì˜ˆì •ì´ì—ìš”. ì•½í˜¼ë°˜ì§€ì™€ ê²°í˜¼ë°˜ì§€ ì„¸íŠ¸ë¡œ 300ë§Œì› ì˜ˆì‚°ì¸ë°, ì–´ë–¤ ê±¸ ì¶”ì²œí•˜ì‹œë‚˜ìš”? ë‹¤ì´ì•„ëª¬ë“œ í¬ê¸°ë³´ë‹¤ëŠ” ë””ìì¸ì´ ì˜ˆìœ ê²Œ ì¢‹ê² ì–´ìš”."

ì „ë¬¸ ìƒë‹´ì‚¬ë¡œì„œ ì´ ê³ ê°ì—ê²Œ ì–´ë–¤ ì¡°ì–¸ê³¼ ì œí’ˆì„ ì¶”ì²œí•˜ì‹œê² ìŠµë‹ˆê¹Œ?"""

    results = []
    
    for model in models:
        print(f"\n--- {model} ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ---")
        
        payload = {
            "model": model,
            "prompt": test_prompt,
            "stream": False
        }
        
        try:
            start_time = time.time()
            
            response = requests.post("http://localhost:11434/api/generate",
                                   json=payload, timeout=60)
            
            if response.status_code == 200:
                end_time = time.time()
                result = response.json()
                answer = result.get('response', '')
                
                processing_time = end_time - start_time
                
                # í’ˆì§ˆ ë¶„ì„
                keywords = ['ì•½í˜¼ë°˜ì§€', 'ê²°í˜¼ë°˜ì§€', '300ë§Œì›', 'ì˜ˆì‚°', 'ë””ìì¸', 'ì¶”ì²œ', 'ë‹¤ì´ì•„ëª¬ë“œ', '25ì‚´']
                found_keywords = [k for k in keywords if k in answer]
                
                quality_score = len(found_keywords) / len(keywords) * 100
                
                print(f"  ì²˜ë¦¬ì‹œê°„: {processing_time:.2f}ì´ˆ")
                print(f"  ì‘ë‹µê¸¸ì´: {len(answer)} ê¸€ì")
                print(f"  í‚¤ì›Œë“œ ì¸ì‹: {len(found_keywords)}/{len(keywords)}ê°œ")
                print(f"  í’ˆì§ˆì ìˆ˜: {quality_score:.1f}%")
                
                # ì‘ë‹µ í’ˆì§ˆ í‰ê°€
                if len(answer) > 200 and quality_score > 50:
                    print(f"  í‰ê°€: ìš°ìˆ˜ âœ“")
                elif len(answer) > 100 and quality_score > 30:
                    print(f"  í‰ê°€: ì–‘í˜¸ â–³")
                else:
                    print(f"  í‰ê°€: ê°œì„ í•„ìš” âœ—")
                
                results.append({
                    'model': model,
                    'time': processing_time,
                    'quality': quality_score,
                    'length': len(answer),
                    'keywords': len(found_keywords)
                })
                
                print(f"  ì‘ë‹µ ìƒ˜í”Œ: {answer[:100]}...")
                
            else:
                print(f"  API ì˜¤ë¥˜: {response.status_code}")
                
        except Exception as e:
            print(f"  í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
    
    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì„ ì •
    if results:
        best_model = max(results, 
                        key=lambda x: x['quality'] * 0.4 + (100/max(x['time'], 1)) * 0.3 + min(x['length']/10, 10) * 0.3)
        
        print(f"\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model['model']}")
        print(f"   í’ˆì§ˆì ìˆ˜: {best_model['quality']:.1f}%")
        print(f"   ì²˜ë¦¬ì†ë„: {best_model['time']:.2f}ì´ˆ")
        print(f"   ì‘ë‹µí’ˆì§ˆ: {best_model['length']} ê¸€ì")
    
    return results

def integrate_to_solomond():
    """ì†”ë¡œëª¬ë“œ AI ì‹œìŠ¤í…œì— í†µí•©"""
    
    print(f"\n=== ì†”ë¡œëª¬ë“œ AI í†µí•© ì¤€ë¹„ ===")
    print(f"1. core/ollama_integration_engine.py ì—…ë°ì´íŠ¸")
    print(f"2. ìµœì‹  ëª¨ë¸ì„ ê¸°ë³¸ ëª¨ë¸ë¡œ ì„¤ì •")
    print(f"3. í•œêµ­ì–´ í”„ë¡¬í”„íŠ¸ ìµœì í™”")
    print(f"4. demo_integrated_system.pyì—ì„œ í…ŒìŠ¤íŠ¸")

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    
    print("ğŸš€ 2025ë…„ ìµœì‹  AI ëª¨ë¸ ì„¤ì¹˜")
    print("="*40)
    
    # 1. Ollama ì°¾ê¸°
    ollama_path = find_ollama()
    
    # 2. ì‚¬ìš© ê°€ëŠ¥í•œ ìµœì‹  ëª¨ë¸ í™•ì¸
    latest_models = check_available_models()
    
    print(f"\nì¶”ì²œ: Qwen2.5ê°€ í•œêµ­ì–´ ì£¼ì–¼ë¦¬ ìƒë‹´ì— ìµœì ì…ë‹ˆë‹¤!")
    
    # 3. ìµœì‹  ëª¨ë¸ ì„¤ì¹˜
    installed = install_recommended_models(ollama_path)
    
    if not installed:
        print(f"\nì„¤ì¹˜ ì‹¤íŒ¨. ìˆ˜ë™ ì„¤ì¹˜ í•´ë³´ì„¸ìš”:")
        print(f"  ollama pull qwen2.5:7b")
        print(f"  ollama pull llama3.2:8b")
        return
    
    print(f"\nâœ“ ì„¤ì¹˜ëœ ìµœì‹  ëª¨ë¸: {installed}")
    
    # 4. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
    results = performance_benchmark(installed)
    
    # 5. ì‹œìŠ¤í…œ í†µí•© ì•ˆë‚´
    integrate_to_solomond()
    
    print(f"\nğŸ‰ ìµœì‹  ëª¨ë¸ ì„¤ì¹˜ ì™„ë£Œ!")
    print(f"ë‹¤ìŒ ë‹¨ê³„:")
    print(f"1. python check_gemma3_status.py - ì„¤ì¹˜ í™•ì¸")
    print(f"2. ì†”ë¡œëª¬ë“œ AI ë©”ì¸ ì‹œìŠ¤í…œì—ì„œ í…ŒìŠ¤íŠ¸")
    print(f"3. ê¸°ì¡´ eeve-korean ëª¨ë¸ê³¼ ì„±ëŠ¥ ë¹„êµ")

if __name__ == "__main__":
    main()