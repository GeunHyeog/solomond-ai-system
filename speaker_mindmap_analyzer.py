#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í™”ìë³„ ëŒ€í™” ë¶„ì„ ë° ë§ˆì¸ë“œë§µ ìƒì„± ì‹œìŠ¤í…œ
Enhanced Speaker Identification í†µí•© ë²„ì „
"""
import os
import sys
import json
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any

# ìµœì í™” ì„¤ì •
os.environ['CUDA_VISIBLE_DEVICES'] = ''
os.environ['OMP_NUM_THREADS'] = '4'

# í–¥ìƒëœ í™”ì êµ¬ë¶„ ì‹œìŠ¤í…œ import
try:
    from enhanced_speaker_identifier import EnhancedSpeakerIdentifier
    ENHANCED_SPEAKER_ID_AVAILABLE = True
    print("[SUCCESS] Enhanced Speaker Identifier ë¡œë“œ ì™„ë£Œ")
except ImportError as e:
    ENHANCED_SPEAKER_ID_AVAILABLE = False
    print(f"[WARNING] Enhanced Speaker Identifier ë¡œë“œ ì‹¤íŒ¨: {e}")

class SpeakerMindmapAnalyzer:
    """í™”ìë³„ ë¶„ì„ ë° ë§ˆì¸ë“œë§µ ìƒì„±ê¸°"""
    
    def __init__(self, expected_speakers=3):
        self.speakers_data = {}
        self.conversation_flow = []
        self.topics_mapping = {}
        self.mindmap_data = {}
        self.expected_speakers = expected_speakers
        
        # í–¥ìƒëœ í™”ì êµ¬ë¶„ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        if ENHANCED_SPEAKER_ID_AVAILABLE:
            self.enhanced_speaker_id = EnhancedSpeakerIdentifier(expected_speakers)
            print(f"[INFO] í–¥ìƒëœ í™”ì êµ¬ë¶„ ì‹œìŠ¤í…œ í™œì„±í™” (ì˜ˆìƒ í™”ì: {expected_speakers}ëª…)")
        else:
            self.enhanced_speaker_id = None
            print("[INFO] ê¸°ë³¸ í™”ì êµ¬ë¶„ ì‹œìŠ¤í…œ ì‚¬ìš©")
        
    def analyze_audio_files(self, audio_files_path="user_files/audio"):
        """ì˜¤ë””ì˜¤ íŒŒì¼ë“¤ ë¶„ì„"""
        print("=== í™”ìë³„ ëŒ€í™” ë¶„ì„ ì‹œì‘ ===")
        
        audio_path = Path(audio_files_path)
        if not audio_path.exists():
            print(f"ì˜¤ë””ì˜¤ í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {audio_files_path}")
            return
        
        # ëª¨ë¸ ë¡œë”©
        try:
            import whisper
            model = whisper.load_model("tiny", device="cpu")
            print("Whisper ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"Whisper ë¡œë”© ì‹¤íŒ¨: {e}")
            return
        
        # ì˜¤ë””ì˜¤ íŒŒì¼ë“¤ ì²˜ë¦¬
        audio_files = list(audio_path.glob("*.m4a")) + list(audio_path.glob("*.wav"))
        
        for i, audio_file in enumerate(audio_files):
            print(f"\\n[{i+1}/{len(audio_files)}] {audio_file.name} ë¶„ì„ ì¤‘...")
            
            try:
                # STT ì²˜ë¦¬
                result = model.transcribe(str(audio_file))
                
                # ì„¸ê·¸ë¨¼íŠ¸ë³„ ë¶„ì„
                if 'segments' in result:
                    self._analyze_segments(audio_file.name, result['segments'])
                else:
                    # ì „ì²´ í…ìŠ¤íŠ¸ ë¶„ì„
                    self._analyze_full_text(audio_file.name, result.get('text', ''))
                
                print(f"  ì™„ë£Œ: {len(result.get('segments', []))}ê°œ ì„¸ê·¸ë¨¼íŠ¸")
                
            except Exception as e:
                print(f"  ì˜¤ë¥˜: {e}")
        
        # í™”ìë³„ ì£¼ì œ ë¶„ì„
        self._extract_topics_by_speaker()
        
        # ë§ˆì¸ë“œë§µ ë°ì´í„° ìƒì„±
        self._generate_mindmap_data()
        
        del model
    
    def _analyze_single_file(self, file_path):
        """ë‹¨ì¼ íŒŒì¼ ë¶„ì„ (Streamlitì—ì„œ ì‚¬ìš©)"""
        try:
            import whisper
            model = whisper.load_model("tiny", device="cpu")
            
            result = model.transcribe(file_path)
            
            filename = Path(file_path).name
            
            # ì„¸ê·¸ë¨¼íŠ¸ë³„ ë¶„ì„
            if 'segments' in result:
                self._analyze_segments(filename, result['segments'])
            else:
                # ì „ì²´ í…ìŠ¤íŠ¸ ë¶„ì„
                self._analyze_full_text(filename, result.get('text', ''))
            
            del model
            return True
            
        except Exception as e:
            print(f"íŒŒì¼ ë¶„ì„ ì˜¤ë¥˜ {file_path}: {e}")
            return False
        
    def _analyze_segments(self, filename, segments):
        """ì„¸ê·¸ë¨¼íŠ¸ë³„ í™”ì ë° ë‚´ìš© ë¶„ì„ (í–¥ìƒëœ ì‹œìŠ¤í…œ ì ìš©)"""
        
        # ì˜ë¯¸ìˆëŠ” ì„¸ê·¸ë¨¼íŠ¸ë§Œ í•„í„°ë§
        meaningful_segments = []
        for segment in segments:
            text = segment.get('text', '').strip()
            if len(text) > 10:  # ì˜ë¯¸ìˆëŠ” ê¸¸ì´ì˜ í…ìŠ¤íŠ¸ë§Œ
                meaningful_segments.append(segment)
        
        if not meaningful_segments:
            return
        
        # í–¥ìƒëœ í™”ì êµ¬ë¶„ ì‹œìŠ¤í…œ ì‚¬ìš©
        if self.enhanced_speaker_id:
            print(f"  [INFO] í–¥ìƒëœ í™”ì êµ¬ë¶„ ì ìš© (ì„¸ê·¸ë¨¼íŠ¸ {len(meaningful_segments)}ê°œ)")
            speaker_segments = self.enhanced_speaker_id.identify_speakers_from_segments(meaningful_segments)
        else:
            # ê¸°ë³¸ ì‹œìŠ¤í…œ ì‚¬ìš©
            print(f"  [INFO] ê¸°ë³¸ í™”ì êµ¬ë¶„ ì ìš© (ì„¸ê·¸ë¨¼íŠ¸ {len(meaningful_segments)}ê°œ)")
            speaker_segments = []
            for segment in meaningful_segments:
                segment_copy = segment.copy()
                segment_copy['speaker'] = self._identify_speaker_fallback(segment.get('text', ''), segment.get('start', 0))
                speaker_segments.append(segment_copy)
        
        # ê²°ê³¼ ì²˜ë¦¬
        for segment in speaker_segments:
            speaker = segment.get('speaker', 'í™”ì_1')
            text = segment.get('text', '').strip()
            start_time = segment.get('start', 0)
            end_time = segment.get('end', start_time + 1)
            
            # í™”ìë³„ ë°ì´í„° ì €ì¥
            if speaker not in self.speakers_data:
                self.speakers_data[speaker] = {
                    'texts': [],
                    'topics': [],
                    'speaking_time': 0,
                    'files': set()
                }
            
            self.speakers_data[speaker]['texts'].append({
                'text': text,
                'timestamp': start_time,
                'duration': end_time - start_time,
                'file': filename
            })
            self.speakers_data[speaker]['files'].add(filename)
            self.speakers_data[speaker]['speaking_time'] += (end_time - start_time)
            
            # ëŒ€í™” íë¦„ì— ì¶”ê°€
            self.conversation_flow.append({
                'speaker': speaker,
                'text': text,
                'timestamp': start_time,
                'duration': end_time - start_time,
                'file': filename
            })
        
        # í™”ì êµ¬ë¶„ í’ˆì§ˆ í‰ê°€
        self._evaluate_speaker_quality(speaker_segments)
    
    def _analyze_full_text(self, filename, full_text):
        """ì „ì²´ í…ìŠ¤íŠ¸ ë¶„ì„ (ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš°)"""
        if len(full_text.strip()) > 20:
            # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
            sentences = [s.strip() for s in full_text.split('.') if len(s.strip()) > 10]
            
            for i, sentence in enumerate(sentences):
                speaker = f"í™”ì_{(i % 3) + 1}"  # 3ëª…ì˜ í™”ìë¡œ ê°€ì •
                
                if speaker not in self.speakers_data:
                    self.speakers_data[speaker] = {
                        'texts': [],
                        'topics': [],
                        'speaking_time': 0,
                        'files': set()
                    }
                
                self.speakers_data[speaker]['texts'].append({
                    'text': sentence,
                    'timestamp': i * 10,  # ê°€ìƒ íƒ€ì„ìŠ¤íƒ¬í”„
                    'file': filename
                })
                self.speakers_data[speaker]['files'].add(filename)
    
    def _identify_speaker_fallback(self, text, timestamp):
        """í™”ì ì‹ë³„ (ê¸°ë³¸ ì‹œìŠ¤í…œ - ì‹œê°„ëŒ€ë³„ ê·œì¹™ ê¸°ë°˜)"""
        # ì‹œê°„ëŒ€ë³„ í™”ì ì¶”ì • (ê¸°ë³¸ ë°©ì‹)
        if timestamp < 30:
            return "í™”ì_1"
        elif timestamp < 60:
            return "í™”ì_2"
        else:
            return "í™”ì_3"
    
    def _evaluate_speaker_quality(self, speaker_segments):
        """í™”ì êµ¬ë¶„ í’ˆì§ˆ í‰ê°€"""
        if not speaker_segments:
            return
        
        # í™”ìë³„ ë°œì–¸ ìˆ˜ ê³„ì‚°
        speaker_counts = {}
        total_duration = 0
        
        for segment in speaker_segments:
            speaker = segment.get('speaker', 'í™”ì_1')
            duration = segment.get('end', 0) - segment.get('start', 0)
            
            if speaker not in speaker_counts:
                speaker_counts[speaker] = {'count': 0, 'duration': 0}
            
            speaker_counts[speaker]['count'] += 1
            speaker_counts[speaker]['duration'] += duration
            total_duration += duration
        
        print(f"  [QUALITY] ê°ì§€ëœ í™”ì ìˆ˜: {len(speaker_counts)}ëª…")
        
        for speaker, data in speaker_counts.items():
            percentage = (data['duration'] / total_duration * 100) if total_duration > 0 else 0
            print(f"    {speaker}: {data['count']}íšŒ ë°œì–¸, {data['duration']:.1f}ì´ˆ ({percentage:.1f}%)")
        
        # í™”ì ë¶„í¬ ê· í˜• í™•ì¸
        durations = [data['duration'] for data in speaker_counts.values()]
        if len(durations) > 1:
            import numpy as np
            balance_std = np.std(durations) / np.mean(durations) if np.mean(durations) > 0 else 0
            if balance_std < 0.5:
                print(f"  [QUALITY] í™”ì ë¶„í¬: ê· í˜•ì  (í¸ì°¨ê³„ìˆ˜: {balance_std:.2f})")
            else:
                print(f"  [QUALITY] í™”ì ë¶„í¬: ë¶ˆê· í˜• (í¸ì°¨ê³„ìˆ˜: {balance_std:.2f})")
    
    def get_speaker_statistics(self):
        """í™”ìë³„ ìƒì„¸ í†µê³„ ë°˜í™˜"""
        stats = {}
        
        for speaker, data in self.speakers_data.items():
            if self.enhanced_speaker_id:
                # í–¥ìƒëœ ì‹œìŠ¤í…œì—ì„œ í™”ì íŠ¹ì„± ë¶„ì„
                combined_text = " ".join([item['text'] for item in data['texts']])
                features = self.enhanced_speaker_id.extract_text_features(combined_text)
                
                stats[speaker] = {
                    'utterance_count': len(data['texts']),
                    'total_speaking_time': data['speaking_time'],
                    'avg_utterance_duration': data['speaking_time'] / len(data['texts']) if data['texts'] else 0,
                    'text_features': features,
                    'speech_style': self.enhanced_speaker_id._classify_speech_style(features),
                    'dominant_patterns': self.enhanced_speaker_id._identify_dominant_patterns(combined_text)
                }
            else:
                # ê¸°ë³¸ í†µê³„
                stats[speaker] = {
                    'utterance_count': len(data['texts']),
                    'total_speaking_time': data['speaking_time'],
                    'avg_utterance_duration': data['speaking_time'] / len(data['texts']) if data['texts'] else 0
                }
        
        return stats
    
    def _extract_topics_by_speaker(self):
        """í™”ìë³„ ì£¼ì œ ì¶”ì¶œ"""
        print("\\ní™”ìë³„ ì£¼ì œ ë¶„ì„ ì¤‘...")
        
        # ì£¼ì œ í‚¤ì›Œë“œ ì‚¬ì „
        topic_keywords = {
            'ë¹„ì¦ˆë‹ˆìŠ¤': ['ì‚¬ì—…', 'ë¹„ì¦ˆë‹ˆìŠ¤', 'ë§¤ì¶œ', 'ìˆ˜ìµ', 'ê³ ê°', 'ë§ˆì¼€íŒ…', 'ë¸Œëœë“œ'],
            'ê¸°ìˆ ': ['ê¸°ìˆ ', 'ì‹œìŠ¤í…œ', 'í”Œë«í¼', 'ê°œë°œ', 'êµ¬í˜„', 'ì†”ë£¨ì…˜'],
            'ì „ëµ': ['ì „ëµ', 'ê³„íš', 'ëª©í‘œ', 'ë°©í–¥', 'ë¹„ì „', 'ë¯¸ì…˜'],
            'ìš´ì˜': ['ìš´ì˜', 'ê´€ë¦¬', 'í”„ë¡œì„¸ìŠ¤', 'ì—…ë¬´', 'íš¨ìœ¨', 'ìµœì í™”'],
            'ì‹œì¥': ['ì‹œì¥', 'ê²½ìŸ', 'íŠ¸ë Œë“œ', 'ë™í–¥', 'ë¶„ì„', 'ì˜ˆì¸¡']
        }
        
        for speaker, data in self.speakers_data.items():
            speaker_topics = {}
            all_text = ' '.join([item['text'] for item in data['texts']])
            
            # ì£¼ì œë³„ ì–¸ê¸‰ ë¹ˆë„ ê³„ì‚°
            for topic, keywords in topic_keywords.items():
                count = sum(all_text.lower().count(keyword) for keyword in keywords)
                if count > 0:
                    speaker_topics[topic] = count
            
            # ìƒìœ„ ì£¼ì œë“¤ ì €ì¥
            sorted_topics = sorted(speaker_topics.items(), key=lambda x: x[1], reverse=True)
            data['topics'] = sorted_topics[:3]  # ìƒìœ„ 3ê°œ ì£¼ì œ
            
            print(f"  {speaker}: {[t[0] for t in sorted_topics[:3]]}")
    
    def _generate_mindmap_data(self):
        """ë§ˆì¸ë“œë§µ ë°ì´í„° ìƒì„±"""
        print("\\në§ˆì¸ë“œë§µ ë°ì´í„° ìƒì„± ì¤‘...")
        
        # ì¤‘ì‹¬ ì£¼ì œ
        self.mindmap_data = {
            'name': 'ëŒ€í™” ì¢…í•© ë¶„ì„',
            'children': []
        }
        
        # í™”ìë³„ ë…¸ë“œ ìƒì„±
        for speaker, data in self.speakers_data.items():
            speaker_node = {
                'name': speaker,
                'size': len(data['texts']) * 10,  # ë°œì–¸ëŸ‰ì— ë¹„ë¡€í•œ í¬ê¸°
                'children': []
            }
            
            # ì£¼ì œë³„ í•˜ìœ„ ë…¸ë“œ
            for topic, count in data['topics']:
                topic_node = {
                    'name': f"{topic} ({count})",
                    'size': count * 5,
                    'children': []
                }
                
                # ì£¼ìš” ë°œì–¸ë“¤
                topic_texts = [
                    item['text'][:50] + "..." if len(item['text']) > 50 else item['text']
                    for item in data['texts'] 
                    if any(keyword in item['text'].lower() for keyword in ['ì‚¬ì—…', 'ê¸°ìˆ ', 'ì „ëµ'] if topic in ['ë¹„ì¦ˆë‹ˆìŠ¤', 'ê¸°ìˆ ', 'ì „ëµ'])
                ][:3]  # ìµœëŒ€ 3ê°œ
                
                for text in topic_texts:
                    topic_node['children'].append({
                        'name': text,
                        'size': 5
                    })
                
                speaker_node['children'].append(topic_node)
            
            self.mindmap_data['children'].append(speaker_node)
    
    def generate_conversation_summary(self):
        """ëŒ€í™” ìš”ì•½ ìƒì„±"""
        print("\\nëŒ€í™” ìš”ì•½ ìƒì„± ì¤‘...")
        
        summary = {
            'overview': {},
            'speaker_analysis': {},
            'topic_flow': [],
            'key_insights': []
        }
        
        # ì „ì²´ ê°œìš”
        summary['overview'] = {
            'total_speakers': len(self.speakers_data),
            'total_utterances': sum(len(data['texts']) for data in self.speakers_data.values()),
            'main_topics': list(set(topic for data in self.speakers_data.values() for topic, _ in data['topics']))
        }
        
        # í™”ìë³„ ë¶„ì„
        for speaker, data in self.speakers_data.items():
            summary['speaker_analysis'][speaker] = {
                'utterance_count': len(data['texts']),
                'main_topics': [topic for topic, _ in data['topics']],
                'key_points': [
                    item['text'][:100] + "..." if len(item['text']) > 100 else item['text']
                    for item in data['texts'][:3]  # ì£¼ìš” ë°œì–¸ 3ê°œ
                ]
            }
        
        # ì£¼ì œ íë¦„ ë¶„ì„
        timeline_topics = []
        for item in sorted(self.conversation_flow, key=lambda x: x['timestamp']):
            # ì£¼ì œ ì‹ë³„
            text_lower = item['text'].lower()
            identified_topic = "ê¸°íƒ€"
            for topic in ['ë¹„ì¦ˆë‹ˆìŠ¤', 'ê¸°ìˆ ', 'ì „ëµ', 'ìš´ì˜', 'ì‹œì¥']:
                if any(keyword in text_lower for keyword in ['ì‚¬ì—…', 'ê¸°ìˆ ', 'ì „ëµ', 'ìš´ì˜', 'ì‹œì¥']):
                    identified_topic = topic
                    break
            
            timeline_topics.append({
                'timestamp': item['timestamp'],
                'speaker': item['speaker'],
                'topic': identified_topic,
                'content': item['text'][:100]
            })
        
        summary['topic_flow'] = timeline_topics[:10]  # ìƒìœ„ 10ê°œ
        
        # ì£¼ìš” ì¸ì‚¬ì´íŠ¸
        insights = []
        
        # í™”ìë³„ íŠ¹ì§•
        for speaker, data in self.speakers_data.items():
            if data['topics']:
                main_topic = data['topics'][0][0]
                insights.append(f"{speaker}ëŠ” ì£¼ë¡œ {main_topic}ì— ëŒ€í•´ ë°œì–¸í•¨")
        
        # ëŒ€í™” íŒ¨í„´
        if len(self.speakers_data) > 1:
            insights.append(f"{len(self.speakers_data)}ëª…ì˜ í™”ìê°€ ì°¸ì—¬í•œ ë‹¤ìê°„ ëŒ€í™”")
        
        summary['key_insights'] = insights
        
        return summary
    
    def save_results(self):
        """ê²°ê³¼ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ë§ˆì¸ë“œë§µ ë°ì´í„° ì €ì¥
        mindmap_filename = f"mindmap_data_{timestamp}.json"
        with open(mindmap_filename, 'w', encoding='utf-8') as f:
            json.dump(self.mindmap_data, f, ensure_ascii=False, indent=2)
        
        # í™”ì ë¶„ì„ ë°ì´í„° ì €ì¥
        speakers_filename = f"speakers_analysis_{timestamp}.json"
        
        # í™”ì ë°ì´í„°ì—ì„œ setì„ listë¡œ ë³€í™˜
        speakers_data_serializable = {}
        for speaker, data in self.speakers_data.items():
            speakers_data_serializable[speaker] = {
                'texts': data['texts'],
                'topics': data['topics'],
                'speaking_time': data['speaking_time'],
                'files': list(data['files'])  # setì„ listë¡œ ë³€í™˜
            }
        
        with open(speakers_filename, 'w', encoding='utf-8') as f:
            json.dump(speakers_data_serializable, f, ensure_ascii=False, indent=2)
        
        # ëŒ€í™” ìš”ì•½ ì €ì¥
        summary = self.generate_conversation_summary()
        summary_filename = f"conversation_summary_{timestamp}.json"
        with open(summary_filename, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        print(f"\\nê²°ê³¼ ì €ì¥ ì™„ë£Œ:")
        print(f"  ë§ˆì¸ë“œë§µ: {mindmap_filename}")
        print(f"  í™”ì ë¶„ì„: {speakers_filename}")
        print(f"  ëŒ€í™” ìš”ì•½: {summary_filename}")
        
        return {
            'mindmap_file': mindmap_filename,
            'speakers_file': speakers_filename,
            'summary_file': summary_filename
        }
    
    def print_analysis_results(self):
        """ë¶„ì„ ê²°ê³¼ ì¶œë ¥"""
        print("\\n" + "="*60)
        print("í™”ìë³„ ëŒ€í™” ë¶„ì„ ê²°ê³¼")
        print("="*60)
        
        for speaker, data in self.speakers_data.items():
            print(f"\\nğŸ‘¤ {speaker}")
            print(f"   ë°œì–¸ ìˆ˜: {len(data['texts'])}íšŒ")
            print(f"   ì°¸ì—¬ íŒŒì¼: {', '.join(data['files'])}")
            print(f"   ì£¼ìš” ì£¼ì œ: {', '.join([t[0] for t in data['topics']])}")
            
            # ì£¼ìš” ë°œì–¸ ì˜ˆì‹œ
            if data['texts']:
                print(f"   ì£¼ìš” ë°œì–¸:")
                for i, text_item in enumerate(data['texts'][:2]):
                    print(f"     {i+1}. {text_item['text'][:80]}...")
        
        print("\\n" + "="*60)
        
        # ëŒ€í™” ìš”ì•½ ì¶œë ¥
        summary = self.generate_conversation_summary()
        
        print("\\nğŸ“Š ëŒ€í™” ê°œìš”:")
        print(f"   ì´ í™”ì ìˆ˜: {summary['overview']['total_speakers']}ëª…")
        print(f"   ì´ ë°œì–¸ ìˆ˜: {summary['overview']['total_utterances']}íšŒ")
        print(f"   ì£¼ìš” ì£¼ì œ: {', '.join(summary['overview']['main_topics'])}")
        
        print("\\nğŸ’¡ ì£¼ìš” ì¸ì‚¬ì´íŠ¸:")
        for insight in summary['key_insights']:
            print(f"   â€¢ {insight}")

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    analyzer = SpeakerMindmapAnalyzer()
    
    # ì˜¤ë””ì˜¤ íŒŒì¼ ë¶„ì„
    analyzer.analyze_audio_files()
    
    # ê²°ê³¼ ì¶œë ¥
    analyzer.print_analysis_results()
    
    # ê²°ê³¼ ì €ì¥
    files = analyzer.save_results()
    
    print(f"\\nâœ… í™”ìë³„ ë¶„ì„ ë° ë§ˆì¸ë“œë§µ ìƒì„± ì™„ë£Œ!")
    print(f"   ìƒì„±ëœ íŒŒì¼: {len(files)}ê°œ")

if __name__ == "__main__":
    main()