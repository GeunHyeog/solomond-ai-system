#!/usr/bin/env python3
"""
Quality Validation System - ì‹¤ì‹œê°„ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§
ë¶„ì„ ê²°ê³¼ì˜ ì •í™•ì„±ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ê²€ì¦í•˜ê³  ê°œì„ í•˜ëŠ” ì‹œìŠ¤í…œ
"""

import json
import time
import logging
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from datetime import datetime, timedelta
import re
import requests
from collections import defaultdict
import numpy as np

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class QualityMetrics:
    """í’ˆì§ˆ ì§€í‘œ ë°ì´í„° í´ë˜ìŠ¤"""
    accuracy_score: float
    completeness_score: float
    relevance_score: float
    clarity_score: float
    actionability_score: float
    overall_score: float
    timestamp: str

class QualityValidator:
    """ë¶„ì„ í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.validation_history = []
        self.quality_thresholds = {
            "accuracy": 0.7,
            "completeness": 0.6,
            "relevance": 0.8,
            "clarity": 0.7,
            "actionability": 0.6,
            "overall": 0.7
        }
        
        # í’ˆì§ˆ í‰ê°€ í‚¤ì›Œë“œ
        self.quality_keywords = {
            "accuracy": {
                "positive": ["ì •í™•í•œ", "êµ¬ì²´ì ì¸", "ê·¼ê±°ê°€", "ì‚¬ì‹¤", "í™•ì‹¤í•œ", "ëª…í™•í•œ"],
                "negative": ["ì¶”ì¸¡", "ì•„ë§ˆë„", "~ê²ƒ ê°™ë‹¤", "ë¶ˆë¶„ëª…", "ëª¨í˜¸í•œ"]
            },
            "completeness": {
                "sections": ["ìš”ì•½", "ë¶„ì„", "ê²°ë¡ ", "ì œì•ˆ", "ë°°ê²½"],
                "min_length": 200
            },
            "relevance": {
                "required": ["í•µì‹¬", "ì¤‘ìš”", "ì£¼ìš”", "ê²°ê³¼"],
                "context_match": ["ê´€ë ¨", "ì—°ê´€", "ë§¥ë½"]
            },
            "clarity": {
                "structure": ["1.", "2.", "3.", "ê°€.", "ë‚˜.", "ë‹¤.", "â—", "â€¢"],
                "connectors": ["ë”°ë¼ì„œ", "ê·¸ëŸ¬ë¯€ë¡œ", "ë˜í•œ", "ë°˜ë©´", "ê²°ê³¼ì ìœ¼ë¡œ"]
            },
            "actionability": {
                "action_words": ["ì œì•ˆ", "ì¶”ì²œ", "ê°œì„ ", "ì‹¤í–‰", "ì ìš©", "êµ¬í˜„", "ìˆ˜í–‰"]
            }
        }
    
    def validate_accuracy(self, analysis_result: str, original_content: str) -> float:
        """ì •í™•ì„± ì ìˆ˜ ê³„ì‚°"""
        if not analysis_result or not original_content:
            return 0.0
        
        score = 0.5  # ê¸°ë³¸ì ìˆ˜
        
        # ê¸ì •ì  ì •í™•ì„± ì§€í‘œ
        positive_indicators = self.quality_keywords["accuracy"]["positive"]
        positive_count = sum(1 for word in positive_indicators 
                           if word in analysis_result.lower())
        score += min(positive_count * 0.1, 0.3)
        
        # ë¶€ì •ì  ì •í™•ì„± ì§€í‘œ (ê°ì )
        negative_indicators = self.quality_keywords["accuracy"]["negative"]
        negative_count = sum(1 for word in negative_indicators 
                           if word in analysis_result.lower())
        score -= min(negative_count * 0.15, 0.4)
        
        # ì›ë³¸ ë‚´ìš©ê³¼ì˜ í‚¤ì›Œë“œ ì¼ì¹˜ë„
        original_keywords = set(re.findall(r'\b\w{4,}\b', original_content.lower()))
        analysis_keywords = set(re.findall(r'\b\w{4,}\b', analysis_result.lower()))
        
        if original_keywords:
            keyword_overlap = len(original_keywords & analysis_keywords) / len(original_keywords)
            score += keyword_overlap * 0.2
        
        return max(0.0, min(1.0, score))
    
    def validate_completeness(self, analysis_result: str) -> float:
        """ì™„ì „ì„± ì ìˆ˜ ê³„ì‚°"""
        if not analysis_result:
            return 0.0
        
        score = 0.0
        
        # ìµœì†Œ ê¸¸ì´ ì²´í¬
        min_length = self.quality_keywords["completeness"]["min_length"]
        if len(analysis_result) >= min_length:
            score += 0.3
        elif len(analysis_result) >= min_length * 0.7:
            score += 0.2
        elif len(analysis_result) >= min_length * 0.5:
            score += 0.1
        
        # í•„ìˆ˜ ì„¹ì…˜ ì²´í¬
        required_sections = self.quality_keywords["completeness"]["sections"]
        section_count = sum(1 for section in required_sections 
                          if section in analysis_result.lower())
        score += min(section_count / len(required_sections), 1.0) * 0.4
        
        # êµ¬ì¡°í™” ì ìˆ˜
        has_structure = any(marker in analysis_result 
                          for marker in ["##", "===", "***", "1.", "2.", "3."])
        if has_structure:
            score += 0.3
        
        return min(1.0, score)
    
    def validate_relevance(self, analysis_result: str, context: str = "") -> float:
        """ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
        if not analysis_result:
            return 0.0
        
        score = 0.5  # ê¸°ë³¸ì ìˆ˜
        
        # í•„ìˆ˜ í‚¤ì›Œë“œ ì²´í¬
        required_keywords = self.quality_keywords["relevance"]["required"]
        keyword_count = sum(1 for word in required_keywords 
                          if word in analysis_result.lower())
        score += min(keyword_count * 0.15, 0.3)
        
        # ì»¨í…ìŠ¤íŠ¸ ë§¤ì¹­ (ì œê³µëœ ê²½ìš°)
        if context:
            context_keywords = self.quality_keywords["relevance"]["context_match"]
            context_match = sum(1 for word in context_keywords 
                              if word in analysis_result.lower())
            score += min(context_match * 0.1, 0.2)
        
        return min(1.0, score)
    
    def validate_clarity(self, analysis_result: str) -> float:
        """ëª…í™•ì„± ì ìˆ˜ ê³„ì‚°"""
        if not analysis_result:
            return 0.0
        
        score = 0.3  # ê¸°ë³¸ì ìˆ˜
        
        # êµ¬ì¡°í™” ì§€í‘œ
        structure_indicators = self.quality_keywords["clarity"]["structure"]
        structure_count = sum(1 for marker in structure_indicators 
                            if marker in analysis_result)
        score += min(structure_count * 0.1, 0.3)
        
        # ì—°ê²°ì–´ ì‚¬ìš©
        connectors = self.quality_keywords["clarity"]["connectors"]
        connector_count = sum(1 for conn in connectors 
                            if conn in analysis_result)
        score += min(connector_count * 0.08, 0.2)
        
        # ë¬¸ì¥ ê¸¸ì´ ì ì •ì„± (ë„ˆë¬´ ê¸¸ê±°ë‚˜ ì§§ì§€ ì•Šì€ì§€)
        sentences = re.split(r'[.!?]+', analysis_result)
        if sentences:
            avg_length = np.mean([len(s.strip()) for s in sentences if s.strip()])
            if 20 <= avg_length <= 100:  # ì ì • ë¬¸ì¥ ê¸¸ì´
                score += 0.2
        
        return min(1.0, score)
    
    def validate_actionability(self, analysis_result: str) -> float:
        """ì‹¤í–‰ê°€ëŠ¥ì„± ì ìˆ˜ ê³„ì‚°"""
        if not analysis_result:
            return 0.0
        
        score = 0.2  # ê¸°ë³¸ì ìˆ˜
        
        # ì‹¤í–‰ ê´€ë ¨ í‚¤ì›Œë“œ
        action_words = self.quality_keywords["actionability"]["action_words"]
        action_count = sum(1 for word in action_words 
                         if word in analysis_result.lower())
        score += min(action_count * 0.15, 0.5)
        
        # êµ¬ì²´ì ì¸ ì œì•ˆ ì„¹ì…˜ ì¡´ì¬ ì—¬ë¶€
        suggestion_patterns = [
            r'ì œì•ˆ.*:',
            r'ì¶”ì²œ.*:',
            r'ê°œì„ .*ë°©ì•ˆ',
            r'ì•¡ì…˜.*í”Œëœ',
            r'ë‹¤ìŒ.*ë‹¨ê³„'
        ]
        
        has_suggestions = any(re.search(pattern, analysis_result.lower()) 
                            for pattern in suggestion_patterns)
        if has_suggestions:
            score += 0.3
        
        return min(1.0, score)
    
    def calculate_quality_metrics(self, analysis_result: str, original_content: str = "", context: str = "") -> QualityMetrics:
        """ì¢…í•© í’ˆì§ˆ ì§€í‘œ ê³„ì‚°"""
        
        accuracy = self.validate_accuracy(analysis_result, original_content)
        completeness = self.validate_completeness(analysis_result)
        relevance = self.validate_relevance(analysis_result, context)
        clarity = self.validate_clarity(analysis_result)
        actionability = self.validate_actionability(analysis_result)
        
        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ì „ì²´ ì ìˆ˜ ê³„ì‚°
        weights = {
            "accuracy": 0.25,
            "completeness": 0.20,
            "relevance": 0.25,
            "clarity": 0.15,
            "actionability": 0.15
        }
        
        overall_score = (
            accuracy * weights["accuracy"] +
            completeness * weights["completeness"] +
            relevance * weights["relevance"] +
            clarity * weights["clarity"] +
            actionability * weights["actionability"]
        )
        
        return QualityMetrics(
            accuracy_score=accuracy,
            completeness_score=completeness,
            relevance_score=relevance,
            clarity_score=clarity,
            actionability_score=actionability,
            overall_score=overall_score,
            timestamp=datetime.now().isoformat()
        )
    
    def generate_improvement_suggestions(self, metrics: QualityMetrics) -> List[str]:
        """í’ˆì§ˆ ê°œì„  ì œì•ˆ ìƒì„±"""
        suggestions = []
        
        if metrics.accuracy_score < self.quality_thresholds["accuracy"]:
            suggestions.append("ğŸ¯ ì •í™•ì„± ê°œì„ : ë” êµ¬ì²´ì ì¸ ê·¼ê±°ì™€ ì‚¬ì‹¤ì„ í¬í•¨í•˜ê³ , ì¶”ì¸¡ì„± í‘œí˜„ì„ ì¤„ì´ì„¸ìš”")
        
        if metrics.completeness_score < self.quality_thresholds["completeness"]:
            suggestions.append("ğŸ“‹ ì™„ì „ì„± ê°œì„ : ë¶„ì„ì— ìš”ì•½, ìƒì„¸ ë¶„ì„, ê²°ë¡ , ì œì•ˆ ì„¹ì…˜ì„ ëª¨ë‘ í¬í•¨í•˜ì„¸ìš”")
        
        if metrics.relevance_score < self.quality_thresholds["relevance"]:
            suggestions.append("ğŸ” ê´€ë ¨ì„± ê°œì„ : í•µì‹¬ ì£¼ì œì™€ ë” ë°€ì ‘í•˜ê²Œ ì—°ê´€ëœ ë‚´ìš©ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”")
        
        if metrics.clarity_score < self.quality_thresholds["clarity"]:
            suggestions.append("âœ¨ ëª…í™•ì„± ê°œì„ : êµ¬ì¡°í™”ëœ í˜•ì‹ê³¼ ë…¼ë¦¬ì  ì—°ê²°ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°€ë…ì„±ì„ ë†’ì´ì„¸ìš”")
        
        if metrics.actionability_score < self.quality_thresholds["actionability"]:
            suggestions.append("ğŸš€ ì‹¤í–‰ê°€ëŠ¥ì„± ê°œì„ : êµ¬ì²´ì ì¸ ì‹¤í–‰ ê³„íšê³¼ ì¶”ì²œì‚¬í•­ì„ í¬í•¨í•˜ì„¸ìš”")
        
        return suggestions
    
    def validate_analysis_quality(self, analysis_result: str, original_content: str = "", context: str = "") -> Dict:
        """ë¶„ì„ í’ˆì§ˆ ì¢…í•© ê²€ì¦"""
        
        start_time = time.time()
        
        # í’ˆì§ˆ ì§€í‘œ ê³„ì‚°
        metrics = self.calculate_quality_metrics(analysis_result, original_content, context)
        
        # ê°œì„  ì œì•ˆ ìƒì„±
        suggestions = self.generate_improvement_suggestions(metrics)
        
        # í’ˆì§ˆ ë“±ê¸‰ ê²°ì •
        if metrics.overall_score >= 0.8:
            quality_grade = "EXCELLENT"
            grade_emoji = "ğŸ†"
        elif metrics.overall_score >= 0.7:
            quality_grade = "GOOD"
            grade_emoji = "âœ…"
        elif metrics.overall_score >= 0.5:
            quality_grade = "FAIR"
            grade_emoji = "âš ï¸"
        else:
            quality_grade = "POOR"
            grade_emoji = "âŒ"
        
        validation_time = time.time() - start_time
        
        # ê²€ì¦ ê²°ê³¼ ì €ì¥
        validation_result = {
            "quality_grade": quality_grade,
            "grade_emoji": grade_emoji,
            "overall_score": round(metrics.overall_score, 3),
            "detailed_scores": {
                "accuracy": round(metrics.accuracy_score, 3),
                "completeness": round(metrics.completeness_score, 3),
                "relevance": round(metrics.relevance_score, 3),
                "clarity": round(metrics.clarity_score, 3),
                "actionability": round(metrics.actionability_score, 3)
            },
            "improvement_suggestions": suggestions,
            "validation_time": round(validation_time, 3),
            "timestamp": metrics.timestamp,
            "thresholds_met": {
                "accuracy": metrics.accuracy_score >= self.quality_thresholds["accuracy"],
                "completeness": metrics.completeness_score >= self.quality_thresholds["completeness"],
                "relevance": metrics.relevance_score >= self.quality_thresholds["relevance"],
                "clarity": metrics.clarity_score >= self.quality_thresholds["clarity"],
                "actionability": metrics.actionability_score >= self.quality_thresholds["actionability"],
                "overall": metrics.overall_score >= self.quality_thresholds["overall"]
            }
        }
        
        # ì´ë ¥ì— ì¶”ê°€
        self.validation_history.append(validation_result)
        
        logger.info(f"í’ˆì§ˆ ê²€ì¦ ì™„ë£Œ: {quality_grade} ({metrics.overall_score:.3f})")
        
        return validation_result
    
    def get_quality_report(self, days: int = 7) -> Dict:
        """í’ˆì§ˆ íŠ¸ë Œë“œ ë³´ê³ ì„œ ìƒì„±"""
        if not self.validation_history:
            return {"message": "ê²€ì¦ ì´ë ¥ì´ ì—†ìŠµë‹ˆë‹¤"}
        
        # ìµœê·¼ Nì¼ ë°ì´í„° í•„í„°ë§
        cutoff_date = datetime.now() - timedelta(days=days)
        recent_validations = [
            v for v in self.validation_history
            if datetime.fromisoformat(v["timestamp"]) >= cutoff_date
        ]
        
        if not recent_validations:
            return {"message": f"ìµœê·¼ {days}ì¼ê°„ ê²€ì¦ ì´ë ¥ì´ ì—†ìŠµë‹ˆë‹¤"}
        
        # í†µê³„ ê³„ì‚°
        total_validations = len(recent_validations)
        avg_score = sum(v["overall_score"] for v in recent_validations) / total_validations
        
        grade_counts = defaultdict(int)
        for v in recent_validations:
            grade_counts[v["quality_grade"]] += 1
        
        # ê°œì„  íŠ¸ë Œë“œ
        scores = [v["overall_score"] for v in recent_validations]
        if len(scores) >= 2:
            trend = "ê°œì„ " if scores[-1] > scores[0] else "ì•…í™”" if scores[-1] < scores[0] else "ìœ ì§€"
        else:
            trend = "ë°ì´í„° ë¶€ì¡±"
        
        return {
            "period": f"ìµœê·¼ {days}ì¼",
            "ì´_ê²€ì¦ìˆ˜": total_validations,
            "í‰ê· _í’ˆì§ˆì ìˆ˜": round(avg_score, 3),
            "í’ˆì§ˆ_ë“±ê¸‰_ë¶„í¬": dict(grade_counts),
            "íŠ¸ë Œë“œ": trend,
            "ìµœê³ _ì ìˆ˜": max(scores),
            "ìµœì €_ì ìˆ˜": min(scores),
            "ê¸°ì¤€_ë‹¬ì„±ë¥ ": {
                criterion: sum(1 for v in recent_validations if v["thresholds_met"][criterion]) / total_validations * 100
                for criterion in self.quality_thresholds.keys()
            }
        }

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
quality_validator = QualityValidator()

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    test_analysis = """
    === ì£¼ì–¼ë¦¬ ì‹œì¥ ë¶„ì„ ê²°ê³¼ ===
    
    ## í•µì‹¬ ìš”ì•½
    ìµœê·¼ ë‹¤ì´ì•„ëª¬ë“œ ë°˜ì§€ ì‹œì¥ì€ í”„ë¦¬ë¯¸ì—„ ì„¸ê·¸ë¨¼íŠ¸ì—ì„œ 15% ì„±ì¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.
    
    ## ìƒì„¸ ë¶„ì„
    1. ì‹œì¥ ë™í–¥: ê³ í’ˆì§ˆ ë‹¤ì´ì•„ëª¬ë“œì— ëŒ€í•œ ìˆ˜ìš” ì¦ê°€
    2. ê³ ê° ì„ í˜¸: í´ë˜ì‹í•œ ë””ìì¸ë³´ë‹¤ ëª¨ë˜í•œ ìŠ¤íƒ€ì¼ ì„ í˜¸
    3. ê°€ê²© ë™í–¥: ì›ìì¬ ê°€ê²© ìƒìŠ¹ìœ¼ë¡œ ì¸í•œ ì œí’ˆ ê°€ê²© 10% ì¸ìƒ
    
    ## ì œì•ˆì‚¬í•­
    - ëª¨ë˜ ë””ìì¸ ë¼ì¸ í™•ëŒ€ ê°œë°œ
    - í”„ë¦¬ë¯¸ì—„ ë§ˆì¼€íŒ… ì „ëµ ìˆ˜ë¦½
    - ê³ ê° ë§ì¶¤ ì„œë¹„ìŠ¤ ê°•í™”
    """
    
    test_original = "ë‹¤ì´ì•„ëª¬ë“œ ë°˜ì§€ ì‹œì¥ ë¶„ì„ì„ ìœ„í•œ íšŒì˜ ë‚´ìš©ì…ë‹ˆë‹¤."
    
    result = quality_validator.validate_analysis_quality(
        analysis_result=test_analysis,
        original_content=test_original,
        context="ì£¼ì–¼ë¦¬ ì‹œì¥ ë¶„ì„"
    )
    
    print("=== í’ˆì§ˆ ê²€ì¦ ê²°ê³¼ ===")
    print(json.dumps(result, ensure_ascii=False, indent=2))
    
    print("\n=== í’ˆì§ˆ ë³´ê³ ì„œ ===")
    report = quality_validator.get_quality_report()
    print(json.dumps(report, ensure_ascii=False, indent=2))