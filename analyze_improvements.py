#!/usr/bin/env python3
"""
ì†”ë¡œëª¬ë“œ AI ì‹œìŠ¤í…œ ê°œì„ ì  ë¶„ì„ ë° ì—…ê·¸ë ˆì´ë“œ
í˜„ì¬ GEMMA3:4b + Qwen3:8bë¥¼ í™œìš©í•œ ì„±ëŠ¥ ìµœì í™”
"""

import requests
import time
import json
from datetime import datetime

def check_current_system_status():
    """í˜„ì¬ ì‹œìŠ¤í…œ ìƒíƒœ ë¶„ì„"""
    print("=== ì†”ë¡œëª¬ë“œ AI ì‹œìŠ¤í…œ í˜„ì¬ ìƒíƒœ ë¶„ì„ ===")
    print(f"ë¶„ì„ ì‹œê°„: {datetime.now()}")
    print()
    
    # 1. Ollama ëª¨ë¸ ìƒíƒœ í™•ì¸
    print("1. ì‚¬ìš© ê°€ëŠ¥í•œ AI ëª¨ë¸ë“¤:")
    try:
        r = requests.get('http://localhost:11434/api/tags')
        if r.status_code == 200:
            models = r.json()['models']
            
            latest_models = []
            standard_models = []
            
            for model in models:
                name = model['name']
                size_gb = model['size'] / (1024**3)
                
                if 'qwen3' in name or 'gemma3' in name:
                    latest_models.append((name, size_gb, "3ì„¸ëŒ€ ìµœì‹ "))
                else:
                    standard_models.append((name, size_gb, "2ì„¸ëŒ€/í‘œì¤€"))
            
            print("   ğŸ¥‡ 3ì„¸ëŒ€ ìµœì‹  ëª¨ë¸ë“¤:")
            for name, size, gen in latest_models:
                print(f"      - {name} ({size:.1f}GB)")
            
            print("   ğŸ¥ˆ 2ì„¸ëŒ€/í‘œì¤€ ëª¨ë¸ë“¤:")
            for name, size, gen in standard_models:
                print(f"      - {name} ({size:.1f}GB)")
                
        else:
            print("   âŒ Ollama ì—°ê²° ì‹¤íŒ¨")
            
    except Exception as e:
        print(f"   âŒ ëª¨ë¸ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
    
    print()
    
    # 2. ì‹œìŠ¤í…œ ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
    print("2. ì„œë¹„ìŠ¤ ìƒíƒœ:")
    services = [
        ("Streamlit UI", "http://localhost:8504"),
        ("Ollama API", "http://localhost:11434/api/tags"),
    ]
    
    for service_name, url in services:
        try:
            r = requests.get(url, timeout=5)
            if r.status_code == 200:
                print(f"   âœ… {service_name}: ì •ìƒ ì‘ë™")
            else:
                print(f"   âš ï¸ {service_name}: ì‘ë‹µ ì˜¤ë¥˜ ({r.status_code})")
        except:
            print(f"   âŒ {service_name}: ì—°ê²° ì‹¤íŒ¨")
    
    print()

def identify_improvement_areas():
    """ê°œì„  í•„ìš” ì˜ì—­ ì‹ë³„"""
    print("=== ê°œì„  í•„ìš” ì˜ì—­ ë¶„ì„ ===")
    
    improvements = {
        "ì„±ëŠ¥ ìµœì í™”": [
            "GEMMA3:4bë¥¼ ë©”ì¸ ë¶„ì„ ì—”ì§„ìœ¼ë¡œ ì „í™˜",
            "Qwen3:8bë¥¼ í•œêµ­ì–´ ì „ë¬¸ ë¶„ì„ìš©ìœ¼ë¡œ íŠ¹í™”",
            "ëª¨ë¸ë³„ ì—­í•  ë¶„ë‹´ ìµœì í™”",
            "ë°°ì¹˜ ì²˜ë¦¬ ì„±ëŠ¥ í–¥ìƒ"
        ],
        "ë¶„ì„ ì •í™•ë„ í–¥ìƒ": [
            "ìµœì‹  ëª¨ë¸ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§",
            "ë‹¤ì¤‘ ëª¨ë¸ ì•™ìƒë¸” ë¶„ì„",
            "ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ê°œì„ ",
            "ë„ë©”ì¸ íŠ¹í™” fine-tuning ì ìš©"
        ],
        "ì‚¬ìš©ì ê²½í—˜ ê°œì„ ": [
            "ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© í‘œì‹œ ê°•í™”",
            "ì—ëŸ¬ í•¸ë“¤ë§ ë° ë³µêµ¬ ìë™í™”",
            "ê²°ê³¼ ì‹œê°í™” ê°œì„ ",
            "ë°˜ì‘ ì†ë„ ìµœì í™”"
        ],
        "ì‹œìŠ¤í…œ ì•ˆì •ì„±": [
            "ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”",
            "ì—ëŸ¬ ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§ ê°•í™”",
            "ë°±ì—… ë° ë³µêµ¬ ì‹œìŠ¤í…œ",
            "ìë™ ì¬ì‹œì‘ ë©”ì»¤ë‹ˆì¦˜"
        ]
    }
    
    for category, items in improvements.items():
        print(f"{category}:")
        for i, item in enumerate(items, 1):
            print(f"   {i}. {item}")
        print()

def test_latest_models_performance():
    """ìµœì‹  ëª¨ë¸ë“¤ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    print("=== ìµœì‹  ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸ ===")
    
    models_to_test = ["qwen3:8b", "gemma3:4b", "gemma2:9b"]
    test_prompt = "í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”: ê²°í˜¼ë°˜ì§€ 200ë§Œì› ì˜ˆì‚°ìœ¼ë¡œ ì–´ë–¤ ì œí’ˆì„ ì¶”ì²œí•˜ì‹œë‚˜ìš”?"
    
    results = []
    
    for model in models_to_test:
        print(f"\ní…ŒìŠ¤íŠ¸ ì¤‘: {model}")
        
        try:
            payload = {
                "model": model,
                "prompt": test_prompt,
                "stream": False
            }
            
            start_time = time.time()
            response = requests.post("http://localhost:11434/api/generate",
                                   json=payload, timeout=60)
            
            if response.status_code == 200:
                end_time = time.time()
                result_data = response.json()
                answer = result_data.get('response', '')
                
                processing_time = end_time - start_time
                
                # í’ˆì§ˆ ë¶„ì„
                keywords = ['ê²°í˜¼ë°˜ì§€', '200ë§Œì›', 'ì˜ˆì‚°', 'ì¶”ì²œ', 'ë‹¤ì´ì•„ëª¬ë“œ', 'ë””ìì¸']
                found_keywords = [k for k in keywords if k in answer]
                quality_score = len(found_keywords) / len(keywords) * 100
                
                results.append({
                    'model': model,
                    'time': processing_time,
                    'quality': quality_score,
                    'length': len(answer),
                    'keywords_found': len(found_keywords)
                })
                
                print(f"   â±ï¸ ì²˜ë¦¬ì‹œê°„: {processing_time:.2f}ì´ˆ")
                print(f"   ğŸ“Š í’ˆì§ˆì ìˆ˜: {quality_score:.1f}%")
                print(f"   ğŸ“ ì‘ë‹µê¸¸ì´: {len(answer)} ê¸€ì")
                print(f"   ğŸ” í‚¤ì›Œë“œ ì¸ì‹: {len(found_keywords)}/{len(keywords)}ê°œ")
                
            else:
                print(f"   âŒ API ì˜¤ë¥˜: {response.status_code}")
                
        except Exception as e:
            print(f"   âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
    
    # ê²°ê³¼ ë¹„êµ
    if results:
        print(f"\n=== ì„±ëŠ¥ ë¹„êµ ê²°ê³¼ ===")
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì„ ì •
        best_quality = max(results, key=lambda x: x['quality'])
        fastest = min(results, key=lambda x: x['time'])
        
        print(f"ğŸ† ìµœê³  í’ˆì§ˆ: {best_quality['model']} ({best_quality['quality']:.1f}%)")
        print(f"âš¡ ìµœê³  ì†ë„: {fastest['model']} ({fastest['time']:.2f}ì´ˆ)")
        
        # ì¢…í•© ì ìˆ˜ (í’ˆì§ˆ 70% + ì†ë„ 30%)
        for result in results:
            speed_score = (10 / max(result['time'], 1)) * 10  # ì†ë„ë¥¼ ì ìˆ˜ë¡œ ë³€í™˜
            total_score = result['quality'] * 0.7 + min(speed_score, 100) * 0.3
            result['total_score'] = total_score
        
        best_overall = max(results, key=lambda x: x['total_score'])
        print(f"ğŸ¯ ì¢…í•© ìµœê³ : {best_overall['model']} ({best_overall['total_score']:.1f}ì )")
        
        return best_overall['model']
    
    return None

def generate_improvement_plan(best_model):
    """ê°œì„  ê³„íš ìƒì„±"""
    print(f"\n=== ê°œì„  ì‹¤í–‰ ê³„íš ===")
    print(f"ìµœì  ëª¨ë¸: {best_model}")
    print()
    
    plan = {
        "ë‹¨ê¸° ê°œì„  (ì¦‰ì‹œ ì ìš©)": [
            f"ollama_integration_engine.pyì—ì„œ korean_chatë¥¼ {best_model}ë¡œ ì„¤ì •",
            "GEMMA3:4bë¥¼ ê°ì • ë¶„ì„ ì „ìš©ìœ¼ë¡œ í™œìš©",
            "í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìµœì‹  ëª¨ë¸ì— ë§ê²Œ ìµœì í™”",
            "ì‘ë‹µ ì†ë„ ê°œì„ ì„ ìœ„í•œ íƒ€ì„ì•„ì›ƒ ì¡°ì •"
        ],
        "ì¤‘ê¸° ê°œì„  (1-2ì£¼)": [
            "ë‹¤ì¤‘ ëª¨ë¸ ì•™ìƒë¸” ì‹œìŠ¤í…œ êµ¬ì¶•",
            "ì‹¤ì‹œê°„ ëª¨ë¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì¶”ê°€",
            "ìë™ ëª¨ë¸ ì„ íƒ ì•Œê³ ë¦¬ì¦˜ ê°œë°œ",
            "ë¶„ì„ ê²°ê³¼ ìºì‹± ì‹œìŠ¤í…œ êµ¬í˜„"
        ],
        "ì¥ê¸° ê°œì„  (1ê°œì›”+)": [
            "ë„ë©”ì¸ íŠ¹í™” fine-tuning ë°ì´í„° êµ¬ì¶•",
            "í•œêµ­ì–´ ì£¼ì–¼ë¦¬ ì „ë¬¸ ëª¨ë¸ í›ˆë ¨",
            "A/B í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬ êµ¬ì¶•",
            "ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ìë™í™” ì‹œìŠ¤í…œ"
        ]
    }
    
    for phase, tasks in plan.items():
        print(f"{phase}:")
        for i, task in enumerate(tasks, 1):
            print(f"   {i}. {task}")
        print()
    
    return plan

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("ì†”ë¡œëª¬ë“œ AI ì‹œìŠ¤í…œ ê°œì„ ì  ë¶„ì„")
    print("=" * 50)
    print()
    
    # 1. í˜„ì¬ ìƒíƒœ ë¶„ì„
    check_current_system_status()
    
    # 2. ê°œì„  ì˜ì—­ ì‹ë³„
    identify_improvement_areas()
    
    # 3. ìµœì‹  ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
    best_model = test_latest_models_performance()
    
    # 4. ê°œì„  ê³„íš ìƒì„±
    if best_model:
        improvement_plan = generate_improvement_plan(best_model)
        
        print("=== ë‹¤ìŒ ë‹¨ê³„ ===")
        print("1. ìœ„ ê°œì„  ê³„íšì— ë”°ë¼ ì‹œìŠ¤í…œ ì—…ê·¸ë ˆì´ë“œ")
        print("2. ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë° ê²€ì¦")
        print("3. ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘ ë° ë°˜ì˜")
        print()
        print("ğŸš€ ìµœì‹  AI ëª¨ë¸ì„ í™œìš©í•œ ì„±ëŠ¥ í–¥ìƒ ì¤€ë¹„ ì™„ë£Œ!")
    
    else:
        print("âš ï¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ì—°ê²° ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

if __name__ == "__main__":
    main()