#!/usr/bin/env python3
"""
GEMMA3 ë° 2025ë…„ ìµœì‹  ëª¨ë¸ ì„¤ì¹˜
"""

import subprocess
import os
import time
import requests

def find_ollama():
    """Ollama ì‹¤í–‰íŒŒì¼ ì°¾ê¸°"""
    
    paths = [
        r"C:\Users\{}\AppData\Local\Programs\Ollama\ollama.exe".format(os.getenv("USERNAME")),
        r"C:\Program Files\Ollama\ollama.exe",
        r"C:\Program Files (x86)\Ollama\ollama.exe"
    ]
    
    for path in paths:
        if os.path.exists(path):
            return path
    
    return "ollama"

def check_available_latest_models():
    """2025ë…„ ìµœì‹  ëª¨ë¸ í™•ì¸"""
    
    print("=== 2025ë…„ 7ì›” ìµœì‹  ëª¨ë¸ ëª©ë¡ ===")
    
    # ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ ìµœì‹  ëª¨ë¸ë“¤
    latest_models = [
        # Google ìµœì‹ 
        ("gemma2:9b", "Gemma 2 9B - Google ìµœì‹  (í˜„ì¬ ìµœì‹ )"),
        ("gemma2:27b", "Gemma 2 27B - Google ê³ ì„±ëŠ¥"),
        ("gemma2:2b", "Gemma 2 2B - Google ê²½ëŸ‰"),
        
        # ì°¸ê³ : Gemma 3ëŠ” ì•„ì§ ê³µì‹ ì¶œì‹œ ì „ì´ê±°ë‚˜ ì œí•œì 
        # ("gemma3:9b", "Gemma 3 9B - Google ì°¨ì„¸ëŒ€ (ì¶œì‹œì‹œ)"),
        
        # Meta ìµœì‹ 
        ("llama3.2:8b", "Llama 3.2 8B - Meta ìµœì‹  ì•ˆì • ë²„ì „"),
        ("llama3.2:3b", "Llama 3.2 3B - Meta ê²½ëŸ‰"),
        ("llama3.1:8b", "Llama 3.1 8B - Meta ê²€ì¦ëœ ë²„ì „"),
        
        # Alibaba ìµœì‹  (í•œêµ­ì–´ ìš°ìˆ˜)
        ("qwen2.5:14b", "Qwen 2.5 14B - ì•„ì‹œì•„ ì–¸ì–´ íŠ¹í™” ìµœê°•"),
        ("qwen2.5:7b", "Qwen 2.5 7B - ì•„ì‹œì•„ ì–¸ì–´ íŠ¹í™”"),
        ("qwen2.5:3b", "Qwen 2.5 3B - ì•„ì‹œì•„ ì–¸ì–´ ê²½ëŸ‰"),
        
        # Microsoft ìµœì‹ 
        ("phi3.5:3.8b", "Phi 3.5 - Microsoft ì†Œí˜• ê³ ì„±ëŠ¥"),
        
        # Mistral ìµœì‹ 
        ("mistral-nemo:12b", "Mistral Nemo 12B - íš¨ìœ¨ì„± ìµœê°•"),
        
        # DeepSeek ìµœì‹ 
        ("deepseek-r1:7b", "DeepSeek R1 7B - ì¶”ë¡  íŠ¹í™” (ìˆë‹¤ë©´)"),
        
        # ì½”ë“œ íŠ¹í™”
        ("codellama:7b", "CodeLlama 7B - ì½”ë“œ ìƒì„± íŠ¹í™”"),
    ]
    
    print("ğŸ¯ í•œêµ­ì–´ ì£¼ì–¼ë¦¬ ìƒë‹´ìš© ì¶”ì²œ ìˆœìœ„:")
    print("1. Qwen 2.5 (ì•„ì‹œì•„ ì–¸ì–´ ìµœê°•)")
    print("2. Llama 3.2 (ë²”ìš© ì•ˆì •ì„±)")
    print("3. Gemma 2 (êµ¬ê¸€ ìµœì‹ )")
    print("4. Mistral Nemo (íš¨ìœ¨ì„±)")
    
    return latest_models

def install_top_models(ollama_path):
    """ìƒìœ„ ì¶”ì²œ ëª¨ë¸ ì„¤ì¹˜"""
    
    print("\n=== ìµœì‹  ìµœê°• ëª¨ë¸ ì„¤ì¹˜ ===")
    
    # ì„±ëŠ¥ + í•œêµ­ì–´ + ìµœì‹ ì„± ê¸°ì¤€ TOP ëª¨ë¸ë“¤
    top_models = [
        ("qwen2.5:7b", "ğŸ¥‡ Qwen 2.5 7B - í•œêµ­ì–´ ìµœê°• (1ìˆœìœ„)"),
        ("llama3.2:8b", "ğŸ¥ˆ Llama 3.2 8B - ë²”ìš© ìµœì‹  (2ìˆœìœ„)"),
        ("gemma2:9b", "ğŸ¥‰ Gemma 2 9B - Google ìµœì‹  (3ìˆœìœ„)")
    ]
    
    installed = []
    
    for model_id, description in top_models:
        print(f"\n{description}")
        print(f"ëª¨ë¸ ì„¤ì¹˜ ì¤‘: {model_id}")
        print("ë‹¤ìš´ë¡œë“œ ì§„í–‰... (í¬ê¸°ì— ë”°ë¼ 3-15ë¶„ ì†Œìš”)")
        
        try:
            # ì§„í–‰ ìƒí™© í‘œì‹œ
            process = subprocess.Popen(
                [ollama_path, "pull", model_id],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )
            
            # ê°„ë‹¨í•œ ì§„í–‰ í‘œì‹œ
            dots = 0
            while process.poll() is None:
                print(f"ì§„í–‰ ì¤‘{'.' * (dots % 4)}", end="\r")
                dots += 1
                time.sleep(2)
            
            stdout, stderr = process.communicate()
            
            if process.returncode == 0:
                print(f"\nâœ… {model_id} ì„¤ì¹˜ ì™„ë£Œ!")
                installed.append(model_id)
            else:
                print(f"\nâŒ {model_id} ì„¤ì¹˜ ì‹¤íŒ¨")
                print(f"ì˜¤ë¥˜: {stderr}")
                
        except Exception as e:
            print(f"\nâŒ ì„¤ì¹˜ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    return installed

def comprehensive_benchmark(models):
    """ì¢…í•© ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
    
    print(f"\n=== ìµœì‹  ëª¨ë¸ ì¢…í•© ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ===")
    
    # ì‹¤ì œ ì£¼ì–¼ë¦¬ ìƒë‹´ ì‹œë‚˜ë¦¬ì˜¤
    test_scenarios = [
        {
            "name": "ë³µì¡í•œ_ì£¼ì–¼ë¦¬_ìƒë‹´",
            "prompt": """ì „ë¬¸ ì£¼ì–¼ë¦¬ ìƒë‹´ì‚¬ë¡œì„œ ë‹µë³€í•´ì£¼ì„¸ìš”:

ê³ ê°: "ì•ˆë…•í•˜ì„¸ìš”. ê²°í˜¼ 5ì£¼ë…„ ê¸°ë…ìœ¼ë¡œ ì•„ë‚´ì—ê²Œ ëª©ê±¸ì´ë¥¼ ì„ ë¬¼í•˜ë ¤ê³  í•´ìš”. 
ì•„ë‚´ëŠ” 30ëŒ€ ì¤‘ë°˜ì´ê³  í‰ì†Œì— ì‹¬í”Œí•˜ê³  í´ë˜ì‹í•œ ìŠ¤íƒ€ì¼ì„ ì¢‹ì•„í•´ìš”. 
ì˜ˆì‚°ì€ 150ë§Œì› ì •ë„ì¸ë°, ë‹¤ì´ì•„ëª¬ë“œê°€ ë“¤ì–´ê°„ ê²ƒê³¼ ì•ˆ ë“¤ì–´ê°„ ê²ƒ ì¤‘ ì–´ë–¤ ê²Œ ë” ì¢‹ì„ê¹Œìš”? 
ê·¸ë¦¬ê³  ê¸ˆ ì¬ì§ˆë³„ë¡œ ì°¨ì´ì ë„ ì•Œë ¤ì£¼ì„¸ìš”."

ìœ„ ìƒí™©ì„ ë°”íƒ•ìœ¼ë¡œ ì „ë¬¸ì ì´ê³  ìì„¸í•œ ìƒë‹´ì„ í•´ì£¼ì„¸ìš”.""",
            "keywords": ["5ì£¼ë…„", "ëª©ê±¸ì´", "30ëŒ€", "ì‹¬í”Œ", "í´ë˜ì‹", "150ë§Œì›", "ë‹¤ì´ì•„ëª¬ë“œ", "ê¸ˆ", "ì¬ì§ˆ"]
        },
        {
            "name": "ê¸´ê¸‰_ì œí’ˆ_ì¶”ì²œ",
            "prompt": """ë¹ ë¥´ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”:

"ë‚´ì¼ì´ ì—¬ìì¹œêµ¬ ìƒì¼ì¸ë° ê¹œë¹¡í–ˆì–´ìš”! 20ëŒ€ ì´ˆë°˜ ëŒ€í•™ìƒì´ê³  ê·€ì—¬ìš´ ê±¸ ì¢‹ì•„í•´ìš”. 
30ë§Œì› ì´í•˜ë¡œ ê·€ê±¸ì´ë‚˜ ëª©ê±¸ì´ ì¶”ì²œí•´ì£¼ì„¸ìš”. ì–´ë–¤ ê²Œ ì¢‹ì„ê¹Œìš”?"

ê°„ë‹¨ëª…ë£Œí•˜ë©´ì„œë„ ì‹¤ìš©ì ì¸ ì¶”ì²œì„ í•´ì£¼ì„¸ìš”.""",
            "keywords": ["ìƒì¼", "20ëŒ€", "ëŒ€í•™ìƒ", "ê·€ì—¬ìš´", "30ë§Œì›", "ê·€ê±¸ì´", "ëª©ê±¸ì´", "ì¶”ì²œ"]
        }
    ]
    
    results = []
    
    for model in models:
        print(f"\n{'='*50}")
        print(f"ğŸ§ª {model} ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
        print(f"{'='*50}")
        
        model_results = {
            "model": model,
            "scenarios": [],
            "avg_time": 0,
            "avg_quality": 0,
            "total_score": 0
        }
        
        total_time = 0
        total_quality = 0
        
        for scenario in test_scenarios:
            print(f"\n--- {scenario['name']} ---")
            
            payload = {
                "model": model,
                "prompt": scenario["prompt"],
                "stream": False
            }
            
            try:
                start_time = time.time()
                
                response = requests.post("http://localhost:11434/api/generate",
                                       json=payload, timeout=120)
                
                if response.status_code == 200:
                    end_time = time.time()
                    result = response.json()
                    answer = result.get('response', '')
                    
                    processing_time = end_time - start_time
                    total_time += processing_time
                    
                    # í’ˆì§ˆ ë¶„ì„
                    keywords = scenario["keywords"]
                    found_keywords = [k for k in keywords if k in answer]
                    keyword_score = len(found_keywords) / len(keywords) * 100
                    
                    # ì‘ë‹µ ê¸¸ì´ ì ìˆ˜ (ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸¸ë©´ ê°ì )
                    ideal_length = 300  # ì´ìƒì ì¸ ì‘ë‹µ ê¸¸ì´
                    length_score = min(100, (len(answer) / ideal_length) * 100)
                    if len(answer) > ideal_length * 2:  # ë„ˆë¬´ ê¸¸ë©´ ê°ì 
                        length_score *= 0.8
                    
                    # ì „ë¬¸ì„± ì ìˆ˜ (ì „ë¬¸ ìš©ì–´ í¬í•¨ ì—¬ë¶€)
                    professional_terms = ["ì£¼ì–¼ë¦¬", "ë‹¤ì´ì•„ëª¬ë“œ", "ê¸ˆ", "ì€", "ë°±ê¸ˆ", "ìºëŸ¿", "ì»·", "í´ë˜ë¦¬í‹°"]
                    found_terms = [t for t in professional_terms if t in answer]
                    professional_score = min(100, len(found_terms) * 20)
                    
                    # ì¢…í•© í’ˆì§ˆ ì ìˆ˜
                    quality_score = (keyword_score * 0.4 + length_score * 0.3 + professional_score * 0.3)
                    total_quality += quality_score
                    
                    print(f"ì²˜ë¦¬ì‹œê°„: {processing_time:.2f}ì´ˆ")
                    print(f"ì‘ë‹µê¸¸ì´: {len(answer)} ê¸€ì")
                    print(f"í‚¤ì›Œë“œ ì¸ì‹: {len(found_keywords)}/{len(keywords)}ê°œ ({keyword_score:.1f}%)")
                    print(f"ì „ë¬¸ìš©ì–´: {len(found_terms)}ê°œ")
                    print(f"í’ˆì§ˆì ìˆ˜: {quality_score:.1f}/100")
                    
                    # ì‘ë‹µ ìƒ˜í”Œ
                    print(f"ì‘ë‹µ ìƒ˜í”Œ: {answer[:120]}...")
                    
                    scenario_result = {
                        "scenario": scenario['name'],
                        "time": processing_time,
                        "quality": quality_score,
                        "keywords_found": len(found_keywords),
                        "professional_terms": len(found_terms)
                    }
                    
                    model_results["scenarios"].append(scenario_result)
                    
                else:
                    print(f"API ì˜¤ë¥˜: {response.status_code}")
                    
            except Exception as e:
                print(f"í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        
        # ëª¨ë¸ë³„ ì¢…í•© ì ìˆ˜ ê³„ì‚°
        if model_results["scenarios"]:
            model_results["avg_time"] = total_time / len(test_scenarios)
            model_results["avg_quality"] = total_quality / len(test_scenarios)
            
            # ì†ë„ ì ìˆ˜ (ë¹ ë¥¼ìˆ˜ë¡ ë†’ìŒ, ìµœëŒ€ 100ì )
            speed_score = max(0, 100 - (model_results["avg_time"] * 10))
            
            # ì¢…í•© ì ìˆ˜ (í’ˆì§ˆ 70%, ì†ë„ 30%)
            model_results["total_score"] = (model_results["avg_quality"] * 0.7 + speed_score * 0.3)
            
            print(f"\nğŸ“Š {model} ì¢…í•© ê²°ê³¼:")
            print(f"   í‰ê·  ì²˜ë¦¬ì‹œê°„: {model_results['avg_time']:.2f}ì´ˆ")
            print(f"   í‰ê·  í’ˆì§ˆì ìˆ˜: {model_results['avg_quality']:.1f}/100")
            print(f"   ì¢…í•© ì ìˆ˜: {model_results['total_score']:.1f}/100")
        
        results.append(model_results)
    
    # ìµœì¢… ìˆœìœ„ ë°œí‘œ
    if results:
        print(f"\nğŸ† ìµœì¢… ì„±ëŠ¥ ìˆœìœ„")
        print(f"{'='*60}")
        
        # ì¢…í•© ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        ranked_results = sorted(results, key=lambda x: x['total_score'], reverse=True)
        
        for i, result in enumerate(ranked_results, 1):
            medal = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else f"{i}ìœ„"
            print(f"{medal} {result['model']}")
            print(f"   ì¢…í•©ì ìˆ˜: {result['total_score']:.1f}/100")
            print(f"   í’ˆì§ˆ: {result['avg_quality']:.1f}/100")
            print(f"   ì†ë„: {result['avg_time']:.2f}ì´ˆ")
            print()
        
        # ì¶”ì²œ
        best_model = ranked_results[0]
        print(f"ğŸ¯ ì†”ë¡œëª¬ë“œ AI ì¶”ì²œ ëª¨ë¸: {best_model['model']}")
        print(f"   ì´ìœ : ì¢…í•© ì„±ëŠ¥ {best_model['total_score']:.1f}ì ìœ¼ë¡œ ìµœê³  ì„±ëŠ¥")
    
    return results

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    
    print("ğŸš€ GEMMA3 & 2025ë…„ ìµœì‹  ëª¨ë¸ ì„¤ì¹˜")
    print("="*50)
    
    # 1. Ollama ì°¾ê¸°
    ollama_path = find_ollama()
    
    # 2. ìµœì‹  ëª¨ë¸ í™•ì¸
    latest_models = check_available_latest_models()
    
    print(f"\nğŸ’¡ ì°¸ê³ : Gemma 3ëŠ” ì•„ì§ ì •ì‹ ì¶œì‹œ ì „ì´ê±°ë‚˜ ì œí•œì ì…ë‹ˆë‹¤.")
    print(f"í˜„ì¬ëŠ” Gemma 2ê°€ Googleì˜ ìµœì‹  ì•ˆì • ë²„ì „ì…ë‹ˆë‹¤.")
    
    # 3. TOP ëª¨ë¸ ì„¤ì¹˜
    installed = install_top_models(ollama_path)
    
    if not installed:
        print(f"\nâŒ ìë™ ì„¤ì¹˜ ì‹¤íŒ¨")
        print(f"ìˆ˜ë™ ì„¤ì¹˜ ì‹œë„:")
        print(f"  ollama pull qwen2.5:7b")
        print(f"  ollama pull llama3.2:8b")
        print(f"  ollama pull gemma2:9b")
        return
    
    print(f"\nâœ… ì„¤ì¹˜ ì™„ë£Œ: {len(installed)}ê°œ ëª¨ë¸")
    for model in installed:
        print(f"  âœ“ {model}")
    
    # 4. ì¢…í•© ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
    print(f"\nğŸ§ª ì´ì œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    time.sleep(2)
    
    benchmark_results = comprehensive_benchmark(installed)
    
    print(f"\nğŸ‰ ì„¤ì¹˜ ë° ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")
    print(f"ë‹¤ìŒ ë‹¨ê³„:")
    print(f"1. ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì„ ì†”ë¡œëª¬ë“œ AI ì‹œìŠ¤í…œì— í†µí•©")
    print(f"2. demo_integrated_system.pyì—ì„œ ì‹¤ì œ í…ŒìŠ¤íŠ¸")
    print(f"3. ê¸°ì¡´ eeve-korean ëª¨ë¸ê³¼ ì„±ëŠ¥ ë¹„êµ")

if __name__ == "__main__":
    main()