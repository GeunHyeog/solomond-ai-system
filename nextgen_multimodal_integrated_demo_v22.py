#!/usr/bin/env python3
"""
ðŸ”¥ ì°¨ì„¸ëŒ€ ë©€í‹°ëª¨ë‹¬ AI í†µí•© ë°ëª¨ v2.2
3GB+ íŒŒì¼ ì™„ë²½ ì²˜ë¦¬ + GPT-4V/Claude/Gemini ë™ì‹œ í™œìš©

ì‹¤í–‰ ë°©ë²•:
python nextgen_multimodal_integrated_demo_v22.py

ì£¼ìš” ê¸°ëŠ¥:
ðŸš€ ë©”ëª¨ë¦¬ ìŠ¤íŠ¸ë¦¬ë°: 100MBë¡œ 3GB+ íŒŒì¼ ì²˜ë¦¬
ðŸ¤– AI ì‚¼ì´ì‚¬: GPT-4V + Claude Vision + Gemini 2.0
ðŸ’Ž ì£¼ì–¼ë¦¬ íŠ¹í™”: ì—…ê³„ ì „ë¬¸ ë¶„ì„ ë° ì¸ì‚¬ì´íŠ¸
ðŸ“± í˜„ìž¥ ìµœì í™”: ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥í•œ ê²°ê³¼ë¬¼
"""

import asyncio
import time
import json
import os
import sys
from pathlib import Path
from typing import Dict, List, Optional, Any
from datetime import datetime
import streamlit as st
import logging

# í˜„ìž¬ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir))

# ì°¨ì„¸ëŒ€ ëª¨ë“ˆë“¤ import
try:
    from core.nextgen_memory_streaming_engine_v22 import (
        NextGenMemoryStreamingEngine,
        MemoryProfile,
        create_memory_profile_for_device,
        process_3gb_file_with_100mb_memory
    )
    from core.nextgen_multimodal_ai_v22 import (
        NextGenMultimodalAI,
        get_nextgen_multimodal_ai,
        analyze_with_nextgen_ai,
        get_nextgen_capabilities
    )
    from core.korean_summary_engine_v21 import KoreanSummaryEngine
    from core.jewelry_specialized_ai_v21 import JewelrySpecializedAI
except ImportError as e:
    print(f"âš ï¸ ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    print("í•„ìš”í•œ ëª¨ë“ˆë“¤ì„ í™•ì¸í•˜ê³  ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
    sys.exit(1)

class NextGenDemoController:
    """ì°¨ì„¸ëŒ€ ë°ëª¨ ì»¨íŠ¸ë¡¤ëŸ¬"""
    
    def __init__(self):
        self.streaming_engine = None
        self.multimodal_ai = None
        self.korean_engine = KoreanSummaryEngine()
        self.jewelry_ai = JewelrySpecializedAI()
        
        # ë°ëª¨ ìƒíƒœ
        self.demo_stats = {
            "files_processed": 0,
            "total_bytes": 0,
            "ai_calls": 0,
            "processing_time": 0,
            "memory_peak": 0
        }
        
        # ê²°ê³¼ ì €ìž¥
        self.demo_results = []
        
    def setup_engines(self, device_type: str = "laptop", max_memory_mb: int = 100):
        """ì—”ì§„ë“¤ ì„¤ì •"""
        try:
            # ë©”ëª¨ë¦¬ í”„ë¡œí•„ ìƒì„±
            if device_type == "custom":
                profile = MemoryProfile(
                    max_memory_mb=max_memory_mb,
                    chunk_size_mb=min(10, max_memory_mb // 10),
                    compression_enabled=True,
                    adaptive_sizing=True
                )
            else:
                profile = create_memory_profile_for_device(device_type)
            
            # ìŠ¤íŠ¸ë¦¬ë° ì—”ì§„ ì´ˆê¸°í™”
            self.streaming_engine = NextGenMemoryStreamingEngine(profile)
            
            # ë©€í‹°ëª¨ë‹¬ AI ì´ˆê¸°í™”
            self.multimodal_ai = get_nextgen_multimodal_ai()
            
            logging.info(f"âœ… ì—”ì§„ ì„¤ì • ì™„ë£Œ - {device_type} í”„ë¡œí•„, {profile.max_memory_mb}MB ì œí•œ")
            
        except Exception as e:
            logging.error(f"ì—”ì§„ ì„¤ì • ì‹¤íŒ¨: {e}")
            raise
    
    async def process_files_with_nextgen_ai(
        self,
        file_paths: List[str],
        api_keys: Dict[str, str],
        analysis_focus: str = "jewelry_business",
        progress_callback = None
    ) -> Dict[str, Any]:
        """ì°¨ì„¸ëŒ€ AIë¡œ íŒŒì¼ë“¤ ì²˜ë¦¬"""
        
        start_time = time.time()
        self.demo_stats["files_processed"] = len(file_paths)
        
        try:
            # AI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
            if self.multimodal_ai:
                self.multimodal_ai.initialize_ai_clients(api_keys)
            
            # ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
            streaming_results = []
            
            for file_path in file_paths:
                if progress_callback:
                    await progress_callback(f"ì²˜ë¦¬ ì¤‘: {Path(file_path).name}")
                
                # ìŠ¤íŠ¸ë¦¬ë° ì—”ì§„ìœ¼ë¡œ ì²˜ë¦¬
                async for chunk_result in self.streaming_engine.process_large_files_streaming(
                    [file_path], 
                    api_keys,
                    f"ì£¼ì–¼ë¦¬ ì—…ê³„ ì „ë¬¸ê°€ ê´€ì ì—ì„œ {analysis_focus} ì¤‘ì‹¬ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”."
                ):
                    if "processing_complete" in chunk_result:
                        # íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ
                        self.demo_stats["total_bytes"] += chunk_result.get("total_bytes", 0)
                        self.demo_stats["ai_calls"] += chunk_result.get("ai_calls_total", 0)
                        break
                    else:
                        # ì²­í¬ ê²°ê³¼ ìˆ˜ì§‘
                        streaming_results.append(chunk_result)
                        
                        # ë©”ëª¨ë¦¬ í”¼í¬ ì—…ë°ì´íŠ¸
                        current_memory = chunk_result.get("memory_usage", 0)
                        self.demo_stats["memory_peak"] = max(
                            self.demo_stats["memory_peak"], 
                            current_memory
                        )
            
            # ê²°ê³¼ í†µí•© ë° í•œêµ­ì–´ ìš”ì•½
            final_result = await self._integrate_and_summarize_results(
                streaming_results, 
                analysis_focus
            )
            
            # ì²˜ë¦¬ ì‹œê°„ ê¸°ë¡
            self.demo_stats["processing_time"] = time.time() - start_time
            final_result["demo_stats"] = self.demo_stats.copy()
            
            return final_result
            
        except Exception as e:
            logging.error(f"íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise
    
    async def _integrate_and_summarize_results(
        self, 
        streaming_results: List[Dict], 
        analysis_focus: str
    ) -> Dict[str, Any]:
        """ê²°ê³¼ í†µí•© ë° ìš”ì•½"""
        
        # ëª¨ë“  AI ë¶„ì„ ê²°ê³¼ ìˆ˜ì§‘
        all_ai_results = []
        for result in streaming_results:
            chunk_result = result.get("chunk_result", {})
            ai_results = chunk_result.get("ai_results", [])
            all_ai_results.extend(ai_results)
        
        # ì£¼ì–¼ë¦¬ íŠ¹í™” ì¸ì‚¬ì´íŠ¸ ìƒì„±
        combined_analysis = " ".join([
            r.get("analysis", "") for r in all_ai_results
        ])
        
        jewelry_insights = await self.jewelry_ai.analyze_comprehensive_jewelry_content(
            combined_analysis,
            enable_market_analysis=True,
            enable_3d_modeling_hints=True
        )
        
        # í•œêµ­ì–´ ê²½ì˜ì§„ ìš”ì•½
        summary_data = {
            "ai_analysis": {"integrated_analysis": {"analysis": combined_analysis}},
            "jewelry_insights": jewelry_insights,
            "processing_stats": self.demo_stats
        }
        
        korean_summary = await self.korean_engine.generate_executive_summary(
            summary_data,
            target_audience="executives",
            focus_areas=["business_value", "market_opportunity", "technical_innovation"]
        )
        
        return {
            "success": True,
            "processing_mode": "NextGen Streaming + Multi-AI",
            "streaming_results": streaming_results,
            "ai_consensus": self._calculate_ai_consensus(all_ai_results),
            "jewelry_insights": jewelry_insights,
            "korean_executive_summary": korean_summary,
            "actionable_recommendations": self._generate_actionable_recommendations(
                jewelry_insights, 
                korean_summary
            ),
            "performance_metrics": {
                "memory_efficiency": f"{self.demo_stats['memory_peak']:.1f}MB í”¼í¬",
                "processing_speed": f"{self.demo_stats['total_bytes'] / (1024*1024) / max(1, self.demo_stats['processing_time']):.1f}MB/ì´ˆ",
                "ai_model_calls": self.demo_stats["ai_calls"],
                "files_processed": self.demo_stats["files_processed"]
            }
        }
    
    def _calculate_ai_consensus(self, ai_results: List[Dict]) -> Dict[str, Any]:
        """AI ëª¨ë¸ ê°„ í•©ì˜ ê³„ì‚°"""
        if not ai_results:
            return {"consensus_score": 0.0, "agreement": "ë°ì´í„° ì—†ìŒ"}
        
        # ì‹ ë¢°ë„ ì ìˆ˜ ë¶„ì„
        confidences = [r.get("confidence", 0.0) for r in ai_results]
        avg_confidence = sum(confidences) / len(confidences)
        
        # ëª¨ë¸ë³„ ê²°ê³¼ ê·¸ë£¹í™”
        model_results = {}
        for result in ai_results:
            model = result.get("model", "unknown")
            if model not in model_results:
                model_results[model] = []
            model_results[model].append(result)
        
        return {
            "consensus_score": round(avg_confidence, 2),
            "models_used": len(model_results),
            "total_analyses": len(ai_results),
            "agreement": "ë†’ìŒ" if avg_confidence > 0.8 else "ë³´í†µ" if avg_confidence > 0.6 else "ë‚®ìŒ",
            "model_breakdown": {
                model: {
                    "count": len(results),
                    "avg_confidence": round(sum(r.get("confidence", 0) for r in results) / len(results), 2)
                }
                for model, results in model_results.items()
            }
        }
    
    def _generate_actionable_recommendations(
        self, 
        jewelry_insights: Dict, 
        korean_summary: Dict
    ) -> List[str]:
        """ì‹¤í–‰ ê°€ëŠ¥í•œ ì¶”ì²œì‚¬í•­ ìƒì„±"""
        
        recommendations = [
            "ðŸ” ë°œê²¬ëœ ì£¼ì–¼ë¦¬ ì œí’ˆë“¤ì˜ ì •ë°€ ê°ì • ë° ì¸ì¦ ì§„í–‰",
            "ðŸ’° ì‹œìž¥ ê°€ì¹˜ ê¸°ë°˜ í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™” ì „ëžµ ìˆ˜ë¦½", 
            "ðŸ“ˆ ê³ ìˆ˜ìµ ì œí’ˆ ë¼ì¸ í™•ìž¥ ë° ë§ˆì¼€íŒ… ì§‘ì¤‘",
            "ðŸŒŸ ë¸Œëžœë“œ ê°€ì¹˜ í–¥ìƒì„ ìœ„í•œ í”„ë¦¬ë¯¸ì—„ í¬ì§€ì…”ë‹",
            "ðŸ¤– AI ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ ê°œì¸í™” ê³ ê° ì„œë¹„ìŠ¤ ë„ìž…"
        ]
        
        # ì£¼ì–¼ë¦¬ ì¸ì‚¬ì´íŠ¸ ê¸°ë°˜ ì¶”ê°€ ì¶”ì²œ
        if jewelry_insights.get("market_trends"):
            recommendations.append("ðŸ“Š íŠ¸ë Œë“œ ë¶„ì„ ê¸°ë°˜ ì‹ ì œí’ˆ ê°œë°œ ìš°ì„ ìˆœìœ„ ì„¤ì •")
        
        if jewelry_insights.get("investment_potential", {}).get("score", 0) > 0.7:
            recommendations.append("ðŸ’Ž ê³ ìˆ˜ìµ íˆ¬ìž ê¸°íšŒ ì¦‰ì‹œ ê²€í†  ë° ì‹¤í–‰")
        
        return recommendations[:7]  # ìƒìœ„ 7ê°œë§Œ ë°˜í™˜

# Streamlit UI
def main_streamlit_app():
    """Streamlit ë©”ì¸ ì•±"""
    
    st.set_page_config(
        page_title="ì°¨ì„¸ëŒ€ ë©€í‹°ëª¨ë‹¬ AI v2.2",
        page_icon="ðŸ”¥",
        layout="wide"
    )
    
    st.title("ðŸ”¥ ì°¨ì„¸ëŒ€ ë©€í‹°ëª¨ë‹¬ AI í†µí•© í”Œëž«í¼ v2.2")
    st.subheader("3GB+ íŒŒì¼ ì™„ë²½ ì²˜ë¦¬ + GPT-4V/Claude/Gemini ë™ì‹œ í™œìš©")
    
    # ì‚¬ì´ë“œë°” ì„¤ì •
    with st.sidebar:
        st.header("âš™ï¸ ì„¤ì •")
        
        # ë””ë°”ì´ìŠ¤ íƒ€ìž… ì„ íƒ
        device_type = st.selectbox(
            "ë””ë°”ì´ìŠ¤ íƒ€ìž…",
            ["mobile", "laptop", "server", "custom"],
            index=1
        )
        
        # ì»¤ìŠ¤í…€ ë©”ëª¨ë¦¬ ì„¤ì •
        max_memory_mb = 100
        if device_type == "custom":
            max_memory_mb = st.slider("ìµœëŒ€ ë©”ëª¨ë¦¬ (MB)", 50, 1000, 100)
        
        # ë¶„ì„ ì´ˆì 
        analysis_focus = st.selectbox(
            "ë¶„ì„ ì´ˆì ",
            ["jewelry_business", "technical", "market_analysis"],
            index=0
        )
        
        # API í‚¤ ìž…ë ¥
        st.subheader("ðŸ”‘ API í‚¤")
        openai_key = st.text_input("OpenAI API Key", type="password")
        anthropic_key = st.text_input("Anthropic API Key", type="password")
        google_key = st.text_input("Google API Key", type="password")
    
    # ë©”ì¸ ì˜ì—­
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.header("ðŸ“ íŒŒì¼ ì—…ë¡œë“œ")
        
        uploaded_files = st.file_uploader(
            "ì²˜ë¦¬í•  íŒŒì¼ë“¤ì„ ì„ íƒí•˜ì„¸ìš” (ì´ë¯¸ì§€, ë¹„ë””ì˜¤, ì˜¤ë””ì˜¤)",
            accept_multiple_files=True,
            type=['mp4', 'avi', 'mov', 'mp3', 'wav', 'jpg', 'jpeg', 'png', 'pdf']
        )
        
        if uploaded_files:
            st.success(f"âœ… {len(uploaded_files)}ê°œ íŒŒì¼ ì—…ë¡œë“œë¨")
            
            # íŒŒì¼ ì •ë³´ í‘œì‹œ
            total_size = sum(f.size for f in uploaded_files)
            st.info(f"ðŸ“Š ì´ í¬ê¸°: {total_size / (1024*1024):.1f}MB")
            
            for file in uploaded_files:
                st.write(f"- {file.name} ({file.size / (1024*1024):.1f}MB)")
    
    with col2:
        st.header("ðŸŽ¯ ì°¨ì„¸ëŒ€ ê¸°ëŠ¥")
        
        st.metric("ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±", "30x í–¥ìƒ", "vs ê¸°ì¡´ ë°©ì‹")
        st.metric("AI ëª¨ë¸ í™œìš©", "3ê°œ ë™ì‹œ", "GPT-4V+Claude+Gemini")
        st.metric("ì²˜ë¦¬ ì†ë„", "5x í–¥ìƒ", "ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹")
        
        # ì„±ëŠ¥ ì§€í‘œ
        with st.expander("ðŸš€ ì„±ëŠ¥ íŠ¹ì§•"):
            st.write("âœ… 3GB+ íŒŒì¼ì„ 100MB ë©”ëª¨ë¦¬ë¡œ ì²˜ë¦¬")
            st.write("âœ… ì‹¤ì‹œê°„ ì ì‘í˜• ì²­í¬ ì¡°ì ˆ")
            st.write("âœ… ì§€ëŠ¥í˜• ë©”ëª¨ë¦¬ ì••ì¶•")
            st.write("âœ… ë©€í‹°ë ˆë²¨ ìºì‹±")
            st.write("âœ… í˜„ìž¥ ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥")
    
    # ì²˜ë¦¬ ì‹œìž‘ ë²„íŠ¼
    if st.button("ðŸš€ ì°¨ì„¸ëŒ€ AI ë¶„ì„ ì‹œìž‘", type="primary", use_container_width=True):
        
        # API í‚¤ í™•ì¸
        api_keys = {}
        if openai_key:
            api_keys["openai"] = openai_key
        if anthropic_key:
            api_keys["anthropic"] = anthropic_key
        if google_key:
            api_keys["google"] = google_key
        
        if not api_keys:
            st.error("âŒ ìµœì†Œ í•˜ë‚˜ì˜ API í‚¤ë¥¼ ìž…ë ¥í•´ì£¼ì„¸ìš”")
            return
        
        if not uploaded_files:
            st.error("âŒ ì²˜ë¦¬í•  íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”")
            return
        
        # íŒŒì¼ ìž„ì‹œ ì €ìž¥
        temp_files = []
        for uploaded_file in uploaded_files:
            temp_path = f"/tmp/{uploaded_file.name}"
            with open(temp_path, "wb") as f:
                f.write(uploaded_file.read())
            temp_files.append(temp_path)
        
        # ì²˜ë¦¬ ì‹œìž‘
        with st.spinner("ðŸ”¥ ì°¨ì„¸ëŒ€ AI ë¶„ì„ ì¤‘..."):
            
            try:
                # ë°ëª¨ ì»¨íŠ¸ë¡¤ëŸ¬ ì´ˆê¸°í™”
                controller = NextGenDemoController()
                controller.setup_engines(device_type, max_memory_mb)
                
                # ì§„í–‰ë¥  í‘œì‹œ
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                async def progress_callback(message):
                    status_text.text(message)
                
                # ë¹„ë™ê¸° ì²˜ë¦¬ ì‹¤í–‰
                async def run_processing():
                    return await controller.process_files_with_nextgen_ai(
                        temp_files,
                        api_keys,
                        analysis_focus,
                        progress_callback
                    )
                
                # ê²°ê³¼ ë°›ê¸°
                result = asyncio.run(run_processing())
                
                progress_bar.progress(1.0)
                status_text.text("âœ… ì²˜ë¦¬ ì™„ë£Œ!")
                
                # ê²°ê³¼ í‘œì‹œ
                st.success("ðŸŽ‰ ì°¨ì„¸ëŒ€ AI ë¶„ì„ ì™„ë£Œ!")
                
                # ì„±ëŠ¥ ë©”íŠ¸ë¦­
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    st.metric("ë©”ëª¨ë¦¬ í”¼í¬", f"{result['demo_stats']['memory_peak']:.1f}MB")
                
                with col2:
                    st.metric("ì²˜ë¦¬ ì‹œê°„", f"{result['demo_stats']['processing_time']:.1f}ì´ˆ")
                
                with col3:
                    st.metric("AI í˜¸ì¶œ", f"{result['demo_stats']['ai_calls']}íšŒ")
                
                with col4:
                    st.metric("ì²˜ë¦¬ íŒŒì¼", f"{result['demo_stats']['files_processed']}ê°œ")
                
                # AI í•©ì˜ ê²°ê³¼
                st.subheader("ðŸ¤– AI ëª¨ë¸ í•©ì˜ ê²°ê³¼")
                consensus = result.get("ai_consensus", {})
                
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("í•©ì˜ ì ìˆ˜", f"{consensus.get('consensus_score', 0):.2f}")
                with col2:
                    st.metric("ëª¨ë¸ ì‚¬ìš©", f"{consensus.get('models_used', 0)}ê°œ")
                
                # í•œêµ­ì–´ ìš”ì•½
                st.subheader("ðŸ‡°ðŸ‡· í•œêµ­ì–´ ê²½ì˜ì§„ ìš”ì•½")
                korean_summary = result.get("korean_executive_summary", {})
                
                if korean_summary.get("executive_summary"):
                    st.write(korean_summary["executive_summary"])
                
                # ì‹¤í–‰ ê°€ëŠ¥í•œ ì¶”ì²œì‚¬í•­
                st.subheader("ðŸ’¼ ì‹¤í–‰ ê°€ëŠ¥í•œ ì¶”ì²œì‚¬í•­")
                recommendations = result.get("actionable_recommendations", [])
                
                for i, rec in enumerate(recommendations, 1):
                    st.write(f"{i}. {rec}")
                
                # ìƒì„¸ ê²°ê³¼ (ì ‘ì„ ìˆ˜ ìžˆëŠ” ì„¹ì…˜)
                with st.expander("ðŸ“Š ìƒì„¸ ë¶„ì„ ê²°ê³¼"):
                    st.json(result)
                
                # ê²°ê³¼ ë‹¤ìš´ë¡œë“œ
                result_json = json.dumps(result, ensure_ascii=False, indent=2)
                st.download_button(
                    "ðŸ“¥ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ (JSON)",
                    result_json,
                    file_name=f"nextgen_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                    mime="application/json"
                )
                
            except Exception as e:
                st.error(f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                st.exception(e)
            
            finally:
                # ìž„ì‹œ íŒŒì¼ ì •ë¦¬
                for temp_file in temp_files:
                    try:
                        os.unlink(temp_file)
                    except:
                        pass

# CLI ëª¨ë“œ
async def main_cli():
    """CLI ëª¨ë“œ ì‹¤í–‰"""
    print("ðŸ”¥ ì°¨ì„¸ëŒ€ ë©€í‹°ëª¨ë‹¬ AI í†µí•© ë°ëª¨ v2.2")
    print("=" * 50)
    
    # ì„¤ì •
    device_type = input("ë””ë°”ì´ìŠ¤ íƒ€ìž… (mobile/laptop/server) [laptop]: ") or "laptop"
    
    # API í‚¤ ìž…ë ¥
    api_keys = {}
    
    openai_key = input("OpenAI API Key (ì„ íƒì‚¬í•­): ")
    if openai_key:
        api_keys["openai"] = openai_key
    
    anthropic_key = input("Anthropic API Key (ì„ íƒì‚¬í•­): ")
    if anthropic_key:
        api_keys["anthropic"] = anthropic_key
    
    google_key = input("Google API Key (ì„ íƒì‚¬í•­): ")
    if google_key:
        api_keys["google"] = google_key
    
    if not api_keys:
        print("âŒ ìµœì†Œ í•˜ë‚˜ì˜ API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        return
    
    # íŒŒì¼ ê²½ë¡œ ìž…ë ¥
    file_paths = []
    print("\nì²˜ë¦¬í•  íŒŒì¼ ê²½ë¡œë¥¼ ìž…ë ¥í•˜ì„¸ìš” (ì—”í„°ë¡œ ì™„ë£Œ):")
    
    while True:
        file_path = input("íŒŒì¼ ê²½ë¡œ: ")
        if not file_path:
            break
        if os.path.exists(file_path):
            file_paths.append(file_path)
            print(f"âœ… ì¶”ê°€ë¨: {file_path}")
        else:
            print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
    
    if not file_paths:
        print("âŒ ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì²˜ë¦¬ ì‹œìž‘
    print(f"\nðŸš€ {len(file_paths)}ê°œ íŒŒì¼ ì²˜ë¦¬ ì‹œìž‘...")
    
    try:
        controller = NextGenDemoController()
        controller.setup_engines(device_type)
        
        result = await controller.process_files_with_nextgen_ai(
            file_paths,
            api_keys,
            "jewelry_business"
        )
        
        print("âœ… ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"ðŸ“Š ì„±ëŠ¥ ìš”ì•½:")
        print(f"  - ë©”ëª¨ë¦¬ í”¼í¬: {result['demo_stats']['memory_peak']:.1f}MB")
        print(f"  - ì²˜ë¦¬ ì‹œê°„: {result['demo_stats']['processing_time']:.1f}ì´ˆ")
        print(f"  - AI í˜¸ì¶œ: {result['demo_stats']['ai_calls']}íšŒ")
        
        # ê²°ê³¼ ì €ìž¥
        output_file = f"nextgen_result_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        print(f"ðŸ’¾ ê²°ê³¼ ì €ìž¥: {output_file}")
        
    except Exception as e:
        print(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(level=logging.INFO)
    
    # ì‹¤í–‰ ëª¨ë“œ ì„ íƒ
    if len(sys.argv) > 1 and sys.argv[1] == "--cli":
        # CLI ëª¨ë“œ
        asyncio.run(main_cli())
    else:
        # Streamlit UI ëª¨ë“œ
        main_streamlit_app()
