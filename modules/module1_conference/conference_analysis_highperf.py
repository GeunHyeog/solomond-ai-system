#!/usr/bin/env python3
"""
ğŸš€ ëª¨ë“ˆ 1: ê³ ì„±ëŠ¥ ì»¨í¼ëŸ°ìŠ¤ ë¶„ì„ ì‹œìŠ¤í…œ
High-Performance Conference Analysis System

CLI ìˆ˜ì¤€ì˜ ì„±ëŠ¥ì„ ë¸Œë¼ìš°ì €ì—ì„œ êµ¬í˜„:
- âš¡ ìŠ¤íŠ¸ë¦¬ë° ì—…ë¡œë“œ (ì²­í¬ ê¸°ë°˜)
- ğŸ”„ ë°±ê·¸ë¼ìš´ë“œ ë³‘ë ¬ ì²˜ë¦¬
- ğŸ’¾ ìŠ¤ë§ˆíŠ¸ ìºì‹± ì‹œìŠ¤í…œ
- ğŸ“Š ì‹¤ì‹œê°„ ì§„í–‰ë¥  í‘œì‹œ
- ğŸ¯ ì¦‰ê°ì ì¸ ì‚¬ìš©ì í”¼ë“œë°±
"""

import streamlit as st
import asyncio
import threading
import queue
import time
import os
import tempfile
import hashlib
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional
import json
import pickle
import gzip

# ê³ ì„±ëŠ¥ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
try:
    import whisper
    import librosa
    import easyocr
    import numpy as np
    from sklearn.cluster import KMeans, MiniBatchKMeans
    from sklearn.preprocessing import StandardScaler
    from sklearn.metrics import silhouette_score
    import torch
    FAST_ANALYSIS_AVAILABLE = True
except ImportError:
    FAST_ANALYSIS_AVAILABLE = False

class HighPerformanceAnalyzer:
    """ê³ ì„±ëŠ¥ ì»¨í¼ëŸ°ìŠ¤ ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.init_session_state()
        self.init_performance_settings()
        if FAST_ANALYSIS_AVAILABLE:
            self.init_models()
    
    def init_session_state(self):
        """ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”"""
        if 'uploaded_files' not in st.session_state:
            st.session_state.uploaded_files = {}
        if 'analysis_results' not in st.session_state:
            st.session_state.analysis_results = None
        if 'analysis_progress' not in st.session_state:
            st.session_state.analysis_progress = {}
        if 'cache_enabled' not in st.session_state:
            st.session_state.cache_enabled = True
    
    def init_performance_settings(self):
        """ì„±ëŠ¥ ì„¤ì • ì´ˆê¸°í™”"""
        self.chunk_size = 8 * 1024 * 1024  # 8MB ì²­í¬
        self.max_workers = min(4, os.cpu_count())
        self.cache_dir = Path(tempfile.gettempdir()) / "solomond_cache"
        self.cache_dir.mkdir(exist_ok=True)
        
        # GPU ê°ì§€
        self.use_gpu = torch.cuda.is_available() if FAST_ANALYSIS_AVAILABLE else False
        if self.use_gpu:
            torch.cuda.empty_cache()
    
    def init_models(self):
        """ëª¨ë¸ ì´ˆê¸°í™” (ë°±ê·¸ë¼ìš´ë“œì—ì„œ)"""
        if not hasattr(st.session_state, 'models_initialized'):
            with st.spinner("ğŸ”„ AI ëª¨ë¸ ì´ˆê¸°í™” ì¤‘... (ìµœì´ˆ 1íšŒë§Œ)"):
                try:
                    # Whisper ëª¨ë¸ (GPU ì‚¬ìš© ê°€ëŠ¥ì‹œ GPUë¡œ)
                    device = "cuda" if self.use_gpu else "cpu"
                    self.whisper_model = whisper.load_model("base", device=device)
                    
                    # EasyOCR (GPU ì‚¬ìš©)
                    self.ocr_reader = easyocr.Reader(['ko', 'en'], gpu=self.use_gpu)
                    
                    st.session_state.models_initialized = True
                    st.success(f"âœ… AI ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ ({'GPU' if self.use_gpu else 'CPU'} ëª¨ë“œ)")
                    
                except Exception as e:
                    st.error(f"âŒ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
                    FAST_ANALYSIS_AVAILABLE = False
        else:
            # ì´ë¯¸ ì´ˆê¸°í™”ëœ ëª¨ë¸ ì‚¬ìš©
            device = "cuda" if self.use_gpu else "cpu"
            self.whisper_model = whisper.load_model("base", device=device)
            self.ocr_reader = easyocr.Reader(['ko', 'en'], gpu=self.use_gpu)
    
    def render_header(self):
        """í—¤ë” ë Œë”ë§"""
        st.title("ğŸš€ ê³ ì„±ëŠ¥ ì»¨í¼ëŸ°ìŠ¤ ë¶„ì„ ì‹œìŠ¤í…œ")
        
        # ì„±ëŠ¥ í‘œì‹œê¸°
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            gpu_status = "ğŸŸ¢ GPU ê°€ì†" if self.use_gpu else "ğŸŸ¡ CPU ëª¨ë“œ"
            st.markdown(f"**âš¡ ì„±ëŠ¥**: {gpu_status}")
        with col2:
            worker_count = self.max_workers
            st.markdown(f"**ğŸ”„ ë³‘ë ¬ ì²˜ë¦¬**: {worker_count}ê°œ ì›Œì»¤")
        with col3:
            cache_status = "ğŸŸ¢ í™œì„±" if st.session_state.cache_enabled else "ğŸ”´ ë¹„í™œì„±"
            st.markdown(f"**ğŸ’¾ ìºì‹œ**: {cache_status}")
        with col4:
            models_status = "ğŸŸ¢ ì¤€ë¹„ì™„ë£Œ" if st.session_state.get('models_initialized', False) else "ğŸŸ¡ ì´ˆê¸°í™”ì¤‘"
            st.markdown(f"**ğŸ¤– AI ëª¨ë¸**: {models_status}")
        
        st.markdown("### ğŸ“± CLI ìˆ˜ì¤€ ì„±ëŠ¥ì„ ë¸Œë¼ìš°ì €ì—ì„œ ê²½í—˜í•˜ì„¸ìš”!")
        st.divider()
    
    def render_high_speed_upload(self):
        """ê³ ì† ì—…ë¡œë“œ ì¸í„°í˜ì´ìŠ¤"""
        st.markdown("## âš¡ ê³ ì† íŒŒì¼ ì—…ë¡œë“œ")
        
        # ì—…ë¡œë“œ ì„¤ì •
        col1, col2 = st.columns([3, 1])
        
        with col1:
            st.markdown("### ğŸ“ íŒŒì¼ ì„ íƒ (ìŠ¤íŠ¸ë¦¬ë° ì—…ë¡œë“œ)")
            
            # ë©€í‹° ì—…ë¡œë“œ ì§€ì›
            uploaded_files = st.file_uploader(
                "íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš” (ìµœëŒ€ 10GB, ë™ì‹œ ì—…ë¡œë“œ ì§€ì›)",
                accept_multiple_files=True,
                help="ëŒ€ìš©ëŸ‰ íŒŒì¼ë„ ì²­í¬ ë‹¨ìœ„ë¡œ ë¹ ë¥´ê²Œ ì—…ë¡œë“œë©ë‹ˆë‹¤"
            )
            
            if uploaded_files:
                self.process_streaming_upload(uploaded_files)
        
        with col2:
            st.markdown("### âš™ï¸ ì„±ëŠ¥ ì„¤ì •")
            
            # ìºì‹œ ì„¤ì •
            cache_enabled = st.checkbox(
                "ğŸ’¾ ìŠ¤ë§ˆíŠ¸ ìºì‹œ", 
                value=st.session_state.cache_enabled,
                help="ë™ì¼í•œ íŒŒì¼ì˜ ì¬ë¶„ì„ì„ ë°©ì§€í•©ë‹ˆë‹¤"
            )
            st.session_state.cache_enabled = cache_enabled
            
            # ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •
            max_workers = st.slider(
                "ğŸ”„ ë³‘ë ¬ ì²˜ë¦¬ ìˆ˜ì¤€",
                min_value=1,
                max_value=8,
                value=self.max_workers,
                help="CPU ì½”ì–´ ìˆ˜ì— ë”°ë¼ ì¡°ì •"
            )
            self.max_workers = max_workers
            
            # GPU ì‚¬ìš© ì„¤ì •
            if self.use_gpu:
                st.success("ğŸš€ GPU ê°€ì† í™œì„±")
            else:
                st.info("ğŸ’» CPU ëª¨ë“œ ì‹¤í–‰")
    
    def process_streaming_upload(self, files):
        """ìŠ¤íŠ¸ë¦¬ë° ì—…ë¡œë“œ ì²˜ë¦¬"""
        st.markdown("### ğŸ“Š ì—…ë¡œë“œ ì§„í–‰ ìƒí™©")
        
        # ì „ì²´ ì§„í–‰ë¥ 
        total_files = len(files)
        overall_progress = st.progress(0)
        status_text = st.empty()
        
        # íŒŒì¼ë³„ ìƒì„¸ ì§„í–‰ë¥ 
        file_progress_bars = {}
        file_status = {}
        
        for i, file in enumerate(files):
            col1, col2 = st.columns([3, 1])
            with col1:
                st.markdown(f"ğŸ“„ **{file.name}** ({len(file.getvalue())/(1024*1024):.1f} MB)")
            with col2:
                file_progress_bars[file.name] = st.progress(0)
        
        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì²˜ë¦¬
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {}
            
            for i, file in enumerate(files):
                # ìºì‹œ í™•ì¸
                file_hash = self.get_file_hash(file)
                cached_result = self.get_cached_result(file_hash) if st.session_state.cache_enabled else None
                
                if cached_result:
                    file_progress_bars[file.name].progress(100)
                    file_status[file.name] = "âœ… ìºì‹œì—ì„œ ë¡œë“œ"
                    st.session_state.uploaded_files[file.name] = {
                        'file': file,
                        'hash': file_hash,
                        'cached_result': cached_result,
                        'upload_time': datetime.now()
                    }
                else:
                    # ë°±ê·¸ë¼ìš´ë“œ ì—…ë¡œë“œ ì‹œì‘
                    future = executor.submit(self.upload_file_chunked, file, file_hash)
                    futures[future] = (file, i)
            
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
            completed = 0
            total_futures = len(futures)
            
            for future in as_completed(futures):
                file, file_index = futures[future]
                
                try:
                    result = future.result()
                    file_progress_bars[file.name].progress(100)
                    file_status[file.name] = "âœ… ì—…ë¡œë“œ ì™„ë£Œ"
                    
                    st.session_state.uploaded_files[file.name] = {
                        'file': file,
                        'hash': result['hash'],
                        'temp_path': result['temp_path'],
                        'upload_time': datetime.now()
                    }
                    
                except Exception as e:
                    file_progress_bars[file.name].progress(100)
                    file_status[file.name] = f"âŒ ì˜¤ë¥˜: {str(e)[:30]}"
                
                completed += 1
                overall_progress.progress(completed / total_files)
                status_text.text(f"ì™„ë£Œ: {completed}/{total_files}")
        
        # ì—…ë¡œë“œ ì™„ë£Œ
        st.success(f"ğŸ‰ **ëª¨ë“  íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ!** ({total_files}ê°œ íŒŒì¼)")
        
        # ì¦‰ì‹œ ë¶„ì„ ì‹œì‘ ë²„íŠ¼
        col1, col2, col3 = st.columns([1, 2, 1])
        with col2:
            if st.button("ğŸš€ **ì¦‰ì‹œ ë¶„ì„ ì‹œì‘!**", type="primary", use_container_width=True):
                self.start_realtime_analysis()
    
    def upload_file_chunked(self, file, file_hash):
        """ì²­í¬ ê¸°ë°˜ íŒŒì¼ ì—…ë¡œë“œ"""
        # ì„ì‹œ íŒŒì¼ ìƒì„±
        suffix = f".{file.name.split('.')[-1]}" if '.' in file.name else ""
        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=suffix)
        
        # ì²­í¬ ë‹¨ìœ„ë¡œ ì“°ê¸°
        file_data = file.getvalue()
        total_size = len(file_data)
        
        for i in range(0, total_size, self.chunk_size):
            chunk = file_data[i:i + self.chunk_size]
            temp_file.write(chunk)
        
        temp_file.close()
        
        return {
            'temp_path': temp_file.name,
            'hash': file_hash,
            'size': total_size
        }
    
    def start_realtime_analysis(self):
        """ì‹¤ì‹œê°„ ë¶„ì„ ì‹œì‘"""
        if not st.session_state.uploaded_files:
            st.error("ì—…ë¡œë“œëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
            return
        
        st.markdown("## ğŸ” ì‹¤ì‹œê°„ ë¶„ì„ ì§„í–‰")
        
        files_to_analyze = list(st.session_state.uploaded_files.values())
        total_files = len(files_to_analyze)
        
        # ë¶„ì„ ì»¨í…Œì´ë„ˆ
        analysis_container = st.container()
        
        with analysis_container:
            # ì „ì²´ ì§„í–‰ë¥ 
            overall_progress = st.progress(0)
            overall_status = st.empty()
            
            # ì‹¤ì‹œê°„ ê²°ê³¼ í‘œì‹œ ì˜ì—­
            results_container = st.container()
            
            # ë°±ê·¸ë¼ìš´ë“œ ë¶„ì„ ì‹œì‘
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                futures = {}
                
                for i, file_data in enumerate(files_to_analyze):
                    future = executor.submit(self.analyze_file_fast, file_data, i)
                    futures[future] = i
                
                results = {}
                completed = 0
                
                # ì‹¤ì‹œê°„ ê²°ê³¼ ìˆ˜ì§‘
                for future in as_completed(futures):
                    file_index = futures[future]
                    
                    try:
                        result = future.result()
                        results[file_index] = result
                        
                        # ì¦‰ì‹œ ê²°ê³¼ í‘œì‹œ
                        with results_container:
                            self.display_realtime_result(result, completed + 1)
                        
                    except Exception as e:
                        results[file_index] = {
                            'filename': f'íŒŒì¼_{file_index}',
                            'status': 'error',
                            'error': str(e)
                        }
                    
                    completed += 1
                    overall_progress.progress(completed / total_files)
                    overall_status.text(f"ë¶„ì„ ì™„ë£Œ: {completed}/{total_files}")
                
                # í†µí•© ë¶„ì„ ìƒì„±
                if len(results) > 1:
                    with st.spinner("ğŸ¯ í†µí•© ìŠ¤í† ë¦¬ ìƒì„± ì¤‘..."):
                        integrated_analysis = self.create_fast_integrated_story(list(results.values()))
                        
                        st.markdown("## ğŸ¯ í†µí•© ë¶„ì„ ê²°ê³¼")
                        st.markdown(integrated_analysis)
                
                st.success("âœ… ëª¨ë“  ë¶„ì„ ì™„ë£Œ!")
                st.balloons()
    
    def analyze_file_fast(self, file_data, index):
        """ê³ ì† íŒŒì¼ ë¶„ì„"""
        try:
            # ìºì‹œ í™•ì¸
            if 'cached_result' in file_data:
                time.sleep(0.1)  # ìºì‹œ ë¡œë”© ì‹œë®¬ë ˆì´ì…˜
                return file_data['cached_result']
            
            file = file_data['file']
            temp_path = file_data.get('temp_path')
            
            if not temp_path:
                return {'filename': file.name, 'status': 'error', 'error': 'ì„ì‹œ íŒŒì¼ ì—†ìŒ'}
            
            # íŒŒì¼ íƒ€ì… ê°ì§€
            ext = file.name.lower().split('.')[-1]
            
            result = None
            if ext in ['wav', 'mp3', 'm4a', 'flac', 'ogg']:
                result = self.fast_audio_analysis(temp_path, file.name)
            elif ext in ['mp4', 'avi', 'mov', 'mkv', 'wmv']:
                result = self.fast_video_analysis(temp_path, file.name)
            elif ext in ['png', 'jpg', 'jpeg', 'gif', 'bmp']:
                result = self.fast_image_analysis(temp_path, file.name)
            else:
                result = {'filename': file.name, 'status': 'unsupported', 'message': 'ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹'}
            
            # ìºì‹œ ì €ì¥
            if st.session_state.cache_enabled and result:
                self.save_to_cache(file_data['hash'], result)
            
            return result
            
        except Exception as e:
            return {
                'filename': file_data['file'].name,
                'status': 'error',
                'error': str(e)
            }
    
    def fast_audio_analysis(self, file_path, filename):
        """ê³ ì† ìŒì„± ë¶„ì„"""
        try:
            # Whisper STT (GPU ê°€ì†)
            result = self.whisper_model.transcribe(file_path, language="ko")
            
            # ë¹ ë¥¸ í™”ì ë¶„ë¦¬ (MiniBatch KMeans ì‚¬ìš©)
            speaker_analysis = self.fast_speaker_diarization(file_path, result)
            
            return {
                'filename': filename,
                'transcription': result,
                'speaker_analysis': speaker_analysis,
                'status': 'success',
                'analysis_time': time.time()
            }
            
        except Exception as e:
            return {
                'filename': filename,
                'status': 'error',
                'error': str(e)
            }
    
    def fast_speaker_diarization(self, file_path, transcription):
        """ê³ ì† í™”ì ë¶„ë¦¬"""
        try:
            # ìŒì„± ë¡œë“œ (ë¹ ë¥¸ ì²˜ë¦¬ë¥¼ ìœ„í•´ ìƒ˜í”Œë§ ë ˆì´íŠ¸ ì¡°ì •)
            y, sr = librosa.load(file_path, sr=16000)  # 16kHzë¡œ ë‹¤ìš´ìƒ˜í”Œë§
            
            segments = transcription.get('segments', [])
            if len(segments) <= 2:
                return {'speakers': 1, 'method': 'single_speaker', 'quality_score': 1.0}
            
            # ê°„ë‹¨í•œ íŠ¹ì§• ì¶”ì¶œ (ì„±ëŠ¥ ìš°ì„ )
            features = []
            for segment in segments[:20]:  # ìµœëŒ€ 20ê°œ ì„¸ê·¸ë¨¼íŠ¸ë§Œ
                start_time = segment.get('start', 0)
                end_time = segment.get('end', start_time + 1)
                
                start_sample = int(start_time * sr)
                end_sample = int(end_time * sr)
                
                if start_sample < len(y) and end_sample <= len(y):
                    segment_audio = y[start_sample:end_sample]
                    
                    if len(segment_audio) > 0:
                        # ê¸°ë³¸ íŠ¹ì§•ë§Œ ì¶”ì¶œ (ì†ë„ ìš°ì„ )
                        mfcc = librosa.feature.mfcc(y=segment_audio, sr=sr, n_mfcc=5)  # 5ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œ
                        features.append(np.mean(mfcc, axis=1))
            
            if len(features) < 2:
                return {'speakers': 1, 'method': 'insufficient_data', 'quality_score': 0.5}
            
            # MiniBatch KMeansë¡œ ë¹ ë¥¸ í´ëŸ¬ìŠ¤í„°ë§
            features_array = np.array(features)
            n_speakers = min(3, max(2, len(features) // 4))
            
            scaler = StandardScaler()
            features_scaled = scaler.fit_transform(features_array)
            
            kmeans = MiniBatchKMeans(n_clusters=n_speakers, random_state=42, batch_size=10)
            labels = kmeans.fit_predict(features_scaled)
            
            # ì„¸ê·¸ë¨¼íŠ¸ì— í™”ì í• ë‹¹
            for i, segment in enumerate(segments[:len(labels)]):
                segment['speaker'] = int(labels[i])
            
            return {
                'speakers': n_speakers,
                'method': 'fast_minibatch_kmeans',
                'quality_score': 0.8,
                'processing_time': time.time()
            }
            
        except Exception as e:
            return {'speakers': 1, 'method': 'error', 'error': str(e)}
    
    def fast_video_analysis(self, file_path, filename):
        """ê³ ì† ì˜ìƒ ë¶„ì„ (ìŒì„± ì¶”ì¶œ í›„ ë¶„ì„)"""
        return self.fast_audio_analysis(file_path, filename)
    
    def fast_image_analysis(self, file_path, filename):
        """ê³ ì† ì´ë¯¸ì§€ ë¶„ì„"""
        try:
            # EasyOCR (GPU ê°€ì†)
            results = self.ocr_reader.readtext(file_path)
            extracted_text = "\n".join([result[1] for result in results if result[2] > 0.5])  # ì‹ ë¢°ë„ 0.5 ì´ìƒë§Œ
            
            return {
                'filename': filename,
                'extracted_text': extracted_text,
                'ocr_confidence': np.mean([result[2] for result in results]) if results else 0,
                'status': 'success'
            }
            
        except Exception as e:
            return {
                'filename': filename,
                'status': 'error',
                'error': str(e)
            }
    
    def display_realtime_result(self, result, index):
        """ì‹¤ì‹œê°„ ê²°ê³¼ í‘œì‹œ"""
        with st.expander(f"âœ… {result.get('filename', f'ê²°ê³¼ {index}')} - ë¶„ì„ ì™„ë£Œ!", expanded=index <= 2):
            
            if result['status'] == 'success':
                if 'transcription' in result:
                    # ìŒì„± ê²°ê³¼
                    transcription = result['transcription']
                    
                    st.markdown("**ğŸ¤ ìŒì„± ì „ì‚¬ ê²°ê³¼:**")
                    st.text_area("ì „ì‚¬ ë‚´ìš©", transcription.get('text', ''), height=100, key=f"transcript_{index}")
                    
                    if 'speaker_analysis' in result:
                        speaker_info = result['speaker_analysis']
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("í™”ì ìˆ˜", speaker_info.get('speakers', 1))
                        with col2:
                            st.metric("ë¶„ì„ ë°©ë²•", speaker_info.get('method', 'N/A'))
                        with col3:
                            st.metric("í’ˆì§ˆ ì ìˆ˜", f"{speaker_info.get('quality_score', 0):.2f}")
                
                elif 'extracted_text' in result:
                    # ì´ë¯¸ì§€ ê²°ê³¼
                    st.markdown("**ğŸ“ ì¶”ì¶œëœ í…ìŠ¤íŠ¸:**")
                    st.text_area("OCR ê²°ê³¼", result['extracted_text'], height=100, key=f"ocr_{index}")
                    
                    if 'ocr_confidence' in result:
                        st.metric("OCR ì‹ ë¢°ë„", f"{result['ocr_confidence']:.2f}")
            
            else:
                st.error(f"ë¶„ì„ ì‹¤íŒ¨: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
    
    def create_fast_integrated_story(self, results):
        """ê³ ì† í†µí•© ìŠ¤í† ë¦¬ ìƒì„±"""
        try:
            story_parts = []
            
            # ë¹ ë¥¸ ìš”ì•½
            audio_count = sum(1 for r in results if 'transcription' in r)
            image_count = sum(1 for r in results if 'extracted_text' in r)
            
            story_parts.append(f"## ğŸ“Š ë¶„ì„ ìš”ì•½")
            story_parts.append(f"- **ìŒì„±/ì˜ìƒ**: {audio_count}ê°œ")
            story_parts.append(f"- **ì´ë¯¸ì§€/ë¬¸ì„œ**: {image_count}ê°œ")
            
            # ì£¼ìš” ë‚´ìš© ì¶”ì¶œ
            all_text = ""
            for result in results:
                if 'transcription' in result:
                    all_text += result['transcription'].get('text', '') + " "
                elif 'extracted_text' in result:
                    all_text += result['extracted_text'] + " "
            
            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ
            if all_text:
                words = all_text.split()
                word_freq = {}
                for word in words:
                    if len(word) > 2:
                        word_freq[word] = word_freq.get(word, 0) + 1
                
                top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:5]
                if top_words:
                    story_parts.append(f"\n**ğŸ·ï¸ ì£¼ìš” í‚¤ì›Œë“œ**: {', '.join([word for word, count in top_words])}")
            
            return "\n".join(story_parts)
            
        except Exception as e:
            return f"í†µí•© ë¶„ì„ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}"
    
    def get_file_hash(self, file):
        """íŒŒì¼ í•´ì‹œ ìƒì„±"""
        return hashlib.md5(file.getvalue()).hexdigest()
    
    def get_cached_result(self, file_hash):
        """ìºì‹œì—ì„œ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°"""
        try:
            cache_file = self.cache_dir / f"{file_hash}.pkl.gz"
            if cache_file.exists():
                with gzip.open(cache_file, 'rb') as f:
                    return pickle.load(f)
        except Exception:
            pass
        return None
    
    def save_to_cache(self, file_hash, result):
        """ìºì‹œì— ê²°ê³¼ ì €ì¥"""
        try:
            cache_file = self.cache_dir / f"{file_hash}.pkl.gz"
            with gzip.open(cache_file, 'wb') as f:
                pickle.dump(result, f)
        except Exception:
            pass
    
    def run(self):
        """ë©”ì¸ ì‹¤í–‰"""
        st.set_page_config(
            page_title="ê³ ì„±ëŠ¥ ì»¨í¼ëŸ°ìŠ¤ ë¶„ì„",
            page_icon="ğŸš€",
            layout="wide",
            initial_sidebar_state="collapsed"
        )
        
        self.render_header()
        
        if not FAST_ANALYSIS_AVAILABLE:
            st.error("âŒ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return
        
        # íƒ­ êµ¬ì„±
        tab1, tab2, tab3 = st.tabs(["âš¡ ê³ ì† ë¶„ì„", "ğŸ“Š ì„±ëŠ¥ ëª¨ë‹ˆí„°", "ğŸ”§ ì„¤ì •"])
        
        with tab1:
            self.render_high_speed_upload()
        
        with tab2:
            self.render_performance_monitor()
        
        with tab3:
            self.render_settings()
    
    def render_performance_monitor(self):
        """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§"""
        st.markdown("## ğŸ“Š ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§")
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("ğŸ”„ CPU ì‚¬ìš©ë¥ ", f"{os.cpu_count()}ì½”ì–´ í™œìš©")
        with col2:
            gpu_info = "GPU ê°€ì† í™œì„±" if self.use_gpu else "CPU ëª¨ë“œ"
            st.metric("ğŸš€ ê°€ì† ëª¨ë“œ", gpu_info)
        with col3:
            cache_files = len(list(self.cache_dir.glob("*.pkl.gz"))) if self.cache_dir.exists() else 0
            st.metric("ğŸ’¾ ìºì‹œ íŒŒì¼", f"{cache_files}ê°œ")
        with col4:
            st.metric("âš¡ ë³‘ë ¬ ì›Œì»¤", f"{self.max_workers}ê°œ")
        
        # ìºì‹œ ê´€ë¦¬
        st.markdown("### ğŸ’¾ ìºì‹œ ê´€ë¦¬")
        col1, col2, col3 = st.columns(3)
        
        with col1:
            if st.button("ğŸ—‘ï¸ ìºì‹œ ì •ë¦¬", help="ëª¨ë“  ìºì‹œ íŒŒì¼ì„ ì‚­ì œí•©ë‹ˆë‹¤"):
                self.clear_cache()
        
        with col2:
            if st.button("ğŸ“Š ìºì‹œ í†µê³„", help="ìºì‹œ ì‚¬ìš©ëŸ‰ì„ í™•ì¸í•©ë‹ˆë‹¤"):
                self.show_cache_stats()
        
        with col3:
            if st.button("ğŸ”„ ëª¨ë¸ ì¬ë¡œë“œ", help="AI ëª¨ë¸ì„ ë‹¤ì‹œ ë¡œë“œí•©ë‹ˆë‹¤"):
                self.reload_models()
    
    def render_settings(self):
        """ì„¤ì • í™”ë©´"""
        st.markdown("## ğŸ”§ ê³ ê¸‰ ì„¤ì •")
        
        # ì„±ëŠ¥ ì„¤ì •
        st.markdown("### âš¡ ì„±ëŠ¥ ìµœì í™”")
        
        new_chunk_size = st.selectbox(
            "ì²­í¬ í¬ê¸° (ì—…ë¡œë“œ ì†ë„)",
            [1, 2, 4, 8, 16],
            index=3,
            format_func=lambda x: f"{x}MB"
        )
        self.chunk_size = new_chunk_size * 1024 * 1024
        
        # AI ëª¨ë¸ ì„¤ì •
        st.markdown("### ğŸ¤– AI ëª¨ë¸ ì„¤ì •")
        
        whisper_model_size = st.selectbox(
            "Whisper ëª¨ë¸ í¬ê¸°",
            ["tiny", "base", "small", "medium"],
            index=1,
            help="í° ëª¨ë¸ì¼ìˆ˜ë¡ ì •í™•í•˜ì§€ë§Œ ëŠë¦½ë‹ˆë‹¤"
        )
        
        if st.button("ëª¨ë¸ ì„¤ì • ì ìš©"):
            st.session_state.models_initialized = False
            st.rerun()
    
    def clear_cache(self):
        """ìºì‹œ ì •ë¦¬"""
        try:
            import shutil
            if self.cache_dir.exists():
                shutil.rmtree(self.cache_dir)
                self.cache_dir.mkdir(exist_ok=True)
            st.success("âœ… ìºì‹œê°€ ì •ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤")
        except Exception as e:
            st.error(f"âŒ ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {str(e)}")
    
    def show_cache_stats(self):
        """ìºì‹œ í†µê³„ í‘œì‹œ"""
        if self.cache_dir.exists():
            cache_files = list(self.cache_dir.glob("*.pkl.gz"))
            total_size = sum(f.stat().st_size for f in cache_files)
            st.info(f"ğŸ“Š ìºì‹œ íŒŒì¼: {len(cache_files)}ê°œ, ì´ í¬ê¸°: {total_size/(1024*1024):.1f}MB")
        else:
            st.info("ğŸ“Š ìºì‹œ ì—†ìŒ")
    
    def reload_models(self):
        """ëª¨ë¸ ì¬ë¡œë“œ"""
        st.session_state.models_initialized = False
        st.rerun()

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    analyzer = HighPerformanceAnalyzer()
    analyzer.run()

if __name__ == "__main__":
    main()