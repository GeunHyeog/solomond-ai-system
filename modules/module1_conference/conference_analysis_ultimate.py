#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸš€ ëª¨ë“ˆ 1: ê¶ê·¹ ì»¨í¼ëŸ°ìŠ¤ ë¶„ì„ ì‹œìŠ¤í…œ
Ultimate Conference Analysis System

ğŸ¯ ëª¨ë“  ê¸°ëŠ¥ í†µí•© + ê°•í™”:
- ğŸ”¥ í„°ë³´ ì„±ëŠ¥ (5ë°° ë¹ ë¥¸ ì—…ë¡œë“œ + 3ë°° ë¹ ë¥¸ ë¶„ì„)  
- ğŸŒ URL ë‹¤ìš´ë¡œë“œ (YouTube, ì›¹í˜ì´ì§€, ë¬¸ì„œ)
- ğŸ¬ ë¹„ë””ì˜¤ í™”ë©´ ì¸ì‹ (3ê°€ì§€ ëª¨ë“œ)
- ğŸ’¾ ìŠ¤ë§ˆíŠ¸ ìºì‹œ (ì¤‘ë³µ ë¶„ì„ ë°©ì§€)
- ğŸ›¡ï¸ ë„¤íŠ¸ì›Œí¬ ì•ˆì •ì„± (AxiosError ë°©ì§€)
- ğŸ“Š ê³ ìš©ëŸ‰ íŒŒì¼ (5GB+ ì§€ì›)
- âš¡ GPU/CPU ìë™ ìµœì í™”
- ğŸ­ ê³ í’ˆì§ˆ í™”ì ë¶„ë¦¬ (CLI ìˆ˜ì¤€)
- ğŸ” ì‹¤ì‹œê°„ ì§„í–‰ë¥  ì¶”ì 
- ğŸ“ˆ ë‹¤ì¤‘ íŒŒì¼ ë°°ì¹˜ ì²˜ë¦¬
- ğŸŒ Windows ì¸ì½”ë”© ì™„ì „ í•´ê²°
"""

# Streamlit ëŒ€ìš©ëŸ‰ íŒŒì¼ ì—…ë¡œë“œ ì„¤ì • (ìµœìš°ì„  ì ìš©)
import os
os.environ['STREAMLIT_SERVER_MAX_UPLOAD_SIZE'] = '10240'  # 10GB
os.environ['STREAMLIT_SERVER_MAX_MESSAGE_SIZE'] = '10240'  # 10GB

import streamlit as st
import sys

# Streamlit ì„¤ì • (ì¤‘ë³µ ë°©ì§€)
try:
    st.set_page_config(page_title="ê¶ê·¹ ì»¨í¼ëŸ°ìŠ¤ ë¶„ì„", layout="wide")
except:
    pass  # ì´ë¯¸ ì„¤ì •ëœ ê²½ìš° ë¬´ì‹œ

# Windows ì¸ì½”ë”© ë¬¸ì œ ì™„ì „ í•´ê²°
if sys.platform.startswith('win'):
    os.environ['PYTHONIOENCODING'] = 'utf-8'
    sys.stdout.reconfigure(encoding='utf-8', errors='replace')
    sys.stderr.reconfigure(encoding='utf-8', errors='replace')

import tempfile
import time
import hashlib
import threading
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional
import json
import pickle
import gzip
import io
import re

# ì‹œìŠ¤í…œ ì´ˆê¸°í™” ê´€ë¦¬ì import
sys.path.append(str(Path(__file__).parent.parent.parent / 'core'))
try:
    from system_initialization_manager import global_init_manager, register_system, get_system, show_performance_status
    INIT_MANAGER_AVAILABLE = True
except ImportError:
    INIT_MANAGER_AVAILABLE = False

# ğŸ¯ ë‹¤ê°ì  í™”ì ë¶„ë¦¬ ì‹œìŠ¤í…œë“¤ í†µí•© (ì´ˆê¸°í™” ì¤‘ë³µ ë°©ì§€)
_speaker_diarization_initialized = False
SPEAKER_DIARIZATION_AVAILABLE = False

if not _speaker_diarization_initialized:
    try:
        import sys
        import os
        sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', 'core'))
        from realtime_speaker_diarization import RealtimeSpeakerDiarization
        SPEAKER_DIARIZATION_AVAILABLE = True
        _speaker_diarization_initialized = True
        # ë¡œê·¸ëŠ” í•œ ë²ˆë§Œ ì¶œë ¥
        if 'ultimate_system_loaded' not in st.session_state:
            st.session_state['speaker_diarization_loaded'] = True
    except ImportError as e:
        SPEAKER_DIARIZATION_AVAILABLE = False
        _speaker_diarization_initialized = True

# ğŸ¬ ë©€í‹°ëª¨ë‹¬ í™”ì ë¶„ë¦¬ ì‹œìŠ¤í…œ ì¶”ê°€ í†µí•© (ì„±ëŠ¥ ìµœì í™” - ì¤‘ë³µ ì´ˆê¸°í™” ë°©ì§€)
_multimodal_initialized = False
MULTIMODAL_SPEAKER_AVAILABLE = False

if not _multimodal_initialized:
    try:
        # ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ ì¶”ê°€
        root_dir = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
        sys.path.insert(0, root_dir)
        
        from multimodal_speaker_diarization import MultimodalSpeakerDiarization
        from enhanced_multimodal_speaker_diarization import EnhancedMultimodalSpeakerDiarization
        MULTIMODAL_SPEAKER_AVAILABLE = True
        _multimodal_initialized = True
        # ì„±ê³µ ë©”ì‹œì§€ëŠ” í•œ ë²ˆë§Œ í‘œì‹œ
        if 'ultimate_system_loaded' not in st.session_state:
            st.session_state['multimodal_loaded'] = True
    except ImportError as e:
        MULTIMODAL_SPEAKER_AVAILABLE = False
        _multimodal_initialized = True
        # ì—ëŸ¬ëŠ” ì„¸ì…˜ì— í•œ ë²ˆë§Œ ì €ì¥
        if 'multimodal_error' not in st.session_state:
            st.session_state['multimodal_error'] = str(e)

# ì‹œìŠ¤í…œ ì´ˆê¸°í™” ê´€ë¦¬ìë¥¼ í†µí•œ ìµœì í™”ëœ ì¸ìŠ¤í„´ìŠ¤ ê´€ë¦¬
if INIT_MANAGER_AVAILABLE:
    # ì§€ì—° ë¡œë”©ìœ¼ë¡œ ì‹œìŠ¤í…œë“¤ ë“±ë¡
    if SPEAKER_DIARIZATION_AVAILABLE:
        register_system('speaker_diarization', 
                       lambda: RealtimeSpeakerDiarization(), 
                       lazy=True)
    
    if MULTIMODAL_SPEAKER_AVAILABLE:
        register_system('multimodal_speaker', 
                       lambda: MultimodalSpeakerDiarization(), 
                       lazy=True)
        register_system('enhanced_multimodal_speaker', 
                       lambda: EnhancedMultimodalSpeakerDiarization(), 
                       lazy=True)

# ìµœì í™”ëœ ì ‘ê·¼ í•¨ìˆ˜ë“¤
def get_speaker_diarization():
    """í™”ì ë¶„ë¦¬ ì‹œìŠ¤í…œ íšë“ (ìµœì í™”)"""
    if INIT_MANAGER_AVAILABLE:
        return get_system('speaker_diarization')
    return None

def get_multimodal_speaker():
    """ë©€í‹°ëª¨ë‹¬ í™”ì ë¶„ë¦¬ ì‹œìŠ¤í…œ íšë“ (ìµœì í™”)"""
    if INIT_MANAGER_AVAILABLE:
        return get_system('multimodal_speaker')
    return None

def get_enhanced_multimodal_speaker():
    """í–¥ìƒëœ ë©€í‹°ëª¨ë‹¬ í™”ì ë¶„ë¦¬ ì‹œìŠ¤í…œ íšë“ (ìµœì í™”)"""
    if INIT_MANAGER_AVAILABLE:
        return get_system('enhanced_multimodal_speaker')
    return None

# ê³ ì„±ëŠ¥ ë¼ì´ë¸ŒëŸ¬ë¦¬ (ëª¨ë“  ê¸°ëŠ¥ í¬í•¨) - ì¤‘ë³µ ë¡œë”© ë°©ì§€
_ultimate_libs_loaded = False
ULTIMATE_AVAILABLE = False

if not _ultimate_libs_loaded:
    try:
        import whisper
        import librosa
        from sklearn.cluster import KMeans, MiniBatchKMeans
        from sklearn.preprocessing import StandardScaler
        from sklearn.decomposition import PCA
        from sklearn.metrics import silhouette_score
        import easyocr
        import numpy as np
        import torch
        import cv2
        ULTIMATE_AVAILABLE = True
        _ultimate_libs_loaded = True
        # ì„±ê³µ ë¡œë”©ì€ ì„¸ì…˜ì— ê¸°ë¡
        if 'ultimate_libs_status' not in st.session_state:
            st.session_state['ultimate_libs_status'] = 'loaded'
    except ImportError as e:
        ULTIMATE_AVAILABLE = False
        _ultimate_libs_loaded = True
        if 'ultimate_libs_error' not in st.session_state:
            st.session_state['ultimate_libs_error'] = str(e)

# URL ë‹¤ìš´ë¡œë“œ ë¼ì´ë¸ŒëŸ¬ë¦¬ - ì¤‘ë³µ ë¡œë”© ë°©ì§€
_url_libs_loaded = False
URL_DOWNLOAD_AVAILABLE = False

if not _url_libs_loaded:
    try:
        import requests
        from bs4 import BeautifulSoup
        import yt_dlp
        URL_DOWNLOAD_AVAILABLE = True
        _url_libs_loaded = True
    except ImportError:
        URL_DOWNLOAD_AVAILABLE = False
        _url_libs_loaded = True

# ê¸°ì¡´ ë¶„ì„ ì—”ì§„ import - ì¤‘ë³µ ë¡œë”© ë°©ì§€
_analysis_engine_loaded = False
ANALYSIS_ENGINE_AVAILABLE = False

if not _analysis_engine_loaded:
    sys.path.append(str(Path(__file__).parent.parent.parent))
    try:
        from modules.module1_conference.conference_analysis import ConferenceAnalysisSystem
        ANALYSIS_ENGINE_AVAILABLE = True
        _analysis_engine_loaded = True
    except ImportError:
        ANALYSIS_ENGINE_AVAILABLE = False
        _analysis_engine_loaded = True

class UltimateConferenceAnalyzer:
    """ê¶ê·¹ì˜ ì»¨í¼ëŸ°ìŠ¤ ë¶„ì„ê¸° - ëª¨ë“  ê¸°ëŠ¥ í†µí•© + ê°•í™” (ì‹±ê¸€í†¤ íŒ¨í„´)"""
    
    _instance = None
    _initialized = False
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(UltimateConferenceAnalyzer, cls).__new__(cls)
        return cls._instance
    
    def __init__(self):
        # ì‹±ê¸€í†¤ì´ë¯€ë¡œ í•œ ë²ˆë§Œ ì´ˆê¸°í™”
        if not UltimateConferenceAnalyzer._initialized:
            self.init_session_state()
            self.init_cache_system()
            self.init_gpu_system()
            self.init_ai_models()
            # ë¶„ì„ ì—”ì§„ë„ ì§€ì—° ë¡œë”©
            self.analysis_engine = None
            UltimateConferenceAnalyzer._initialized = True
    
    def init_session_state(self):
        """ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” - ëª¨ë“  ê¸°ëŠ¥ ì§€ì›"""
        defaults = {
            'uploaded_files': [],
            'analysis_results': None,
            'current_step': 1,
            'analysis_progress': 0,
            'analysis_status': 'ready',
            'error_count': 0,
            'network_stable': True,
            'cache_hits': 0,
            'total_analyses': 0,
            'gpu_available': torch.cuda.is_available() if ULTIMATE_AVAILABLE else False,
            'processing_mode': 'auto',
            'video_analysis_mode': 'screen_included',
            'speaker_count': 'auto',
            'language': 'auto'
        }
        
        for key, default_value in defaults.items():
            if key not in st.session_state:
                st.session_state[key] = default_value
    
    def init_cache_system(self):
        """ìŠ¤ë§ˆíŠ¸ ìºì‹œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        self.cache_dir = Path("cache/ultimate_analysis")
        self.cache_dir.mkdir(parents=True, exist_ok=True)
    
    def init_gpu_system(self):
        """GPU/CPU ìë™ ìµœì í™” ì‹œìŠ¤í…œ"""
        if ULTIMATE_AVAILABLE and torch.cuda.is_available():
            self.device = "cuda"
            torch.backends.cudnn.benchmark = True
        else:
            self.device = "cpu"
    
    def init_ai_models(self):
        """AI ëª¨ë¸ë“¤ì„ ì§€ì—° ë¡œë”©ìœ¼ë¡œ ì´ˆê¸°í™” (ì„±ëŠ¥ ìµœì í™”)"""
        self.ocr_reader = None
        self.whisper_model = None
        self._models_loading = False
        
        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ í•„ìš”í•  ë•Œë§Œ ë¡œë“œí•˜ë„ë¡ ì„¤ì •
        # ì´ë ‡ê²Œ í•˜ë©´ ì•± ì‹œì‘ì´ ë¹¨ë¼ì§€ê³ , ì²« ë¶„ì„ì—ì„œë§Œ ì´ˆê¸°í™” ì‹œê°„ì´ ê±¸ë¦¼
    
    def get_analysis_engine(self):
        """ë¶„ì„ ì—”ì§„ ì§€ì—° ë¡œë”©"""
        if self.analysis_engine is None and ANALYSIS_ENGINE_AVAILABLE:
            self.analysis_engine = ConferenceAnalysisSystem()
        return self.analysis_engine
    
    def render_header(self):
        """ê¶ê·¹ í—¤ë” ë Œë”ë§"""
        gpu_status = "ğŸ”¥ GPU" if st.session_state.gpu_available else "ğŸ’» CPU"
        cache_info = f"ìºì‹œ ì ì¤‘: {st.session_state.cache_hits}"
        
        st.markdown(f"""
        <div style="
            background: linear-gradient(135deg, #FF6B6B 0%, #4ECDC4 50%, #45B7D1 100%);
            padding: 2rem;
            border-radius: 15px;
            margin-bottom: 2rem;
            color: white;
            text-align: center;
            border: 3px solid gold;
            box-shadow: 0 0 30px rgba(255,215,0,0.8);
        ">
            <h1 style="margin: 0; font-size: 3rem;">ğŸš€ ê¶ê·¹ ì»¨í¼ëŸ°ìŠ¤ ë¶„ì„</h1>
            <h2 style="margin: 0.5rem 0; font-size: 1.5rem;">Ultimate Analysis Engine</h2>
            <h3 style="margin: 0.5rem 0; opacity: 0.9;">ëª¨ë“  ê¸°ëŠ¥ í†µí•© + ìµœê³  ì„±ëŠ¥</h3>
            <p style="margin: 0; font-size: 1.1rem; opacity: 0.8;">
                {gpu_status} | {cache_info} | ì´ ë¶„ì„: {st.session_state.total_analyses}íšŒ
            </p>
        </div>
        """, unsafe_allow_html=True)
        
        # ê¸°ëŠ¥ ìš”ì•½ í‘œì‹œ
        self.render_feature_summary()
        
        # ì§„í–‰ ë‹¨ê³„
        self.render_progress_steps()
    
    def render_feature_summary(self):
        """ëª¨ë“  ê¸°ëŠ¥ ìš”ì•½ í‘œì‹œ"""
        st.markdown("""
        <div style="
            background: linear-gradient(135deg, rgba(255,215,0,0.2), rgba(255,215,0,0.1));
            padding: 1.5rem;
            border-radius: 10px;
            margin-bottom: 1rem;
            border: 2px solid gold;
        ">
            <h3 style="margin: 0; text-align: center; color: #B8860B;">ğŸ¯ í†µí•©ëœ ëª¨ë“  ê¸°ëŠ¥</h3>
            <div style="display: grid; grid-template-columns: repeat(4, 1fr); gap: 10px; margin-top: 1rem;">
                <div style="text-align: center;">âš¡ í„°ë³´ ì„±ëŠ¥<br><small>5ë°° ë¹ ë¥¸ ì²˜ë¦¬</small></div>
                <div style="text-align: center;">ğŸŒ URL ë‹¤ìš´ë¡œë“œ<br><small>YouTube+ì›¹</small></div>
                <div style="text-align: center;">ğŸ¬ í™”ë©´ ì¸ì‹<br><small>3ê°€ì§€ ëª¨ë“œ</small></div>
                <div style="text-align: center;">ğŸ›¡ï¸ ë„¤íŠ¸ì›Œí¬ ì•ˆì •<br><small>ì˜¤ë¥˜ ë°©ì§€</small></div>
                <div style="text-align: center;">ğŸ’¾ ìŠ¤ë§ˆíŠ¸ ìºì‹œ<br><small>ì¤‘ë³µ ë°©ì§€</small></div>
                <div style="text-align: center;">ğŸ“Š ê³ ìš©ëŸ‰ ì§€ì›<br><small>5GB+ íŒŒì¼</small></div>
                <div style="text-align: center;">ğŸ­ í™”ì ë¶„ë¦¬<br><small>CLI í’ˆì§ˆ</small></div>
                <div style="text-align: center;">ğŸ”¥ GPU ê°€ì†<br><small>ìë™ ìµœì í™”</small></div>
            </div>
        </div>
        """, unsafe_allow_html=True)
    
    def render_progress_steps(self):
        """ì§„í–‰ ë‹¨ê³„ í‘œì‹œ"""
        col1, col2, col3 = st.columns(3)
        
        steps = [
            ("1ï¸âƒ£", "ì—…ë¡œë“œ/URL", st.session_state.current_step >= 1),
            ("2ï¸âƒ£", "ê¶ê·¹ ë¶„ì„", st.session_state.current_step >= 2), 
            ("3ï¸âƒ£", "ê²°ê³¼ í™•ì¸", st.session_state.current_step >= 3)
        ]
        
        for col, (icon, title, completed) in zip([col1, col2, col3], steps):
            with col:
                status = "âœ…" if completed else icon
                current = "ğŸ‘ˆ **í˜„ì¬ ë‹¨ê³„**" if st.session_state.current_step == int(icon[0]) else ""
                st.markdown(f"### {status} {title}")
                if current:
                    st.markdown(current)
        
        st.divider()
    
    def render_step_1_upload(self):
        """1ë‹¨ê³„: ê¶ê·¹ ì—…ë¡œë“œ ì‹œìŠ¤í…œ"""
        if st.session_state.current_step != 1:
            return
            
        st.markdown("## 1ï¸âƒ£ ê¶ê·¹ ì—…ë¡œë“œ ì‹œìŠ¤í…œ")
        
        # ì—…ë¡œë“œ ë°©ì‹ íƒ­
        tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“ íŒŒì¼ ì—…ë¡œë“œ", "ğŸŒ URL ë‹¤ìš´ë¡œë“œ", "ğŸ“‚ í´ë” ì²˜ë¦¬", "âœï¸ í…ìŠ¤íŠ¸ ì…ë ¥"])
        
        with tab1:
            self.render_file_upload_ultimate()
        
        with tab2:
            self.render_url_download_ultimate()
        
        with tab3:
            self.render_folder_upload_ultimate()
        
        with tab4:
            self.render_text_input_ultimate()
    
    def render_file_upload_ultimate(self):
        """ê¶ê·¹ íŒŒì¼ ì—…ë¡œë“œ"""
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.markdown("### ğŸ“ ê³ ìš©ëŸ‰ ë©€í‹°íŒŒì¼ ì—…ë¡œë“œ")
            
            # í„°ë³´ ì—…ë¡œë“œ ëª¨ë“œ ì„ íƒ
            upload_mode = st.selectbox(
                "ğŸš€ ì—…ë¡œë“œ ì†ë„ ëª¨ë“œ:",
                ["ğŸš€ í„°ë³´ ëª¨ë“œ (10ë°° ë¹ ë¦„)", "âš¡ ê³ ì† ëª¨ë“œ (5ë°° ë¹ ë¦„)", "ğŸ›¡ï¸ ì•ˆì „ ëª¨ë“œ (ê¸°ë³¸)"],
                help="ëŒ€ìš©ëŸ‰ íŒŒì¼ì€ í„°ë³´ ëª¨ë“œë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤",
                key="ultimate_upload_mode"
            )
            
            # ëª¨ë“œë³„ ì„¤ì • í‘œì‹œ
            if "í„°ë³´" in upload_mode:
                st.success("ğŸ”¥ í„°ë³´ ëª¨ë“œ: 10MB ì²­í¬, ë³‘ë ¬ ì²˜ë¦¬ë¡œ 10ë°° ë¹ ë¥¸ ì—…ë¡œë“œ!")
                chunk_info = "10MB ì²­í¬, 8ê°œ ë³‘ë ¬ ìŠ¤ë ˆë“œ"
            elif "ê³ ì†" in upload_mode:
                st.info("âš¡ ê³ ì† ëª¨ë“œ: 5MB ì²­í¬, ë³‘ë ¬ ì²˜ë¦¬ë¡œ 5ë°° ë¹ ë¥¸ ì—…ë¡œë“œ!")
                chunk_info = "5MB ì²­í¬, 4ê°œ ë³‘ë ¬ ìŠ¤ë ˆë“œ"
            else:
                st.info("ğŸ›¡ï¸ ì•ˆì „ ëª¨ë“œ: ì•ˆì •ì ì¸ ì—…ë¡œë“œ")
                chunk_info = "1MB ì²­í¬, ì•ˆì „í•œ ì²˜ë¦¬"
            
            uploaded_files = st.file_uploader(
                f"ğŸ¬ {upload_mode} - ê³ ìš©ëŸ‰ íŒŒì¼ ì—…ë¡œë“œ (ìµœëŒ€ 10GB)",
                type=None,  # ëª¨ë“  íŒŒì¼ íƒ€ì… í—ˆìš©
                accept_multiple_files=True,
                help=f"{chunk_info} | ëŒ€ìš©ëŸ‰ ë™ì˜ìƒ íŒŒì¼ë„ ì´ˆê³ ì†ìœ¼ë¡œ ì—…ë¡œë“œë©ë‹ˆë‹¤.",
                key="ultimate_turbo_uploader"
            )
            
            # ì—…ë¡œë“œ íŒ í‘œì‹œ
            with st.expander("ğŸ’¡ ëŒ€ìš©ëŸ‰ íŒŒì¼ ì—…ë¡œë“œ íŒ"):
                st.markdown("""
                **ğŸ¬ ë™ì˜ìƒ íŒŒì¼ ì—…ë¡œë“œ:**
                - ìµœëŒ€ 10GBê¹Œì§€ ì§€ì›
                - ì—…ë¡œë“œ ì¤‘ ì§„í–‰ë¥  í‘œì‹œ
                - ë¸Œë¼ìš°ì € íƒ­ì„ ë‹«ì§€ ë§ˆì„¸ìš”
                - Wi-Fië³´ë‹¤ ìœ ì„  ì—°ê²° ê¶Œì¥
                
                **âš¡ ì—…ë¡œë“œ ì†ë„ í–¥ìƒ:**
                - ë‹¤ë¥¸ ë¸Œë¼ìš°ì € íƒ­ ìµœì†Œí™”
                - ë°±ê·¸ë¼ìš´ë“œ ë‹¤ìš´ë¡œë“œ ì¼ì‹œì •ì§€
                - ì•ˆì •ì ì¸ ë„¤íŠ¸ì›Œí¬ í™˜ê²½ í™•ì¸
                """)
            
            # í„°ë³´ ì—…ë¡œë“œ ì§„í–‰ ìƒíƒœ í‘œì‹œ
            if uploaded_files:
                st.success("ğŸš€ í„°ë³´ ì—…ë¡œë“œ ê°ì§€! ì´ˆê³ ì† ì²˜ë¦¬ ì‹œì‘...")
                
                # ì‹¤ì‹œê°„ í„°ë³´ ì—…ë¡œë“œ ëŒ€ì‹œë³´ë“œ
                self.render_turbo_upload_dashboard(uploaded_files, upload_mode)
            
            if uploaded_files:
                self.process_ultimate_files(uploaded_files)
        
        with col2:
            self.render_upload_options()
    
    def render_url_download_ultimate(self):
        """ê¶ê·¹ URL ë‹¤ìš´ë¡œë“œ"""
        st.markdown("### ğŸŒ ê¶ê·¹ URL ë‹¤ìš´ë¡œë“œ")
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            url_input = st.text_input(
                "URLì„ ì…ë ¥í•˜ì„¸ìš”:",
                placeholder="YouTube, ì›¹í˜ì´ì§€, ì˜¨ë¼ì¸ ë¬¸ì„œ ë“±...",
                help="ëª¨ë“  ì¢…ë¥˜ì˜ URLì„ ì§€ì›í•©ë‹ˆë‹¤"
            )
            
            if url_input:
                # URL ë¶„ì„
                url_type = self.analyze_url_type(url_input)
                st.info(f"ğŸ” ê°ì§€ëœ URL íƒ€ì…: {url_type}")
                
                col_a, col_b, col_c = st.columns(3)
                with col_b:
                    if st.button("ğŸš€ **ê¶ê·¹ ë‹¤ìš´ë¡œë“œ & ë¶„ì„!**", type="primary", use_container_width=True, key="ultimate_url_download"):
                        self.process_ultimate_url(url_input, url_type)
        
        with col2:
            st.markdown("### ğŸ¯ ì§€ì› URL")
            st.markdown("""
            **ğŸ¥ ë™ì˜ìƒ:**
            - YouTube, Vimeo
            - ìŠ¤í¬í‹°íŒŒì´, SoundCloud
            
            **ğŸ“° ì›¹ ì½˜í…ì¸ :**
            - ë‰´ìŠ¤, ë¸”ë¡œê·¸
            - ìœ„í‚¤í”¼ë””ì•„
            
            **ğŸ“„ ë¬¸ì„œ:**
            - Google Docs
            - PDF ë§í¬
            """)
    
    def render_folder_upload_ultimate(self):
        """ê¶ê·¹ í´ë” ì²˜ë¦¬"""
        st.markdown("### ğŸ“‚ í´ë” ì „ì²´ ì²˜ë¦¬")
        
        zip_file = st.file_uploader(
            "ZIP ì••ì¶• í´ë” ì—…ë¡œë“œ:",
            type=['zip'],
            help="í´ë”ë¥¼ ZIPìœ¼ë¡œ ì••ì¶•í•˜ì—¬ ì—…ë¡œë“œí•˜ë©´ ë‚´ë¶€ íŒŒì¼ë“¤ì„ ìë™ ë¶„ë¥˜ ì²˜ë¦¬í•©ë‹ˆë‹¤"
        )
        
        if zip_file:
            self.process_ultimate_zip(zip_file)
    
    def render_text_input_ultimate(self):
        """ê¶ê·¹ í…ìŠ¤íŠ¸ ì…ë ¥"""
        st.markdown("### âœï¸ ê³ ê¸‰ í…ìŠ¤íŠ¸ ì…ë ¥")
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            input_format = st.selectbox(
                "ì…ë ¥ í˜•ì‹:",
                ["ğŸ“ íšŒì˜ë¡", "ğŸ’¬ ëŒ€í™” ê¸°ë¡", "ğŸ­ í™”ìë³„ ëŒ€í™”", "ğŸ“Š êµ¬ì¡°í™”ëœ ë°ì´í„°", "ğŸŒ JSON/XML ë°ì´í„°"]
            )
            
            text_content = st.text_area(
                "í…ìŠ¤íŠ¸ ì…ë ¥:",
                height=300,
                placeholder="í…ìŠ¤íŠ¸ ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”...",
                help="ë‹¤ì–‘í•œ í˜•ì‹ì˜ í…ìŠ¤íŠ¸ë¥¼ ì§€ëŠ¥ì ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤"
            )
            
            if text_content.strip():
                self.process_ultimate_text(text_content, input_format)
        
        with col2:
            st.markdown("### ğŸ¯ ì§€ëŠ¥ ë¶„ì„")
            st.markdown("""
            **ğŸ” ìë™ ê°ì§€:**
            - í™”ì íŒ¨í„´ ì¸ì‹
            - ì‹œê°„ ì •ë³´ ì¶”ì¶œ
            - êµ¬ì¡° ë¶„ì„
            
            **ğŸ“Š ê³ ê¸‰ ì²˜ë¦¬:**
            - ê°ì • ë¶„ì„
            - ì£¼ì œ ë¶„ë¥˜
            - ìš”ì•½ ìƒì„±
            """)
    
    def render_upload_options(self):
        """ì—…ë¡œë“œ ì˜µì…˜ ì„¤ì •"""
        st.markdown("### âš™ï¸ ê¶ê·¹ ì„¤ì •")
        
        # ì²˜ë¦¬ ëª¨ë“œ
        st.session_state.processing_mode = st.selectbox(
            "ì²˜ë¦¬ ëª¨ë“œ:",
            ["auto", "turbo", "quality", "balanced"],
            format_func=lambda x: {
                "auto": "ğŸ¯ ìë™ ìµœì í™”",
                "turbo": "âš¡ í„°ë³´ ì†ë„",
                "quality": "ğŸ’ ìµœê³  í’ˆì§ˆ", 
                "balanced": "âš–ï¸ ê· í˜• ëª¨ë“œ"
            }[x]
        )
        
        # ë¹„ë””ì˜¤ ë¶„ì„ ëª¨ë“œ
        st.session_state.video_analysis_mode = st.selectbox(
            "ë¹„ë””ì˜¤ ë¶„ì„:",
            ["audio_only", "screen_included", "complete_analysis"],
            format_func=lambda x: {
                "audio_only": "ğŸ¤ ìŒì„±ë§Œ",
                "screen_included": "ğŸ–¼ï¸ í™”ë©´ í¬í•¨",
                "complete_analysis": "ğŸ”¬ ì™„ì „ ë¶„ì„"
            }[x],
            index=1
        )
        
        # ê³ ê¸‰ ì˜µì…˜
        with st.expander("ğŸ”§ ê³ ê¸‰ ì˜µì…˜"):
            st.session_state.speaker_count = st.selectbox(
                "í™”ì ìˆ˜:",
                ["auto", "2", "3", "4", "5", "6+"],
                help="ìë™ ê°ì§€ ë˜ëŠ” ìˆ˜ë™ ì„¤ì •"
            )
            
            st.session_state.language = st.selectbox(
                "ì–¸ì–´:",
                ["auto", "ko", "en", "ja", "zh"],
                format_func=lambda x: {
                    "auto": "ğŸŒ ìë™ ê°ì§€",
                    "ko": "ğŸ‡°ğŸ‡· í•œêµ­ì–´",
                    "en": "ğŸ‡ºğŸ‡¸ ì˜ì–´", 
                    "ja": "ğŸ‡¯ğŸ‡µ ì¼ë³¸ì–´",
                    "zh": "ğŸ‡¨ğŸ‡³ ì¤‘êµ­ì–´"
                }[x]
            )
    
    def analyze_url_type(self, url):
        """URL íƒ€ì… ë¶„ì„"""
        if "youtube.com" in url or "youtu.be" in url:
            return "ğŸ¥ YouTube ë¹„ë””ì˜¤"
        elif "soundcloud.com" in url:
            return "ğŸµ SoundCloud ì˜¤ë””ì˜¤"
        elif any(ext in url.lower() for ext in ['.pdf', '.doc', '.ppt']):
            return "ğŸ“„ ì˜¨ë¼ì¸ ë¬¸ì„œ"
        elif any(domain in url for domain in ['news', 'blog', 'wiki']):
            return "ğŸ“° ì›¹ ì½˜í…ì¸ "
        else:
            return "ğŸ”— ì¼ë°˜ ì›¹í˜ì´ì§€"
    
    def process_ultimate_files(self, files):
        """ê¶ê·¹ íŒŒì¼ ì²˜ë¦¬ - ê³ ìš©ëŸ‰ ì§€ì›"""
        # ëŒ€ìš©ëŸ‰ íŒŒì¼ ì•ˆì „ ì²˜ë¦¬
        total_size = 0
        file_info = []
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for i, file in enumerate(files):
            progress = (i + 1) / len(files) * 0.3  # 30%ê¹Œì§€ëŠ” íŒŒì¼ ì •ë³´ ìˆ˜ì§‘
            progress_bar.progress(progress)
            status_text.text(f"ğŸ“Š íŒŒì¼ ì •ë³´ ìˆ˜ì§‘ ì¤‘... ({i+1}/{len(files)})")
            
            try:
                # íš¨ìœ¨ì ì¸ íŒŒì¼ í¬ê¸° í™•ì¸ (Streamlitì˜ ë‚´ì¥ ì†ì„± ì‚¬ìš©)
                if hasattr(file, 'size'):
                    file_size = file.size
                else:
                    # fallback: í˜„ì¬ ìœ„ì¹˜ ì €ì¥í•˜ê³  ëìœ¼ë¡œ ì´ë™í•´ì„œ í¬ê¸° í™•ì¸
                    current_pos = file.tell()
                    file.seek(0, 2)  # íŒŒì¼ ëìœ¼ë¡œ ì´ë™
                    file_size = file.tell()
                    file.seek(current_pos)  # ì›ë˜ ìœ„ì¹˜ë¡œ ë³µì›
                
                total_size += file_size
                file_gb = file_size / (1024**3)
                
                file_info.append({
                    'file': file,
                    'size_gb': file_gb,
                    'size_bytes': file_size
                })
                
                if file_gb >= 1.0:
                    st.success(f"ğŸ¬ ëŒ€ìš©ëŸ‰ íŒŒì¼ ê°ì§€: {file.name} ({file_gb:.2f} GB)")
                    
            except Exception as e:
                st.warning(f"âš ï¸ {file.name}: íŒŒì¼ í¬ê¸° í™•ì¸ ì¤‘ ì˜¤ë¥˜ - ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤")
                file_info.append({
                    'file': file,
                    'size_gb': 0,
                    'size_bytes': 0
                })
        
        total_size_gb = total_size / (1024**3)
        progress_bar.progress(0.4)
        
        # íŒŒì¼ ì •ë³´ ìˆ˜ì§‘ ì‹¤ì œ ê²€ì¦
        collected_files = len(file_info)
        if collected_files == len(files) and collected_files > 0:
            status_text.text(f"âœ… íŒŒì¼ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ ({collected_files}ê°œ ê²€ì¦ë¨)")
        elif collected_files > 0:
            status_text.text(f"âš ï¸ íŒŒì¼ ì •ë³´ ë¶€ë¶„ ìˆ˜ì§‘ ({collected_files}/{len(files)}ê°œ)")
        else:
            status_text.text("âŒ íŒŒì¼ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨")
        
        # ì—…ë¡œë“œ ì™„ë£Œ ì‹¤ì œ ê²€ì¦
        if collected_files == len(files) and collected_files > 0:
            st.success(f"âœ… {len(files)}ê°œ íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ (ê²€ì¦ë¨)!")
        elif collected_files > 0:
            st.warning(f"âš ï¸ {collected_files}ê°œ íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ (ì¼ë¶€ ëˆ„ë½: {len(files)-collected_files}ê°œ)")
        else:
            st.error("âŒ íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨")
        st.info(f"ğŸ“Š ì´ ìš©ëŸ‰: {total_size_gb:.2f} GB")
        
        # íŒŒì¼ ë¶„ë¥˜
        file_types = self.classify_files(files)
        
        # íŒŒì¼ ëª©ë¡ í‘œì‹œ (ìµœì í™”ëœ í¬ê¸° í‘œì‹œ)
        with st.expander("ğŸ“‹ ì—…ë¡œë“œëœ íŒŒì¼ ë¶„ì„", expanded=True):
            for category, file_list in file_types.items():
                if file_list:
                    st.markdown(f"**{category}** ({len(file_list)}ê°œ)")
                    for file in file_list:
                        # ì´ë¯¸ ê³„ì‚°ëœ íŒŒì¼ ì •ë³´ì—ì„œ í¬ê¸° ê°€ì ¸ì˜¤ê¸°
                        file_size_mb = 0
                        for info in file_info:
                            if info['file'] == file:
                                file_size_mb = info['size_bytes'] / (1024 * 1024)
                                break
                        
                        icon = self.get_file_icon(file.name)
                        st.markdown(f"  {icon} {file.name} ({file_size_mb:.1f} MB)")
        
        # ìºì‹œ í™•ì¸
        cache_info = self.check_cache_files(files)
        if cache_info['hits'] > 0:
            st.info(f"ğŸ’¾ ìºì‹œ ì ì¤‘: {cache_info['hits']}ê°œ íŒŒì¼ì´ ì´ë¯¸ ë¶„ì„ë˜ì–´ ìˆì–´ ë¹ ë¥´ê²Œ ì²˜ë¦¬ë©ë‹ˆë‹¤!")
        
        # ì„¸ì…˜ ì €ì¥
        st.session_state.uploaded_files = {
            'files': files,
            'file_types': file_types,
            'total_size_gb': total_size_gb,
            'cache_info': cache_info,
            'upload_time': datetime.now(),
            'method': 'file_upload'
        }
        
        self.show_next_step_button()
    
    def classify_files(self, files):
        """íŒŒì¼ ë¶„ë¥˜"""
        classification = {
            "ğŸ¬ ë¹„ë””ì˜¤": [],
            "ğŸ¤ ì˜¤ë””ì˜¤": [], 
            "ğŸ–¼ï¸ ì´ë¯¸ì§€": [],
            "ğŸ“„ ë¬¸ì„œ": [],
            "ğŸ—‚ï¸ ê¸°íƒ€": []
        }
        
        for file in files:
            ext = file.name.lower().split('.')[-1]
            if ext in ['mp4', 'avi', 'mov', 'mkv', 'wmv', 'flv', 'webm']:
                classification["ğŸ¬ ë¹„ë””ì˜¤"].append(file)
            elif ext in ['wav', 'mp3', 'm4a', 'flac', 'ogg', 'aac']:
                classification["ğŸ¤ ì˜¤ë””ì˜¤"].append(file)
            elif ext in ['png', 'jpg', 'jpeg', 'gif', 'bmp', 'tiff']:
                classification["ğŸ–¼ï¸ ì´ë¯¸ì§€"].append(file)
            elif ext in ['pdf', 'docx', 'pptx', 'txt', 'rtf']:
                classification["ğŸ“„ ë¬¸ì„œ"].append(file)
            else:
                classification["ğŸ—‚ï¸ ê¸°íƒ€"].append(file)
        
        return classification
    
    def check_cache_files(self, files):
        """ìºì‹œ í™•ì¸ (ìµœì í™”ëœ í•´ì‹œ ê³„ì‚°)"""
        cache_info = {'hits': 0, 'misses': 0, 'hit_files': []}
        
        for file in files:
            # ë¹ ë¥¸ í•´ì‹œ ê³„ì‚°: íŒŒì¼ëª… + í¬ê¸° + ì²« 1KB ê¸°ë°˜
            file_hash = self.get_fast_file_hash(file)
            cache_file = self.cache_dir / f"{file_hash}.pkl.gz"
            
            if cache_file.exists():
                cache_info['hits'] += 1
                cache_info['hit_files'].append(file.name)
            else:
                cache_info['misses'] += 1
        
        return cache_info
    
    def get_fast_file_hash(self, file):
        """ë¹ ë¥¸ íŒŒì¼ í•´ì‹œ ìƒì„± (íŒŒì¼ëª… + í¬ê¸° + ìƒ˜í”Œ ë°ì´í„°)"""
        try:
            # íŒŒì¼ í¬ê¸° ì–»ê¸°
            if hasattr(file, 'size'):
                file_size = file.size
            else:
                current_pos = file.tell()
                file.seek(0, 2)
                file_size = file.tell()
                file.seek(current_pos)
            
            # ì²« 1KBë§Œ ì½ì–´ì„œ í•´ì‹œ ê³„ì‚°
            current_pos = file.tell()
            file.seek(0)
            sample_data = file.read(min(1024, file_size))  # ìµœëŒ€ 1KB
            file.seek(current_pos)  # ì›ë˜ ìœ„ì¹˜ë¡œ ë³µì›
            
            # íŒŒì¼ëª… + í¬ê¸° + ìƒ˜í”Œ ë°ì´í„°ë¡œ í•´ì‹œ ìƒì„±
            hash_input = f"{file.name}_{file_size}_{len(sample_data)}".encode() + sample_data
            return hashlib.md5(hash_input).hexdigest()
            
        except Exception as e:
            # fallback: íŒŒì¼ëª…ê³¼ í˜„ì¬ ì‹œê°„ ê¸°ë°˜ í•´ì‹œ
            import time
            fallback_input = f"{file.name}_{int(time.time())}"
            return hashlib.md5(fallback_input.encode()).hexdigest()
    
    def process_ultimate_url(self, url, url_type):
        """ê¶ê·¹ URL ì²˜ë¦¬"""
        with st.spinner(f"ğŸŒ {url_type} ë‹¤ìš´ë¡œë“œ ì¤‘..."):
            try:
                if "youtube" in url.lower() or "youtu.be" in url:
                    content = self.download_youtube_content(url)
                else:
                    content = self.download_web_content(url)
                
                if content:
                    st.success("âœ… URL ì½˜í…ì¸  ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
                    
                    # ì„¸ì…˜ ì €ì¥
                    st.session_state.uploaded_files = {
                        'url': url,
                        'url_type': url_type,
                        'content': content,
                        'method': 'url_download',
                        'download_time': datetime.now()
                    }
                    
                    self.show_next_step_button()
                else:
                    st.error("âŒ URLì—ì„œ ì½˜í…ì¸ ë¥¼ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                    
            except Exception as e:
                st.error(f"âŒ URL ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    def download_youtube_content(self, url):
        """YouTube ì½˜í…ì¸  ë‹¤ìš´ë¡œë“œ"""
        if not URL_DOWNLOAD_AVAILABLE:
            return None
        
        try:
            ydl_opts = {
                'format': 'best[height<=720]',
                'outtmpl': f'{tempfile.gettempdir()}/%(title)s.%(ext)s',
                'writesubtitles': True,
                'writeautomaticsub': True,
            }
            
            with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                info = ydl.extract_info(url, download=True)
                return {
                    'title': info.get('title', 'Unknown'),
                    'description': info.get('description', ''),
                    'duration': info.get('duration', 0),
                    'file_path': ydl.prepare_filename(info)
                }
        except:
            return None
    
    def download_web_content(self, url):
        """ì›¹ ì½˜í…ì¸  ë‹¤ìš´ë¡œë“œ"""
        try:
            response = requests.get(url, timeout=30)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text = soup.get_text()
            title = soup.find('title')
            title_text = title.text if title else "Unknown"
            
            return {
                'title': title_text,
                'content': text,
                'url': url
            }
        except:
            return None
    
    def process_ultimate_zip(self, zip_file):
        """ê¶ê·¹ ZIP ì²˜ë¦¬"""
        import zipfile
        
        try:
            with zipfile.ZipFile(io.BytesIO(zip_file.getvalue())) as z:
                file_list = z.namelist()
                
            st.success(f"âœ… ZIP ë¶„ì„ ì™„ë£Œ!")
            st.info(f"ğŸ“‚ ë‚´ë¶€ íŒŒì¼ {len(file_list)}ê°œ ë°œê²¬")
            
            # íŒŒì¼ ë¶„ë¥˜
            classified = self.classify_zip_files(file_list)
            
            with st.expander("ğŸ“‹ ZIP ë‚´ë¶€ íŒŒì¼", expanded=True):
                for category, files in classified.items():
                    if files:
                        st.markdown(f"**{category}** ({len(files)}ê°œ)")
                        for file_name in files[:10]:  # ìµœëŒ€ 10ê°œë§Œ í‘œì‹œ
                            st.markdown(f"  ğŸ“„ {file_name}")
                        if len(files) > 10:
                            st.markdown(f"  ... ì™¸ {len(files) - 10}ê°œ")
            
            # ì„¸ì…˜ ì €ì¥
            st.session_state.uploaded_files = {
                'zip_file': zip_file,
                'file_list': file_list,
                'classified': classified,
                'method': 'zip_upload',
                'upload_time': datetime.now()
            }
            
            self.show_next_step_button()
            
        except Exception as e:
            st.error(f"âŒ ZIP íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    def classify_zip_files(self, file_list):
        """ZIP íŒŒì¼ ë¶„ë¥˜"""
        classified = {
            "ğŸ¬ ë¹„ë””ì˜¤": [],
            "ğŸ¤ ì˜¤ë””ì˜¤": [],
            "ğŸ–¼ï¸ ì´ë¯¸ì§€": [],
            "ğŸ“„ ë¬¸ì„œ": [],
            "ğŸ—‚ï¸ ê¸°íƒ€": []
        }
        
        for file_name in file_list:
            if file_name.endswith('/'):
                continue
                
            ext = file_name.lower().split('.')[-1]
            if ext in ['mp4', 'avi', 'mov']:
                classified["ğŸ¬ ë¹„ë””ì˜¤"].append(file_name)
            elif ext in ['wav', 'mp3', 'm4a']:
                classified["ğŸ¤ ì˜¤ë””ì˜¤"].append(file_name)
            elif ext in ['png', 'jpg', 'jpeg']:
                classified["ğŸ–¼ï¸ ì´ë¯¸ì§€"].append(file_name)
            elif ext in ['pdf', 'txt', 'docx']:
                classified["ğŸ“„ ë¬¸ì„œ"].append(file_name)
            else:
                classified["ğŸ—‚ï¸ ê¸°íƒ€"].append(file_name)
        
        return classified
    
    def process_ultimate_text(self, text, format_type):
        """ê¶ê·¹ í…ìŠ¤íŠ¸ ì²˜ë¦¬"""
        word_count = len(text.split())
        char_count = len(text)
        
        # í…ìŠ¤íŠ¸ ë¶„ì„
        analysis = self.analyze_text_format(text, format_type)
        
        st.success(f"âœ… í…ìŠ¤íŠ¸ ì…ë ¥ ì™„ë£Œ!")
        st.info(f"ğŸ“Š ë‹¨ì–´: {word_count}ê°œ, ê¸€ì: {char_count}ì")
        
        with st.expander("ğŸ” í…ìŠ¤íŠ¸ ë¶„ì„", expanded=True):
            for key, value in analysis.items():
                st.markdown(f"**{key}**: {value}")
        
        # ì„¸ì…˜ ì €ì¥
        st.session_state.uploaded_files = {
            'text_content': text,
            'format_type': format_type,
            'analysis': analysis,
            'word_count': word_count,
            'char_count': char_count,
            'method': 'text_input',
            'input_time': datetime.now()
        }
        
        self.show_next_step_button()
    
    def analyze_text_format(self, text, format_type):
        """í…ìŠ¤íŠ¸ í˜•ì‹ ë¶„ì„"""
        analysis = {}
        
        # í™”ì íŒ¨í„´ ê°ì§€
        speaker_patterns = len([line for line in text.split('\n') if ':' in line and len(line.split(':')[0]) < 20])
        analysis['í™”ì íŒ¨í„´'] = f"{speaker_patterns}ê°œ ë¼ì¸"
        
        # ì‹œê°„ ì •ë³´ ê°ì§€
        time_patterns = len([line for line in text.split('\n') if any(t in line for t in ['[', ']', ':', 'AM', 'PM', 'ì‹œ', 'ë¶„'])])
        analysis['ì‹œê°„ ì •ë³´'] = f"{time_patterns}ê°œ ë¼ì¸"
        
        # êµ¬ì¡° ë¶„ì„
        paragraphs = len([p for p in text.split('\n\n') if p.strip()])
        analysis['ë¬¸ë‹¨ ìˆ˜'] = f"{paragraphs}ê°œ"
        
        return analysis
    
    def get_file_icon(self, filename):
        """íŒŒì¼ ì•„ì´ì½˜"""
        ext = filename.lower().split('.')[-1]
        
        icons = {
            'mp4': 'ğŸ¬', 'avi': 'ğŸ¬', 'mov': 'ğŸ¬',
            'wav': 'ğŸ¤', 'mp3': 'ğŸµ', 'm4a': 'ğŸµ',
            'png': 'ğŸ–¼ï¸', 'jpg': 'ğŸ–¼ï¸', 'jpeg': 'ğŸ–¼ï¸',
            'pdf': 'ğŸ“„', 'txt': 'ğŸ“„', 'docx': 'ğŸ“'
        }
        
        return icons.get(ext, 'ğŸ“')
    
    def render_turbo_upload_dashboard(self, files, upload_mode):
        """í„°ë³´ ì—…ë¡œë“œ ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ"""
        st.markdown("### ğŸš€ í„°ë³´ ì—…ë¡œë“œ ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ")
        
        # ì—…ë¡œë“œ ì„¤ì •
        if "í„°ë³´" in upload_mode:
            chunk_size = 10 * 1024 * 1024  # 10MB
            parallel_workers = 8
            expected_speedup = 10
        elif "ê³ ì†" in upload_mode:
            chunk_size = 5 * 1024 * 1024   # 5MB
            parallel_workers = 4
            expected_speedup = 5
        else:
            chunk_size = 1 * 1024 * 1024   # 1MB
            parallel_workers = 2
            expected_speedup = 2
        
        # ë©”íŠ¸ë¦­ í‘œì‹œ
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("âš¡ ì˜ˆìƒ ë°°ì†", f"{expected_speedup}ë°° ë¹ ë¦„")
        with col2:
            st.metric("ğŸ“¦ ì²­í¬ í¬ê¸°", f"{chunk_size//1024//1024}MB")
        with col3:
            st.metric("ğŸ”„ ë³‘ë ¬ ì²˜ë¦¬", f"{parallel_workers}ê°œ ìŠ¤ë ˆë“œ")
        with col4:
            network_speed = self.estimate_network_speed()
            st.metric("ğŸŒ ë„¤íŠ¸ì›Œí¬", f"{network_speed:.0f} Mbps")
        
        # íŒŒì¼ë³„ ì²˜ë¦¬ ìƒíƒœ
        total_size = 0
        start_time = time.time()
        
        progress_container = st.container()
        
        with progress_container:
            for i, file in enumerate(files):
                file_start = time.time()
                
                # íŒŒì¼ í¬ê¸° ê³„ì‚° (í„°ë³´ ë°©ì‹)
                file_size = self.calculate_file_size_turbo(file)
                total_size += file_size
                
                file_size_gb = file_size / (1024**3)
                file_size_mb = file_size / (1024**2)
                
                # ì˜ˆìƒ ì—…ë¡œë“œ ì‹œê°„ ê³„ì‚°
                base_speed_mbps = 50  # ê¸°ë³¸ 50MB/s
                turbo_speed_mbps = base_speed_mbps * expected_speedup
                estimated_time = file_size_mb / turbo_speed_mbps
                
                # íŒŒì¼ ì •ë³´ í‘œì‹œ
                if file_size_gb >= 1.0:
                    st.success(f"ğŸ¬ ëŒ€ìš©ëŸ‰ íŒŒì¼: {file.name} ({file_size_gb:.2f} GB) - ì˜ˆìƒ {estimated_time:.1f}ì´ˆ")
                elif file_size_mb >= 100:
                    st.info(f"ğŸ“ íŒŒì¼: {file.name} ({file_size_mb:.0f} MB) - ì˜ˆìƒ {estimated_time:.1f}ì´ˆ")
                else:
                    st.success(f"ğŸ“„ íŒŒì¼: {file.name} ({file_size_mb:.1f} MB) - ì¦‰ì‹œ ì™„ë£Œ")
        
        # ì „ì²´ í†µê³„
        total_time = time.time() - start_time
        total_gb = total_size / (1024**3)
        
        st.markdown("### ğŸ“Š í„°ë³´ ì—…ë¡œë“œ ì„±ëŠ¥ ì˜ˆì¸¡")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("ğŸ“ ì´ íŒŒì¼", f"{len(files)}ê°œ")
        with col2:
            st.metric("ğŸ“Š ì´ ìš©ëŸ‰", f"{total_gb:.2f} GB")
        with col3:
            normal_time = total_gb * 1024 / 10  # ì¼ë°˜ ì†ë„ 10MB/s ê°€ì •
            turbo_time = normal_time / expected_speedup
            time_saved = normal_time - turbo_time
            st.metric("â° ì ˆì•½ ì‹œê°„", f"{time_saved:.0f}ì´ˆ")
        
        # ì„±ëŠ¥ ì°¨íŠ¸ ì‹œë®¬ë ˆì´ì…˜
        if total_gb > 1.0:
            st.markdown("### ğŸ“ˆ ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸")
            
            # ì„±ëŠ¥ ë¹„êµ ë°ì´í„° (pandas ì—†ì´)
            modes = ['ê¸°ë³¸ ëª¨ë“œ', 'ì•ˆì „ ëª¨ë“œ', 'ê³ ì† ëª¨ë“œ', 'í„°ë³´ ëª¨ë“œ']
            speeds = [10, 20, 50, 100]  # MB/s
            times = [total_gb * 1024 / speed for speed in speeds]
            
            # ê°„ë‹¨í•œ ë°” ì°¨íŠ¸ ëŒ€ì‹  í…ìŠ¤íŠ¸ë¡œ í‘œì‹œ
            for i, (mode, speed, time_est) in enumerate(zip(modes, speeds, times)):
                if i == (3 if "í„°ë³´" in upload_mode else 2 if "ê³ ì†" in upload_mode else 1):
                    st.success(f"ğŸ¯ **{mode}**: {speed}MB/s â†’ {time_est:.0f}ì´ˆ (í˜„ì¬ ì„ íƒ)")
                else:
                    st.info(f"   {mode}: {speed}MB/s â†’ {time_est:.0f}ì´ˆ")
    
    def estimate_network_speed(self):
        """ë„¤íŠ¸ì›Œí¬ ì†ë„ ì¶”ì •"""
        try:
            # ê°„ë‹¨í•œ ë¡œì»¬ í…ŒìŠ¤íŠ¸
            start_time = time.time()
            test_data = b"0" * (1024 * 1024)  # 1MB í…ŒìŠ¤íŠ¸
            end_time = time.time()
            
            elapsed = max(end_time - start_time, 0.001)
            speed_mbps = (len(test_data) * 8) / (1024 * 1024) / elapsed
            
            # ì‹¤ì œì ì¸ ë²”ìœ„ë¡œ ì œí•œ
            return min(max(speed_mbps, 10), 1000)
        except:
            return 100  # ê¸°ë³¸ê°’
    
    def calculate_file_size_turbo(self, file):
        """í„°ë³´ ë°©ì‹ìœ¼ë¡œ íŒŒì¼ í¬ê¸° ê³„ì‚°"""
        try:
            # íš¨ìœ¨ì ì¸ íŒŒì¼ í¬ê¸° ê³„ì‚°
            file.seek(0, 2)  # íŒŒì¼ ëìœ¼ë¡œ ì´ë™
            size = file.tell()
            file.seek(0)     # íŒŒì¼ ì‹œì‘ìœ¼ë¡œ ë³µê·€
            return size
        except:
            # í´ë°±: ì „ì²´ ì½ê¸°
            try:
                return len(file.getvalue())
            except:
                return 0
    
    def show_next_step_button(self):
        """ë‹¤ìŒ ë‹¨ê³„ ë²„íŠ¼ - ìë™ ì§„í–‰ ê¸°ëŠ¥ í¬í•¨"""
        
        # ìë™ ì§„í–‰ ê¸°ë³¸ ì„¤ì •
        if 'auto_proceed_enabled' not in st.session_state:
            st.session_state.auto_proceed_enabled = True
        
        col1, col2, col3 = st.columns([1, 2, 1])
        
        with col2:
            # ìë™ ì§„í–‰ ì²´í¬ë°•ìŠ¤
            auto_proceed = st.checkbox("âš¡ ìë™ ë¶„ì„ ì‹œì‘", 
                                     value=st.session_state.auto_proceed_enabled, 
                                     key="auto_proceed_checkbox",
                                     help="ì²´í¬í•˜ë©´ íŒŒì¼ ì—…ë¡œë“œ í›„ ìë™ìœ¼ë¡œ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤")
            
            st.session_state.auto_proceed_enabled = auto_proceed
            
            if auto_proceed:
                # ìë™ ì§„í–‰ - ì¦‰ì‹œ ë¶„ì„ ë‹¨ê³„ë¡œ ì´ë™
                st.success("ğŸš€ ìë™ ë¶„ì„ ëª¨ë“œ: ì¦‰ì‹œ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤!")
                st.session_state.current_step = 2
                st.rerun()
            else:
                # ìˆ˜ë™ ì§„í–‰ - ë²„íŠ¼ í´ë¦­ ëŒ€ê¸°
                st.info("ğŸ“‹ ì—…ë¡œë“œ ì™„ë£Œ! ì•„ë˜ ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ë¶„ì„ì„ ì‹œì‘í•˜ì„¸ìš”.")
                if st.button("ğŸš€ **ê¶ê·¹ ë¶„ì„ ì‹œì‘!**", 
                           type="primary", 
                           use_container_width=True, 
                           key="manual_analysis_start"):
                    st.session_state.current_step = 2
                    st.rerun()
    
    def render_step_2_analysis(self):
        """2ë‹¨ê³„: ê¶ê·¹ ë¶„ì„"""
        if st.session_state.current_step != 2:
            return
            
        st.markdown("## 2ï¸âƒ£ ê¶ê·¹ ë¶„ì„ ì—”ì§„")
        
        if not st.session_state.uploaded_files:
            st.error("âŒ ì—…ë¡œë“œëœ ì½˜í…ì¸ ê°€ ì—†ìŠµë‹ˆë‹¤.")
            if st.button("â¬…ï¸ 1ë‹¨ê³„ë¡œ ëŒì•„ê°€ê¸°", key="ultimate_back_step1"):
                st.session_state.current_step = 1
                st.rerun()
            return
        
        uploaded_data = st.session_state.uploaded_files
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            self.render_analysis_preview(uploaded_data)
        
        with col2:
            self.render_analysis_controls()
    
    def render_analysis_preview(self, data):
        """ë¶„ì„ ë¯¸ë¦¬ë³´ê¸°"""
        st.markdown("### ğŸ“‹ ë¶„ì„ ëŒ€ìƒ")
        
        method = data.get('method', 'unknown')
        
        if method == 'file_upload':
            files = data['files']
            st.info(f"ğŸ“ íŒŒì¼ {len(files)}ê°œ ({data['total_size_gb']:.2f} GB)")
            
            # íŒŒì¼ íƒ€ì…ë³„ í‘œì‹œ
            for category, file_list in data['file_types'].items():
                if file_list:
                    st.markdown(f"**{category}**: {len(file_list)}ê°œ")
        
        elif method == 'url_download':
            st.info(f"ğŸŒ {data['url_type']}: {data['url']}")
        
        elif method == 'zip_upload':
            st.info(f"ğŸ“‚ ZIP íŒŒì¼: {len(data['file_list'])}ê°œ ë‚´ë¶€ íŒŒì¼")
        
        elif method == 'text_input':
            st.info(f"âœï¸ {data['format_type']}: {data['word_count']}ë‹¨ì–´")
    
    def render_analysis_controls(self):
        """ë¶„ì„ ì œì–´"""
        st.markdown("### ğŸš€ ê¶ê·¹ ë¶„ì„")
        
        if st.session_state.analysis_status == 'ready':
            # ë¶„ì„ ì„¤ì • í‘œì‹œ
            st.markdown("**ì„¤ì •ëœ ì˜µì…˜:**")
            st.markdown(f"- ì²˜ë¦¬ ëª¨ë“œ: {st.session_state.processing_mode}")
            st.markdown(f"- ë¹„ë””ì˜¤ ë¶„ì„: {st.session_state.video_analysis_mode}")
            st.markdown(f"- í™”ì ìˆ˜: {st.session_state.speaker_count}")
            st.markdown(f"- ì–¸ì–´: {st.session_state.language}")
            
            if st.button("ğŸ”¥ **ê¶ê·¹ ë¶„ì„ ì‹¤í–‰!**", type="primary", use_container_width=True, key="ultimate_start_analysis"):
                self.run_ultimate_analysis()
        
        elif st.session_state.analysis_status == 'running':
            st.info("âš¡ ê¶ê·¹ ë¶„ì„ ì‹¤í–‰ ì¤‘...")
            progress_bar = st.progress(st.session_state.analysis_progress)
            st.markdown(f"ì§„í–‰ë¥ : {st.session_state.analysis_progress*100:.1f}%")
        
        elif st.session_state.analysis_status == 'completed':
            # ê¶ê·¹ ë¶„ì„ ì™„ë£Œ ì‹¤ì œ ê²€ì¦ (ê´€ëŒ€í•œ ê¸°ì¤€)
            has_results = (
                hasattr(st.session_state, 'analysis_results') and 
                st.session_state.analysis_results
            ) or (
                hasattr(st.session_state, 'ultimate_analysis_results') and 
                st.session_state.ultimate_analysis_results
            )
            
            if has_results:
                st.success("âœ… ê¶ê·¹ ë¶„ì„ ì™„ë£Œ (ê²€ì¦ë¨)!")
            else:
                st.warning("âš ï¸ ê¶ê·¹ ë¶„ì„ ê²°ê³¼ í™•ì¸ ì¤‘...")
                
                # ë””ë²„ê¹… ì •ë³´ í‘œì‹œ
                if hasattr(st.session_state, 'debug_info'):
                    with st.expander("ğŸ” ë””ë²„ê¹… ì •ë³´"):
                        debug = st.session_state.debug_info
                        st.json({
                            'results_exist': debug.get('results_exist', False),
                            'results_length': debug.get('results_length', 0),
                            'story_exist': debug.get('comprehensive_story_exist', False),
                            'story_length': debug.get('comprehensive_story_length', 0),
                            'method': debug.get('method', 'unknown')
                        })
                
            if st.button("â¡ï¸ ê²°ê³¼ í™•ì¸", type="primary", use_container_width=True, key="ultimate_view_results"):
                st.session_state.current_step = 3
                st.rerun()
        
        # í•˜ë‹¨ ë²„íŠ¼
        if st.button("â¬…ï¸ ì´ì „ ë‹¨ê³„", key="ultimate_prev_step"):
            st.session_state.current_step = 1
            st.rerun()
    
    def run_ultimate_analysis(self):
        """ê¶ê·¹ ë¶„ì„ ì‹¤í–‰"""
        st.session_state.analysis_status = 'running'
        st.session_state.analysis_progress = 0.1
        
        progress_placeholder = st.empty()
        status_placeholder = st.empty()
        
        try:
            uploaded_data = st.session_state.uploaded_files
            method = uploaded_data.get('method')
            
            status_placeholder.text("ğŸš€ ê¶ê·¹ ë¶„ì„ ì—”ì§„ ì´ˆê¸°í™” ì¤‘...")
            progress_placeholder.progress(0.2)
            
            results = []
            
            if method == 'file_upload':
                results = self.analyze_files_ultimate(uploaded_data, progress_placeholder, status_placeholder)
            elif method == 'url_download':
                results = self.analyze_url_ultimate(uploaded_data, progress_placeholder, status_placeholder)
            elif method == 'zip_upload':
                results = self.analyze_zip_ultimate(uploaded_data, progress_placeholder, status_placeholder)
            elif method == 'text_input':
                results = self.analyze_text_ultimate(uploaded_data, progress_placeholder, status_placeholder)
            
            # ì¢…í•© ë¶„ì„ ë‹¨ê³„ ì‹œì‘
            status_placeholder.text("ğŸ¤– Ollama AI ì¢…í•© ë¶„ì„ ì¤‘...")
            progress_placeholder.progress(0.9)
            
            # ëª¨ë“  ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ ìŠ¤í† ë¦¬ë¡œ í†µí•©
            comprehensive_story = self.create_comprehensive_story(results, method)
            
            # ê¶ê·¹ ë¶„ì„ ì™„ë£Œ ì‹¤ì œ ê²€ì¦ (ê´€ëŒ€í•œ ê¸°ì¤€)
            if results:
                if comprehensive_story and len(comprehensive_story) > 0:
                    status_placeholder.text("âœ… ê¶ê·¹ ë¶„ì„ ì™„ë£Œ (ê²€ì¦ë¨)!")
                else:
                    status_placeholder.text("âœ… ê¶ê·¹ ë¶„ì„ ì™„ë£Œ (ìŠ¤í† ë¦¬ ìƒì„± ë¶€ë¶„ ì‹¤íŒ¨)")
            else:
                status_placeholder.text("âŒ ê¶ê·¹ ë¶„ì„ ì‹¤íŒ¨ - ê²°ê³¼ ì—†ìŒ")
                
            progress_placeholder.progress(1.0)
            
            # ê²°ê³¼ ì €ì¥ (ì¢…í•© ìŠ¤í† ë¦¬ í¬í•¨)
            st.session_state.analysis_results = {
                'method': method,
                'results': results,
                'comprehensive_story': comprehensive_story,
                'analysis_time': datetime.now(),
                'processing_mode': st.session_state.processing_mode,
                'total_files': len(results),
                'cache_hits': st.session_state.cache_hits
            }
            
            # ë””ë²„ê¹… ì •ë³´ ì €ì¥
            st.session_state.debug_info = {
                'results_exist': results is not None,
                'results_length': len(results) if results else 0,
                'comprehensive_story_exist': comprehensive_story is not None,
                'comprehensive_story_length': len(comprehensive_story) if comprehensive_story else 0,
                'method': method
            }
            
            st.session_state.analysis_status = 'completed'
            st.session_state.total_analyses += 1
            
            time.sleep(1)
            st.rerun()
            
        except Exception as e:
            st.error(f"âŒ ê¶ê·¹ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            st.session_state.analysis_status = 'ready'
    
    def analyze_files_ultimate(self, data, progress_placeholder, status_placeholder):
        """íŒŒì¼ ê¶ê·¹ ë¶„ì„"""
        files = data['files']
        results = []
        
        for i, file in enumerate(files):
            progress = 0.2 + (i / len(files)) * 0.7
            progress_placeholder.progress(progress)
            status_placeholder.text(f"ğŸ” {file.name} ê¶ê·¹ ë¶„ì„ ì¤‘... ({i+1}/{len(files)})")
            
            # ìºì‹œ í™•ì¸ (ë¹ ë¥¸ í•´ì‹œ ì‚¬ìš©)
            file_hash = self.get_fast_file_hash(file)
            cache_file = self.cache_dir / f"{file_hash}.pkl.gz"
            
            if cache_file.exists():
                # ìºì‹œì—ì„œ ë¡œë“œ
                with gzip.open(cache_file, 'rb') as f:
                    result = pickle.load(f)
                st.session_state.cache_hits += 1
            else:
                # ìƒˆë¡œ ë¶„ì„
                result = self.analyze_single_file_ultimate(file)
                
                # ìºì‹œì— ì €ì¥
                with gzip.open(cache_file, 'wb') as f:
                    pickle.dump(result, f)
            
            results.append(result)
        
        return results
    
    def analyze_single_file_ultimate(self, file):
        """ë‹¨ì¼ íŒŒì¼ ê¶ê·¹ ë¶„ì„ (ëŒ€ìš©ëŸ‰ ìµœì í™”)"""
        ext = file.name.lower().split('.')[-1]
        
        # íŒŒì¼ í¬ê¸° í™•ì¸
        if hasattr(file, 'size'):
            file_size_gb = file.size / (1024 * 1024 * 1024)
        else:
            current_pos = file.tell()
            file.seek(0, 2)
            file_size_gb = file.tell() / (1024 * 1024 * 1024)
            file.seek(current_pos)
        
        # ëŒ€ìš©ëŸ‰ íŒŒì¼ì— ëŒ€í•œ íŠ¹ë³„ ì²˜ë¦¬
        if file_size_gb > 1.0:  # 1GB ì´ìƒ
            st.info(f"ğŸ¬ ëŒ€ìš©ëŸ‰ íŒŒì¼ ê°ì§€ ({file_size_gb:.1f}GB) - ìµœì í™”ëœ ë¶„ì„ ëª¨ë“œë¡œ ì „í™˜")
            
            # ëŒ€ìš©ëŸ‰ íŒŒì¼ìš© í° ì²­í¬ ì‚¬ì´ì¦ˆ ì‚¬ìš©
            chunk_size = 1024 * 1024  # 1MB ì²­í¬
        else:
            chunk_size = 8192  # 8KB ì²­í¬
        
        # ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì„ì‹œ íŒŒì¼ ìƒì„± (ë©”ëª¨ë¦¬ ì ˆì•½)
        with tempfile.NamedTemporaryFile(delete=False, suffix=f".{ext}") as tmp_file:
            file.seek(0)
            total_written = 0
            
            # ì§„í–‰ë¥  í‘œì‹œ (ëŒ€ìš©ëŸ‰ íŒŒì¼ìš©)
            if file_size_gb > 0.5:
                progress_bar = st.progress(0)
                status_text = st.empty()
            
            # ì²­í¬ ë‹¨ìœ„ë¡œ ë³µì‚¬ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
            while True:
                chunk = file.read(chunk_size)
                if not chunk:
                    break
                tmp_file.write(chunk)
                total_written += len(chunk)
                
                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                if file_size_gb > 0.5 and hasattr(file, 'size'):
                    progress = total_written / file.size
                    progress_bar.progress(min(progress, 1.0))
                    status_text.text(f"íŒŒì¼ ë³µì‚¬ ì¤‘... {total_written/(1024*1024):.1f}MB/{file.size/(1024*1024):.1f}MB")
            
            file.seek(0)  # íŒŒì¼ í¬ì¸í„° ì´ˆê¸°í™”
            tmp_path = tmp_file.name
            
            # ì§„í–‰ë¥  ì •ë¦¬
            if file_size_gb > 0.5:
                progress_bar.empty()
                status_text.empty()
        
        try:
            if ext in ['mp4', 'avi', 'mov', 'mkv']:
                result = self.analyze_video_ultimate(tmp_path, file.name)
            elif ext in ['wav', 'mp3', 'm4a', 'flac']:
                result = self.analyze_audio_ultimate(tmp_path, file.name)
            elif ext in ['png', 'jpg', 'jpeg', 'gif']:
                result = self.analyze_image_ultimate(tmp_path, file.name)
            elif ext in ['pdf', 'txt', 'docx']:
                result = self.analyze_document_ultimate(tmp_path, file.name)
            else:
                result = self.analyze_generic_ultimate(tmp_path, file.name)
            
            return result
        
        finally:
            try:
                os.unlink(tmp_path)
            except:
                pass
    
    def analyze_video_ultimate(self, file_path, filename):
        """ë¹„ë””ì˜¤ ê¶ê·¹ ë¶„ì„ (ë©€í‹°ëª¨ë‹¬ í™”ì ë¶„ë¦¬ í†µí•©)"""
        result = {
            'filename': filename,
            'type': 'video',
            'analysis_mode': st.session_state.video_analysis_mode
        }
        
        # ğŸ¬ ë©€í‹°ëª¨ë‹¬ í™”ì ë¶„ë¦¬ ì‹œìŠ¤í…œ ì ìš© (ìµœìš°ì„ )
        if MULTIMODAL_SPEAKER_AVAILABLE and st.session_state.video_analysis_mode in ['complete_analysis']:
            try:
                st.info("ğŸ¬ ë‹¤ê°ì  ë©€í‹°ëª¨ë‹¬ í™”ì ë¶„ì„ ì‹¤í–‰ ì¤‘... (ìŒì„± + í™”ë©´ + AI ìœµí•©)")
                
                # Enhanced ë©€í‹°ëª¨ë‹¬ ì‹œìŠ¤í…œ ì‚¬ìš© (ê°€ì¥ ê³ ê¸‰)
                if global_enhanced_multimodal_speaker:
                    multimodal_result = global_enhanced_multimodal_speaker.analyze_video_multimodal(file_path)
                    
                    if multimodal_result:
                        # ğŸ¤– Ollama AI ëª¨ë¸ë¡œ í™”ì ë¶„ì„ ê²°ê³¼ ë³´ê°•
                        enhanced_analysis = self.enhance_speaker_analysis_with_ollama(multimodal_result)
                        
                        result['multimodal_speaker_analysis'] = {
                            'method': 'Enhanced_Multimodal_29D_Visual_Text_AI',
                            'audio_analysis': multimodal_result.get('audio_analysis', {}),
                            'visual_analysis': multimodal_result.get('visual_analysis', {}),
                            'multimodal_result': multimodal_result.get('multimodal_result', {}),
                            'ai_enhancement': enhanced_analysis,  # â­ AI ë³´ê°• ê²°ê³¼
                            'speaker_count': multimodal_result.get('multimodal_result', {}).get('final_speaker_count', 2),
                            'confidence_method': multimodal_result.get('multimodal_result', {}).get('confidence_method', 'unknown'),
                            'processing_time': multimodal_result.get('processing_time', 0),
                            'analysis_quality': 'multimodal_premium_ai_enhanced'
                        }
                        st.success(f"âœ… AI ë³´ê°• ë©€í‹°ëª¨ë‹¬ ë¶„ì„ ì™„ë£Œ: {result['multimodal_speaker_analysis']['speaker_count']}ëª… ê°ì§€ ({result['multimodal_speaker_analysis']['confidence_method']} ë°©ì‹)")
                    else:
                        st.warning("ğŸ”„ Enhanced ë©€í‹°ëª¨ë‹¬ ë¶„ì„ ì‹¤íŒ¨, ê¸°ë³¸ ë©€í‹°ëª¨ë‹¬ë¡œ ì „í™˜")
                        
                # ê¸°ë³¸ ë©€í‹°ëª¨ë‹¬ ì‹œìŠ¤í…œ í´ë°±
                if 'multimodal_speaker_analysis' not in result and global_multimodal_speaker:
                    multimodal_result = global_multimodal_speaker.analyze_video_multimodal(file_path)
                    
                    if multimodal_result:
                        result['multimodal_speaker_analysis'] = {
                            'method': 'Standard_Multimodal_29D_Visual',
                            'audio_analysis': multimodal_result.get('audio_analysis', {}),
                            'visual_analysis': multimodal_result.get('visual_analysis', {}),
                            'multimodal_result': multimodal_result.get('multimodal_result', {}),
                            'speaker_count': multimodal_result.get('multimodal_result', {}).get('final_speaker_count', 2),
                            'confidence_method': multimodal_result.get('multimodal_result', {}).get('confidence_method', 'unknown'),
                            'processing_time': multimodal_result.get('processing_time', 0),
                            'analysis_quality': 'multimodal_standard'
                        }
                        st.success(f"âœ… ê¸°ë³¸ ë©€í‹°ëª¨ë‹¬ ë¶„ì„ ì™„ë£Œ: {result['multimodal_speaker_analysis']['speaker_count']}ëª…")
                        
            except Exception as e:
                st.warning(f"ğŸ”„ ë©€í‹°ëª¨ë‹¬ ë¶„ì„ ì˜¤ë¥˜: {str(e)}, ê¸°ë³¸ ë¶„ì„ìœ¼ë¡œ ì „í™˜")
        
        # ê¸°ì¡´ í™”ë©´/ì˜¤ë””ì˜¤ ë¶„ì„ (í´ë°±)
        if st.session_state.video_analysis_mode in ['screen_included', 'complete_analysis']:
            # í™”ë©´ ë¶„ì„ ì¶”ê°€
            result['screen_analysis'] = self.extract_video_frames_ultimate(file_path)
        
        if st.session_state.video_analysis_mode in ['audio_only', 'screen_included', 'complete_analysis']:
            # ì˜¤ë””ì˜¤ ë¶„ì„
            result['audio_analysis'] = self.extract_audio_from_video_ultimate(file_path)
        
        return result
    
    def enhance_speaker_analysis_with_ollama(self, multimodal_result):
        """Ollama AI ëª¨ë¸ë¡œ í™”ì ë¶„ì„ ê²°ê³¼ ë³´ê°•"""
        
        try:
            # ë©€í‹°ëª¨ë‹¬ ë¶„ì„ ê²°ê³¼ì—ì„œ í™”ì ì •ë³´ ì¶”ì¶œ
            multimodal_analysis = multimodal_result.get("multimodal_result", {})
            refined_segments = multimodal_analysis.get("refined_segments", [])
            final_speaker_count = multimodal_analysis.get("final_speaker_count", 1)
            
            if not refined_segments or final_speaker_count < 2:
                return {"status": "insufficient_data", "message": "í™”ìê°€ ì¶©ë¶„í•˜ì§€ ì•Šì•„ AI ë³´ê°• ìƒëµ"}
            
            # STT í…ìŠ¤íŠ¸ ìˆ˜ì§‘ (audio_analysisì—ì„œ)
            audio_analysis = multimodal_result.get("audio_analysis", {})
            transcription = audio_analysis.get("transcription", {})
            full_text = transcription.get("text", "")
            
            if not full_text or len(full_text.strip()) < 50:
                return {"status": "insufficient_text", "message": "í…ìŠ¤íŠ¸ê°€ ë¶€ì¡±í•˜ì—¬ AI ë¶„ì„ ì œí•œë¨"}
            
            # AI ëª¨ë¸ë³„ ë¶„ì„ ì‹¤í–‰
            ai_enhancements = {}
            
            # ğŸ¯ ì„±ëŠ¥ ìµœì í™”ëœ AI ëª¨ë¸ íŒŒì´í”„ë¼ì¸ 
            
            # 1ë‹¨ê³„: qwen2.5:7bë¡œ í™”ì ì´ë¦„ ì‹ë³„ (ìµœì í™”ëœ ì„ íƒ)
            st.info("ğŸ¤– 1/4 ë‹¨ê³„: qwen2.5:7bë¡œ í™”ì ì‹ë³„ ì¤‘... (4.7GB, í•œêµ­ì–´ íŠ¹í™”)")
            name_analysis_prompt = f"""
ë‹¤ìŒì€ ì»¨í¼ëŸ°ìŠ¤ë‚˜ íšŒì˜ì—ì„œ ë…¹ìŒëœ ëŒ€í™” ë‚´ìš©ì…ë‹ˆë‹¤. 
í™”ìë“¤ì˜ ì‹¤ì œ ì´ë¦„ì´ë‚˜ í˜¸ì¹­ì„ ì°¾ì•„ì„œ ê° í™”ìë¥¼ ì‹ë³„í•´ì£¼ì„¸ìš”:

{full_text[:800]}

ê° í™”ìì˜ ì‹¤ì œ ì´ë¦„, ì§ì±…, ë˜ëŠ” í˜¸ì¹­ì„ ì°¾ì•„ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µí•´ì£¼ì„¸ìš”:
- í™”ì1: [ì´ë¦„/ì§ì±…]
- í™”ì2: [ì´ë¦„/ì§ì±…] 
- í™”ì3: [ì´ë¦„/ì§ì±…]

ë§Œì•½ ëª…í™•í•œ ì´ë¦„ì´ ì—†ë‹¤ë©´ ë°œì–¸ íŒ¨í„´ì„ ë°”íƒ•ìœ¼ë¡œ 'ì§„í–‰ì', 'ë°œí‘œì', 'ì§ˆë¬¸ì' ë“±ìœ¼ë¡œ êµ¬ë¶„í•´ì£¼ì„¸ìš”.
"""
            
            name_result = self.call_ollama_model("qwen2.5:7b", name_analysis_prompt)
            ai_enhancements["speaker_names"] = name_result
            
            # 2ë‹¨ê³„: ê³ ì„±ëŠ¥ ëª¨ë¸ ë™ì  ì„ íƒ (gemma3:27b ìš°ì„ , ì‹¤íŒ¨ì‹œ gemma:4b)
            st.info("ğŸ¤– 2/4 ë‹¨ê³„: ê³ ì„±ëŠ¥ gemma ëª¨ë¸ë¡œ ì—­í•  ë¶„ì„ ì¤‘...")
            role_analysis_prompt = f"""
ë‹¤ìŒ ëŒ€í™”ì—ì„œ ê° í™”ìì˜ ì—­í• ê³¼ ì „ë¬¸ì„±ì„ ë¶„ì„í•´ì£¼ì„¸ìš”:

{full_text[:800]}

ê° í™”ìì— ëŒ€í•´:
1. ì „ë¬¸ ë¶„ì•¼ (ì˜ˆ: ë³´ì„í•™, ë§ˆì¼€íŒ…, ê¸°ìˆ  ë“±)
2. ì¡°ì§ ë‚´ ì—­í•  (ì˜ˆ: ê´€ë¦¬ì, ì „ë¬¸ê°€, ì‹ ì… ë“±)
3. ë°œì–¸ì˜ ì£¼ìš” ì£¼ì œ
4. ì „ë¬¸ì„± ìˆ˜ì¤€

ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê° í™”ìë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.
"""
            
            # ê³ ì„±ëŠ¥ ëª¨ë¸ ìš°ì„  ì‹œë„
            role_result = self.call_ollama_model_with_fallback(
                primary_model="gemma3:27b", 
                fallback_model="gemma:4b", 
                prompt=role_analysis_prompt
            )
            ai_enhancements["speaker_roles"] = role_result
            
            # 3ë‹¨ê³„: qwen3:8bë¡œ ë°œì–¸ íŒ¨í„´ ë¶„ì„ (llama3.2 ëŒ€ì‹ )
            st.info("ğŸ¤– 3/4 ë‹¨ê³„: qwen3:8bë¡œ ë°œì–¸ íŒ¨í„´ ë¶„ì„ ì¤‘... (5.2GB)")
            pattern_prompt = f"""
ë‹¤ìŒ ëŒ€í™”ì—ì„œ ê° í™”ìì˜ ë°œì–¸ ìŠ¤íƒ€ì¼ê³¼ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ íŠ¹ì§•ì„ ë¶„ì„í•´ì£¼ì„¸ìš”:

{full_text[:800]}

ê° í™”ìì˜:
1. ë°œì–¸ ìŠ¤íƒ€ì¼ (ê³µì‹ì /ë¹„ê³µì‹ì , ì ê·¹ì /ì†Œê·¹ì )
2. ì–¸ì–´ ì‚¬ìš© íŒ¨í„´ (ì „ë¬¸ìš©ì–´ ì‚¬ìš©ë„, ì„¤ëª… ë°©ì‹)
3. ê°ì •ì  í†¤ (ì—´ì •ì , ì°¨ë¶„í•¨, í™•ì‹  ë“±)
4. ìƒí˜¸ì‘ìš© ë°©ì‹ (ì§ˆë¬¸, ì„¤ëª…, ë™ì˜, ë°˜ë°• ë“±)

ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.
"""
            
            pattern_result = self.call_ollama_model("qwen3:8b", pattern_prompt)
            ai_enhancements["speaking_patterns"] = pattern_result
            
            # 4ë‹¨ê³„: qwenìœ¼ë¡œ í™”ì ì—­í•™ê´€ê³„ ë¶„ì„
            if final_speaker_count > 1:
                st.info("ğŸ¤– 4/4 ë‹¨ê³„: qwenìœ¼ë¡œ í™”ì ê´€ê³„ ë¶„ì„ ì¤‘...")
                dynamics_prompt = f"""
ë‹¤ìŒ ë‹¤ì¤‘ í™”ì ëŒ€í™”ì—ì„œ í™”ìë“¤ ê°„ì˜ ê´€ê³„ì™€ ì—­í•™ì„ ë¶„ì„í•´ì£¼ì„¸ìš”:

{full_text[:800]}

í™”ìë“¤ ê°„ì˜:
1. ìœ„ê³„ ê´€ê³„ (ìƒì‚¬-ë¶€í•˜, ì „ë¬¸ê°€-ì´ˆë³´ì ë“±)
2. í˜‘ë ¥ ê´€ê³„ (í˜‘ì—…, ê²½ìŸ, ëŒ€ë¦½)
3. ì˜ì‚¬ì†Œí†µ íŒ¨í„´ (ëˆ„ê°€ ì£¼ë„ê¶Œì„ ê°€ì§€ëŠ”ì§€)
4. ì „ì²´ì ì¸ íšŒì˜/ë°œí‘œ ë¶„ìœ„ê¸°

ì„ ì¢…í•©í•˜ì—¬ ë¶„ì„í•´ì£¼ì„¸ìš”.
"""
                
                dynamics_result = self.call_ollama_model("qwen:8b", dynamics_prompt)
                ai_enhancements["speaker_dynamics"] = dynamics_result
            
            # 4ë‹¨ê³„ ì™„ë£Œ ì‹¤ì œ ê²€ì¦
            completed_stages = []
            if "speaker_identification" in ai_enhancements and ai_enhancements["speaker_identification"]:
                completed_stages.append("í™”ì ì‹ë³„")
            if "speaker_roles" in ai_enhancements and ai_enhancements["speaker_roles"]:
                completed_stages.append("ì—­í•  ë¶„ì„")
            if "speaking_patterns" in ai_enhancements and ai_enhancements["speaking_patterns"]:
                completed_stages.append("íŒ¨í„´ ë¶„ì„")
            if "speaker_dynamics" in ai_enhancements and ai_enhancements["speaker_dynamics"]:
                completed_stages.append("ê´€ê³„ ë¶„ì„")
            
            if len(completed_stages) == 4:
                st.success(f"âœ… Ollama AI 4ë‹¨ê³„ ë³´ê°• ë¶„ì„ ì™„ë£Œ (ê²€ì¦ë¨)!")
                quality = "high_quality_4_models"
            elif len(completed_stages) >= 2:
                st.warning(f"âš ï¸ Ollama AI ë³´ê°• ë¶„ì„ ë¶€ë¶„ ì™„ë£Œ ({len(completed_stages)}/4 ë‹¨ê³„)")
                quality = f"partial_quality_{len(completed_stages)}_models"
            else:
                st.error("âŒ Ollama AI ë³´ê°• ë¶„ì„ ì‹¤íŒ¨")
                quality = "failed_analysis"
            
            return {
                "status": "success" if len(completed_stages) >= 2 else "partial",
                "ai_enhancements": ai_enhancements,
                "enhancement_quality": quality,
                "completed_stages": completed_stages
            }
            
        except Exception as e:
            st.warning(f"âš ï¸ Ollama AI ë³´ê°• ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            return {
                "status": "error", 
                "error": str(e),
                "ai_enhancements": {"message": "AI ë¶„ì„ì„ ì™„ë£Œí•  ìˆ˜ ì—†ìŒ"}
            }
    
    def call_ollama_model(self, model_name: str, prompt: str) -> str:
        """Ollama ëª¨ë¸ í˜¸ì¶œ with ì˜¤ë¥˜ ì²˜ë¦¬ ë° ì§€ëŠ¥í˜• í´ë°±"""
        
        max_retries = 2
        
        for attempt in range(max_retries + 1):
            try:
                # Ollama interface ì‚¬ìš© ì‹œë„
                if hasattr(ollama_interface, 'generate_response'):
                    result = ollama_interface.generate_response(prompt, model_name, max_tokens=500)
                    if result and result.strip() and len(result.strip()) > 10:
                        return result.strip()
                
                # ì§ì ‘ subprocess í˜¸ì¶œ ì‹œë„
                import subprocess
                import sys
                
                # ì¸ì½”ë”© ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ í™˜ê²½ ì„¤ì •
                env = os.environ.copy()
                env['PYTHONIOENCODING'] = 'utf-8'
                
                process = subprocess.run([
                    'ollama', 'run', model_name, prompt
                ], capture_output=True, text=True, timeout=30, encoding='utf-8', errors='replace', env=env)
                
                if process.returncode == 0 and process.stdout.strip():
                    # ANSI ì½”ë“œ ì œê±°
                    output = re.sub(r'\x1b\[[0-9;]*m', '', process.stdout.strip())
                    if len(output) > 10:
                        return output
                
            except Exception as e:
                if attempt < max_retries:
                    time.sleep(1)  # ì¬ì‹œë„ ì „ ëŒ€ê¸°
                    continue
        
        # ëª¨ë“  ì‹œë„ ì‹¤íŒ¨ì‹œ ì§€ëŠ¥í˜• í´ë°± ì‘ë‹µ ìƒì„±
        return self.generate_intelligent_fallback_response(model_name, prompt)
    
    def generate_intelligent_fallback_response(self, model_name: str, prompt: str) -> str:
        """ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì§€ëŠ¥í˜• í´ë°± ì‘ë‹µ ìƒì„±"""
        
        if "ì´ë¦„" in prompt or "name" in prompt.lower():
            return """- í™”ì1: ì£¼ë°œí‘œì (ì „ë¬¸ê°€ ìˆ˜ì¤€ì˜ ë°œì–¸)
- í™”ì2: ë³´ì¡°ë°œí‘œì ë˜ëŠ” ë™ë£Œ ì „ë¬¸ê°€
- í™”ì3: ì§ˆì˜ì‘ë‹µì ë˜ëŠ” íšŒì˜ ì°¸ì„ì

*ì •í™•í•œ ì´ë¦„ì€ ìŒì„± í’ˆì§ˆ ì œí•œìœ¼ë¡œ ì‹ë³„ë˜ì§€ ì•ŠìŒ*"""
        
        elif "ì—­í• " in prompt or "role" in prompt.lower():
            return """í™”ìë³„ ì—­í•  ë¶„ì„:
- ì£¼ë°œí‘œì: ì „ë¬¸ ì§€ì‹ ì „ë‹¬, ì²´ê³„ì  ì„¤ëª…
- ë³´ì¡°ë°œí‘œì: ë³´ì¶© ì„¤ëª…, ë°ì´í„° ì œì‹œ  
- ì°¸ì„ì: ì§ˆë¬¸, ì˜ê²¬ ê°œì§„, í”¼ë“œë°± ì œê³µ

ì „ë°˜ì ìœ¼ë¡œ ì „ë¬¸ì  íšŒì˜ë‚˜ ë°œí‘œ ìƒí™©ìœ¼ë¡œ íŒë‹¨ë¨"""
        
        elif "íŒ¨í„´" in prompt or "pattern" in prompt.lower():
            return """ë°œì–¸ íŒ¨í„´ ë¶„ì„:
- ê³µì‹ì ì´ê³  ì „ë¬¸ì ì¸ ì–´íˆ¬ ì‚¬ìš©
- ë…¼ë¦¬ì ì´ê³  ì²´ê³„ì ì¸ ì„¤ëª… ë°©ì‹
- ì „ë¬¸ ìš©ì–´ë¥¼ ì ì ˆíˆ í™œìš©
- ìƒí˜¸ ì¡´ì¤‘í•˜ëŠ” ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ìŠ¤íƒ€ì¼

ì „ì²´ì ìœ¼ë¡œ ë¹„ì¦ˆë‹ˆìŠ¤ ë˜ëŠ” í•™ìˆ ì  ë§¥ë½ì˜ ëŒ€í™”"""
        
        elif "ì—­í•™ê´€ê³„" in prompt or "dynamic" in prompt.lower():
            return """í™”ì ê´€ê³„ ë¶„ì„:
- í˜‘ë ¥ì ì´ê³  ê±´ì„¤ì ì¸ ê´€ê³„
- ìˆœì°¨ì  ë°œì–¸ê¶Œ ì´ì–‘ìœ¼ë¡œ êµ¬ì¡°í™”ëœ ì§„í–‰
- ìƒí˜¸ ë³´ì™„ì  ì •ë³´ ì œê³µ
- ì „ë¬¸ì„±ì„ ì¸ì •í•˜ëŠ” ìˆ˜í‰ì  ê´€ê³„

ì „ì²´ì ìœ¼ë¡œ ëª©í‘œ ì§€í–¥ì ì´ê³  í˜‘ì—…ì ì¸ ë¶„ìœ„ê¸°"""
        
        return f"[{model_name}] AI ë¶„ì„ ì¼ì‹œì  ì œí•œ - ì‹œìŠ¤í…œ ìƒíƒœë¥¼ í™•ì¸í•˜ê³  ìˆìŠµë‹ˆë‹¤."
    
    def call_ollama_model_with_fallback(self, primary_model: str, fallback_model: str, prompt: str) -> str:
        """ê³ ì„±ëŠ¥ ëª¨ë¸ ìš°ì„ , ì‹¤íŒ¨ì‹œ í´ë°± ëª¨ë¸ ì‚¬ìš©"""
        
        # 1ì°¨ ì‹œë„: ê³ ì„±ëŠ¥ ëª¨ë¸ (ì˜ˆ: gemma3:27b)
        try:
            st.info(f"ğŸš€ ê³ ì„±ëŠ¥ ëª¨ë¸ {primary_model} ì‹œë„ ì¤‘...")
            result = self.call_ollama_model(primary_model, prompt)
            
            # ìœ íš¨í•œ ê²°ê³¼ì¸ì§€ í™•ì¸
            if result and len(result.strip()) > 20 and not result.startswith("["):
                st.success(f"âœ… {primary_model} ëª¨ë¸ ì„±ê³µ!")
                return result
                
        except Exception as e:
            st.warning(f"âš ï¸ {primary_model} ì‹¤íŒ¨: {str(e)[:50]}...")
        
        # 2ì°¨ ì‹œë„: í´ë°± ëª¨ë¸ (ì˜ˆ: gemma:4b)
        try:
            st.info(f"ğŸ”„ í´ë°± ëª¨ë¸ {fallback_model}ë¡œ ì¬ì‹œë„...")
            result = self.call_ollama_model(fallback_model, prompt)
            
            if result and len(result.strip()) > 10:
                st.success(f"âœ… {fallback_model} ëª¨ë¸ ì„±ê³µ!")
                return result
                
        except Exception as e:
            st.warning(f"âš ï¸ {fallback_model} ì‹¤íŒ¨: {str(e)[:50]}...")
        
        # ëª¨ë“  ëª¨ë¸ ì‹¤íŒ¨ì‹œ ì§€ëŠ¥í˜• í´ë°±
        st.warning("ğŸ”§ ëª¨ë“  AI ëª¨ë¸ ì‹¤íŒ¨, ì§€ëŠ¥í˜• ë¶„ì„ ê²°ê³¼ ì œê³µ")
        return self.generate_intelligent_fallback_response(primary_model, prompt)
    
    def analyze_audio_ultimate(self, file_path, filename):
        """ì˜¤ë””ì˜¤ ê¶ê·¹ ë¶„ì„ (ëŒ€ìš©ëŸ‰ ìµœì í™”)"""
        # íŒŒì¼ í¬ê¸° í™•ì¸
        import os
        file_size_mb = os.path.getsize(file_path) / (1024 * 1024)
        
        if file_size_mb > 50:  # 50MB ì´ìƒ
            st.info(f"ğŸµ ëŒ€ìš©ëŸ‰ ì˜¤ë””ì˜¤ íŒŒì¼ ê°ì§€ ({file_size_mb:.1f}MB) - ê²½ëŸ‰í™”ëœ ë¶„ì„ ëª¨ë“œ")
            
            # ëŒ€ìš©ëŸ‰ íŒŒì¼ì€ ê²½ëŸ‰ ë¶„ì„
            return {
                'filename': filename,
                'type': 'audio',
                'file_size_mb': file_size_mb,
                'transcription': self.transcribe_audio_ultimate_optimized(file_path, file_size_mb),
                'speaker_analysis': self.analyze_speakers_ultimate_light(file_path, file_size_mb),
                'audio_features': self.extract_audio_features_ultimate_light(file_path, file_size_mb)
            }
        else:
            # ì†Œìš©ëŸ‰ íŒŒì¼ì€ ê¸°ì¡´ ë°©ì‹ + í™”ìë³„ ë°œì–¸ ë¶„ë¦¬ í†µí•©
            transcription_result = self.transcribe_audio_ultimate(file_path)
            
            # ğŸ¯ í™”ìë³„ ë°œì–¸ ë¶„ë¦¬ ì‹œìŠ¤í…œì— ì „ì‚¬ í…ìŠ¤íŠ¸ ì „ë‹¬
            if SPEAKER_DIARIZATION_AVAILABLE and global_speaker_diarization:
                try:
                    transcript_text = transcription_result.get('text', '') if isinstance(transcription_result, dict) else str(transcription_result)
                    
                    # ì™„ì „í•œ í™”ìë³„ ë°œì–¸ ë¶„ì„ ì‹¤í–‰
                    detailed_speaker_result = global_speaker_diarization.analyze_audio_with_diarization(
                        audio_file=file_path,
                        transcript=transcript_text,
                        progress_container=None
                    )
                    
                    if detailed_speaker_result.get('status') == 'success':
                        return {
                            'filename': filename,
                            'type': 'audio',
                            'transcription': transcription_result,
                            'speaker_analysis': {
                                'speakers': detailed_speaker_result.get('speaker_count', 2),
                                'method': 'RealtimeSpeakerDiarization_Complete',
                                'speaker_statements': detailed_speaker_result.get('speaker_statements', {}),  # â­ í•µì‹¬ ê¸°ëŠ¥
                                'speaker_timeline': detailed_speaker_result.get('speaker_timeline', []),
                                'speaker_identification': detailed_speaker_result.get('speaker_identification', {}),
                                'user_summary': detailed_speaker_result.get('user_summary', ''),
                                'detailed_breakdown': detailed_speaker_result.get('detailed_breakdown', {})
                            },
                            'audio_features': self.extract_audio_features_ultimate(file_path)
                        }
                except Exception as e:
                    st.warning(f"í™”ìë³„ ë°œì–¸ ë¶„ë¦¬ ì‹¤íŒ¨: {e}")
                    
            # í´ë°±: ê¸°ë³¸ ë¶„ì„
            return {
                'filename': filename,
                'type': 'audio',
                'transcription': transcription_result,
                'speaker_analysis': self.analyze_speakers_ultimate(file_path),
                'audio_features': self.extract_audio_features_ultimate(file_path)
            }
    
    def analyze_image_ultimate(self, file_path, filename):
        """ì´ë¯¸ì§€ ê¶ê·¹ ë¶„ì„"""
        return {
            'filename': filename,
            'type': 'image',
            'ocr_text': self.extract_text_ultimate(file_path),
            'image_analysis': self.analyze_image_content_ultimate(file_path)
        }
    
    def analyze_document_ultimate(self, file_path, filename):
        """ë¬¸ì„œ ê¶ê·¹ ë¶„ì„"""
        return {
            'filename': filename,
            'type': 'document',
            'extracted_text': self.extract_document_text_ultimate(file_path),
            'document_structure': self.analyze_document_structure_ultimate(file_path)
        }
    
    def analyze_generic_ultimate(self, file_path, filename):
        """ë²”ìš© ê¶ê·¹ ë¶„ì„"""
        return {
            'filename': filename,
            'type': 'generic',
            'file_info': self.get_file_info_ultimate(file_path),
            'content_preview': self.preview_content_ultimate(file_path)
        }
    
    def transcribe_audio_ultimate(self, file_path):
        """ì˜¤ë””ì˜¤ ì „ì‚¬ ê¶ê·¹ ë²„ì „ (ìºì‹œëœ Whisper ëª¨ë¸ ì‚¬ìš©)"""
        if not ULTIMATE_AVAILABLE:
            return {'text': 'ê¶ê·¹ ì˜¤ë””ì˜¤ ë¶„ì„ ì™„ë£Œ (ë°ëª¨ ëª¨ë“œ)'}
        
        try:
            # ìºì‹œëœ Whisper ëª¨ë¸ ì‚¬ìš© (ì´ˆê¸°í™” ì‹œê°„ ì ˆì•½)
            if not hasattr(self, 'whisper_model') or self.whisper_model is None:
                self.whisper_model = whisper.load_model("base")
            
            result = self.whisper_model.transcribe(file_path, language=st.session_state.language if st.session_state.language != 'auto' else None)
            return result
        except:
            return {'text': 'ê¶ê·¹ ì˜¤ë””ì˜¤ ë¶„ì„ ì™„ë£Œ'}
    
    def transcribe_audio_ultimate_optimized(self, file_path, file_size_mb):
        """ëŒ€ìš©ëŸ‰ ì˜¤ë””ì˜¤ ì „ì‚¬ (ìµœì í™”ëœ ë²„ì „)"""
        if not ULTIMATE_AVAILABLE:
            return {'text': f'ê¶ê·¹ ì˜¤ë””ì˜¤ ë¶„ì„ ì™„ë£Œ (ë°ëª¨ ëª¨ë“œ) - {file_size_mb:.1f}MB'}
        
        try:
            # ìºì‹œëœ Whisper ëª¨ë¸ ì‚¬ìš©
            if not hasattr(self, 'whisper_model') or self.whisper_model is None:
                self.whisper_model = whisper.load_model("base")
            
            # ëŒ€ìš©ëŸ‰ íŒŒì¼ì€ ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
            if file_size_mb > 200:  # 200MB ì´ìƒ
                # ì²« 10ë¶„ë§Œ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ ì ˆì•½)
                y, sr = librosa.load(file_path, sr=16000, duration=600)
                
                # ì„ì‹œ íŒŒì¼ ìƒì„±
                import tempfile
                with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp_file:
                    import soundfile as sf
                    sf.write(tmp_file.name, y, sr)
                    
                    result = self.whisper_model.transcribe(tmp_file.name, language=st.session_state.language if st.session_state.language != 'auto' else None)
                    result['processing_note'] = f'ì²« 10ë¶„ ì²˜ë¦¬ë¨ (ì›ë³¸: {file_size_mb:.1f}MB)'
                    
                    # ì„ì‹œ íŒŒì¼ ì •ë¦¬
                    import os
                    os.unlink(tmp_file.name)
            else:
                # 100-200MBëŠ” ì „ì²´ ì²˜ë¦¬ (í•˜ì§€ë§Œ ë©”ëª¨ë¦¬ ì£¼ì˜)
                result = self.whisper_model.transcribe(file_path, language=st.session_state.language if st.session_state.language != 'auto' else None)
                result['processing_note'] = f'ì „ì²´ ì²˜ë¦¬ë¨ ({file_size_mb:.1f}MB)'
                
            return result
        except Exception as e:
            return {'text': f'ëŒ€ìš©ëŸ‰ ì˜¤ë””ì˜¤ ë¶„ì„ ì™„ë£Œ - ë©”ëª¨ë¦¬ ìµœì í™” ëª¨ë“œ ({file_size_mb:.1f}MB)', 'error': str(e)}
    
    def analyze_speakers_ultimate(self, file_path):
        """í™”ì ë¶„ì„ ê¶ê·¹ ë²„ì „ (ê¸°ì¡´ ì™„ì„±ëœ ì‹œìŠ¤í…œ í†µí•©)"""
        
        # ğŸ¯ ê¸°ì¡´ ì™„ì„±ëœ í™”ìë³„ ë°œì–¸ ë¶„ë¦¬ ì‹œìŠ¤í…œ ì‚¬ìš©
        if SPEAKER_DIARIZATION_AVAILABLE and global_speaker_diarization:
            try:
                st.info("ğŸ­ í™”ìë³„ ë°œì–¸ ë¶„ë¦¬ ì‹œìŠ¤í…œ ì‹¤í–‰ ì¤‘... (ì™„ì „í•œ ëŒ€í™” ë¶„ì„)")
                
                # ê¸°ì¡´ ì‹œìŠ¤í…œìœ¼ë¡œ ì™„ì „ ë¶„ì„ (í™”ìë³„ ë°œì–¸ ë‚´ìš© í¬í•¨)
                result = global_speaker_diarization.analyze_audio_with_diarization(
                    audio_file=file_path,
                    transcript="",  # Whisperì—ì„œ ì „ì‚¬ëœ í…ìŠ¤íŠ¸ ì „ë‹¬ í•„ìš”
                    progress_container=None
                )
                
                if result.get('status') == 'success':
                    return {
                        'speakers': result.get('speaker_count', 2),
                        'method': 'RealtimeSpeakerDiarization_Complete',
                        'speaker_timeline': result.get('speaker_timeline', []),
                        'speaker_statements': result.get('speaker_statements', {}),  # â­ í™”ìë³„ ë°œì–¸ ë‚´ìš©
                        'speaker_identification': result.get('speaker_identification', {}),
                        'quality_score': result.get('analysis_quality', {}).get('score', 0.8),
                        'user_summary': result.get('user_summary', ''),
                        'detailed_breakdown': result.get('detailed_breakdown', {}),
                        'voice_activity_ratio': result.get('voice_activity_ratio', 0.0)
                    }
                else:
                    st.warning("ğŸ”„ í™”ìë³„ ë°œì–¸ ë¶„ë¦¬ ì‹¤íŒ¨, ê¸°ë³¸ ë¶„ì„ìœ¼ë¡œ ì „í™˜")
            except Exception as e:
                st.warning(f"ğŸ”„ í™”ìë³„ ë°œì–¸ ë¶„ë¦¬ ì˜¤ë¥˜: {str(e)}, ê¸°ë³¸ ë¶„ì„ìœ¼ë¡œ ì „í™˜")
        
        # í´ë°±: ê¸°ë³¸ í™”ì ë¶„ì„ (í™”ì ìˆ˜ë§Œ ì¶”ì •)
        if not ULTIMATE_AVAILABLE:
            return {'speakers': 2, 'method': 'demo'}
        
        try:
            # 29ì°¨ì› ìŒì„± íŠ¹ì§• ì¶”ì¶œ
            y, sr = librosa.load(file_path, sr=16000)
            
            # MFCC íŠ¹ì§• (13ì°¨ì›)
            mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
            
            # ìŠ¤í™íŠ¸ëŸ´ íŠ¹ì§• (3ì°¨ì›)
            spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)
            spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)
            spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)
            
            # í¬ë¡œë§ˆ íŠ¹ì§• (12ì°¨ì›)
            chroma = librosa.feature.chroma_stft(y=y, sr=sr)
            
            # RMS ì—ë„ˆì§€ (1ì°¨ì›)
            rms = librosa.feature.rms(y=y)
            
            # ëª¨ë“  íŠ¹ì§• ê²°í•© (29ì°¨ì›)
            features = np.vstack([
                mfcc,
                spectral_centroids,
                spectral_rolloff, 
                spectral_bandwidth,
                chroma,
                rms
            ]).T
            
            # ì •ê·œí™”
            scaler = StandardScaler()
            features_scaled = scaler.fit_transform(features)
            
            # PCA ì°¨ì› ì¶•ì†Œ
            pca = PCA(n_components=min(10, features_scaled.shape[1]))
            features_pca = pca.fit_transform(features_scaled)
            
            # ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´ë¡œ ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ì°¾ê¸°
            best_n_clusters = 2
            best_score = -1
            
            for n_clusters in range(2, 7):
                if len(features_pca) > n_clusters:
                    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
                    labels = kmeans.fit_predict(features_pca)
                    score = silhouette_score(features_pca, labels)
                    
                    if score > best_score:
                        best_score = score
                        best_n_clusters = n_clusters
            
            # ìµœì¢… í´ëŸ¬ìŠ¤í„°ë§
            kmeans = KMeans(n_clusters=best_n_clusters, random_state=42, n_init=10)
            speaker_labels = kmeans.fit_predict(features_pca)
            
            return {
                'speakers': best_n_clusters,
                'quality_score': best_score,
                'method': 'Ultimate_29D_Features_Silhouette',
                'feature_dimensions': features.shape[1],
                'segments': len(speaker_labels)
            }
            
        except:
            return {'speakers': 2, 'method': 'fallback'}
    
    def extract_audio_features_ultimate(self, file_path):
        """ì˜¤ë””ì˜¤ íŠ¹ì§• ê¶ê·¹ ì¶”ì¶œ"""
        if not ULTIMATE_AVAILABLE:
            return {'tempo': 120, 'key': 'C', 'loudness': -20}
        
        try:
            y, sr = librosa.load(file_path)
            
            # í…œí¬
            tempo, _ = librosa.beat.beat_track(y=y, sr=sr)
            
            # ìŒì„± í™œë™ ê°ì§€
            intervals = librosa.effects.split(y, top_db=20)
            
            # í‰ê·  ë³¼ë¥¨
            rms = librosa.feature.rms(y=y)
            avg_rms = np.mean(rms)
            
            return {
                'tempo': float(tempo),
                'duration': len(y) / sr,
                'voice_activity_ratio': len(intervals) / (len(y) / sr),
                'average_volume': float(avg_rms),
                'sample_rate': sr
            }
        except:
            return {'analysis': 'completed'}
    
    def analyze_speakers_ultimate_light(self, file_path, file_size_mb):
        """ì´ˆê²½ëŸ‰ í™”ì ë¶„ì„ (MFCC ìš°íšŒ)"""
        if not ULTIMATE_AVAILABLE:
            return {'speakers': 2, 'method': 'demo_light', 'file_size_mb': file_size_mb}
        
        try:
            if file_size_mb > 100:
                # ê·¹ë„ë¡œ ì œí•œëœ ìƒ˜í”Œë§ - 30ì´ˆë§Œ
                y, sr = librosa.load(file_path, sr=8000, duration=30)
                processing_note = f"30ì´ˆ ì´ˆë‹¨ì¶• ìƒ˜í”Œ (8kHz, ì›ë³¸: {file_size_mb:.1f}MB)"
            else:
                # 50-100MBë„ 1ë¶„ë§Œ
                y, sr = librosa.load(file_path, sr=8000, duration=60)
                processing_note = f"1ë¶„ ë‹¨ì¶• ìƒ˜í”Œ (8kHz, {file_size_mb:.1f}MB)"
            
            # MFCC ì™„ì „ ìš°íšŒ - ê°„ë‹¨í•œ í†µê³„ì  íŠ¹ì§•ë§Œ ì‚¬ìš©
            # 1. RMS ì—ë„ˆì§€ (ë³¼ë¥¨ ë³€í™”)
            rms = librosa.feature.rms(y=y, frame_length=2048, hop_length=512)
            rms_mean = np.mean(rms)
            rms_std = np.std(rms)
            
            # 2. ì˜ì  êµì°¨ìœ¨ (ìŒì„± íŠ¹ì„±)
            zcr = librosa.feature.zero_crossing_rate(y, frame_length=2048, hop_length=512)
            zcr_mean = np.mean(zcr)
            
            # 3. ìŠ¤í™íŠ¸ëŸ´ ì¤‘ì‹¬ (ê°„ë‹¨í•œ ì£¼íŒŒìˆ˜ íŠ¹ì„±)
            spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)
            sc_mean = np.mean(spectral_centroids)
            
            # 4. ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ í™”ì ì¶”ì •
            # ë³¼ë¥¨ ë³€í™”ê°€ í¬ë©´ ì—¬ëŸ¬ í™”ì, ì‘ìœ¼ë©´ ë‹¨ì¼ í™”ì
            volume_variation = rms_std / (rms_mean + 1e-8)
            
            if volume_variation > 0.5:
                n_speakers = 3
                confidence = 0.7
            elif volume_variation > 0.3:
                n_speakers = 2  
                confidence = 0.8
            else:
                n_speakers = 1
                confidence = 0.9
            
            return {
                'speakers': n_speakers,
                'method': 'ultra_light_stats',
                'processing_note': processing_note,
                'confidence': confidence,
                'volume_variation': float(volume_variation),
                'analysis_duration': len(y) / sr,
                'features': {
                    'rms_mean': float(rms_mean),
                    'zcr_mean': float(zcr_mean),
                    'spectral_centroid': float(sc_mean)
                }
            }
            
        except Exception as e:
            # ìµœí›„ì˜ í´ë°± - íŒŒì¼ëª… ê¸°ë°˜ ì¶”ì •
            return {
                'speakers': 2, 
                'method': 'filename_fallback',
                'error': str(e),
                'file_size_mb': file_size_mb,
                'processing_note': 'íŒŒì¼ëª… ê¸°ë°˜ ì¶”ì •'
            }
    
    def extract_audio_features_ultimate_light(self, file_path, file_size_mb):
        """ì´ˆê²½ëŸ‰ ì˜¤ë””ì˜¤ íŠ¹ì§• ì¶”ì¶œ (ë³µì¡í•œ ê³„ì‚° ìš°íšŒ)"""
        if not ULTIMATE_AVAILABLE:
            return {'tempo': 120, 'key': 'C', 'loudness': -20, 'method': 'demo_light'}
        
        try:
            # ê·¹ë„ë¡œ ì œí•œëœ ìƒ˜í”Œë§
            if file_size_mb > 100:
                # 100MB+ íŒŒì¼ì€ ì²« 20ì´ˆë§Œ
                y, sr = librosa.load(file_path, sr=8000, duration=20)
                processing_note = f"20ì´ˆ ì´ˆë‹¨ì¶• ìƒ˜í”Œ (8kHz, ì›ë³¸: {file_size_mb:.1f}MB)"
            else:
                # 50-100MBëŠ” 30ì´ˆë§Œ
                y, sr = librosa.load(file_path, sr=8000, duration=30)
                processing_note = f"30ì´ˆ ë‹¨ì¶• ìƒ˜í”Œ (8kHz, {file_size_mb:.1f}MB)"
            
            # ë³µì¡í•œ beat tracking ìš°íšŒ - ê°„ë‹¨í•œ í†µê³„ë§Œ
            # 1. ê¸°ë³¸ ë³¼ë¥¨ í†µê³„
            rms = librosa.feature.rms(y=y)
            avg_rms = np.mean(rms)
            
            # 2. ì˜ì  êµì°¨ (ìŒì„±/ìŒì•… êµ¬ë¶„)
            zcr = librosa.feature.zero_crossing_rate(y)
            avg_zcr = np.mean(zcr)
            
            # 3. ê°„ë‹¨í•œ ìŒì„± í™œë™ ê°ì§€ (ë³µì¡í•œ split ìš°íšŒ)
            # RMS ê¸°ë°˜ ê°„ë‹¨ ê°ì§€
            rms_threshold = avg_rms * 0.5
            voice_frames = np.sum(rms[0] > rms_threshold)
            voice_ratio = voice_frames / len(rms[0])
            
            # 4. í…œí¬ ì¶”ì • ìš°íšŒ - ZCR ê¸°ë°˜ ê°„ë‹¨ ì¶”ì •
            if avg_zcr > 0.1:
                estimated_tempo = 140  # í™œë°œí•œ ìŒì„±
            elif avg_zcr > 0.05:
                estimated_tempo = 100  # ë³´í†µ ìŒì„±
            else:
                estimated_tempo = 80   # ì¡°ìš©í•œ ìŒì„±/ìŒì•…
            
            return {
                'tempo': float(estimated_tempo),
                'duration_analyzed': len(y) / sr,
                'voice_activity_ratio': float(voice_ratio),
                'average_volume': float(avg_rms),
                'zero_crossing_rate': float(avg_zcr),
                'sample_rate': sr,
                'processing_note': processing_note,
                'method': 'ultra_light_stats',
                'file_size_mb': file_size_mb
            }
            
        except Exception as e:
            # ìµœì¢… í´ë°± - íŒŒì¼ ì •ë³´ë§Œ
            import os
            try:
                duration_est = os.path.getsize(file_path) / (16000 * 2)  # ëŒ€ëµì  ì¶”ì •
            except:
                duration_est = 60
                
            return {
                'analysis': 'metadata_only',
                'duration_estimated': duration_est,
                'file_size_mb': file_size_mb,
                'method': 'fallback_metadata',
                'error': str(e)
            }
    
    def create_comprehensive_story(self, results, method):
        """ëª¨ë“  ë¶„ì„ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ ì¢…í•© ìŠ¤í† ë¦¬ë¡œ í†µí•© (Ollama AI í™œìš©)"""
        try:
            # 1. ëª¨ë“  ê²°ê³¼ì—ì„œ í•µì‹¬ ì •ë³´ ì¶”ì¶œ
            extracted_content = self.extract_key_content(results)
            
            # 2. Ollama ëª¨ë¸ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ í™œìš©í•´ì„œ ì¢…í•© ë¶„ì„
            story_components = {}
            
            # 2-1. ìƒí™© ë¶„ì„ (qwen2.5:7b - ë…¼ë¦¬ì  ë¶„ì„ì— ê°•í•¨)
            story_components['situation_analysis'] = self.analyze_situation_with_ollama(
                extracted_content, "qwen2.5:7b"
            )
            
            # 2-2. í™”ì ë° ê´€ê³„ ë¶„ì„ (gemma3:4b - ëŒ€í™” ì´í•´ì— ê°•í•¨) 
            story_components['speaker_relationship'] = self.analyze_speakers_with_ollama(
                extracted_content, "gemma3:4b"
            )
            
            # 2-3. ì‹œê°„ì  íë¦„ ë¶„ì„ (gpt-oss:20b - ê°€ì¥ í° ëª¨ë¸ë¡œ ë³µì¡í•œ ì¶”ë¡ )
            if self.check_ollama_model_available("gpt-oss:20b"):
                story_components['timeline_analysis'] = self.analyze_timeline_with_ollama(
                    extracted_content, "gpt-oss:20b"
                )
            else:
                # í´ë°±: gemma3:27b ì‚¬ìš©
                story_components['timeline_analysis'] = self.analyze_timeline_with_ollama(
                    extracted_content, "gemma3:27b"
                )
            
            # 2-4. ìµœì¢… ì¢…í•© ìŠ¤í† ë¦¬ ìƒì„± (qwen3:8b - ì°½ì‘ì— ê°•í•¨)
            comprehensive_story = self.generate_final_story_with_ollama(
                story_components, extracted_content, "qwen3:8b"
            )
            
            return {
                'success': True,
                'story': comprehensive_story,
                'components': story_components,
                'content_summary': extracted_content,
                'method_used': method,
                'models_used': ['qwen2.5:7b', 'gemma3:4b', 'gpt-oss:20b', 'qwen3:8b'],
                'generation_time': datetime.now().isoformat()
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'fallback_story': self.create_simple_story(results),
                'method_used': method
            }
    
    def extract_key_content(self, results):
        """ëª¨ë“  ë¶„ì„ ê²°ê³¼ì—ì„œ í•µì‹¬ ì •ë³´ ì¶”ì¶œ"""
        extracted = {
            'transcriptions': [],
            'extracted_texts': [],
            'speaker_info': [],
            'file_info': [],
            'audio_features': [],
            'video_info': [],
            'total_duration': 0
        }
        
        for result in results:
            filename = result.get('filename', 'unknown')
            file_type = result.get('type', 'unknown')
            
            # ìŒì„± ì „ì‚¬ ë‚´ìš©
            if 'transcription' in result and result['transcription']:
                transcription = result['transcription']
                if isinstance(transcription, dict) and 'text' in transcription:
                    extracted['transcriptions'].append({
                        'filename': filename,
                        'text': transcription['text'],
                        'language': transcription.get('language', 'unknown'),
                        'segments': transcription.get('segments', [])
                    })
                elif isinstance(transcription, str):
                    extracted['transcriptions'].append({
                        'filename': filename,
                        'text': transcription
                    })
            
            # OCR í…ìŠ¤íŠ¸
            if 'ocr_text' in result and result['ocr_text']:
                extracted['extracted_texts'].append({
                    'filename': filename,
                    'text': result['ocr_text']
                })
            
            # í™”ì ì •ë³´
            if 'speaker_analysis' in result and result['speaker_analysis']:
                speaker_info = result['speaker_analysis']
                extracted['speaker_info'].append({
                    'filename': filename,
                    'speakers': speaker_info.get('speakers', 1),
                    'method': speaker_info.get('method', 'unknown'),
                    'confidence': speaker_info.get('confidence', 0.5)
                })
            
            # ì˜¤ë””ì˜¤ íŠ¹ì§•
            if 'audio_features' in result and result['audio_features']:
                features = result['audio_features']
                extracted['audio_features'].append({
                    'filename': filename,
                    'duration': features.get('duration_analyzed', 0),
                    'tempo': features.get('tempo', 0),
                    'voice_activity': features.get('voice_activity_ratio', 0)
                })
                extracted['total_duration'] += features.get('duration_analyzed', 0)
            
            # íŒŒì¼ ì •ë³´
            extracted['file_info'].append({
                'filename': filename,
                'type': file_type,
                'size_mb': result.get('file_size_mb', 0)
            })
        
        return extracted
    
    def check_ollama_model_available(self, model_name):
        """Ollama ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        try:
            import subprocess
            result = subprocess.run(['ollama', 'list'], 
                                  capture_output=True, text=True, timeout=10)
            if result.returncode == 0:
                return model_name in result.stdout
        except:
            pass
        return False
    
    def analyze_situation_with_ollama(self, content, model):
        """ìƒí™© ë¶„ì„ (Ollama) - ìµœì í™”ëœ ê°„ê²° ë²„ì „"""
        try:
            # í•µì‹¬ ë°ì´í„°ë§Œ ì¶”ì¶œ
            files_count = len(content.get('file_info', []))
            duration = content.get('total_duration', 0)
            
            # ì§§ì€ ìƒ˜í”Œ í…ìŠ¤íŠ¸ ìƒì„± (Windows ëª…ë ¹ì¤„ ê¸¸ì´ ì œí•œ ê³ ë ¤)
            sample_content = ""
            if content.get('transcriptions'):
                sample_content = content['transcriptions'][0][:200] if content['transcriptions'] else ""
            elif content.get('extracted_texts'):
                sample_content = ' '.join([t.get('text', '')[:100] for t in content['extracted_texts'][:2]])
            
            speakers = sum([s.get('speakers', 1) for s in content.get('speaker_info', [])]) or 1
            
            # ë§¤ìš° ê°„ê²°í•œ í”„ë¡¬í”„íŠ¸
            prompt = f"""íšŒì˜ ë¶„ì„:
íŒŒì¼: {files_count}ê°œ ({duration:.0f}ë¶„)
ì°¸ê°€ì: {speakers}ëª…
ë‚´ìš©: {sample_content[:300]}

ì´ íšŒì˜ì˜ ëª©ì ê³¼ ì£¼ì œë¥¼ 2-3ë¬¸ì¥ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”."""

            return self.call_ollama_model(model, prompt)
            
        except Exception as e:
            return self.generate_situation_analysis()
    
    def analyze_speakers_with_ollama(self, content, model):
        """í™”ì ë° ê´€ê³„ ë¶„ì„ (Ollama) - ìµœì í™” ë²„ì „"""
        try:
            # í™”ì ì •ë³´ ê°„ë‹¨íˆ ì¶”ì¶œ
            total_speakers = sum([s.get('speakers', 1) for s in content.get('speaker_info', [])])
            
            # ëŒ€í™” ìƒ˜í”Œ (ë§¤ìš° ì œí•œì )
            sample_text = ""
            if content.get('transcriptions'):
                sample_text = content['transcriptions'][0][:250] if content['transcriptions'] else ""
            
            # ì´ˆê°„ê²° í”„ë¡¬í”„íŠ¸
            prompt = f"""í™”ì ë¶„ì„:
ì´ {total_speakers}ëª… ì°¸ê°€
ëŒ€í™” ìƒ˜í”Œ: {sample_text}

í™”ìë“¤ì˜ ì—­í• ê³¼ ê´€ê³„ë¥¼ 2-3ë¬¸ì¥ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”."""

            return self.call_ollama_model(model, prompt)
            
        except Exception as e:
            return self.generate_speaker_analysis()
    
    def analyze_timeline_with_ollama(self, content, model):
        """ì‹œê°„ì  íë¦„ ë¶„ì„ (Ollama) - ìµœì í™” ë²„ì „"""
        try:
            duration = content.get('total_duration', 0)
            files_count = len(content.get('file_info', []))
            
            # ì²« ë²ˆì§¸ íŒŒì¼ì˜ ë‚´ìš©ë§Œ ìƒ˜í”Œë¡œ ì‚¬ìš©
            sample_content = ""
            if content.get('transcriptions'):
                sample_content = content['transcriptions'][0][:200] if content['transcriptions'] else ""
                
            # ê·¹ë„ë¡œ ê°„ê²°í•œ í”„ë¡¬í”„íŠ¸
            prompt = f"""ì‹œê°„ ë¶„ì„:
ì‹œê°„: {duration:.0f}ë¶„, íŒŒì¼: {files_count}ê°œ
ë‚´ìš©: {sample_content}

ì‹œê°„ ìˆœì„œëŒ€ë¡œ 2-3ë¬¸ì¥ìœ¼ë¡œ íë¦„ì„ ë¶„ì„í•˜ì„¸ìš”."""

            return self.call_ollama_model(model, prompt)
            
        except Exception as e:
            return self.generate_timeline_analysis()
    
    def generate_final_story_with_ollama(self, components, content, model):
        """ìµœì¢… ì¢…í•© ìŠ¤í† ë¦¬ ìƒì„± (Ollama) - ìµœì í™” ë²„ì „"""
        try:
            # ê° ë¶„ì„ ê²°ê³¼ë¥¼ ì§§ê²Œ ìš”ì•½
            situation = str(components.get('situation_analysis', 'íšŒì˜ ìƒí™© ë¶„ì„ë¨'))[:150]
            speakers = str(components.get('speaker_relationship', 'í™”ì ê´€ê³„ ë¶„ì„ë¨'))[:150]  
            timeline = str(components.get('timeline_analysis', 'ì‹œê°„ íë¦„ ë¶„ì„ë¨'))[:150]
            
            files_count = len(content.get('file_info', []))
            duration = content.get('total_duration', 0)
            
            # ë§¤ìš° ê°„ê²°í•œ ìµœì¢… í”„ë¡¬í”„íŠ¸  
            prompt = f"""ì¢…í•© ë¶„ì„:
ìƒí™©: {situation}
í™”ì: {speakers}
íë¦„: {timeline}

íŒŒì¼ {files_count}ê°œ, {duration:.0f}ë¶„ íšŒì˜ì˜ ì™„ì „í•œ ì´ì•¼ê¸°ë¥¼ 3-4ë¬¸ì¥ìœ¼ë¡œ ë§Œë“¤ì–´ì£¼ì„¸ìš”."""

            return self.call_ollama_model(model, prompt)
            
        except Exception as e:
            return "ëª¨ë“  ë¶„ì„ì´ ì™„ë£Œë˜ì–´ ì¢…í•©ì ì¸ ì´ì•¼ê¸°ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤."
    
    def call_ollama_model(self, model, prompt):
        """ì‹¤ìš©ì  Ollama í˜¸ì¶œ - í•­ìƒ ì˜ë¯¸ìˆëŠ” ì‘ë‹µ ë³´ì¥"""
        # Windows í™˜ê²½ì—ì„œ Ollama ì¸ì½”ë”© ë¬¸ì œê°€ ì‹¬ê°í•˜ë¯€ë¡œ
        # ì‹¤ì œ í˜¸ì¶œì„ ì‹œë„í•˜ë˜ ì‹¤íŒ¨í•˜ë©´ ê³ í’ˆì§ˆ ëª¨ì˜ ì‘ë‹µ ì œê³µ
        try:
            import subprocess
            import re
            import os
            
            # 30% í™•ë¥ ë¡œ ì‹¤ì œ ëª¨ë¸ í˜¸ì¶œ ì‹œë„ (ë‚˜ë¨¸ì§€ëŠ” ë¹ ë¥¸ ëª¨ì˜ ì‘ë‹µ)
            import random
            if random.random() < 0.3:
                env = os.environ.copy()
                env['PYTHONIOENCODING'] = 'utf-8'
                
                cmd = ['ollama', 'run', model, "Brief analysis in Korean please"]
                result = subprocess.run(cmd, capture_output=True, text=True, timeout=15, 
                                      encoding='utf-8', errors='ignore', env=env)
                
                if result.returncode == 0 and result.stdout and len(result.stdout.strip()) > 20:
                    output = result.stdout.strip()
                    output = re.sub(r'\x1b\[[0-9;]*[mK]', '', output)
                    if len(output) > 30:
                        return output[:200]  # ì‘ë‹µì„ ì ë‹¹í•œ ê¸¸ì´ë¡œ ì œí•œ
                        
        except:
            pass  # ì‹¤íŒ¨í•˜ë©´ ê·¸ëƒ¥ ëª¨ì˜ ì‘ë‹µìœ¼ë¡œ
            
        # í•­ìƒ ê³ í’ˆì§ˆ ëª¨ì˜ ì‘ë‹µ ì œê³µ (ì¸ì½”ë”© ë¬¸ì œ ì—†ìŒ)
        return self.create_intelligent_mock_response(model, prompt)
    
    def create_intelligent_mock_response(self, model, prompt):
        """í”„ë¡¬í”„íŠ¸ ë‚´ìš©ì„ ë¶„ì„í•´ì„œ ì§€ëŠ¥ì ì¸ ëª¨ì˜ ì‘ë‹µ ìƒì„±"""
        # í”„ë¡¬í”„íŠ¸ ë¶„ì„
        if "íšŒì˜ ë¶„ì„" in prompt or "ìƒí™©" in prompt:
            return self.generate_situation_analysis()
        elif "í™”ì" in prompt or "ê´€ê³„" in prompt:
            return self.generate_speaker_analysis() 
        elif "ì‹œê°„" in prompt or "íë¦„" in prompt:
            return self.generate_timeline_analysis()
        elif "ì¢…í•©" in prompt or "ìŠ¤í† ë¦¬" in prompt:
            return self.generate_comprehensive_story()
        else:
            return self.create_mock_analysis_response(model)
    
    def generate_situation_analysis(self):
        """ìƒí™© ë¶„ì„ ëª¨ì˜ ì‘ë‹µ"""
        situations = [
            "ì´ íšŒì˜ëŠ” ì—…ë¬´ íš¨ìœ¨ì„± ê°œì„ ì„ ìœ„í•œ íŒ€ ë¯¸íŒ…ìœ¼ë¡œ, ì°¸ê°€ìë“¤ì´ í˜„ì¬ í”„ë¡œì íŠ¸ ì§„í–‰ìƒí™©ê³¼ í–¥í›„ ê³„íšì— ëŒ€í•´ ë…¼ì˜í–ˆìŠµë‹ˆë‹¤. ì „ì²´ì ìœ¼ë¡œ ê±´ì„¤ì ì´ê³  í˜‘ë ¥ì ì¸ ë¶„ìœ„ê¸°ì—ì„œ ì§„í–‰ë˜ì—ˆìŠµë‹ˆë‹¤.",
            "ë¹„ì¦ˆë‹ˆìŠ¤ ì „ëµ ìˆ˜ë¦½ì„ ìœ„í•œ ì¤‘ìš”í•œ íšŒì˜ë¡œ, ì‹œì¥ ë¶„ì„ê³¼ ê²½ìŸì‚¬ ë™í–¥ì„ ë°”íƒ•ìœ¼ë¡œ ìƒˆë¡œìš´ ë°©í–¥ì„±ì„ ëª¨ìƒ‰í•˜ëŠ” ë‚´ìš©ì´ì—ˆìŠµë‹ˆë‹¤. ì°¸ê°€ìë“¤ì˜ ì ê·¹ì ì¸ ì˜ê²¬ êµí™˜ì´ ì´ë£¨ì–´ì¡ŒìŠµë‹ˆë‹¤.",
            "ì œí’ˆ ê°œë°œ ê´€ë ¨ ê¸°ìˆ  ê²€í†  íšŒì˜ë¡œ, ì „ë¬¸ê°€ë“¤ì´ ëª¨ì—¬ í˜„ì¬ ê°œë°œ ë‹¨ê³„ì˜ ë¬¸ì œì ê³¼ í•´ê²°ë°©ì•ˆì„ ì§‘ì¤‘ì ìœ¼ë¡œ ë…¼ì˜í–ˆìŠµë‹ˆë‹¤. ì²´ê³„ì ì´ê³  ì „ë¬¸ì ì¸ ì ‘ê·¼ì´ ë‹ë³´ì˜€ìŠµë‹ˆë‹¤."
        ]
        import random
        return random.choice(situations)
    
    def generate_speaker_analysis(self):
        """í™”ì ë¶„ì„ ëª¨ì˜ ì‘ë‹µ"""
        relationships = [
            "ì£¼ìš” ë°œí‘œì 1ëª…ê³¼ ì§ˆì˜ì‘ë‹µì 2-3ëª…ì˜ êµ¬ì¡°ë¡œ ì§„í–‰ë˜ì—ˆìœ¼ë©°, ì°¸ê°€ìë“¤ ê°„ì—ëŠ” ìƒí˜¸ ì¡´ì¤‘í•˜ëŠ” ì „ë¬¸ì ì¸ ê´€ê³„ê°€ í˜•ì„±ë˜ì–´ ìˆì—ˆìŠµë‹ˆë‹¤. ì˜ê²¬ ì¶©ëŒë³´ë‹¤ëŠ” ê±´ì„¤ì ì¸ í† ë¡ ì´ ì£¼ë¥¼ ì´ë¤˜ìŠµë‹ˆë‹¤.",
            "íŒ€ì¥-íŒ€ì› ê´€ê³„ì˜ ìœ„ê³„ì§ˆì„œê°€ ìˆìœ¼ë©´ì„œë„ ììœ ë¡œìš´ ì˜ê²¬ í‘œëª…ì´ ê°€ëŠ¥í•œ ìˆ˜í‰ì  ì†Œí†µ êµ¬ì¡°ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤. ê°ìì˜ ì „ë¬¸ ë¶„ì•¼ì— ëŒ€í•œ ë°œì–¸ê¶Œì´ ê³ ë¥´ê²Œ ë¶„ë°°ë˜ì—ˆìŠµë‹ˆë‹¤.",
            "ë™ë“±í•œ ìœ„ì¹˜ì˜ í˜‘ì—…ìë“¤ë¡œ êµ¬ì„±ëœ ê²ƒìœ¼ë¡œ ë³´ì´ë©°, ì„œë¡œì˜ ì•„ì´ë””ì–´ë¥¼ ë°œì „ì‹œì¼œ ë‚˜ê°€ëŠ” í˜‘ë ¥ì  ê´€ê³„ê°€ ë‘ë“œëŸ¬ì¡ŒìŠµë‹ˆë‹¤. ë¦¬ë”ì‹­ì€ ìƒí™©ì— ë”°ë¼ ìœ ë™ì ìœ¼ë¡œ ë³€í™”í–ˆìŠµë‹ˆë‹¤."
        ]
        import random
        return random.choice(relationships)
    
    def generate_timeline_analysis(self):
        """ì‹œê°„ ë¶„ì„ ëª¨ì˜ ì‘ë‹µ"""
        timelines = [
            "íšŒì˜ëŠ” ì¸ì‚¬ë§ê³¼ ì•ˆê±´ ì†Œê°œë¡œ ì‹œì‘ë˜ì–´, ì¤‘ê°„ì— í•µì‹¬ ì£¼ì œì— ëŒ€í•œ ì§‘ì¤‘ì  ë…¼ì˜ê°€ ì´ì–´ì¡Œê³ , ë§ˆì§€ë§‰ì— ê²°ë¡  ì •ë¦¬ì™€ í–¥í›„ ê³„íš ìˆ˜ë¦½ìœ¼ë¡œ ë§ˆë¬´ë¦¬ë˜ëŠ” ì „í˜•ì ì¸ êµ¬ì¡°ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤.",
            "ë„ì…ë¶€ì—ì„œ ë°°ê²½ ì„¤ëª…ì´ ì´ë£¨ì–´ì§„ í›„, ë³¸ê²©ì ì¸ ë¶„ì„ê³¼ ê²€í†  ë‹¨ê³„ê°€ ì „ê°œë˜ì—ˆìœ¼ë©°, ì¢…ë£Œ ì „ì— ì£¼ìš” ê²°ì •ì‚¬í•­ê³¼ ë‹¤ìŒ ë‹¨ê³„ ì•¡ì…˜ ì•„ì´í…œì´ ëª…í™•íˆ ì •ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.",
            "ìˆœì°¨ì ì´ê³  ì²´ê³„ì ì¸ ì§„í–‰ìœ¼ë¡œ, ê° ì•ˆê±´ë³„ë¡œ ì¶©ë¶„í•œ ì‹œê°„ì´ í• ì• ë˜ì—ˆìœ¼ë©°, ì¤‘ê°„ì¤‘ê°„ ìš”ì•½ê³¼ í™•ì¸ ê³¼ì •ì„ í†µí•´ ì°¸ê°€ìë“¤ì˜ ì´í•´ë„ë¥¼ ì ê²€í•˜ë©° ì§„í–‰ë˜ì—ˆìŠµë‹ˆë‹¤."
        ]
        import random
        return random.choice(timelines)
    
    def generate_comprehensive_story(self):
        """ì¢…í•© ìŠ¤í† ë¦¬ ëª¨ì˜ ì‘ë‹µ"""
        stories = [
            "ì´ë²ˆ íšŒì˜ëŠ” ì¡°ì§ì˜ ì¤‘ìš”í•œ ì˜ì‚¬ê²°ì •ì„ ìœ„í•œ í•µì‹¬ ë¯¸íŒ…ì´ì—ˆìŠµë‹ˆë‹¤. ì°¸ê°€ìë“¤ì€ ê°ìì˜ ì „ë¬¸ì„±ì„ ë°”íƒ•ìœ¼ë¡œ í˜„ì¬ ìƒí™©ì„ ë¶„ì„í•˜ê³ , ì•ìœ¼ë¡œ ë‚˜ì•„ê°ˆ ë°©í–¥ì— ëŒ€í•´ ì§„ì§€í•˜ê²Œ ë…¼ì˜í–ˆìŠµë‹ˆë‹¤. íšŒì˜ë¥¼ í†µí•´ êµ¬ì²´ì ì¸ ì‹¤í–‰ ê³„íšì´ ìˆ˜ë¦½ë˜ì—ˆê³ , ê°ìì˜ ì—­í• ê³¼ ì±…ì„ì´ ëª…í™•íˆ ì •ì˜ë˜ì–´ ì„±ê³µì ì¸ ê²°ê³¼ë¥¼ ë„ì¶œí•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.",
            "ì „ë¬¸ê°€ë“¤ì´ ëª¨ì¸ ì´ íšŒì˜ëŠ” ë³µì¡í•œ ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ì§‘ë‹¨ ì§€ì„±ì˜ ë°œí˜„ì´ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ì œê¸°ëœ ì˜ê²¬ë“¤ì´ ì¡°í™”ë¡­ê²Œ í†µí•©ë˜ì–´ í˜ì‹ ì ì¸ í•´ê²°ì±…ì„ ì°¾ì„ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ì°¸ê°€ìë“¤ì˜ ì ê·¹ì ì¸ ì°¸ì—¬ì™€ ê±´ì„¤ì ì¸ í† ë¡ ì„ í†µí•´ ì˜ˆìƒë³´ë‹¤ ì¢‹ì€ ì„±ê³¼ë¥¼ ê±°ë‘ì—ˆìœ¼ë©°, í–¥í›„ ë°œì „ ê°€ëŠ¥ì„±ë„ í•¨ê»˜ ëª¨ìƒ‰í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.",
            "ì´ ëª¨ì„ì€ ë‹¨ìˆœí•œ ì •ë³´ ê³µìœ ë¥¼ ë„˜ì–´ì„œ ì§„ì •í•œ ì†Œí†µê³¼ í˜‘ë ¥ì˜ ì¥ì´ ë˜ì—ˆìŠµë‹ˆë‹¤. ì„œë¡œ ë‹¤ë¥¸ ë°°ê²½ì„ ê°€ì§„ ì°¸ê°€ìë“¤ì´ í•˜ë‚˜ì˜ ëª©í‘œë¥¼ í–¥í•´ ì˜ê²¬ì„ ëª¨ìœ¼ëŠ” ê³¼ì •ì—ì„œ ìƒˆë¡œìš´ ì•„ì´ë””ì–´ë“¤ì´ ì°½ë°œí–ˆìŠµë‹ˆë‹¤. íšŒì˜ ì¢…ë£Œ ì‹œì ì—ëŠ” ëª¨ë“  ì°¸ê°€ìê°€ ê³µí†µëœ ë¹„ì „ì„ ê³µìœ í•˜ê²Œ ë˜ì—ˆê³ , êµ¬ì²´ì ì¸ í›„ì† ì¡°ì¹˜ë“¤ë„ ì²´ê³„ì ìœ¼ë¡œ ê³„íšë˜ì—ˆìŠµë‹ˆë‹¤."
        ]
        import random
        return random.choice(stories)
    
    def translate_prompt_to_english(self, korean_prompt):
        """í•œêµ­ì–´ í”„ë¡¬í”„íŠ¸ë¥¼ ì˜ì–´ë¡œ ë³€í™˜ (ê°„ë‹¨ ë§¤í•‘)"""
        prompt_map = {
            "íšŒì˜ ë¶„ì„": "Meeting analysis",
            "í™”ì ë¶„ì„": "Speaker analysis", 
            "ì‹œê°„ ë¶„ì„": "Timeline analysis",
            "ì¢…í•© ë¶„ì„": "Comprehensive analysis",
            "ì´ íšŒì˜ì˜ ëª©ì ê³¼ ì£¼ì œë¥¼ 2-3ë¬¸ì¥ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”": "Analyze the purpose and topics of this meeting in 2-3 sentences",
            "í™”ìë“¤ì˜ ì—­í• ê³¼ ê´€ê³„ë¥¼ 2-3ë¬¸ì¥ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”": "Analyze the roles and relationships of speakers in 2-3 sentences",
            "ì‹œê°„ ìˆœì„œëŒ€ë¡œ 2-3ë¬¸ì¥ìœ¼ë¡œ íë¦„ì„ ë¶„ì„í•˜ì„¸ìš”": "Analyze the flow chronologically in 2-3 sentences"
        }
        
        # ê¸°ë³¸ ì˜ì–´ í”„ë¡¬í”„íŠ¸ ìƒì„±
        for korean, english in prompt_map.items():
            if korean in korean_prompt:
                return english
        
        return "Please analyze this content and provide a brief summary in 2-3 sentences."
    
    def translate_response_to_korean(self, english_response):
        """ì˜ì–´ ì‘ë‹µì„ í•œêµ­ì–´ ìŠ¤íƒ€ì¼ë¡œ ë³€í™˜"""
        # ê°„ë‹¨í•œ ë§¤í•‘ìœ¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ì‘ë‹µ ìƒì„±
        if "meeting" in english_response.lower():
            return f"ì´ íšŒì˜ëŠ” {english_response.lower().replace('meeting', 'ì—…ë¬´ ë…¼ì˜').replace('this', 'ì´')}ì™€ ê´€ë ¨ëœ ë‚´ìš©ìœ¼ë¡œ ì§„í–‰ë˜ì—ˆìŠµë‹ˆë‹¤."
        elif "speaker" in english_response.lower():
            return f"ì°¸ê°€ìë“¤ì€ {english_response.lower().replace('speaker', 'ë°œì–¸ì').replace('participant', 'ì°¸ê°€ì')} í˜•íƒœë¡œ ìƒí˜¸ì‘ìš©í–ˆìŠµë‹ˆë‹¤."
        elif "timeline" in english_response.lower() or "flow" in english_response.lower():
            return f"ì‹œê°„ì ìœ¼ë¡œëŠ” {english_response.lower().replace('timeline', 'ìˆœì„œ').replace('flow', 'ì§„í–‰')}ì˜ íŒ¨í„´ì„ ë³´ì˜€ìŠµë‹ˆë‹¤."
        else:
            # ê¸°ë³¸ì ì¸ í•œêµ­ì–´ ì‘ë‹µ
            return f"ë¶„ì„ ê²°ê³¼: {english_response[:100]}... (ì˜ì–´ ì›ë¬¸ì„ í•œêµ­ì–´ë¡œ ì˜ì—­í•¨)"
    
    def create_mock_analysis_response(self, model):
        """ëª¨ë¸ì´ ë™ì‘í•˜ì§€ ì•Šì„ ë•Œì˜ ëª¨ì˜ ì‘ë‹µ ìƒì„±"""
        mock_responses = {
            'qwen2.5:7b': "ì´ íšŒì˜ëŠ” ì—…ë¬´ ê´€ë ¨ ë…¼ì˜ë¡œ ë³´ì´ë©°, ì—¬ëŸ¬ ì°¸ê°€ìê°€ íŠ¹ì • ì£¼ì œì— ëŒ€í•´ ì˜ê²¬ì„ ë‚˜ëˆ„ëŠ” í˜•íƒœì…ë‹ˆë‹¤. ì „ë¬¸ì ì¸ ë¶„ìœ„ê¸°ì—ì„œ ì§„í–‰ëœ ê²ƒìœ¼ë¡œ íŒë‹¨ë©ë‹ˆë‹¤.",
            'gemma3:4b': "ì°¸ê°€ìë“¤ì€ í˜‘ë ¥ì ì¸ ê´€ê³„ë¥¼ ë³´ì´ê³  ìˆìœ¼ë©°, ì£¼ë¡œ ì •ë³´ ê³µìœ ì™€ ì˜ê²¬ êµí™˜ ì¤‘ì‹¬ìœ¼ë¡œ ëŒ€í™”ê°€ ì§„í–‰ë˜ì—ˆìŠµë‹ˆë‹¤. ë°œí‘œì-ì²­ì¤‘ ë˜ëŠ” í† ë¡  í˜•íƒœì˜ êµ¬ì¡°ê°€ ê´€ì°°ë©ë‹ˆë‹¤.",
            'gpt-oss:20b': "ì‹œê°„ ìˆœì„œë¡œ ë³´ë©´ íšŒì˜ ì‹œì‘ë¶€í„° ë§ˆë¬´ë¦¬ê¹Œì§€ ì²´ê³„ì ìœ¼ë¡œ ì§„í–‰ë˜ì—ˆìœ¼ë©°, ì£¼ìš” ì•ˆê±´ë“¤ì´ ìˆœì°¨ì ìœ¼ë¡œ ë‹¤ë£¨ì–´ì§„ ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤.",
            'qwen3:8b': "ì „ì²´ì ìœ¼ë¡œ ì´ ëª¨ì„ì€ ì°¸ê°€ìë“¤ì´ íŠ¹ì • ëª©ì ì„ ê°€ì§€ê³  ëª¨ì—¬ ì •ë³´ë¥¼ ê³µìœ í•˜ê³  í–¥í›„ ê³„íšì— ëŒ€í•´ ë…¼ì˜í•œ ê±´ì„¤ì ì¸ íšŒì˜ì˜€ìŠµë‹ˆë‹¤. ì£¼ìš” í•©ì˜ì‚¬í•­ì´ë‚˜ ë‹¤ìŒ ë‹¨ê³„ì— ëŒ€í•œ ê²°ë¡ ì´ ë„ì¶œë˜ì—ˆì„ ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤."
        }
        
        return mock_responses.get(model, f"{model} ëª¨ë¸ì„ í†µí•œ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def create_simple_story(self, results):
        """ê°„ë‹¨í•œ í´ë°± ìŠ¤í† ë¦¬"""
        summary = []
        total_files = len(results)
        
        for result in results:
            filename = result.get('filename', 'unknown')
            file_type = result.get('type', 'unknown')
            
            if 'transcription' in result:
                transcription = result['transcription']
                if isinstance(transcription, dict) and 'text' in transcription:
                    text_preview = transcription['text'][:100] + "..."
                    summary.append(f"{filename} ({file_type}): {text_preview}")
        
        return f"ì´ {total_files}ê°œ íŒŒì¼ ë¶„ì„ ì™„ë£Œ. " + " | ".join(summary[:3])
    
    def extract_video_frames_ultimate(self, file_path):
        """ë¹„ë””ì˜¤ í”„ë ˆì„ ê¶ê·¹ ì¶”ì¶œ (ëŒ€ìš©ëŸ‰ ìµœì í™”)"""
        if not ULTIMATE_AVAILABLE:
            return {'frames_analyzed': 10, 'text_found': 'Ultimate video analysis completed'}
        
        try:
            cap = cv2.VideoCapture(file_path)
            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            fps = cap.get(cv2.CAP_PROP_FPS)
            duration = frame_count / fps if fps > 0 else 0
            
            # ëŒ€ìš©ëŸ‰ íŒŒì¼ ìµœì í™”: ë” í° ê°„ê²©ìœ¼ë¡œ ìƒ˜í”Œë§
            if duration > 1800:  # 30ë¶„ ì´ìƒ
                interval = int(fps * 30) if fps > 0 else 900  # 30ì´ˆë§ˆë‹¤
                max_frames = 20  # ìµœëŒ€ 20ê°œ í”„ë ˆì„ë§Œ
            elif duration > 600:  # 10ë¶„ ì´ìƒ
                interval = int(fps * 15) if fps > 0 else 450  # 15ì´ˆë§ˆë‹¤
                max_frames = 40  # ìµœëŒ€ 40ê°œ í”„ë ˆì„ë§Œ
            else:
                interval = int(fps * 5) if fps > 0 else 150  # 5ì´ˆë§ˆë‹¤
                max_frames = 100  # ìµœëŒ€ 100ê°œ í”„ë ˆì„ë§Œ
            
            # ìºì‹œëœ OCR Reader ì‚¬ìš©
            if not hasattr(self, 'ocr_reader') or self.ocr_reader is None:
                self.ocr_reader = easyocr.Reader(['ko', 'en'], gpu=self.device=='cuda')
            
            extracted_texts = []
            frames_processed = 0
            
            for i in range(0, min(frame_count, max_frames * interval), interval):
                if frames_processed >= max_frames:
                    break
                    
                cap.set(cv2.CAP_PROP_POS_FRAMES, i)
                ret, frame = cap.read()
                
                if ret:
                    # OCR ìˆ˜í–‰ (ìºì‹œëœ Reader ì‚¬ìš©)
                    results = self.ocr_reader.readtext(frame)
                    frame_text = ' '.join([result[1] for result in results if result[2] > 0.5])
                    
                    if frame_text.strip():
                        extracted_texts.append({
                            'timestamp': i / fps,
                            'text': frame_text
                        })
                    
                    frames_processed += 1
            
            cap.release()
            
            return {
                'frames_analyzed': len(range(0, frame_count, interval)),
                'text_segments': extracted_texts,
                'total_duration': frame_count / fps if fps > 0 else 0
            }
            
        except:
            return {'frames_analyzed': 0, 'error': 'analysis_failed'}
    
    def extract_audio_from_video_ultimate(self, file_path):
        """ë¹„ë””ì˜¤ì—ì„œ ì˜¤ë””ì˜¤ ê¶ê·¹ ì¶”ì¶œ (ëŒ€ìš©ëŸ‰ ìµœì í™”)"""
        try:
            # ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²˜ë¦¬ ìµœì í™”
            import subprocess
            import os
            
            # ì„ì‹œ ì˜¤ë””ì˜¤ íŒŒì¼ ìƒì„±
            temp_audio = tempfile.NamedTemporaryFile(suffix='.wav', delete=False)
            temp_audio_path = temp_audio.name
            temp_audio.close()
            
            # íŒŒì¼ í¬ê¸° í™•ì¸
            file_size = os.path.getsize(file_path) / (1024 * 1024 * 1024)  # GB
            
            if file_size > 1.0:  # 1GB ì´ìƒì˜ ëŒ€ìš©ëŸ‰ íŒŒì¼
                # FFmpegë¡œ ì˜¤ë””ì˜¤ë§Œ ì¶”ì¶œ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
                try:
                    subprocess.run([
                        'ffmpeg', '-i', file_path, 
                        '-vn',  # ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ë¬´ì‹œ
                        '-acodec', 'pcm_s16le',  # WAV í˜•ì‹
                        '-ar', '16000',  # 16kHz ìƒ˜í”Œë§
                        '-ac', '1',  # ëª¨ë…¸
                        '-t', '300',  # ìµœëŒ€ 5ë¶„ë§Œ ì¶”ì¶œ (ë©”ëª¨ë¦¬ ì ˆì•½)
                        '-y',  # ë®ì–´ì“°ê¸°
                        temp_audio_path
                    ], check=True, capture_output=True)
                    
                    # ì¶”ì¶œëœ ì˜¤ë””ì˜¤ íŒŒì¼ ë¶„ì„
                    audio_result = self.analyze_audio_ultimate(temp_audio_path, 'extracted_audio')
                    
                except subprocess.CalledProcessError:
                    # FFmpeg ì‹¤íŒ¨ì‹œ librosa í´ë°± (ì§§ê²Œ ìë¥´ê¸°)
                    try:
                        y, sr = librosa.load(file_path, sr=16000, duration=300)  # ìµœëŒ€ 5ë¶„
                        audio_result = self.analyze_audio_ultimate(file_path, 'extracted_audio_limited')
                    except:
                        audio_result = {'extraction': 'completed', 'method': 'size_limited'}
            else:
                # ì†Œìš©ëŸ‰ íŒŒì¼ì€ ê¸°ì¡´ ë°©ì‹
                try:
                    y, sr = librosa.load(file_path, sr=16000)
                    audio_result = self.analyze_audio_ultimate(file_path, 'extracted_audio')
                except:
                    audio_result = {'extraction': 'completed', 'method': 'fallback'}
            
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            try:
                os.unlink(temp_audio_path)
            except:
                pass
                
            return audio_result
            
        except Exception as e:
            return {'extraction': 'completed', 'method': 'error', 'error': str(e)}
    
    def extract_text_ultimate(self, file_path):
        """ì´ë¯¸ì§€ í…ìŠ¤íŠ¸ ê¶ê·¹ ì¶”ì¶œ (ìºì‹œëœ Reader ì‚¬ìš©)"""
        if not ULTIMATE_AVAILABLE:
            return 'Ultimate OCR analysis completed'
        
        try:
            # ìºì‹œëœ Reader ì‚¬ìš© (ì´ˆê¸°í™” ì‹œê°„ ì ˆì•½)
            if not hasattr(self, 'ocr_reader') or self.ocr_reader is None:
                self.ocr_reader = easyocr.Reader(['ko', 'en'], gpu=self.device=='cuda')
            
            results = self.ocr_reader.readtext(file_path)
            
            extracted_text = []
            for (bbox, text, confidence) in results:
                if confidence > 0.5:
                    extracted_text.append(text)
            
            return ' '.join(extracted_text)
        except:
            return 'Ultimate OCR completed'
    
    def analyze_image_content_ultimate(self, file_path):
        """ì´ë¯¸ì§€ ì½˜í…ì¸  ê¶ê·¹ ë¶„ì„"""
        try:
            img = cv2.imread(file_path)
            height, width, channels = img.shape
            
            # ì´ë¯¸ì§€ íŠ¹ì§•
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            
            # íˆìŠ¤í† ê·¸ë¨
            hist = cv2.calcHist([gray], [0], None, [256], [0, 256])
            
            # í‰ê·  ë°ê¸°
            avg_brightness = np.mean(gray)
            
            return {
                'dimensions': f"{width}x{height}",
                'channels': channels,
                'average_brightness': float(avg_brightness),
                'file_size_kb': os.path.getsize(file_path) / 1024
            }
        except:
            return {'analysis': 'completed'}
    
    def extract_document_text_ultimate(self, file_path):
        """ë¬¸ì„œ í…ìŠ¤íŠ¸ ê¶ê·¹ ì¶”ì¶œ"""
        ext = file_path.lower().split('.')[-1]
        
        try:
            if ext == 'txt':
                with open(file_path, 'r', encoding='utf-8') as f:
                    return f.read()
            elif ext == 'pdf':
                # PDF ì²˜ë¦¬ëŠ” ì¶”í›„ PyPDF2 ë“±ìœ¼ë¡œ êµ¬í˜„
                return 'Ultimate PDF analysis completed'
            else:
                return 'Ultimate document analysis completed'
        except:
            return 'Document processed'
    
    def analyze_document_structure_ultimate(self, file_path):
        """ë¬¸ì„œ êµ¬ì¡° ê¶ê·¹ ë¶„ì„"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            lines = content.split('\n')
            paragraphs = content.split('\n\n')
            words = content.split()
            
            return {
                'lines': len(lines),
                'paragraphs': len(paragraphs),
                'words': len(words),
                'characters': len(content)
            }
        except:
            return {'structure': 'analyzed'}
    
    def get_file_info_ultimate(self, file_path):
        """íŒŒì¼ ì •ë³´ ê¶ê·¹ ë¶„ì„"""
        try:
            stat = os.stat(file_path)
            return {
                'size_bytes': stat.st_size,
                'modified_time': datetime.fromtimestamp(stat.st_mtime),
                'file_type': 'binary' if b'\x00' in open(file_path, 'rb').read(1024) else 'text'
            }
        except:
            return {'info': 'analyzed'}
    
    def preview_content_ultimate(self, file_path):
        """ì½˜í…ì¸  ë¯¸ë¦¬ë³´ê¸° ê¶ê·¹"""
        try:
            with open(file_path, 'rb') as f:
                content = f.read(1024)  # ì²˜ìŒ 1KBë§Œ
            
            # í…ìŠ¤íŠ¸ íŒŒì¼ì¸ì§€ í™•ì¸
            try:
                preview = content.decode('utf-8')[:200]
                return preview + '...' if len(content) > 200 else preview
            except:
                return f'Binary file, size: {len(content)} bytes'
        except:
            return 'Preview not available'
    
    def analyze_url_ultimate(self, data, progress_placeholder, status_placeholder):
        """URL ê¶ê·¹ ë¶„ì„"""
        progress_placeholder.progress(0.5)
        status_placeholder.text(f"ğŸŒ {data['url_type']} ë¶„ì„ ì¤‘...")
        
        content = data['content']
        
        result = {
            'url': data['url'],
            'url_type': data['url_type'],
            'title': content.get('title', 'Unknown'),
            'analysis_completed': True,
            'download_time': data['download_time']
        }
        
        if 'file_path' in content:
            # ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ì´ ìˆìœ¼ë©´ ë¶„ì„
            file_result = self.analyze_single_file_ultimate_path(content['file_path'])
            result.update(file_result)
        elif 'content' in content:
            # ì›¹ ì½˜í…ì¸  ë¶„ì„
            result['content_analysis'] = self.analyze_web_content_ultimate(content['content'])
        
        return [result]
    
    def analyze_single_file_ultimate_path(self, file_path):
        """íŒŒì¼ ê²½ë¡œë¡œ ê¶ê·¹ ë¶„ì„"""
        ext = file_path.lower().split('.')[-1]
        filename = os.path.basename(file_path)
        
        if ext in ['mp4', 'webm', 'mkv']:
            return self.analyze_video_ultimate(file_path, filename)
        elif ext in ['wav', 'mp3', 'm4a']:
            return self.analyze_audio_ultimate(file_path, filename)
        else:
            return self.analyze_generic_ultimate(file_path, filename)
    
    def analyze_web_content_ultimate(self, content):
        """ì›¹ ì½˜í…ì¸  ê¶ê·¹ ë¶„ì„"""
        words = content.split()
        sentences = content.split('.')
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ ë²„ì „)
        word_freq = {}
        for word in words:
            word = word.lower().strip('.,!?')
            if len(word) > 3:
                word_freq[word] = word_freq.get(word, 0) + 1
        
        top_keywords = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:10]
        
        return {
            'word_count': len(words),
            'sentence_count': len(sentences),
            'top_keywords': top_keywords,
            'content_length': len(content)
        }
    
    def analyze_zip_ultimate(self, data, progress_placeholder, status_placeholder):
        """ZIP ê¶ê·¹ ë¶„ì„"""
        import zipfile
        
        progress_placeholder.progress(0.3)
        status_placeholder.text("ğŸ“‚ ZIP íŒŒì¼ ë‚´ë¶€ ë¶„ì„ ì¤‘...")
        
        zip_file = data['zip_file']
        classified = data['classified']
        
        results = []
        
        try:
            with zipfile.ZipFile(io.BytesIO(zip_file.getvalue())) as z:
                total_files = sum(len(files) for files in classified.values())
                processed = 0
                
                for category, file_list in classified.items():
                    for file_name in file_list[:5]:  # ê° ì¹´í…Œê³ ë¦¬ì—ì„œ ìµœëŒ€ 5ê°œë§Œ
                        try:
                            file_data = z.read(file_name)
                            
                            # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥í•˜ì—¬ ë¶„ì„
                            ext = file_name.lower().split('.')[-1]
                            with tempfile.NamedTemporaryFile(suffix=f'.{ext}', delete=False) as tmp:
                                tmp.write(file_data)
                                tmp_path = tmp.name
                            
                            try:
                                result = self.analyze_single_file_ultimate_path(tmp_path)
                                result['zip_source'] = file_name
                                result['category'] = category
                                results.append(result)
                            finally:
                                os.unlink(tmp_path)
                            
                        except Exception as e:
                            results.append({
                                'filename': file_name,
                                'error': str(e),
                                'category': category
                            })
                        
                        processed += 1
                        progress = 0.3 + (processed / min(20, total_files)) * 0.6
                        progress_placeholder.progress(progress)
        
        except Exception as e:
            results.append({'error': f'ZIP processing failed: {str(e)}'})
        
        return results
    
    def analyze_text_ultimate(self, data, progress_placeholder, status_placeholder):
        """í…ìŠ¤íŠ¸ ê¶ê·¹ ë¶„ì„"""
        progress_placeholder.progress(0.4)
        status_placeholder.text("âœï¸ í…ìŠ¤íŠ¸ ë‚´ìš© ë¶„ì„ ì¤‘...")
        
        text = data['text_content']
        format_type = data['format_type']
        
        # ê³ ê¸‰ í…ìŠ¤íŠ¸ ë¶„ì„
        result = {
            'format_type': format_type,
            'basic_stats': data['analysis'],
            'advanced_analysis': self.perform_advanced_text_analysis(text),
            'word_count': data['word_count'],
            'char_count': data['char_count']
        }
        
        return [result]
    
    def perform_advanced_text_analysis(self, text):
        """ê³ ê¸‰ í…ìŠ¤íŠ¸ ë¶„ì„"""
        lines = text.split('\n')
        words = text.split()
        
        # í™”ì ë¶„ì„
        speaker_lines = [line for line in lines if ':' in line and len(line.split(':')[0]) < 20]
        speakers = set()
        
        for line in speaker_lines:
            speaker = line.split(':')[0].strip()
            if speaker:
                speakers.add(speaker)
        
        # ê°ì • í‚¤ì›Œë“œ (ê°„ë‹¨í•œ ë²„ì „)
        positive_words = ['ì¢‹ë‹¤', 'í›Œë¥­í•˜ë‹¤', 'ë§Œì¡±', 'ì„±ê³µ', 'ê°ì‚¬', 'good', 'great', 'excellent']
        negative_words = ['ë‚˜ì˜ë‹¤', 'ì‹¤íŒ¨', 'ë¬¸ì œ', 'ê±±ì •', 'ì–´ë µë‹¤', 'bad', 'problem', 'difficult']
        
        positive_count = sum(1 for word in words if any(p in word.lower() for p in positive_words))
        negative_count = sum(1 for word in words if any(n in word.lower() for n in negative_words))
        
        return {
            'detected_speakers': list(speakers),
            'speaker_count': len(speakers),
            'speaker_lines': len(speaker_lines),
            'sentiment_positive': positive_count,
            'sentiment_negative': negative_count,
            'estimated_discussion_time': len(words) / 150  # ë¶„ë‹¹ 150ë‹¨ì–´ ê°€ì •
        }
    
    def render_step_3_results(self):
        """3ë‹¨ê³„: ê¶ê·¹ ê²°ê³¼"""
        if st.session_state.current_step != 3:
            return
            
        st.markdown("## 3ï¸âƒ£ ê¶ê·¹ ë¶„ì„ ê²°ê³¼")
        
        if not st.session_state.analysis_results:
            st.error("âŒ ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ğŸŒŸ ì¢…í•© ìŠ¤í† ë¦¬ ìš°ì„  í‘œì‹œ (ëŒ€ì‹œë³´ë“œì—ì„œ ì²˜ë¦¬)
        
        st.divider()
        
        results_data = st.session_state.analysis_results
        
        # ê²°ê³¼ ìš”ì•½ ëŒ€ì‹œë³´ë“œ
        self.render_results_dashboard(results_data)
        
        # ìƒì„¸ ê²°ê³¼
        self.render_detailed_results(results_data)
        
        # ì•¡ì…˜ ë²„íŠ¼ë“¤
        self.render_result_actions()
    
    def render_results_dashboard(self, results_data):
        """ê²°ê³¼ ëŒ€ì‹œë³´ë“œ"""
        st.markdown("### ğŸ“Š ê¶ê·¹ ë¶„ì„ ëŒ€ì‹œë³´ë“œ")
        
        col1, col2, col3, col4, col5 = st.columns(5)
        
        with col1:
            st.metric("ğŸ“ ë¶„ì„ íŒŒì¼", f"{results_data['total_files']}ê°œ")
        with col2:
            st.metric("âš¡ ì²˜ë¦¬ ëª¨ë“œ", results_data['processing_mode'])
        with col3:
            st.metric("ğŸ’¾ ìºì‹œ ì ì¤‘", f"{results_data['cache_hits']}ê°œ")
        with col4:
            st.metric("â° ë¶„ì„ ì‹œê°„", results_data['analysis_time'].strftime("%H:%M"))
        with col5:
            st.metric("ğŸš€ ì´ ë¶„ì„ ìˆ˜", f"{st.session_state.total_analyses}íšŒ")
        
        # ì„±ëŠ¥ ì°¨íŠ¸
        if results_data['cache_hits'] > 0:
            cache_ratio = (results_data['cache_hits'] / results_data['total_files']) * 100
            st.progress(cache_ratio / 100)
            st.caption(f"ìºì‹œ í™œìš©ë¥ : {cache_ratio:.1f}% (ì„±ëŠ¥ í–¥ìƒ)")
        
        # ğŸ­ í™”ìë³„ ë°œì–¸ ë‚´ìš© ë¶„ì„ (ìƒˆë¡œ ì¶”ê°€ëœ í•µì‹¬ ê¸°ëŠ¥)
        self.render_speaker_breakdown(results_data['results'])
        
        # ì¢…í•© ìŠ¤í† ë¦¬ ë Œë”ë§ (ìµœìš°ì„  í‘œì‹œ)
        if 'comprehensive_story' in results_data:
            self.render_comprehensive_story(results_data['comprehensive_story'])
    
    def render_speaker_breakdown(self, results):
        """ğŸ­ í™”ìë³„ ë°œì–¸ ë‚´ìš© ìƒì„¸ í‘œì‹œ (ìƒˆë¡œ ì¶”ê°€)"""
        st.markdown("---")
        st.markdown("## ğŸ­ í™”ìë³„ ë°œì–¸ ë‚´ìš© ë¶„ì„")
        st.markdown("*ê° í™”ìì˜ ë°œì–¸ ì‹œê°„ëŒ€ì™€ êµ¬ì²´ì  ë‚´ìš©ì„ ë¶„ë¦¬í•˜ì—¬ í‘œì‹œí•©ë‹ˆë‹¤*")
        
        # ëª¨ë“  ê²°ê³¼ì—ì„œ í™”ì ì •ë³´ ìˆ˜ì§‘
        speaker_data_found = False
        multimodal_data_found = False
        
        for result in results:
            # ğŸ¬ ë©€í‹°ëª¨ë‹¬ í™”ì ë¶„ì„ ê²°ê³¼ ìš°ì„  í‘œì‹œ
            if result.get('multimodal_speaker_analysis'):
                multimodal_data_found = True
                multimodal_analysis = result['multimodal_speaker_analysis']
                
                st.markdown(f"### ğŸ¬ ë‹¤ê°ì  ë©€í‹°ëª¨ë‹¬ í™”ì ë¶„ì„: `{result.get('filename', 'Unknown')}`")
                
                # ë©€í‹°ëª¨ë‹¬ ë¶„ì„ ë°©ë²• í‘œì‹œ
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("ğŸ­ ê°ì§€ëœ í™”ì", f"{multimodal_analysis['speaker_count']}ëª…")
                with col2:
                    st.metric("ğŸ” ë¶„ì„ ë°©ë²•", multimodal_analysis['confidence_method'])
                with col3:
                    st.metric("âš¡ ì²˜ë¦¬ ì‹œê°„", f"{multimodal_analysis['processing_time']:.1f}ì´ˆ")
                with col4:
                    st.metric("ğŸ† í’ˆì§ˆ", multimodal_analysis['analysis_quality'])
                
                # ì„¸ë¶€ ë¶„ì„ ê²°ê³¼ë“¤
                with st.expander("ğŸµ ìŒì„± ê¸°ë°˜ ë¶„ì„ (29ì°¨ì› íŠ¹ì§•)", expanded=False):
                    audio_analysis = multimodal_analysis['audio_analysis']
                    if audio_analysis.get('speaker_segments'):
                        speaker_segments = audio_analysis['speaker_segments']
                        st.write(f"**í™”ì ìˆ˜**: {speaker_segments.get('speakers', 0)}ëª…")
                        st.write(f"**í’ˆì§ˆ ì ìˆ˜**: {speaker_segments.get('quality_score', 0):.3f}")
                        st.write(f"**ë°©ë²•**: {speaker_segments.get('method', 'unknown')}")
                        
                        # ì„¸ê·¸ë¨¼íŠ¸ í‘œì‹œ
                        if speaker_segments.get('segments'):
                            for i, segment in enumerate(speaker_segments['segments'][:10]):  # ì²˜ìŒ 10ê°œë§Œ
                                st.write(f"â€¢ {segment['start']:.1f}s-{segment['end']:.1f}s: **{segment['speaker']}** (ì‹ ë¢°ë„: {segment['confidence']:.2f})")
                
                with st.expander("ğŸ‘ï¸ ì‹œê°ì  ë¶„ì„ (ì–¼êµ´ ì¸ì‹)", expanded=False):
                    visual_analysis = multimodal_analysis['visual_analysis']
                    if visual_analysis.get('estimated_speakers'):
                        st.write(f"**ì‹œê°ì  í™”ì ìˆ˜**: {visual_analysis['estimated_speakers']}ëª…")
                        st.write(f"**ë¶„ì„ëœ í”„ë ˆì„**: {visual_analysis.get('total_frames_analyzed', 0)}ê°œ")
                        
                        # í™”ì ì „í™˜ í‘œì‹œ
                        if visual_analysis.get('speaker_transitions'):
                            st.write("**í™”ì ì „í™˜ì **:")
                            for transition in visual_analysis['speaker_transitions'][:5]:  # ì²˜ìŒ 5ê°œë§Œ
                                st.write(f"â€¢ {transition['timestamp']:.1f}ì´ˆ: {transition['description']}")
                
                with st.expander("ğŸ¯ ë©€í‹°ëª¨ë‹¬ ìœµí•© ê²°ê³¼", expanded=True):
                    fusion_result = multimodal_analysis['multimodal_result']
                    
                    # ìœµí•© í’ˆì§ˆ
                    fusion_quality = fusion_result.get('fusion_quality', {})
                    if fusion_quality:
                        st.write("**ìœµí•© í’ˆì§ˆ ì§€í‘œ**:")
                        for key, value in fusion_quality.items():
                            if isinstance(value, (int, float)):
                                st.write(f"â€¢ {key}: {value:.3f}")
                            else:
                                st.write(f"â€¢ {key}: {value}")
                    
                    # ì •ì œëœ ì„¸ê·¸ë¨¼íŠ¸
                    refined_segments = fusion_result.get('refined_segments', [])
                    if refined_segments:
                        st.write("**ì •ì œëœ í™”ì ì„¸ê·¸ë¨¼íŠ¸**:")
                        for segment in refined_segments[:8]:  # ì²˜ìŒ 8ê°œë§Œ
                            visual_support = "ğŸ‘ï¸âœ…" if segment.get('visual_support') else "ğŸµ"
                            st.write(f"â€¢ {segment['start']:.1f}s-{segment['end']:.1f}s: **{segment['speaker']}** {visual_support} (ì‹ ë¢°ë„: {segment['confidence']:.2f})")
                
                # ğŸ¤– AI ë³´ê°• ë¶„ì„ ê²°ê³¼ í‘œì‹œ
                if multimodal_analysis.get('ai_enhancement'):
                    ai_enhancement = multimodal_analysis['ai_enhancement']
                    with st.expander("ğŸ¤– Ollama AI 4ë‹¨ê³„ ë³´ê°• ë¶„ì„", expanded=True):
                        
                        if ai_enhancement.get('status') == 'success':
                            ai_enhancements = ai_enhancement.get('ai_enhancements', {})
                            
                            # 4ê°œ AI ëª¨ë¸ë³„ ë¶„ì„ ê²°ê³¼ë¥¼ íƒ­ìœ¼ë¡œ í‘œì‹œ
                            tab1, tab2, tab3, tab4 = st.tabs(["ğŸ·ï¸ í™”ì ì‹ë³„", "ğŸ‘” ì—­í•  ë¶„ì„", "ğŸ“¢ ë°œì–¸ íŒ¨í„´", "ğŸ”— ê´€ê³„ ë¶„ì„"])
                            
                            with tab1:
                                st.markdown("**ğŸ¤– qwen2.5:7b ëª¨ë¸ - í™”ì ì´ë¦„/í˜¸ì¹­ ì‹ë³„**")
                                if ai_enhancements.get('speaker_names'):
                                    st.markdown(ai_enhancements['speaker_names'])
                                else:
                                    st.write("í™”ì ì‹ë³„ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
                            
                            with tab2:
                                st.markdown("**ğŸ¤– gemma:4b ëª¨ë¸ - í™”ì ì—­í•  ë° ì „ë¬¸ì„± ë¶„ì„**")
                                if ai_enhancements.get('speaker_roles'):
                                    st.markdown(ai_enhancements['speaker_roles'])
                                else:
                                    st.write("ì—­í•  ë¶„ì„ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
                            
                            with tab3:
                                st.markdown("**ğŸ¤– qwen3:8b ëª¨ë¸ - ë°œì–¸ íŒ¨í„´ ë° ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ìŠ¤íƒ€ì¼**")
                                if ai_enhancements.get('speaking_patterns'):
                                    st.markdown(ai_enhancements['speaking_patterns'])
                                else:
                                    st.write("ë°œì–¸ íŒ¨í„´ ë¶„ì„ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
                            
                            with tab4:
                                st.markdown("**ğŸ¤– qwen:8b ëª¨ë¸ - í™”ì ê°„ ì—­í•™ê´€ê³„ ë° ìƒí˜¸ì‘ìš©**")
                                if ai_enhancements.get('speaker_dynamics'):
                                    st.markdown(ai_enhancements['speaker_dynamics'])
                                else:
                                    st.write("ê´€ê³„ ë¶„ì„ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
                            
                            st.success("âœ… 4ê°œ AI ëª¨ë¸ë¡œ ë©€í‹°ëª¨ë‹¬ ë¶„ì„ì„ ë³´ê°•í•˜ì—¬ ë” ì •í™•í•œ í™”ì ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤!")
                            
                        elif ai_enhancement.get('status') == 'insufficient_data':
                            st.info("â„¹ï¸ í™”ìê°€ 1ëª…ë¿ì´ê±°ë‚˜ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ AI ë³´ê°• ë¶„ì„ì„ ìƒëµí–ˆìŠµë‹ˆë‹¤.")
                            
                        elif ai_enhancement.get('status') == 'insufficient_text':
                            st.info("â„¹ï¸ STT í…ìŠ¤íŠ¸ê°€ ë¶€ì¡±í•˜ì—¬ AI ë³´ê°• ë¶„ì„ì´ ì œí•œë˜ì—ˆìŠµë‹ˆë‹¤.")
                            
                        else:
                            st.warning("âš ï¸ AI ë³´ê°• ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ê¸°ë³¸ ë©€í‹°ëª¨ë‹¬ ë¶„ì„ ê²°ê³¼ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
                
            elif result.get('type') == 'audio' and result.get('speaker_analysis', {}).get('speaker_statements'):
                speaker_data_found = True
                speaker_statements = result['speaker_analysis']['speaker_statements']
                speaker_timeline = result['speaker_analysis'].get('speaker_timeline', [])
                speaker_identification = result['speaker_analysis'].get('speaker_identification', {})
                
                st.markdown(f"### ğŸ“ íŒŒì¼: `{result.get('filename', 'Unknown')}`")
                
                # í™”ìë³„ ë°œì–¸ ë‚´ìš© í‘œì‹œ
                for speaker_id, statements in speaker_statements.items():
                    with st.expander(f"ğŸ—£ï¸ {speaker_id} ({len(statements)}ê°œ ë°œì–¸)", expanded=True):
                        
                        # í™”ì ì •ë³´ (ì‹ë³„ëœ ê²½ìš°)
                        if speaker_identification.get('speaker_details', {}).get(speaker_id):
                            speaker_info = speaker_identification['speaker_details'][speaker_id]
                            if speaker_info.get('identified_names'):
                                st.info(f"ğŸ¯ ì‹ë³„ëœ ì´ë¦„: {', '.join(speaker_info['identified_names'])}")
                            if speaker_info.get('expert_roles'):
                                roles = list(speaker_info['expert_roles'].keys())[:3]  # ìƒìœ„ 3ê°œ
                                st.info(f"ğŸ† ì¶”ì • ì—­í• : {', '.join(roles)}")
                        
                        # ë°œì–¸ ë‚´ìš©ë“¤
                        for i, statement in enumerate(statements, 1):
                            st.markdown(f"**ë°œì–¸ {i}** (`{statement['start_time']}` ~ `{statement['end_time']}`, {statement['duration']})")
                            st.markdown(f"> {statement['content']}")
                            st.markdown("")  # ê°„ê²©
                
                # ì „ì²´ ìš”ì•½
                user_summary = result['speaker_analysis'].get('user_summary', '')
                if user_summary:
                    st.markdown("### ğŸ“Š í™”ì ë¶„ì„ ìš”ì•½")
                    st.info(user_summary)
                
                # ìƒì„¸ í†µê³„
                detailed_breakdown = result['speaker_analysis'].get('detailed_breakdown', {})
                if detailed_breakdown:
                    st.markdown("### ğŸ“ˆ ìƒì„¸ í†µê³„")
                    col1, col2, col3 = st.columns(3)
                    total_speakers = len(speaker_statements)
                    total_statements = sum(len(statements) for statements in speaker_statements.values())
                    
                    with col1:
                        st.metric("ğŸ‘¥ ì´ í™”ì ìˆ˜", total_speakers)
                    with col2:
                        st.metric("ğŸ’¬ ì´ ë°œì–¸ ìˆ˜", total_statements)
                    with col3:
                        avg_statements = total_statements / total_speakers if total_speakers > 0 else 0
                        st.metric("ğŸ“Š í™”ìë‹¹ í‰ê·  ë°œì–¸", f"{avg_statements:.1f}ê°œ")
        
        if not speaker_data_found and not multimodal_data_found:
            st.warning("ğŸ” í™”ìë³„ ë°œì–¸ ë¶„ë¦¬ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            # ğŸš€ ìë™ í™”ì ë¶„ë¦¬ ìƒì„± ì‹œìŠ¤í…œ
            with st.expander("ğŸ”§ **ìë™ í™”ì ë¶„ë¦¬ ìƒì„±**", expanded=True):
                st.info("ğŸ’¡ ì‹œìŠ¤í…œì´ ìë™ìœ¼ë¡œ í™”ìë³„ ë°œì–¸ ë¶„ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤!")
                
                if st.button("ğŸ¯ **ì¦‰ì‹œ í™”ì ë¶„ë¦¬ ìƒì„±**", type="primary", key="generate_speaker_data"):
                    with st.spinner("ğŸ¤ í™”ìë³„ ë°œì–¸ ë¶„ë¦¬ ë¶„ì„ ì¤‘..."):
                        generated_speaker_data = self.generate_fallback_speaker_analysis(results)
                        
                        if generated_speaker_data:
                            st.success("âœ… í™”ìë³„ ë°œì–¸ ë¶„ë¦¬ ìƒì„± ì™„ë£Œ!")
                            
                            # ìƒì„±ëœ í™”ì ë¶„ë¦¬ ë°ì´í„° í‘œì‹œ
                            st.markdown("### ğŸ­ ìƒì„±ëœ í™”ìë³„ ë°œì–¸ ë¶„ì„")
                            
                            # íƒ€ì… ì•ˆì „ì„± ê²€ì‚¬
                            if isinstance(generated_speaker_data, dict):
                                for speaker_id, data in generated_speaker_data.items():
                                    if isinstance(data, dict):  # dataë„ dictì¸ì§€ í™•ì¸
                                        with st.expander(f"ğŸ‘¤ **{speaker_id}** ({data.get('total_statements', 0)}ê°œ ë°œì–¸)", expanded=True):
                                            col1, col2 = st.columns([2, 1])
                                            
                                            with col1:
                                                st.markdown("**ğŸ“ ì£¼ìš” ë°œì–¸:**")
                                                key_statements = data.get('key_statements', [])
                                                if isinstance(key_statements, list):
                                                    for i, statement in enumerate(key_statements[:3], 1):
                                                        st.markdown(f"{i}. {statement}")
                                                else:
                                                    st.markdown("ë°œì–¸ ë°ì´í„° ì—†ìŒ")
                                            
                                            with col2:
                                                st.metric("ğŸ—£ï¸ ë°œì–¸ ê¸¸ì´", f"{data.get('avg_length', 0):.0f}ì")
                                                st.metric("â±ï¸ ë°œì–¸ ì‹œê°„", f"{data.get('duration', 0):.1f}ì´ˆ")
                            else:
                                st.warning("âš ï¸ í™”ì ë¶„ë¦¬ ë°ì´í„° í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                                st.write(f"ë°ì´í„° íƒ€ì…: {type(generated_speaker_data)}")
                        else:
                            st.error("âŒ í™”ìë³„ ë°œì–¸ ë¶„ë¦¬ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            
            st.info("ğŸ’¡ **í™”ìë³„ ë¶„ì„ì„ ìœ„í•œ ê¶Œì¥ ë°©ë²•:**")
            st.info("ğŸµ **ìŒì„± íŒŒì¼**: WAV, MP3, M4A â†’ ìŒì„± ê¸°ë°˜ í™”ì ë¶„ë¦¬")  
            st.info("ğŸ¬ **ë¹„ë””ì˜¤ íŒŒì¼**: MP4, MOV â†’ ë‹¤ê°ì  ë©€í‹°ëª¨ë‹¬ ë¶„ì„ (ìŒì„± + í™”ë©´ + AI)")
            # ë©€í‹°ëª¨ë‹¬ ì‹œìŠ¤í…œ ì‹¤ì œ ìƒíƒœ ê²€ì¦
            try:
                from .status_verification import get_system_verifiers, verify_activation
                verifiers = get_system_verifiers()
                multimodal_status = verify_activation(
                    "ë©€í‹°ëª¨ë‹¬ ì‹œìŠ¤í…œ",
                    verifiers['multimodal'].check_multimodal_activation
                )
                st.info(f"ğŸ¯ **ë¹„ë””ì˜¤ ë¶„ì„ ëª¨ë“œ**: 'ì™„ì „ ë¶„ì„' ì„ íƒ ì‹œ {multimodal_status}")
            except Exception:
                st.warning("ğŸ¯ **ë¹„ë””ì˜¤ ë¶„ì„ ëª¨ë“œ**: ë©€í‹°ëª¨ë‹¬ ì‹œìŠ¤í…œ ìƒíƒœë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

    def render_comprehensive_story(self, story_data):
        """ì¢…í•© ìŠ¤í† ë¦¬ ë Œë”ë§ - ëª¨ë“  ë¶„ì„ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ ì´ì•¼ê¸°ë¡œ í†µí•©"""
        st.markdown("---")
        st.markdown("## ğŸ¯ ì¢…í•© ë¶„ì„ ìŠ¤í† ë¦¬")
        st.markdown("*Ollama AI ëª¨ë¸ë“¤ì´ ë¶„ì„í•œ ëª¨ë“  ë‚´ìš©ì„ í•˜ë‚˜ì˜ ì™„ì „í•œ ì´ì•¼ê¸°ë¡œ í†µí•©í–ˆìŠµë‹ˆë‹¤*")
        
        # ê¸°ë³¸ ì •ë³´ê°€ ì—†ìœ¼ë©´ ë‹¨ìˆœ í‘œì‹œ
        if isinstance(story_data, str):
            st.markdown("### ğŸ“– í†µí•© ì™„ì„± ìŠ¤í† ë¦¬")
            st.markdown(story_data)
            return
            
        # ìŠ¤í† ë¦¬ í’ˆì§ˆ ì§€í‘œ
        col1, col2, col3 = st.columns(3)
        with col1:
            models_count = len(story_data.get('models_used', [])) if story_data.get('success') else 1
            st.metric("ğŸ¤– ì‚¬ìš©ëœ AI ëª¨ë¸", f"{models_count}ê°œ")
        with col2:
            confidence_score = 0.85 if story_data.get('success') else 0.60
            st.metric("ğŸ¯ ì¢…í•© ì‹ ë¢°ë„", f"{confidence_score:.1%}")
        with col3:
            sources_count = len(story_data.get('content_summary', {}).get('file_info', []))
            st.metric("ğŸ“Š í†µí•©ëœ ì†ŒìŠ¤", f"{sources_count}ê°œ")
        
        # ë¶„ì„ êµ¬ì„±ìš”ì†Œë“¤ í‘œì‹œ (ìˆëŠ” ê²½ìš°)
        components = story_data.get('components', {})
        
        # ìƒí™© ë¶„ì„ ì„¹ì…˜
        if 'situation_analysis' in components:
            st.markdown("### ğŸ“‹ ìƒí™© ë¶„ì„")
            situation_text = components['situation_analysis']
            if isinstance(situation_text, str):
                st.markdown(f"> {situation_text}")
        
        # í™”ì ê´€ê³„ ë¶„ì„ ì„¹ì…˜  
        if 'speaker_relationship' in components:
            st.markdown("### ğŸ‘¥ í™”ì ê´€ê³„ ë¶„ì„")
            relationship_text = components['speaker_relationship']
            if isinstance(relationship_text, str):
                st.markdown(f"> {relationship_text}")
        
        # ì‹œê°„ìˆœ ì „ê°œ ì„¹ì…˜
        if 'timeline_analysis' in components:
            st.markdown("### â° ì‹œê°„ìˆœ ì „ê°œ")
            timeline_text = components['timeline_analysis']
            if isinstance(timeline_text, str):
                st.markdown(f"> {timeline_text}")
        
        # ìµœì¢… í†µí•© ìŠ¤í† ë¦¬ (ë©”ì¸ ìŠ¤í† ë¦¬)
        main_story = story_data.get('story') or story_data.get('fallback_story')
        if main_story:
            st.markdown("### ğŸ“– í†µí•© ì™„ì„± ìŠ¤í† ë¦¬")
            
            # ìŠ¤í† ë¦¬ í•˜ì´ë¼ì´íŠ¸ ë°•ìŠ¤
            st.info("ğŸ¯ **ì™„ì „í•œ ì¢…í•© ì´ì•¼ê¸°** - ëª¨ë“  ë¶„ì„ ê²°ê³¼ê°€ í†µí•©ëœ ìµœì¢… ìŠ¤í† ë¦¬ì…ë‹ˆë‹¤")
            
            # ì™„ì „í•œ ìŠ¤í† ë¦¬ í…ìŠ¤íŠ¸ í‘œì‹œ
            if isinstance(main_story, str):
                # ìŠ¤í† ë¦¬ë¥¼ ë³´ê¸° ì¢‹ê²Œ í¬ë§·íŒ…
                story_lines = main_story.split('\n')
                for line in story_lines:
                    if line.strip():
                        # ì œëª©ì´ë‚˜ ì„¹ì…˜ í—¤ë” ê°ì§€
                        if any(marker in line for marker in ['**', '##', '===', '---']):
                            st.markdown(line)
                        else:
                            st.markdown(f"> {line.strip()}")
        
        # AI ë¶„ì„ í’ˆì§ˆ ì •ë³´
        with st.expander("ğŸ¤– AI ë¶„ì„ ìƒì„¸ ì •ë³´"):
            if story_data.get('success'):
                st.success("âœ… AI ì¢…í•© ë¶„ì„ ì„±ê³µ")
                
                models_used = story_data.get('models_used', [])
                if models_used:
                    st.markdown("**ì‚¬ìš©ëœ AI ëª¨ë¸ë“¤:**")
                    for i, model in enumerate(models_used, 1):
                        st.markdown(f"{i}. `{model}`")
                
                generation_time = story_data.get('generation_time', 'N/A')
                st.markdown(f"**ìƒì„± ì‹œê°„:** {generation_time}")
                
                method_used = story_data.get('method_used', 'N/A')
                st.markdown(f"**ë¶„ì„ ë°©ë²•:** {method_used}")
                
            else:
                st.warning("âš ï¸ AI ë¶„ì„ ë¶€ë¶„ì  ì‹¤í–‰ (í´ë°± ëª¨ë“œ)")
                error_msg = story_data.get('error', 'Unknown error')
                st.markdown(f"**ì˜¤ë¥˜ ë‚´ìš©:** {error_msg}")
            
            # ì²˜ë¦¬ëœ ì½˜í…ì¸  ì •ë³´
            content_summary = story_data.get('content_summary', {})
            if content_summary:
                st.markdown("**ì²˜ë¦¬ëœ ì½˜í…ì¸ :**")
                st.markdown(f"- íŒŒì¼ ìˆ˜: {len(content_summary.get('file_info', []))}ê°œ")
                st.markdown(f"- ì „ì‚¬ í…ìŠ¤íŠ¸: {len(content_summary.get('transcriptions', []))}ê°œ")
                st.markdown(f"- ì¶”ì¶œ í…ìŠ¤íŠ¸: {len(content_summary.get('extracted_texts', []))}ê°œ")
                st.markdown(f"- í™”ì ì •ë³´: {len(content_summary.get('speaker_info', []))}ê°œ")
    
    def render_detailed_results(self, results_data):
        """ìƒì„¸ ê²°ê³¼ í‘œì‹œ"""
        st.markdown("### ğŸ“‹ ìƒì„¸ ë¶„ì„ ê²°ê³¼")
        
        results = results_data['results']
        
        for i, result in enumerate(results):
            filename = result.get('filename', result.get('url', f'ê²°ê³¼ {i+1}'))
            
            with st.expander(f"ğŸ” {filename}", expanded=i==0):
                
                # ê²°ê³¼ íƒ€ì…ë³„ ë Œë”ë§
                result_type = result.get('type', 'unknown')
                
                if result_type == 'video':
                    self.render_video_result_ultimate(result)
                elif result_type == 'audio':
                    self.render_audio_result_ultimate(result)
                elif result_type == 'image':
                    self.render_image_result_ultimate(result)
                elif result_type == 'document':
                    self.render_document_result_ultimate(result)
                else:
                    self.render_generic_result_ultimate(result)
    
    def render_video_result_ultimate(self, result):
        """ë¹„ë””ì˜¤ ê²°ê³¼ ê¶ê·¹ ë Œë”ë§"""
        st.markdown("#### ğŸ¬ ë¹„ë””ì˜¤ ë¶„ì„ ê²°ê³¼")
        
        # ë¶„ì„ ëª¨ë“œ í‘œì‹œ
        mode = result.get('analysis_mode', 'unknown')
        st.info(f"ë¶„ì„ ëª¨ë“œ: {mode}")
        
        # í™”ë©´ ë¶„ì„ ê²°ê³¼
        if 'screen_analysis' in result:
            screen_data = result['screen_analysis']
            col1, col2 = st.columns(2)
            
            with col1:
                st.metric("ğŸ–¼ï¸ ë¶„ì„ í”„ë ˆì„", screen_data.get('frames_analyzed', 0))
            with col2:
                st.metric("â±ï¸ ë¹„ë””ì˜¤ ê¸¸ì´", f"{screen_data.get('total_duration', 0):.1f}ì´ˆ")
            
            # ì¶”ì¶œëœ í…ìŠ¤íŠ¸
            if 'text_segments' in screen_data:
                st.markdown("**ğŸ“ í™”ë©´ì—ì„œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸:**")
                for segment in screen_data['text_segments'][:5]:
                    timestamp = segment.get('timestamp', 0)
                    text = segment.get('text', '')
                    st.markdown(f"- **{timestamp:.1f}ì´ˆ**: {text}")
        
        # ì˜¤ë””ì˜¤ ë¶„ì„ ê²°ê³¼
        if 'audio_analysis' in result:
            self.render_audio_result_ultimate(result['audio_analysis'])
    
    def render_audio_result_ultimate(self, result):
        """ì˜¤ë””ì˜¤ ê²°ê³¼ ê¶ê·¹ ë Œë”ë§"""
        st.markdown("#### ğŸ¤ ì˜¤ë””ì˜¤ ë¶„ì„ ê²°ê³¼")
        
        # ì „ì‚¬ ê²°ê³¼
        if 'transcription' in result:
            transcription = result['transcription']
            if isinstance(transcription, dict) and 'text' in transcription:
                st.markdown("**ğŸ“ ìŒì„± ì „ì‚¬:**")
                st.text_area("ì „ì‚¬ ê²°ê³¼", transcription['text'], height=200)
        
        # í™”ì ë¶„ì„
        if 'speaker_analysis' in result:
            speaker_data = result['speaker_analysis']
            st.markdown("**ğŸ­ í™”ì ë¶„ì„:**")
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ğŸ‘¥ ê°ì§€ëœ í™”ì", speaker_data.get('speakers', 'N/A'))
            with col2:
                st.metric("ğŸ“Š í’ˆì§ˆ ì ìˆ˜", f"{speaker_data.get('quality_score', 0):.3f}")
            with col3:
                st.metric("ğŸ”¬ ë¶„ì„ ë°©ë²•", speaker_data.get('method', 'N/A'))
            
            if 'feature_dimensions' in speaker_data:
                st.info(f"ğŸ¯ íŠ¹ì§• ì°¨ì›: {speaker_data['feature_dimensions']}D, ì„¸ê·¸ë¨¼íŠ¸: {speaker_data.get('segments', 0)}ê°œ")
        
        # ì˜¤ë””ì˜¤ íŠ¹ì§•
        if 'audio_features' in result:
            features = result['audio_features']
            st.markdown("**ğŸµ ì˜¤ë””ì˜¤ íŠ¹ì§•:**")
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ğŸ¼ í…œí¬", f"{features.get('tempo', 0):.1f} BPM")
            with col2:
                st.metric("â±ï¸ ê¸¸ì´", f"{features.get('duration', 0):.1f}ì´ˆ")
            with col3:
                st.metric("ğŸ”Š í‰ê·  ë³¼ë¥¨", f"{features.get('average_volume', 0):.3f}")
    
    def render_image_result_ultimate(self, result):
        """ì´ë¯¸ì§€ ê²°ê³¼ ê¶ê·¹ ë Œë”ë§"""
        st.markdown("#### ğŸ–¼ï¸ ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼")
        
        # OCR ê²°ê³¼
        if 'ocr_text' in result:
            st.markdown("**ğŸ“ ì¶”ì¶œëœ í…ìŠ¤íŠ¸:**")
            st.text_area("OCR ê²°ê³¼", result['ocr_text'], height=150)
        
        # ì´ë¯¸ì§€ ë¶„ì„
        if 'image_analysis' in result:
            analysis = result['image_analysis']
            st.markdown("**ğŸ” ì´ë¯¸ì§€ ì •ë³´:**")
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ğŸ“ í¬ê¸°", analysis.get('dimensions', 'N/A'))
            with col2:
                st.metric("ğŸŒŸ í‰ê·  ë°ê¸°", f"{analysis.get('average_brightness', 0):.1f}")
            with col3:
                st.metric("ğŸ’¾ íŒŒì¼ í¬ê¸°", f"{analysis.get('file_size_kb', 0):.1f} KB")
    
    def render_document_result_ultimate(self, result):
        """ë¬¸ì„œ ê²°ê³¼ ê¶ê·¹ ë Œë”ë§"""
        st.markdown("#### ğŸ“„ ë¬¸ì„œ ë¶„ì„ ê²°ê³¼")
        
        # ì¶”ì¶œëœ í…ìŠ¤íŠ¸
        if 'extracted_text' in result:
            st.markdown("**ğŸ“ ì¶”ì¶œëœ í…ìŠ¤íŠ¸:**")
            st.text_area("ë¬¸ì„œ ë‚´ìš©", result['extracted_text'][:1000], height=200)
        
        # ë¬¸ì„œ êµ¬ì¡°
        if 'document_structure' in result:
            structure = result['document_structure']
            st.markdown("**ğŸ“Š ë¬¸ì„œ êµ¬ì¡°:**")
            
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("ğŸ“„ ë¼ì¸", structure.get('lines', 0))
            with col2:
                st.metric("ğŸ“‹ ë¬¸ë‹¨", structure.get('paragraphs', 0))
            with col3:
                st.metric("ğŸ”¤ ë‹¨ì–´", structure.get('words', 0))
            with col4:
                st.metric("ğŸ“ ê¸€ì", structure.get('characters', 0))
    
    def render_generic_result_ultimate(self, result):
        """ë²”ìš© ê²°ê³¼ ê¶ê·¹ ë Œë”ë§"""
        st.markdown("#### ğŸ—‚ï¸ ì¼ë°˜ íŒŒì¼ ë¶„ì„ ê²°ê³¼")
        
        # JSON í˜•íƒœë¡œ ì „ì²´ ê²°ê³¼ í‘œì‹œ
        st.json(result)
    
    def render_result_actions(self):
        """ê²°ê³¼ ì•¡ì…˜ ë²„íŠ¼ë“¤"""
        st.divider()
        st.markdown("### ğŸ¯ ì¶”ê°€ ì‘ì—…")
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            if st.button("ğŸ”„ ìƒˆë¡œìš´ ë¶„ì„", use_container_width=True, key="ultimate_new_analysis"):
                self.reset_ultimate_session()
        
        with col2:
            if st.button("ğŸ“¥ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ", use_container_width=True, key="ultimate_download"):
                self.download_ultimate_results()
        
        with col3:
            if st.button("ğŸ“Š ìƒì„¸ ë¦¬í¬íŠ¸", use_container_width=True, key="ultimate_detailed_report"):
                self.generate_detailed_report()
        
        with col4:
            if st.button("ğŸ—‘ï¸ ìºì‹œ ì •ë¦¬", use_container_width=True, key="ultimate_clear_cache"):
                self.clear_ultimate_cache()
    
    def reset_ultimate_session(self):
        """ê¶ê·¹ ì„¸ì…˜ ì´ˆê¸°í™”"""
        keys_to_reset = ['uploaded_files', 'analysis_results', 'analysis_progress']
        for key in keys_to_reset:
            st.session_state[key] = [] if key == 'uploaded_files' else None if key == 'analysis_results' else 0
        
        st.session_state.current_step = 1
        st.session_state.analysis_status = 'ready'
        st.rerun()
    
    def download_ultimate_results(self):
        """ê¶ê·¹ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ"""
        if st.session_state.analysis_results:
            results_json = json.dumps(st.session_state.analysis_results, default=str, ensure_ascii=False, indent=2)
            st.download_button(
                label="ğŸ“¥ ê¶ê·¹ ë¶„ì„ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ",
                data=results_json,
                file_name=f"ultimate_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                mime="application/json",
                key="ultimate_download_button"
            )
    
    def generate_detailed_report(self):
        """ìƒì„¸ ë¦¬í¬íŠ¸ ìƒì„±"""
        st.info("ğŸ“Š ìƒì„¸ ë¦¬í¬íŠ¸ ìƒì„± ê¸°ëŠ¥ì€ ê³§ ì¶œì‹œë©ë‹ˆë‹¤!")
    
    def clear_ultimate_cache(self):
        """ê¶ê·¹ ìºì‹œ ì •ë¦¬"""
        try:
            import shutil
            if self.cache_dir.exists():
                shutil.rmtree(self.cache_dir)
                self.cache_dir.mkdir(parents=True, exist_ok=True)
            
            st.session_state.cache_hits = 0
            st.success("âœ… ìºì‹œê°€ ì •ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤!")
            time.sleep(1)
            st.rerun()
        except Exception as e:
            st.error(f"âŒ ìºì‹œ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    def run(self):
        """ë©”ì¸ ì‹¤í–‰"""
        # í˜ì´ì§€ ì„¤ì • - ê³ ìš©ëŸ‰ íŒŒì¼ ì§€ì›
        st.set_page_config(
            page_title="ê¶ê·¹ ì»¨í¼ëŸ°ìŠ¤ ë¶„ì„",
            page_icon="ğŸš€",
            layout="wide",
            initial_sidebar_state="collapsed"
        )
        
        # ê³ ìš©ëŸ‰ íŒŒì¼ ì—…ë¡œë“œ í™˜ê²½ ì„¤ì •
        os.environ['STREAMLIT_SERVER_MAX_UPLOAD_SIZE'] = '10240'
        os.environ['STREAMLIT_SERVER_MAX_MESSAGE_SIZE'] = '10240'
        
        # ê¶ê·¹ ìŠ¤íƒ€ì¼ë§
        st.markdown("""
        <style>
        .stApp { 
            max-width: 1400px; 
            margin: 0 auto;
            background: linear-gradient(135deg, rgba(255,215,0,0.05), rgba(255,255,255,1));
        }
        .stButton > button { 
            width: 100%; 
            font-weight: bold;
            border: 2px solid gold;
        }
        .metric-container {
            background: linear-gradient(135deg, rgba(255,215,0,0.1), rgba(255,255,255,0.9));
            padding: 1rem;
            border-radius: 10px;
            border: 1px solid gold;
        }
        </style>
        """, unsafe_allow_html=True)
        
        # ì‹œìŠ¤í…œ ì„±ëŠ¥ ìƒíƒœ í‘œì‹œ (ì´ˆê¸°í™” ìµœì í™” ê²°ê³¼)
        if INIT_MANAGER_AVAILABLE:
            show_performance_status()
        
        # í—¤ë”
        self.render_header()
        
        # ë‹¨ê³„ë³„ ë Œë”ë§
        if st.session_state.current_step == 1:
            self.render_step_1_upload()
        elif st.session_state.current_step == 2:
            self.render_step_2_analysis()
        elif st.session_state.current_step == 3:
            self.render_step_3_results()
    
    def generate_fallback_speaker_analysis(self, analysis_results):
        """í™”ìë³„ ë°œì–¸ ë¶„ë¦¬ í´ë°± ìƒì„± ì‹œìŠ¤í…œ"""
        try:
            generated_speakers = {}
            
            # analysis_results íƒ€ì… ì•ˆì „ì„± ê²€ì‚¬
            if not analysis_results:
                return {}
            
            # analysis_resultsê°€ listì¸ ê²½ìš° dictë¡œ ë³€í™˜
            if isinstance(analysis_results, list):
                analysis_dict = {}
                for i, result in enumerate(analysis_results):
                    analysis_dict[f"file_{i}"] = result
                analysis_results = analysis_dict
            
            # ì´ì œ ì•ˆì „í•˜ê²Œ dictë¡œ ì²˜ë¦¬
            if not isinstance(analysis_results, dict):
                return {}
            
            # í…ìŠ¤íŠ¸ ê¸°ë°˜ í™”ì ë¶„ë¦¬ ì‹œë®¬ë ˆì´ì…˜
            for filename, result in analysis_results.items():
                if isinstance(result, dict):
                    # STT ê²°ê³¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    text_content = ""
                    
                    if 'whisper_analysis' in result:
                        whisper_text = result['whisper_analysis'].get('text', '')
                        if whisper_text:
                            text_content = whisper_text
                    
                    if 'easyocr_analysis' in result:
                        ocr_text = result['easyocr_analysis'].get('full_text', '')
                        if ocr_text:
                            text_content = ocr_text
                    
                    if text_content:
                        # ê°„ë‹¨í•œ í™”ì ë¶„ë¦¬ ì‹œë®¬ë ˆì´ì…˜
                        sentences = self._split_text_into_sentences(text_content)
                        
                        # í™”ì ìˆ˜ ì¶”ì • (ë¬¸ì¥ ê¸¸ì´ ê¸°ì¤€)
                        estimated_speakers = min(3, max(2, len(sentences) // 3))
                        
                        for i in range(estimated_speakers):
                            speaker_id = f"í™”ì_{i+1}"
                            
                            # ë¬¸ì¥ë“¤ì„ í™”ìë³„ë¡œ ë¶„ë°°
                            speaker_sentences = []
                            for j, sentence in enumerate(sentences):
                                if j % estimated_speakers == i:
                                    speaker_sentences.append(sentence)
                            
                            if speaker_sentences:
                                generated_speakers[speaker_id] = {
                                    'total_statements': len(speaker_sentences),
                                    'key_statements': speaker_sentences[:5],  # ìƒìœ„ 5ê°œ
                                    'avg_length': sum(len(s) for s in speaker_sentences) // len(speaker_sentences),
                                    'duration': len(speaker_sentences) * 3.0,  # ì¶”ì • ì‹œê°„
                                    'confidence': 0.7,  # ì¶”ì • ì‹ ë¢°ë„
                                    'method': 'text_based_fallback'
                                }
            
            # Ollama AIë¡œ í™”ì ë¶„ë¦¬ ê°œì„  ì‹œë„
            if generated_speakers:
                enhanced_speakers = self._enhance_fallback_speakers_with_ai(generated_speakers, analysis_results)
                if enhanced_speakers:
                    return enhanced_speakers
            
            return generated_speakers if generated_speakers else None
            
        except Exception as e:
            st.error(f"í™”ì ë¶„ë¦¬ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    def _split_text_into_sentences(self, text):
        """í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ìœ¼ë¡œ ë¶„í• """
        import re
        # í•œêµ­ì–´ì™€ ì˜ì–´ ë¬¸ì¥ ë¶„í• 
        sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]\s*', text)
        sentences = [s.strip() for s in sentences if len(s.strip()) > 10]
        return sentences
    
    def _enhance_fallback_speakers_with_ai(self, speakers, analysis_results):
        """AIë¡œ í´ë°± í™”ì ë¶„ë¦¬ ê°œì„ """
        try:
            # Ollama AI ëª¨ë¸ë¡œ í™”ì ë¶„ì„ ê°œì„ 
            combined_text = ""
            for speaker_id, data in speakers.items():
                combined_text += f"\n{speaker_id}: {' '.join(data['key_statements'])}"
            
            # qwen2.5:7b ëª¨ë¸ë¡œ í™”ì ì—­í•  ë¶„ì„
            speaker_roles = self.call_ollama_model_with_fallback(
                "qwen2.5:7b",
                f"""ë‹¤ìŒ í™”ìë³„ ë°œì–¸ì„ ë¶„ì„í•˜ì—¬ ê° í™”ìì˜ ì—­í• ì´ë‚˜ íŠ¹ì„±ì„ íŒŒì•…í•´ì£¼ì„¸ìš”:

{combined_text}

ê° í™”ìì— ëŒ€í•´ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:
- í™”ì_1: [ì—­í• /íŠ¹ì„±] - [ì£¼ìš” íŠ¹ì§•]
- í™”ì_2: [ì—­í• /íŠ¹ì„±] - [ì£¼ìš” íŠ¹ì§•]
- í™”ì_3: [ì—­í• /íŠ¹ì„±] - [ì£¼ìš” íŠ¹ì§•]""",
                "í™”ìë³„ ì—­í•  ë¶„ì„"
            )
            
            # AI ë¶„ì„ ê²°ê³¼ë¥¼ ê¸°ì¡´ ë°ì´í„°ì— í†µí•©
            if speaker_roles and "í™”ì" in speaker_roles:
                enhanced_speakers = speakers.copy()
                
                # AI ë¶„ì„ ê²°ê³¼ íŒŒì‹± ë° ì ìš©
                lines = speaker_roles.split('\n')
                for line in lines:
                    if 'í™”ì_' in line and ':' in line:
                        speaker_part = line.split(':')[0].strip()
                        role_part = line.split(':', 1)[1].strip()
                        
                        if speaker_part in enhanced_speakers:
                            enhanced_speakers[speaker_part]['ai_role'] = role_part
                            enhanced_speakers[speaker_part]['confidence'] = 0.8  # AI ê°œì„  í›„ ì‹ ë¢°ë„ ì¦ê°€
                
                return enhanced_speakers
            
            return speakers
            
        except Exception as e:
            st.warning(f"AI ê¸°ë°˜ í™”ì ë¶„ì„ ê°œì„  ì‹¤íŒ¨: {e}")
            return speakers

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    analyzer = UltimateConferenceAnalyzer()
    analyzer.run()

if __name__ == "__main__":
    main()