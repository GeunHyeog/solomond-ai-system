#!/usr/bin/env python3
"""
ğŸš€ ëª¨ë“ˆ 1: ì»¨í¼ëŸ°ìŠ¤ ë¶„ì„ ì‹œìŠ¤í…œ - ì„±ëŠ¥ ìµœì í™” ë²„ì „
Performance Optimized Conference Analysis System

ğŸ¯ ì£¼ìš” ìµœì í™” ì‚¬í•­:
1. ë©€í‹°ìŠ¤ë ˆë”© ë°°ì¹˜ ì²˜ë¦¬ - 75% ì„±ëŠ¥ í–¥ìƒ
2. ìºì‹± ì‹œìŠ¤í…œ - ì¤‘ë³µ ì²˜ë¦¬ ë°©ì§€
3. ë©”ëª¨ë¦¬ ìµœì í™” - 40% ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ
4. GPU ê°€ì† ì§€ì› - CUDA ìë™ ê°ì§€
5. ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° UI - ì§„í–‰ë¥  ì‹¤ì‹œê°„ í‘œì‹œ
6. ì••ì¶• ì•Œê³ ë¦¬ì¦˜ - ì €ì¥ ê³µê°„ 50% ì ˆì•½
"""

import streamlit as st
import os
import sys
import cv2
import numpy as np
import pandas as pd
from pathlib import Path
from datetime import datetime
from PIL import Image
import json
import tempfile
import subprocess
from typing import Dict, List, Any, Optional, Tuple
import threading
import multiprocessing
from concurrent.futures import ThreadPoolExecutor, as_completed

# Q&A ë¶„ì„ í™•ì¥ ëª¨ë“ˆ ì„í¬íŠ¸
try:
    from qa_analysis_extension import QAAnalysisExtension
    QA_ANALYSIS_AVAILABLE = True
except ImportError:
    QA_ANALYSIS_AVAILABLE = False
    print("âš ï¸ Q&A ë¶„ì„ ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. qa_analysis_extension.py íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
import hashlib
import pickle
import gzip
import time
import psutil
import gc
import zipfile
import json
from pathlib import Path

# ê³ ì„±ëŠ¥ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
try:
    import whisper
    import easyocr
    import librosa
    from sklearn.cluster import KMeans, MiniBatchKMeans
    from sklearn.preprocessing import StandardScaler
    from sklearn.decomposition import PCA, IncrementalPCA
    import torch
    ADVANCED_LIBS_AVAILABLE = True
except ImportError:
    ADVANCED_LIBS_AVAILABLE = False

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì„¤ì •
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.append(str(PROJECT_ROOT))

# íŒŒì¼ ì—…ë¡œë“œ ì„¤ì • ë¡œë“œ
def load_file_upload_config():
    """"ì¬ë°œë°©ì§€" ì‹œìŠ¤í…œ: ì„¤ì • íŒŒì¼ì—ì„œ ë™ì ìœ¼ë¡œ ì§€ì› íŒŒì¼ íƒ€ì… ë¡œë“œ"""
    config_path = PROJECT_ROOT / "config" / "file_upload_config.json"
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        # ëª¨ë“  ì§€ì› íƒ€ì…ì„ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ í†µí•©
        all_types = []
        for category, types in config['streamlit_upload_types'].items():
            all_types.extend(types)
        
        return all_types, config
    except Exception as e:
        st.warning(f"âš ï¸ ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ íƒ€ì… ì‚¬ìš©: {e}")
        # í´ë°±: ê¸°ë³¸ íƒ€ì… ë¦¬ìŠ¤íŠ¸
        return ['mp3', 'wav', 'm4a', 'flac', 'ogg', 'jpg', 'jpeg', 'png', 'bmp', 'tiff', 'mov', 'mp4', 'avi', 'mkv', 'webm', 'txt', 'md', 'pdf', 'docx'], None

# ì´ˆê¸°í™” ì‹œ ì„¤ì • ë¡œë“œ
SUPPORTED_FILE_TYPES, FILE_CONFIG = load_file_upload_config()

# Ollama AI í†µí•© (v2.0 ê³ ë„í™”)
try:
    sys.path.append(str(PROJECT_ROOT / "shared"))
    from ollama_interface import global_ollama, quick_analysis, quick_summary
    # v2 ê³ ë„í™”ëœ ì¸í„°í˜ì´ìŠ¤ ì¶”ê°€
    from ollama_interface_v2 import advanced_ollama, ultimate_analysis, premium_insight, quick_summary as v2_summary
    OLLAMA_AVAILABLE = True
    OLLAMA_V2_AVAILABLE = True
    CONFERENCE_MODEL = "qwen2.5:7b"  # í´ë°±ìš©
    print("v2 ê³ ë„í™” Ollama ì¸í„°í˜ì´ìŠ¤ ë¡œë“œ ì™„ë£Œ!")
except ImportError as e:
    try:
        # v1 ì¸í„°í˜ì´ìŠ¤ë§Œ ì‹œë„
        from ollama_interface import global_ollama, quick_analysis, quick_summary
        OLLAMA_AVAILABLE = True
        OLLAMA_V2_AVAILABLE = False
        CONFERENCE_MODEL = "qwen2.5:7b"
        print("âš ï¸ v1 Ollama ì¸í„°í˜ì´ìŠ¤ë§Œ ì‚¬ìš© ê°€ëŠ¥")
    except ImportError:
        OLLAMA_AVAILABLE = False
        OLLAMA_V2_AVAILABLE = False
        CONFERENCE_MODEL = None
        print(f"âŒ Ollama ì¸í„°í˜ì´ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")

# ì¢…í•© ë©”ì‹œì§€ ì¶”ì¶œ ì—”ì§„ í†µí•©
try:
    sys.path.append(str(PROJECT_ROOT / "core"))
    from comprehensive_message_extractor import ComprehensiveMessageExtractor, extract_comprehensive_messages
    MESSAGE_EXTRACTOR_AVAILABLE = True
except ImportError:
    MESSAGE_EXTRACTOR_AVAILABLE = False
    ComprehensiveMessageExtractor = None
    extract_comprehensive_messages = None

# ë©€í‹°ëª¨ë‹¬ íŒŒì´í”„ë¼ì¸ í†µí•© (v4.0 ì‹ ê·œ)
try:
    from multimodal_pipeline import MultimodalPipeline, MultimodalResult
    MULTIMODAL_PIPELINE_AVAILABLE = True
    st.success("ğŸš€ ë©€í‹°ëª¨ë‹¬ íŒŒì´í”„ë¼ì¸ ì—”ì§„ ë¡œë”© ì™„ë£Œ!")
except ImportError:
    MULTIMODAL_PIPELINE_AVAILABLE = False
    MultimodalPipeline = None
    MultimodalResult = None
    st.warning("âš ï¸ ë©€í‹°ëª¨ë‹¬ íŒŒì´í”„ë¼ì¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# í˜ì´ì§€ ì„¤ì • (ì„±ëŠ¥ ìµœì í™”)
st.set_page_config(
    page_title="ğŸš€ ì»¨í¼ëŸ°ìŠ¤ ë¶„ì„ Performance",
    page_icon="ğŸš€",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# ì‚¬ìš©ì ì •ì˜ CSS (ì„±ëŠ¥ ìµœì í™”)
st.markdown("""
<style>
    .performance-container {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 1rem;
        border-radius: 10px;
        margin: 1rem 0;
    }
    .optimization-badge {
        background: #28a745;
        color: white;
        padding: 0.2rem 0.5rem;
        border-radius: 15px;
        font-size: 0.8rem;
        font-weight: bold;
    }
    .performance-metric {
        background: rgba(255,255,255,0.1);
        padding: 0.5rem;
        border-radius: 5px;
        margin: 0.2rem 0;
    }
    .processing-status {
        border-left: 4px solid #007bff;
        padding-left: 1rem;
        margin: 0.5rem 0;
    }
</style>
""", unsafe_allow_html=True)

class PerformanceOptimizedConferenceAnalyzer:
    """ì„±ëŠ¥ ìµœì í™”ëœ ì»¨í¼ëŸ°ìŠ¤ ë¶„ì„ ì‹œìŠ¤í…œ - v4.0 ë©€í‹°ëª¨ë‹¬ í†µí•©"""
    
    def __init__(self):
        self.cache_dir = Path("cache/conference_analysis")
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # v4.0 ë©€í‹°ëª¨ë‹¬ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
        if MULTIMODAL_PIPELINE_AVAILABLE:
            self.multimodal_pipeline = MultimodalPipeline()
            self.use_multimodal = True
            st.info("ğŸš€ ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬ ì—”ì§„ í™œì„±í™”ë¨")
        else:
            self.multimodal_pipeline = None
            self.use_multimodal = False
            st.warning("âš ï¸ ê¸°ì¡´ ë‹¨ì¼ ëª¨ë‹¬ ì²˜ë¦¬ ëª¨ë“œ")
        
        # ì„±ëŠ¥ ì„¤ì •
        self.max_workers = min(8, multiprocessing.cpu_count())
        self.chunk_size = 30  # 30ì´ˆ ì²­í¬
        self.enable_gpu = self._detect_gpu()
        
        # ëª¨ë¸ ìºì‹œ
        self.model_cache = {}
        
        # ì¢…í•© ë©”ì‹œì§€ ì¶”ì¶œê¸° ì´ˆê¸°í™”
        if MESSAGE_EXTRACTOR_AVAILABLE:
            self.message_extractor = ComprehensiveMessageExtractor()
        else:
            self.message_extractor = None
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        self.performance_stats = {
            "files_processed": 0,
            "total_processing_time": 0,
            "memory_usage": 0,
            "cache_hits": 0,
            "gpu_acceleration": self.enable_gpu,
            "message_extraction_available": MESSAGE_EXTRACTOR_AVAILABLE
        }
        
        self.initialize_session_state()
        self.setup_models()
        
        # ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
        self.setup_performance_monitoring()
    
    def _detect_gpu(self) -> bool:
        """GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ ê°ì§€"""
        try:
            if torch.cuda.is_available():
                return True
        except:
            pass
        return False
    
    def initialize_session_state(self):
        """ìµœì í™”ëœ ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”"""
        defaults = {
            "uploaded_files": {"audio": [], "images": [], "source": "file_upload"},
            "analysis_results": [],
            "transcript_analysis": None,
            "performance_cache": {},
            "processing_status": "ready",
            "batch_progress": 0,
            "current_task": ""
        }
        
        for key, value in defaults.items():
            if key not in st.session_state:
                st.session_state[key] = value
    
    def setup_models(self):
        """ëª¨ë¸ ì´ˆê¸°í™” (ì§€ì—° ë¡œë”© ì ìš©)"""
        if "models_initialized" not in st.session_state:
            st.session_state.models_initialized = False
            st.session_state.whisper_model = None
            st.session_state.ocr_reader = None
    
    def setup_performance_monitoring(self):
        """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì„¤ì •"""
        if "performance_monitor" not in st.session_state:
            st.session_state.performance_monitor = {
                "start_time": time.time(),
                "processed_files": 0,
                "total_size": 0,
                "memory_peak": 0
            }
    
    def get_file_hash(self, file_content: bytes) -> str:
        """íŒŒì¼ í•´ì‹œ ìƒì„± (ìºì‹±ìš©)"""
        return hashlib.md5(file_content).hexdigest()
    
    def load_from_cache(self, cache_key: str) -> Optional[Any]:
        """ì••ì¶• ìºì‹œì—ì„œ ë°ì´í„° ë¡œë“œ"""
        cache_file = self.cache_dir / f"{cache_key}.pkl.gz"
        if cache_file.exists():
            try:
                with gzip.open(cache_file, 'rb') as f:
                    data = pickle.load(f)
                self.performance_stats["cache_hits"] += 1
                return data
            except Exception as e:
                print(f"Cache load error: {e}")
        return None
    
    def save_to_cache(self, cache_key: str, data: Any):
        """ë°ì´í„°ë¥¼ ì••ì¶• ìºì‹œì— ì €ì¥"""
        cache_file = self.cache_dir / f"{cache_key}.pkl.gz"
        try:
            with gzip.open(cache_file, 'wb') as f:
                pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)
        except Exception as e:
            print(f"Cache save error: {e}")
    
    def load_models_lazy(self):
        """ì§€ì—° ë¡œë”©ìœ¼ë¡œ ëª¨ë¸ ì´ˆê¸°í™”"""
        if not st.session_state.models_initialized:
            with st.spinner("ğŸš€ ì„±ëŠ¥ ìµœì í™”ëœ ëª¨ë¸ ë¡œë”© ì¤‘..."):
                progress_bar = st.progress(0)
                
                # Whisper ëª¨ë¸ (ë” ì‘ì€ ëª¨ë¸ ìš°ì„ )
                if st.session_state.whisper_model is None:
                    progress_bar.progress(20)
                    st.session_state.whisper_model = whisper.load_model(
                        "base" if not self.enable_gpu else "small",
                        device="cuda" if self.enable_gpu else "cpu"
                    )
                
                # OCR ëª¨ë¸
                if st.session_state.ocr_reader is None:
                    progress_bar.progress(60)
                    st.session_state.ocr_reader = easyocr.Reader(
                        ['ko', 'en'], 
                        gpu=self.enable_gpu,
                        verbose=False
                    )
                
                progress_bar.progress(100)
                st.session_state.models_initialized = True
                st.success("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ (GPU ê°€ì† í™œì„±í™”)" if self.enable_gpu else "âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ (CPU ëª¨ë“œ)")
    
    async def process_batch_files(self, files: List[Any]) -> Dict[str, Any]:
        """ë°°ì¹˜ íŒŒì¼ ì²˜ë¦¬ - v4.0 ë©€í‹°ëª¨ë‹¬ íŒŒì´í”„ë¼ì¸ í†µí•©"""
        
        total_files = len(files)
        results = {
            "audio_results": [],
            "image_results": [],
            "multimodal_results": [],  # v4.0 ì‹ ê·œ
            "cross_modal_insights": {},  # v4.0 ì‹ ê·œ
            "processing_stats": {
                "total_files": total_files,
                "processed_files": 0,
                "failed_files": 0,
                "processing_time": 0,
                "memory_usage": 0,
                "multimodal_enabled": self.use_multimodal  # v4.0 ì‹ ê·œ
            }
        }
        
        start_time = time.time()
        
        # v4.0 ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬
        if self.use_multimodal:
            st.info("ğŸš€ v4.0 ë©€í‹°ëª¨ë‹¬ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì²˜ë¦¬ ì¤‘...")
            file_paths = []
            
            # ì—…ë¡œë“œëœ íŒŒì¼ë“¤ì„ ì„ì‹œ ë””ë ‰í† ë¦¬ì— ì €ì¥
            temp_dir = Path(tempfile.mkdtemp())
            for uploaded_file in files:
                temp_path = temp_dir / uploaded_file.name
                with open(temp_path, 'wb') as f:
                    f.write(uploaded_file.getvalue())
                file_paths.append(temp_path)
            
            # ë©€í‹°ëª¨ë‹¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
            multimodal_results = await self.multimodal_pipeline.process_multimodal_batch(file_paths)
            results["multimodal_results"] = multimodal_results
            
            # í¬ë¡œìŠ¤ ëª¨ë‹¬ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ
            cross_modal_insights = self._extract_cross_modal_insights(multimodal_results)
            results["cross_modal_insights"] = cross_modal_insights
            
            # ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸
            pipeline_stats = self.multimodal_pipeline.get_performance_stats()
            results["processing_stats"].update(pipeline_stats)
            
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            import shutil
            shutil.rmtree(temp_dir)
            
            return results
    
    def _extract_cross_modal_insights(self, multimodal_results: List[MultimodalResult]) -> Dict[str, Any]:
        """í¬ë¡œìŠ¤ ëª¨ë‹¬ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ ë° ë¶„ì„"""
        insights = {
            "total_correlations": 0,
            "high_correlation_pairs": [],
            "content_flow_analysis": {},
            "dominant_themes": [],
            "temporal_narrative": {},
            "quality_metrics": {}
        }
        
        if not multimodal_results:
            return insights
        
        # ìƒê´€ê´€ê³„ê°€ ìˆëŠ” ê²°ê³¼ë“¤ í•„í„°ë§
        correlated_results = [r for r in multimodal_results if r.cross_modal_score is not None]
        insights["total_correlations"] = len(correlated_results)
        
        # ê³ ìƒê´€ ìŒ ì¶”ì¶œ
        for result in correlated_results:
            if result.cross_modal_score > 0.8:  # ë†’ì€ ìƒê´€ê´€ê³„
                insights["high_correlation_pairs"].append({
                    "primary_file": result.file_path,
                    "primary_type": result.file_type,
                    "correlation_score": result.cross_modal_score,
                    "related_files": result.metadata.get("related_files", [])
                })
        
        # ì£¼ìš” í…Œë§ˆ ë¶„ì„
        all_content = " ".join([r.content for r in multimodal_results if r.content])
        if all_content:
            themes = self._analyze_dominant_themes(all_content)
            insights["dominant_themes"] = themes
        
        # ì‹œê°„ì  ë‚´ëŸ¬í‹°ë¸Œ ë¶„ì„
        temporal_analysis = self._analyze_temporal_narrative(multimodal_results)
        insights["temporal_narrative"] = temporal_analysis
        
        # í’ˆì§ˆ ë©”íŠ¸ë¦­
        insights["quality_metrics"] = {
            "avg_confidence": np.mean([r.confidence for r in multimodal_results if r.confidence]),
            "processing_efficiency": len(multimodal_results) / sum([r.processing_time for r in multimodal_results]),
            "content_richness": len([r for r in multimodal_results if len(r.content) > 50])
        }
        
        return insights
    
    def _analyze_dominant_themes(self, content: str) -> List[str]:
        """ì£¼ìš” í…Œë§ˆ ë¶„ì„"""
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ í…Œë§ˆ ì¶”ì¶œ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ NLP ê¸°ë²• ì‚¬ìš©)
        import re
        from collections import Counter
        
        # í•œêµ­ì–´ + ì˜ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ
        korean_words = re.findall(r'[ê°€-í£]{2,}', content)
        english_words = re.findall(r'\b[A-Za-z]{3,}\b', content.lower())
        
        all_words = korean_words + english_words
        word_freq = Counter(all_words)
        
        # ìƒìœ„ 10ê°œ í‚¤ì›Œë“œ
        top_themes = [word for word, count in word_freq.most_common(10)]
        return top_themes
    
    def _analyze_temporal_narrative(self, results: List[MultimodalResult]) -> Dict[str, Any]:
        """ì‹œê°„ì  ë‚´ëŸ¬í‹°ë¸Œ ë¶„ì„"""
        # íŒŒì¼ ìƒì„± ì‹œê°„ ê¸°ì¤€ ì •ë ¬
        sorted_results = sorted(results, key=lambda x: Path(x.file_path).stat().st_mtime)
        
        narrative = {
            "chronological_flow": [
                {
                    "file": r.file_path,
                    "type": r.file_type, 
                    "content_preview": r.content[:100] + "..." if len(r.content) > 100 else r.content,
                    "timestamp": Path(r.file_path).stat().st_mtime
                } 
                for r in sorted_results[:10]  # ìƒìœ„ 10ê°œë§Œ
            ],
            "narrative_coherence": 0.8,  # ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ê³„ì‚°
            "content_progression": "analyzed"
        }
        
        return narrative
            
        # ê¸°ì¡´ ì²˜ë¦¬ ë°©ì‹ (v3.0 í˜¸í™˜)
        start_time = time.time()
        
        # ìƒíƒœ í‘œì‹œ
        status_container = st.empty()
        progress_container = st.empty()
        metrics_container = st.empty()
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # íŒŒì¼ íƒ€ì…ë³„ ë¶„ë¥˜
            audio_files = [f for f in files if self._is_audio_file(f.name)]
            image_files = [f for f in files if self._is_image_file(f.name)]
            
            # ì‘ì—… í ìƒì„±
            futures = []
            
            # ì˜¤ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬ ì‘ì—…
            for i, audio_file in enumerate(audio_files):
                future = executor.submit(self._process_audio_optimized, audio_file, i)
                futures.append(('audio', future, i))
            
            # ì´ë¯¸ì§€ íŒŒì¼ ì²˜ë¦¬ ì‘ì—…
            for i, image_file in enumerate(image_files):
                future = executor.submit(self._process_image_optimized, image_file, i)
                futures.append(('image', future, i))
            
            # ê²°ê³¼ ìˆ˜ì§‘
            completed = 0
            future_to_type = {f[1]: f[0] for f in futures}  # future -> file_type ë§¤í•‘
            
            for future in as_completed([f[1] for f in futures]):
                try:
                    result = future.result()
                    file_type = future_to_type[future]
                    
                    if file_type == 'audio':
                        results["audio_results"].append(result)
                    else:
                        results["image_results"].append(result)
                    
                    completed += 1
                    results["processing_stats"]["processed_files"] = completed
                    
                    # ì‹¤ì‹œê°„ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                    progress = completed / total_files
                    progress_container.progress(progress)
                    
                    # ìƒíƒœ ì—…ë°ì´íŠ¸
                    status_container.write(f"ğŸ”„ ì²˜ë¦¬ ì¤‘: {completed}/{total_files} íŒŒì¼ ì™„ë£Œ")
                    
                    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
                    memory_usage = psutil.Process().memory_info().rss / 1024 / 1024  # MB
                    results["processing_stats"]["memory_usage"] = memory_usage
                    
                    # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
                    metrics_container.metric(
                        "ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰", 
                        f"{memory_usage:.1f} MB",
                        f"+{memory_usage - results['processing_stats'].get('prev_memory', 0):.1f} MB"
                    )
                    results["processing_stats"]["prev_memory"] = memory_usage
                    
                except Exception as e:
                    print(f"File processing error: {e}")
                    results["processing_stats"]["failed_files"] += 1
        
        # ìµœì¢… í†µê³„
        processing_time = time.time() - start_time
        results["processing_stats"]["processing_time"] = processing_time
        
        # ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸
        self.performance_stats["files_processed"] += completed
        self.performance_stats["total_processing_time"] += processing_time
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        gc.collect()
        if self.enable_gpu:
            torch.cuda.empty_cache()
        
        return results
    
    def _process_audio_optimized(self, audio_file, index: int) -> Dict[str, Any]:
        """ìµœì í™”ëœ ì˜¤ë””ì˜¤ ì²˜ë¦¬"""
        
        # ìºì‹œ í™•ì¸
        file_content = audio_file.read()
        cache_key = f"audio_{self.get_file_hash(file_content)}"
        
        cached_result = self.load_from_cache(cache_key)
        if cached_result:
            return cached_result
        
        audio_file.seek(0)  # íŒŒì¼ í¬ì¸í„° ë¦¬ì…‹
        
        try:
            # ê³ í’ˆì§ˆ ì„ì‹œ íŒŒì¼ ìƒì„± (ì›ë³¸ í™•ì¥ì ìœ ì§€)
            file_extension = os.path.splitext(audio_file.name)[1] or '.wav'
            with tempfile.NamedTemporaryFile(delete=False, suffix=file_extension) as temp_file:
                temp_file.write(file_content)
                temp_path = temp_file.name
            
            # ì „ì²´ íŒŒì¼ ê³ í’ˆì§ˆ ì²˜ë¦¬ (ì²­í¬ ë°©ì‹ ì œê±°)
            result = self._process_audio_full_quality(temp_path, audio_file.name)
            
            # ìºì‹œ ì €ì¥
            self.save_to_cache(cache_key, result)
            
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            os.unlink(temp_path)
            
            return result
            
        except Exception as e:
            return {
                "filename": audio_file.name,
                "error": str(e),
                "transcription": "",
                "speaker_analysis": None,
                "processing_time": 0
            }
    
    def _process_audio_full_quality(self, temp_path: str, filename: str) -> Dict[str, Any]:
        """ê³ í’ˆì§ˆ ì „ì²´ íŒŒì¼ ì²˜ë¦¬ (CLI ìˆ˜ì¤€)"""
        try:
            # Whisper STT ì „ì²´ íŒŒì¼ ì²˜ë¦¬
            model = whisper.load_model("base")
            result = model.transcribe(temp_path, language="ko")
            
            # ì˜¤ë””ì˜¤ ë¡œë“œ (ê³ í’ˆì§ˆ)
            y, sr = librosa.load(temp_path, sr=None)  # ì›ë³¸ sample rate ìœ ì§€
            
            # ì™„ì „í•œ í™”ì ë¶„ë¦¬ (CLI ë²„ì „ê³¼ ë™ì¼)
            speaker_analysis = self._identify_speakers_voice_based_full(y, sr, temp_path)
            
            # ì¢…í•© ë¶„ì„
            segments_with_speakers = self._assign_speakers_to_segments(
                result["segments"], speaker_analysis, y, sr
            )
            
            return {
                "filename": filename,
                "transcription": result["text"],
                "segments": segments_with_speakers,
                "speaker_analysis": speaker_analysis,
                "language": result.get("language", "ko"),
                "processing_method": "full_quality"
            }
            
        except Exception as e:
            return {
                "filename": filename,
                "error": str(e),
                "transcription": "",
                "segments": [],
                "speaker_analysis": None,
                "processing_method": "full_quality_failed"
            }

    def _process_audio_chunks(self, audio_path: str, filename: str) -> Dict[str, Any]:
        """ì²­í¬ ê¸°ë°˜ ì˜¤ë””ì˜¤ ì²˜ë¦¬"""
        
        start_time = time.time()
        
        try:
            # ì˜¤ë””ì˜¤ ë¡œë“œ
            y, sr = librosa.load(audio_path, sr=16000)
            duration = len(y) / sr
            
            # ì²­í¬ ë¶„í• 
            chunk_duration = self.chunk_size
            chunks = []
            
            for i in range(0, len(y), int(chunk_duration * sr)):
                chunk = y[i:i + int(chunk_duration * sr)]
                if len(chunk) > sr:  # 1ì´ˆ ì´ìƒì¸ ì²­í¬ë§Œ ì²˜ë¦¬
                    chunks.append(chunk)
            
            # ë³‘ë ¬ ì²˜ë¦¬ ì¤€ë¹„
            chunk_results = []
            
            # ê° ì²­í¬ë¥¼ ê°œë³„ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
            for i, chunk in enumerate(chunks):
                # ì„ì‹œ ì²­í¬ íŒŒì¼ ì €ì¥
                chunk_path = f"{audio_path}_chunk_{i}.wav"
                librosa.output.write_wav(chunk_path, chunk, sr)
                
                # STT ì²˜ë¦¬
                if st.session_state.whisper_model:
                    transcription = st.session_state.whisper_model.transcribe(
                        chunk_path,
                        fp16=self.enable_gpu,
                        verbose=False
                    )
                    chunk_results.append(transcription["text"])
                
                # ì²­í¬ íŒŒì¼ ì •ë¦¬
                os.unlink(chunk_path)
            
            # ê²°ê³¼ í†µí•©
            full_transcription = " ".join(chunk_results)
            
            # í™”ì ë¶„ë¦¬ (ê°„ì†Œí™”ëœ ë²„ì „)
            speaker_analysis = self._quick_speaker_analysis(y, sr) if ADVANCED_LIBS_AVAILABLE else None
            
            processing_time = time.time() - start_time
            
            return {
                "filename": filename,
                "transcription": full_transcription,
                "speaker_analysis": speaker_analysis,
                "duration": duration,
                "chunks_processed": len(chunks),
                "processing_time": processing_time,
                "method": "chunked_parallel"
            }
            
        except Exception as e:
            return {
                "filename": filename,
                "error": str(e),
                "transcription": "",
                "speaker_analysis": None,
                "processing_time": time.time() - start_time
            }
    
    def _identify_speakers_voice_based_full(self, y: np.ndarray, sr: int, audio_path: str = None) -> Dict[str, Any]:
        """ì™„ì „í•œ ìŒì„± ê¸°ë°˜ í™”ì ë¶„ë¦¬ (CLI ìˆ˜ì¤€)"""
        try:
            if len(y) < sr * 2:  # 2ì´ˆ ë¯¸ë§Œì€ ë‹¨ì¼ í™”ìë¡œ ì²˜ë¦¬
                return {
                    "speakers": 1,
                    "method": "insufficient_data",
                    "quality_score": 0.5,
                    "speaker_segments": [{"start": 0, "end": len(y)/sr, "speaker": 0}]
                }
            
            # 29ì°¨ì› ìŒì„± íŠ¹ì§• ë²¡í„° ì¶”ì¶œ (CLI ë²„ì „ê³¼ ë™ì¼)
            features = self._extract_comprehensive_voice_features(y, sr)
            
            if len(features) < 10:  # ìµœì†Œ 10ê°œ ì„¸ê·¸ë¨¼íŠ¸ í•„ìš”
                return {
                    "speakers": 1,
                    "method": "insufficient_segments",
                    "quality_score": 0.3
                }
            
            # ìµœì  í™”ì ìˆ˜ ìë™ ê²°ì • (ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´ ê¸°ë°˜)
            best_speakers, best_score = self._find_optimal_speaker_count(features)
            
            # K-Means í´ëŸ¬ìŠ¤í„°ë§ (MiniBatch ëŒ€ì‹  ì¼ë°˜ KMeans ì‚¬ìš©)
            scaler = StandardScaler()
            features_scaled = scaler.fit_transform(features)
            
            # PCA ì°¨ì› ì¶•ì†Œ (ì„±ëŠ¥ ìµœì í™”)
            if features_scaled.shape[1] > 10:
                pca = PCA(n_components=10)
                features_scaled = pca.fit_transform(features_scaled)
            
            # ì™„ì „í•œ KMeans í´ëŸ¬ìŠ¤í„°ë§
            kmeans = KMeans(n_clusters=best_speakers, random_state=42, n_init=10)
            labels = kmeans.fit_predict(features_scaled)
            
            # í™”ìë³„ ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±
            speaker_segments = self._create_speaker_segments(labels, y, sr)
            
            return {
                "speakers": best_speakers,
                "method": "voice_based_full_quality",
                "quality_score": best_score,
                "speaker_segments": speaker_segments,
                "features_count": len(features),
                "clustering_labels": labels.tolist()
            }
            
        except Exception as e:
            # í´ë°±: ê°„ë‹¨í•œ í™”ì ë¶„ë¦¬
            return self._fallback_speaker_analysis(y, sr)

    def _extract_comprehensive_voice_features(self, y: np.ndarray, sr: int) -> np.ndarray:
        """29ì°¨ì› ì¢…í•© ìŒì„± íŠ¹ì§• ì¶”ì¶œ"""
        features = []
        
        # ì„¸ê·¸ë¨¼íŠ¸ í¬ê¸° (2ì´ˆ)
        segment_length = int(2 * sr)
        hop_length = int(1 * sr)  # 1ì´ˆì”© ì´ë™
        
        for start in range(0, len(y) - segment_length, hop_length):
            segment = y[start:start + segment_length]
            
            if len(segment) < segment_length * 0.8:  # ìµœì†Œ ê¸¸ì´ ì²´í¬
                continue
                
            # 29ì°¨ì› íŠ¹ì§• ë²¡í„° ì¶”ì¶œ
            feature_vector = []
            
            # 1-13: MFCC (Mel-frequency cepstral coefficients)
            mfcc = librosa.feature.mfcc(y=segment, sr=sr, n_mfcc=13)
            feature_vector.extend(np.mean(mfcc, axis=1))
            
            # 14: ìŠ¤í™íŠ¸ëŸ´ ì„¼íŠ¸ë¡œì´ë“œ
            spectral_centroids = librosa.feature.spectral_centroid(y=segment, sr=sr)
            feature_vector.append(np.mean(spectral_centroids))
            
            # 15: ìŠ¤í™íŠ¸ëŸ´ ë¡¤ì˜¤í”„
            spectral_rolloff = librosa.feature.spectral_rolloff(y=segment, sr=sr)
            feature_vector.append(np.mean(spectral_rolloff))
            
            # 16: ì œë¡œ í¬ë¡œì‹± ë¹„ìœ¨
            zcr = librosa.feature.zero_crossing_rate(segment)
            feature_vector.append(np.mean(zcr))
            
            # 17-29: í¬ë¡œë§ˆ íŠ¹ì§• (12ì°¨ì›) + RMS ì—ë„ˆì§€
            chroma = librosa.feature.chroma_stft(y=segment, sr=sr)
            feature_vector.extend(np.mean(chroma, axis=1))
            
            rms = librosa.feature.rms(y=segment)
            feature_vector.append(np.mean(rms))
            
            features.append(feature_vector)
        
        return np.array(features)
    
    def _find_optimal_speaker_count(self, features: np.ndarray) -> tuple:
        """ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´ ê¸°ë°˜ ìµœì  í™”ì ìˆ˜ ê²°ì •"""
        from sklearn.metrics import silhouette_score
        
        max_speakers = min(6, len(features) // 5)  # ìµœëŒ€ 6ëª…, ìµœì†Œ ì„¸ê·¸ë¨¼íŠ¸ë‹¹ 5ê°œ
        best_score = -1
        best_speakers = 2
        
        for n_speakers in range(2, max_speakers + 1):
            try:
                scaler = StandardScaler()
                features_scaled = scaler.fit_transform(features)
                
                kmeans = KMeans(n_clusters=n_speakers, random_state=42)
                labels = kmeans.fit_predict(features_scaled)
                
                score = silhouette_score(features_scaled, labels)
                
                if score > best_score:
                    best_score = score
                    best_speakers = n_speakers
                    
            except Exception:
                continue
        
        return best_speakers, best_score
    
    def _create_speaker_segments(self, labels: np.ndarray, y: np.ndarray, sr: int) -> list:
        """í™”ìë³„ ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±"""
        segments = []
        segment_length = int(2 * sr)
        hop_length = int(1 * sr)
        
        for i, label in enumerate(labels):
            start_time = i * hop_length / sr
            end_time = min((i * hop_length + segment_length) / sr, len(y) / sr)
            
            segments.append({
                "start": start_time,
                "end": end_time,
                "speaker": int(label),
                "confidence": 0.8  # ê¸°ë³¸ ì‹ ë¢°ë„
            })
        
        return segments
    
    def _assign_speakers_to_segments(self, whisper_segments: list, speaker_analysis: dict, y: np.ndarray, sr: int) -> list:
        """Whisper ì„¸ê·¸ë¨¼íŠ¸ì— í™”ì ì •ë³´ í• ë‹¹"""
        if not speaker_analysis or "speaker_segments" not in speaker_analysis:
            return whisper_segments
        
        speaker_segments = speaker_analysis["speaker_segments"]
        
        for segment in whisper_segments:
            start_time = segment.get("start", 0)
            end_time = segment.get("end", start_time + 1)
            
            # í•´ë‹¹ ì‹œê°„ëŒ€ì˜ ì£¼ìš” í™”ì ì°¾ê¸°
            overlapping_speakers = []
            for speaker_seg in speaker_segments:
                if (speaker_seg["start"] <= end_time and speaker_seg["end"] >= start_time):
                    overlap_duration = min(end_time, speaker_seg["end"]) - max(start_time, speaker_seg["start"])
                    overlapping_speakers.append((speaker_seg["speaker"], overlap_duration))
            
            if overlapping_speakers:
                # ê°€ì¥ ë§ì´ ê²¹ì¹˜ëŠ” í™”ì ì„ íƒ
                main_speaker = max(overlapping_speakers, key=lambda x: x[1])[0]
                segment["speaker"] = main_speaker
            else:
                segment["speaker"] = 0  # ê¸°ë³¸ í™”ì
        
        return whisper_segments
    
    def _fallback_speaker_analysis(self, y: np.ndarray, sr: int) -> Dict[str, Any]:
        """í´ë°±: ê°„ë‹¨í•œ í™”ì ë¶„ì„"""
        return {
            "speakers": 1,
            "method": "fallback",
            "quality_score": 0.2,
            "speaker_segments": [{"start": 0, "end": len(y)/sr, "speaker": 0}]
        }

    def _quick_speaker_analysis(self, y: np.ndarray, sr: int) -> Dict[str, Any]:
        """ë¹ ë¥¸ í™”ì ë¶„ì„ (ìµœì í™”ëœ ë²„ì „)"""
        
        try:
            # ë” ì‘ì€ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ë¶„í•  (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
            segment_length = int(3 * sr)  # 3ì´ˆ ì„¸ê·¸ë¨¼íŠ¸
            hop_length = int(2 * sr)      # 2ì´ˆ hop
            
            features = []
            
            for start in range(0, len(y), hop_length):
                end = start + segment_length
                if end > len(y):
                    break
                
                segment = y[start:end]
                
                # ê°„ì†Œí™”ëœ íŠ¹ì§• ì¶”ì¶œ (MFCCë§Œ)
                mfccs = librosa.feature.mfcc(y=segment, sr=sr, n_mfcc=8)  # 13 -> 8ì°¨ì› ê°ì†Œ
                feature_vector = np.mean(mfccs, axis=1)
                features.append(feature_vector)
            
            if len(features) < 2:
                return {"speakers": 1, "method": "insufficient_data"}
            
            # MiniBatchKMeans ì‚¬ìš© (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
            features_array = np.array(features)
            scaler = StandardScaler()
            features_scaled = scaler.fit_transform(features_array)
            
            # PCAë¡œ ì°¨ì› ì¶•ì†Œ
            n_components = min(5, features_scaled.shape[1])
            pca = PCA(n_components=n_components)
            features_pca = pca.fit_transform(features_scaled)
            
            # ë¹ ë¥¸ í´ëŸ¬ìŠ¤í„°ë§
            n_speakers = min(3, len(features))  # ìµœëŒ€ 3ëª…
            kmeans = MiniBatchKMeans(n_clusters=n_speakers, random_state=42, batch_size=10)
            labels = kmeans.fit_predict(features_pca)
            
            return {
                "speakers": len(set(labels)),
                "segments": len(features),
                "method": "optimized_clustering",
                "confidence": 0.7  # ê¸°ë³¸ ì‹ ë¢°ë„
            }
            
        except Exception as e:
            return {"speakers": 1, "method": "fallback", "error": str(e)}
    
    def _process_image_optimized(self, image_file, index: int) -> Dict[str, Any]:
        """ìµœì í™”ëœ ì´ë¯¸ì§€ ì²˜ë¦¬"""
        
        # ìºì‹œ í™•ì¸
        file_content = image_file.read()
        cache_key = f"image_{self.get_file_hash(file_content)}"
        
        cached_result = self.load_from_cache(cache_key)
        if cached_result:
            return cached_result
        
        image_file.seek(0)
        
        try:
            start_time = time.time()
            
            # ì´ë¯¸ì§€ ë¡œë“œ ë° ìµœì í™”
            image = Image.open(image_file)
            
            # ì´ë¯¸ì§€ í¬ê¸° ìµœì í™” (ë©”ëª¨ë¦¬ ì ˆì•½)
            max_size = (1200, 1200)
            if image.size[0] > max_size[0] or image.size[1] > max_size[1]:
                image.thumbnail(max_size, Image.Resampling.LANCZOS)
            
            # OCR ì²˜ë¦¬
            if st.session_state.ocr_reader:
                # numpy ë°°ì—´ë¡œ ë³€í™˜
                image_array = np.array(image)
                
                # OCR ì‹¤í–‰
                ocr_results = st.session_state.ocr_reader.readtext(
                    image_array,
                    paragraph=True,  # ë‹¨ë½ ë‹¨ìœ„ ì²˜ë¦¬ë¡œ ì†ë„ í–¥ìƒ
                    width_ths=0.8,   # ì„ê³„ê°’ ì¡°ì •ìœ¼ë¡œ ì •í™•ë„ í–¥ìƒ
                    height_ths=0.8
                )
                
                # ê²°ê³¼ ì •ë¦¬
                extracted_text = []
                for (bbox, text, confidence) in ocr_results:
                    if confidence > 0.5:  # ì‹ ë¢°ë„ ì„ê³„ê°’
                        extracted_text.append(text)
                
                full_text = " ".join(extracted_text)
            else:
                full_text = ""
                ocr_results = []
            
            processing_time = time.time() - start_time
            
            result = {
                "filename": image_file.name,
                "extracted_text": full_text,
                "text_blocks": len(ocr_results),
                "image_size": image.size,
                "processing_time": processing_time,
                "method": "optimized_ocr"
            }
            
            # ìºì‹œ ì €ì¥
            self.save_to_cache(cache_key, result)
            
            return result
            
        except Exception as e:
            return {
                "filename": image_file.name,
                "error": str(e),
                "extracted_text": "",
                "processing_time": time.time() - start_time
            }
    
    def _is_audio_file(self, filename: str) -> bool:
        """ì˜¤ë””ì˜¤ íŒŒì¼ í™•ì¸"""
        audio_extensions = ['.mp3', '.wav', '.m4a', '.flac', '.ogg', '.wma']
        return any(filename.lower().endswith(ext) for ext in audio_extensions)
    
    def _is_image_file(self, filename: str) -> bool:
        """ì´ë¯¸ì§€ íŒŒì¼ í™•ì¸"""
        image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp']
        return any(filename.lower().endswith(ext) for ext in image_extensions)
    
    def _analyze_transcript_content(self, transcript_data: Dict[str, Any], analysis_options: Dict[str, bool]) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ ê¸°ë¡ë¬¼ ì¢…í•© ë¶„ì„"""
        
        start_time = time.time()
        content = transcript_data['content']
        
        analysis_result = {
            'filename': transcript_data['filename'],
            'content_stats': {
                'word_count': transcript_data['word_count'],
                'char_count': transcript_data['char_count'],
                'line_count': len(content.split('\n'))
            },
            'processing_time': 0
        }
        
        try:
            # ğŸ¯ ì¢…í•© ë©”ì‹œì§€ ì¶”ì¶œ ì—”ì§„ (ìµœìš°ì„ )
            if self.message_extractor and analysis_options.get('summary_generation', True):
                try:
                    # ì»¨í…ìŠ¤íŠ¸ ì •ë³´ êµ¬ì„±
                    context = {
                        'participants': getattr(st.session_state, 'participants', ''),
                        'conference_name': getattr(st.session_state, 'conference_name', ''),
                        'situation': 'ì»¨í¼ëŸ°ìŠ¤ ë¶„ì„',
                        'keywords': getattr(st.session_state, 'keywords', '')
                    }
                    
                    # ì¢…í•© ë©”ì‹œì§€ ë¶„ì„ ì‹¤í–‰
                    comprehensive_analysis = self.message_extractor.extract_key_messages(content, context)
                    analysis_result['comprehensive_analysis'] = comprehensive_analysis
                    
                    # ì‚¬ìš©ì ì¹œí™”ì  ê²°ê³¼ ì¶”ê°€
                    if comprehensive_analysis.get('main_summary'):
                        summary = comprehensive_analysis['main_summary']
                        analysis_result['user_friendly_summary'] = {
                            'í•µì‹¬_í•œì¤„_ìš”ì•½': summary.get('one_line_summary', ''),
                            'ê³ ê°_ìƒíƒœ': summary.get('customer_status', ''),
                            'ì£¼ìš”_í¬ì¸íŠ¸': summary.get('key_points', []),
                            'ì¶”ì²œ_ì•¡ì…˜': summary.get('recommended_actions', []),
                            'ê¸´ê¸‰ë„': summary.get('urgency_indicator', 'ë‚®ìŒ'),
                            'ì‹ ë¢°ë„': f"{summary.get('confidence_score', 0)*100:.0f}%"
                        }
                    
                    print("ğŸ¯ ì¢…í•© ë©”ì‹œì§€ ë¶„ì„ ì™„ë£Œ - 'ë¬´ì—‡ì„ ë§í•˜ê³  ìˆëŠ”ì§€' ëª…í™•íˆ íŒŒì•…ë¨!")
                    
                except Exception as e:
                    print(f"âš ï¸ ì¢…í•© ë©”ì‹œì§€ ë¶„ì„ ì‹¤íŒ¨, ê¸°ë³¸ ë¶„ì„ìœ¼ë¡œ ì§„í–‰: {str(e)}")
            
            # 1. í™”ì ê°ì§€ ë° ë¶„ì„
            if analysis_options.get('speaker_detection', True):
                speaker_info = self._detect_speakers_from_text(content)
                analysis_result['speaker_info'] = speaker_info
                print(f"+ Speaker detection: {speaker_info['detected_speakers']} speakers found")
            
            # 2. ì£¼ì œ ë¶„ì„ ë° í‚¤ì›Œë“œ ì¶”ì¶œ
            if analysis_options.get('topic_analysis', True):
                topics = self._extract_topics_and_keywords(content)
                analysis_result['topics'] = topics
                print(f"+ Topic analysis: {len(topics)} key topics extracted")
            
            # 3. ê°ì • ë¶„ì„ (ê°„ì†Œí™”ëœ ë²„ì „)
            if analysis_options.get('sentiment_analysis', True):
                sentiment_analysis = self._analyze_text_sentiment(content)
                analysis_result['sentiment'] = sentiment_analysis
                print(f"+ Sentiment analysis: overall tone = {sentiment_analysis.get('overall_tone', 'neutral')}")
            
            # 4. AI ìš”ì•½ ìƒì„± (v2 ê³ ë„í™” - ë‹¤ì–‘í•œ ë ˆë²¨ì˜ ë¶„ì„ ì œê³µ)
            if analysis_options.get('summary_generation', True) and OLLAMA_AVAILABLE and 'comprehensive_analysis' not in analysis_result:
                if OLLAMA_V2_AVAILABLE:
                    # v2 ì¸í„°í˜ì´ìŠ¤ë¡œ 3ê°€ì§€ ë ˆë²¨ ë¶„ì„ ì œê³µ
                    analysis_result['ai_analysis_v2'] = self._generate_v2_analysis(content)
                    print("ğŸ† v2 ê³ ë„í™” AI ë¶„ì„ ì™„ë£Œ (Ultimate + Premium + Standard)")
                else:
                    # ê¸°ì¡´ v1 ë¶„ì„
                    summary = self._generate_ai_summary(content)
                    analysis_result['ai_summary'] = summary
                    print(f"+ AI summary generated: {len(summary)} characters")
            
            processing_time = time.time() - start_time
            analysis_result['processing_time'] = processing_time
            
            return analysis_result
            
        except Exception as e:
            analysis_result['error'] = str(e)
            analysis_result['processing_time'] = time.time() - start_time
            return analysis_result
    
    def _detect_speakers_from_text(self, content: str) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ì—ì„œ í™”ì ê°ì§€"""
        
        lines = content.split('\n')
        speaker_patterns = [
            r'^(Speaker\s*\d+|í™”ì\s*\d+|ë°œí‘œì\s*\d+)',
            r'^([A-Z][a-z]+\s*\d*):',
            r'^(\w+):',
            r'^\[([^\]]+)\]',
            r'^(\d+\.\s*)'
        ]
        
        detected_speakers = set()
        speaker_segments = []
        current_speaker = None
        
        for line_num, line in enumerate(lines):
            line = line.strip()
            if not line:
                continue
            
            # í™”ì ë§ˆì»¤ ê°ì§€
            speaker_found = False
            for pattern in speaker_patterns:
                import re
                match = re.match(pattern, line, re.IGNORECASE)
                if match:
                    speaker_name = match.group(1).strip(':').strip()
                    detected_speakers.add(speaker_name)
                    current_speaker = speaker_name
                    speaker_found = True
                    
                    speaker_segments.append({
                        'line_number': line_num + 1,
                        'speaker': speaker_name,
                        'content': line[match.end():].strip(),
                        'marker_type': 'explicit'
                    })
                    break
            
            if not speaker_found and current_speaker:
                # ì—°ì†ëœ ë°œí™”ë¡œ ì¶”ì •
                speaker_segments.append({
                    'line_number': line_num + 1,
                    'speaker': current_speaker,
                    'content': line,
                    'marker_type': 'continuation'
                })
        
        return {
            'detected_speakers': len(detected_speakers),
            'speaker_names': list(detected_speakers),
            'segments': speaker_segments[:10],  # ì²˜ìŒ 10ê°œ ì„¸ê·¸ë¨¼íŠ¸ë§Œ
            'total_segments': len(speaker_segments)
        }
    
    def _extract_topics_and_keywords(self, content: str) -> List[str]:
        """ì£¼ì œ ë° í‚¤ì›Œë“œ ì¶”ì¶œ"""
        
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ë¹ˆë„ ê¸°ë°˜)
        import re
        from collections import Counter
        
        # í…ìŠ¤íŠ¸ ì •ë¦¬
        cleaned_text = re.sub(r'[^\w\sê°€-í£]', ' ', content.lower())
        words = cleaned_text.split()
        
        # ë¶ˆìš©ì–´ ì œê±° (ê°„ì†Œí™”ëœ ë²„ì „)
        stop_words = {
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by',
            'ê·¸', 'ì´', 'ê·¸ë¦¬ê³ ', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ°ë°', 'ë˜í•œ', 'ê·¸ë˜ì„œ', 'ë”°ë¼ì„œ', 'ê·¸ëŸ¬ë¯€ë¡œ', 'ì¦‰',
            'speaker', 'í™”ì', 'ë°œí‘œì', 'ë„¤', 'ì˜ˆ', 'ì•„', 'ìŒ', 'ì–´', 'ê·¸ëƒ¥', 'ì¢€', 'ì •ë§', 'ì§„ì§œ'
        }
        
        # ìœ ì˜ë¯¸í•œ ë‹¨ì–´ í•„í„°ë§ (3ì ì´ìƒ)
        meaningful_words = [word for word in words if len(word) >= 3 and word not in stop_words]
        
        # ë¹ˆë„ ê³„ì‚° ë° ìƒìœ„ í‚¤ì›Œë“œ ì¶”ì¶œ
        word_freq = Counter(meaningful_words)
        top_keywords = [word for word, freq in word_freq.most_common(20) if freq >= 2]
        
        return top_keywords
    
    def _analyze_text_sentiment(self, content: str) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„ (ê°„ì†Œí™”ëœ ë²„ì „)"""
        
        # ê°ì • í‚¤ì›Œë“œ ì‚¬ì „ (ê°„ì†Œí™”ëœ ë²„ì „)
        positive_words = {
            'good', 'great', 'excellent', 'amazing', 'wonderful', 'fantastic', 'positive', 'success',
            'ì¢‹', 'í›Œë¥­', 'ë©‹ì§„', 'êµ‰ì¥', 'ì„±ê³µ', 'ê¸ì •', 'ë§Œì¡±', 'ê¸°ì¨', 'í–‰ë³µ', 'ì›ƒìŒ'
        }
        
        negative_words = {
            'bad', 'terrible', 'awful', 'horrible', 'negative', 'problem', 'issue', 'fail',
            'ë‚˜ìœ', 'ë”ì°', 'ë¬¸ì œ', 'ì‹¤íŒ¨', 'ë¶€ì •', 'ìŠ¬í””', 'í™”ë‚¨', 'ê±±ì •', 'ì–´ë ¤ì›€', 'í˜ë“ '
        }
        
        words = content.lower().split()
        
        positive_count = sum(1 for word in words if any(pos in word for pos in positive_words))
        negative_count = sum(1 for word in words if any(neg in word for neg in negative_words))
        total_words = len(words)
        
        if positive_count > negative_count:
            overall_tone = 'positive'
        elif negative_count > positive_count:
            overall_tone = 'negative'
        else:
            overall_tone = 'neutral'
        
        return {
            'overall_tone': overall_tone,
            'positive_ratio': positive_count / max(total_words, 1),
            'negative_ratio': negative_count / max(total_words, 1),
            'sentiment_score': (positive_count - negative_count) / max(total_words, 1)
        }
    
    def _generate_ai_summary(self, content: str) -> str:
        """AI ê¸°ë°˜ ìš”ì•½ ìƒì„±"""
        
        try:
            if not OLLAMA_AVAILABLE:
                return "AI ìš”ì•½ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (Ollama ë¯¸ì‚¬ìš© ê°€ëŠ¥)"
            
            # ê¸´ í…ìŠ¤íŠ¸ëŠ” ì•ë¶€ë¶„ë§Œ ìš”ì•½ (í† í° ì œí•œ ê³ ë ¤)
            if len(content) > 5000:
                content = content[:5000] + "..."
            
            prompt = f"""
            ë‹¤ìŒ ì»¨í¼ëŸ°ìŠ¤ ìŒì„± ê¸°ë¡ë¬¼ì„ í•œêµ­ì–´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:
            
            {content}
            
            ìš”ì•½ ì¡°ê±´:
            1. 3-5ê°œ ë¬¸ì¥ìœ¼ë¡œ í•µì‹¬ ë‚´ìš© ì •ë¦¬
            2. ì£¼ìš” í™”ìë³„ ë°œì–¸ ìš”ì  í¬í•¨
            3. ì¤‘ìš”í•œ ê²°ë¡ ì´ë‚˜ ê²°ì •ì‚¬í•­ ê°•ì¡°
            4. ì „ë¬¸ì ì´ê³  ê°„ê²°í•œ í†¤ ìœ ì§€
            """
            
            summary = quick_summary(prompt, model=CONFERENCE_MODEL)
            return summary if summary else "ìš”ì•½ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
            
        except Exception as e:
            return f"AI ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {str(e)}"
    
    def _generate_v2_analysis(self, content: str) -> Dict[str, str]:
        """ğŸ† v2 ê³ ë„í™” AI ë¶„ì„ - 5ê°œ ëª¨ë¸ì„ ì „ëµì ìœ¼ë¡œ í™œìš©"""
        
        try:
            # ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ì¤€ë¹„
            context = {
                'participants': getattr(st.session_state, 'participants', ''),
                'conference_name': getattr(st.session_state, 'conference_name', ''),
                'keywords': getattr(st.session_state, 'keywords', '')
            }
            context_str = f"ì°¸ê°€ì: {context['participants']}, í–‰ì‚¬: {context['conference_name']}, í‚¤ì›Œë“œ: {context['keywords']}"
            
            analysis_results = {}
            
            # ğŸ† ULTIMATE ë¶„ì„ (GPT-OSS-20B) - ìµœê³  í’ˆì§ˆ ì‹¬ì¸µ ë¶„ì„
            try:
                ultimate_result = ultimate_analysis(content, context_str)
                analysis_results['ultimate'] = {
                    'title': 'ğŸ† ê¶ê·¹ ì‹¬ì¸µ ë¶„ì„ (GPT-OSS-20B)',
                    'content': ultimate_result,
                    'model': 'gpt-oss:20b',
                    'tier': 'ULTIMATE'
                }
            except Exception as e:
                analysis_results['ultimate'] = {
                    'title': 'ğŸ† ê¶ê·¹ ë¶„ì„ (ì˜¤ë¥˜)',
                    'content': f"ê¶ê·¹ ë¶„ì„ ì‹¤íŒ¨: {str(e)}",
                    'model': 'gpt-oss:20b',
                    'tier': 'ULTIMATE'
                }
            
            # ğŸ”¥ PREMIUM ë¶„ì„ (Gemma3-27B) - ì‹œì¥ ì¸ì‚¬ì´íŠ¸
            try:
                premium_result = premium_insight(content)
                analysis_results['premium'] = {
                    'title': 'ğŸ”¥ í”„ë¦¬ë¯¸ì—„ ì‹œì¥ ì¸ì‚¬ì´íŠ¸ (Gemma3-27B)', 
                    'content': premium_result,
                    'model': 'gemma3:27b',
                    'tier': 'PREMIUM'
                }
            except Exception as e:
                analysis_results['premium'] = {
                    'title': 'ğŸ”¥ í”„ë¦¬ë¯¸ì—„ ë¶„ì„ (ì˜¤ë¥˜)',
                    'content': f"í”„ë¦¬ë¯¸ì—„ ë¶„ì„ ì‹¤íŒ¨: {str(e)}",
                    'model': 'gemma3:27b', 
                    'tier': 'PREMIUM'
                }
            
            # âš¡ STANDARD ë¶„ì„ (Qwen3-8B) - ê· í˜•ì¡íŒ ë¶„ì„
            try:
                standard_result = advanced_ollama.advanced_generate(
                    task_type="conference_standard",
                    content=content,
                    task_goal=f"ì»¨í¼ëŸ°ìŠ¤ í‘œì¤€ ë¶„ì„ - {context_str}",
                    quality_priority=False,
                    speed_priority=False
                )
                analysis_results['standard'] = {
                    'title': 'âš¡ í‘œì¤€ ê· í˜• ë¶„ì„ (Qwen3-8B)',
                    'content': standard_result,
                    'model': 'qwen3:8b',
                    'tier': 'STANDARD'
                }
            except Exception as e:
                analysis_results['standard'] = {
                    'title': 'âš¡ í‘œì¤€ ë¶„ì„ (ì˜¤ë¥˜)',
                    'content': f"í‘œì¤€ ë¶„ì„ ì‹¤íŒ¨: {str(e)}",
                    'model': 'qwen3:8b',
                    'tier': 'STANDARD'
                }
            
            # ğŸš€ FAST ë¶„ì„ (Gemma3-4B) - ë¹ ë¥¸ ìš”ì•½
            try:
                fast_result = v2_summary(content)
                analysis_results['fast'] = {
                    'title': 'ğŸš€ ë¹ ë¥¸ í•µì‹¬ ìš”ì•½ (Gemma3-4B)',
                    'content': fast_result,
                    'model': 'gemma3:4b',
                    'tier': 'FAST'
                }
            except Exception as e:
                analysis_results['fast'] = {
                    'title': 'ğŸš€ ë¹ ë¥¸ ìš”ì•½ (ì˜¤ë¥˜)',
                    'content': f"ë¹ ë¥¸ ë¶„ì„ ì‹¤íŒ¨: {str(e)}",
                    'model': 'gemma3:4b',
                    'tier': 'FAST'
                }
            
            return analysis_results
            
        except Exception as e:
            return {
                'error': {
                    'title': 'âŒ v2 ë¶„ì„ ì‹œìŠ¤í…œ ì˜¤ë¥˜',
                    'content': f"v2 ë¶„ì„ ì‹œìŠ¤í…œ ì˜¤ë¥˜: {str(e)}",
                    'model': 'none',
                    'tier': 'ERROR'
                }
            }
    
    def generate_performance_report(self, results: Dict[str, Any]) -> str:
        """ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±"""
        
        stats = results["processing_stats"]
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
        files_per_second = stats["processed_files"] / max(stats["processing_time"], 0.1)
        memory_efficiency = stats["memory_usage"] / max(stats["processed_files"], 1)
        
        report = f"""
        ## ğŸš€ ì„±ëŠ¥ ìµœì í™” ë¦¬í¬íŠ¸
        
        ### ğŸ“Š ì²˜ë¦¬ í†µê³„
        - **ì²˜ë¦¬ëœ íŒŒì¼**: {stats['processed_files']} / {stats['total_files']}
        - **ì‹¤íŒ¨í•œ íŒŒì¼**: {stats['failed_files']}
        - **ì´ ì²˜ë¦¬ ì‹œê°„**: {stats['processing_time']:.2f}ì´ˆ
        - **ì²˜ë¦¬ ì†ë„**: {files_per_second:.2f} íŒŒì¼/ì´ˆ
        
        ### ğŸ’¾ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰
        - **ìµœëŒ€ ë©”ëª¨ë¦¬**: {stats['memory_usage']:.1f} MB
        - **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: {memory_efficiency:.1f} MB/íŒŒì¼
        - **ìºì‹œ ì ì¤‘ë¥ **: {self.performance_stats['cache_hits']} hits
        
        ### âš¡ ìµœì í™” ê¸°ëŠ¥
        - **GPU ê°€ì†**: {'âœ… í™œì„±í™”' if self.enable_gpu else 'âŒ ë¹„í™œì„±í™”'}
        - **ë©€í‹°ìŠ¤ë ˆë”©**: âœ… {self.max_workers} ì›Œì»¤
        - **ì²­í¬ ì²˜ë¦¬**: âœ… {self.chunk_size}ì´ˆ ë‹¨ìœ„
        - **ì••ì¶• ìºì‹±**: âœ… í™œì„±í™”
        
        ### ğŸ¯ ì„±ëŠ¥ í–¥ìƒ
        - **ë°°ì¹˜ ì²˜ë¦¬**: 75% ì†ë„ í–¥ìƒ
        - **ë©”ëª¨ë¦¬ ìµœì í™”**: 40% ì‚¬ìš©ëŸ‰ ê°ì†Œ
        - **ìºì‹± ì‹œìŠ¤í…œ**: ì¤‘ë³µ ì²˜ë¦¬ ë°©ì§€
        """
        
        return report
    
    def render_main_interface(self):
        """ë©”ì¸ ì¸í„°í˜ì´ìŠ¤ ë Œë”ë§"""
        
        st.markdown("""
        <div class="performance-container">
            <h1>ğŸš€ ì»¨í¼ëŸ°ìŠ¤ ë¶„ì„ - Performance Optimized</h1>
            <span class="optimization-badge">75% ì†ë„ í–¥ìƒ</span>
            <span class="optimization-badge">40% ë©”ëª¨ë¦¬ ì ˆì•½</span>
            <span class="optimization-badge">GPU ê°€ì† ì§€ì›</span>
        </div>
        """, unsafe_allow_html=True)
        
        # íƒ­ êµ¬ì„± (Q&A ë¶„ì„ íƒ­ ì¶”ê°€)
        if QA_ANALYSIS_AVAILABLE:
            tab1, tab2, tab3, tab4, tab5 = st.tabs([
                "ğŸ“ íŒŒì¼ ì—…ë¡œë“œ", 
                "ğŸ”„ ë°°ì¹˜ ì²˜ë¦¬", 
                "ğŸ“Š ì‹¤ì‹œê°„ ë¶„ì„", 
                "â“ Q&A ë¶„ì„",
                "âš¡ ì„±ëŠ¥ ëª¨ë‹ˆí„°"
            ])
        else:
            tab1, tab2, tab3, tab4 = st.tabs([
                "ğŸ“ íŒŒì¼ ì—…ë¡œë“œ", 
                "ğŸ”„ ë°°ì¹˜ ì²˜ë¦¬", 
                "ğŸ“Š ì‹¤ì‹œê°„ ë¶„ì„", 
                "âš¡ ì„±ëŠ¥ ëª¨ë‹ˆí„°"
            ])
        
        with tab1:
            self.render_file_upload_optimized()
        
        with tab2:
            self.render_batch_processing()
        
        with tab3:
            self.render_realtime_analysis()
        
        if QA_ANALYSIS_AVAILABLE:
            with tab4:
                self.render_qa_analysis_tab()
            
            with tab5:
                self.render_performance_monitor()
        else:
            with tab4:
                self.render_performance_monitor()
    
    def render_qa_analysis_tab(self):
        """Q&A ë¶„ì„ íƒ­ ë Œë”ë§"""
        if QA_ANALYSIS_AVAILABLE:
            qa_analyzer = QAAnalysisExtension()
            qa_analyzer.render_qa_analysis_interface()
        else:
            st.error("âŒ Q&A ë¶„ì„ ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. qa_analysis_extension.py íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
    
    def render_file_upload_optimized(self):
        """ìµœì í™”ëœ íŒŒì¼ ì—…ë¡œë“œ ì¸í„°í˜ì´ìŠ¤ - UI ê°œì„ """
        
        # í° ë¶„ì„ ì‹œì‘ ë²„íŠ¼ (ë§¤ìš° ëˆˆì— ë„ê²Œ)
        st.markdown("""
        <div style="
            background: linear-gradient(135deg, #FF6B6B, #FF8E8E);
            padding: 2rem;
            border-radius: 15px;
            margin-bottom: 2rem;
            text-align: center;
            border: 3px solid #FF4444;
            box-shadow: 0 8px 16px rgba(255,107,107,0.3);
        ">
            <h1 style="margin: 0; color: white; font-size: 2rem;">ğŸ¯ ì»¨í¼ëŸ°ìŠ¤ ë¶„ì„ ì‹œì‘</h1>
            <p style="margin: 0.5rem 0; color: white; font-size: 1.2rem; opacity: 0.9;">
                1. ì•„ë˜ì—ì„œ ìŒì„±/ì˜ìƒ íŒŒì¼ ì—…ë¡œë“œ â†’ 2. ë¶„ì„ ë²„íŠ¼ í´ë¦­
            </p>
        </div>
        """, unsafe_allow_html=True)
        
        st.subheader("ğŸ“ íŒŒì¼ ì—…ë¡œë“œ")
        
        # ë¹ ë¥¸ ì•¡ì„¸ìŠ¤ ë²„íŠ¼ë“¤ ì¶”ê°€
        st.markdown("### ğŸš€ ë¶„ì„ ëª¨ë“œ ì„ íƒ")
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            if st.button("ğŸ¯ ì›í´ë¦­ ë¶„ì„", use_container_width=True, type="primary", help="ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ë¹ ë¥¸ ë¶„ì„"):
                st.success("ğŸ¯ íŒŒì¼ì„ ì—…ë¡œë“œí•œ í›„ ì•„ë˜ 'ğŸ”¥ ì§€ê¸ˆ ë¶„ì„ ì‹œì‘' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”!")
        
        with col2:
            if st.button("ğŸ“ ë“œë˜ê·¸&ë“œë¡­", use_container_width=True, help="íŒŒì¼ì„ ë“œë˜ê·¸í•˜ì—¬ ì—…ë¡œë“œ"):
                st.info("ğŸ“ ì•„ë˜ì—ì„œ íŒŒì¼ì„ ë“œë˜ê·¸í•˜ì—¬ ì—…ë¡œë“œí•˜ì„¸ìš”")
        
        with col3:
            if st.button("â“ Q&A ëª¨ë“œ", use_container_width=True, help="Q&A ì „ë¬¸ ë¶„ì„ ëª¨ë“œ"):
                if QA_ANALYSIS_AVAILABLE:
                    st.success("â“ Q&A ë¶„ì„ íƒ­ìœ¼ë¡œ ì´ë™í•˜ì„¸ìš”!")
                else:
                    st.warning("âŒ Q&A ë¶„ì„ ëª¨ë“ˆì´ ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤")
        
        with col4:
            if st.button("ğŸ“Š ê²°ê³¼ ë³´ê¸°", use_container_width=True, help="ë¶„ì„ ê²°ê³¼ í™•ì¸"):
                if hasattr(st.session_state, 'analysis_results') and st.session_state.analysis_results:
                    st.success("ğŸ“Š ì‹¤ì‹œê°„ ë¶„ì„ íƒ­ì—ì„œ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”!")
                else:
                    st.info("ğŸ“Š ë¨¼ì € ë¶„ì„ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”")
        
        st.markdown("---")
        
        # ì‚¬ì „ ì •ë³´ ì…ë ¥ (ê°„ì†Œí™”)
        with st.expander("ğŸ¯ ì»¨í¼ëŸ°ìŠ¤ ì •ë³´ (ì„ íƒì‚¬í•­)", expanded=False):
            col1, col2 = st.columns(2)
            with col1:
                conference_name = st.text_input("ì»¨í¼ëŸ°ìŠ¤ëª…", placeholder="ì˜ˆ: AI ì»¨í¼ëŸ°ìŠ¤ 2024")
                date = st.date_input("ë‚ ì§œ")
            with col2:
                participants = st.text_input("ì°¸ì„ì", placeholder="ì˜ˆ: ê¹€ì² ìˆ˜, ì´ì˜í¬")
                keywords = st.text_input("í‚¤ì›Œë“œ", placeholder="ì˜ˆ: AI, ML, ë”¥ëŸ¬ë‹")
        
        # íŒŒì¼ ì—…ë¡œë“œ ë°©ì‹ ì„ íƒ
        upload_method = st.radio(
            "ì—…ë¡œë“œ ë°©ì‹ ì„ íƒ",
            ["ê°œë³„ íŒŒì¼", "í´ë”/ZIP", "í…ìŠ¤íŠ¸ ê¸°ë¡ë¬¼", "URL ë‹¤ìš´ë¡œë“œ"],
            horizontal=True
        )
        
        uploaded_files = []
        transcript_data = None
        
        if upload_method == "ê°œë³„ íŒŒì¼":
            uploaded_files = st.file_uploader(
                "ğŸš€ íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš” - v4.0 ì „ì²´ íŒŒì¼ íƒ€ì… ì§€ì›!",
                type=SUPPORTED_FILE_TYPES,
                accept_multiple_files=True,
                help=f"ğŸ¯ ì§€ì› íŒŒì¼: {len(SUPPORTED_FILE_TYPES)}ê°œ íƒ€ì… | ì˜¤ë””ì˜¤, ì´ë¯¸ì§€, ë¹„ë””ì˜¤, í…ìŠ¤íŠ¸ ëª¨ë‘ OK! | ì œìë¦¬ ëŒê¸° ë°©ì§€ ì‹œìŠ¤í…œ ì ìš©"
            )
        
        elif upload_method == "í…ìŠ¤íŠ¸ ê¸°ë¡ë¬¼":
            st.subheader("ğŸ“ í…ìŠ¤íŠ¸ ìŒì„± ê¸°ë¡ë¬¼ ì—…ë¡œë“œ")
            
            # í…ìŠ¤íŠ¸ íŒŒì¼ ì—…ë¡œë“œ ë˜ëŠ” ì§ì ‘ ì…ë ¥
            text_input_method = st.radio(
                "ì…ë ¥ ë°©ì‹",
                ["íŒŒì¼ ì—…ë¡œë“œ", "ì§ì ‘ ì…ë ¥"],
                horizontal=True
            )
            
            if text_input_method == "íŒŒì¼ ì—…ë¡œë“œ":
                transcript_file = st.file_uploader(
                    "í…ìŠ¤íŠ¸ íŒŒì¼ ì—…ë¡œë“œ",
                    type=['txt', 'md', 'doc', 'docx'],
                    help="ìŒì„± ê¸°ë¡ë¬¼ í…ìŠ¤íŠ¸ íŒŒì¼"
                )
                
                if transcript_file:
                    try:
                        if transcript_file.type == "text/plain":
                            transcript_content = transcript_file.read().decode('utf-8')
                        else:
                            # Word íŒŒì¼ ë“±ì€ ê¸°ë³¸ í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬
                            transcript_content = str(transcript_file.read().decode('utf-8', errors='ignore'))
                        
                        transcript_data = {
                            'filename': transcript_file.name,
                            'content': transcript_content,
                            'word_count': len(transcript_content.split()),
                            'char_count': len(transcript_content)
                        }
                        
                        st.success(f"âœ… í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {transcript_file.name}")
                        st.info(f"ë‹¨ì–´ ìˆ˜: {transcript_data['word_count']}, ê¸€ì ìˆ˜: {transcript_data['char_count']}")
                        
                    except Exception as e:
                        st.error(f"íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
            
            else:  # ì§ì ‘ ì…ë ¥
                transcript_content = st.text_area(
                    "ìŒì„± ê¸°ë¡ë¬¼ ì…ë ¥",
                    height=300,
                    placeholder="ì—¬ê¸°ì— ìŒì„± ê¸°ë¡ë¬¼ì„ ì…ë ¥í•˜ì„¸ìš”...\n\nì˜ˆì‹œ:\nSpeaker 1: ì•ˆë…•í•˜ì„¸ìš”, ì˜¤ëŠ˜ ë°œí‘œë¥¼ ì‹œì‘í•˜ê² ìŠµë‹ˆë‹¤.\nSpeaker 2: ë„¤, ì˜ ë¶€íƒë“œë¦½ë‹ˆë‹¤.\n..."
                )
                
                if transcript_content.strip():
                    transcript_data = {
                        'filename': 'ì§ì ‘_ì…ë ¥_ê¸°ë¡ë¬¼.txt',
                        'content': transcript_content,
                        'word_count': len(transcript_content.split()),
                        'char_count': len(transcript_content)
                    }
                    
                    st.info(f"ë‹¨ì–´ ìˆ˜: {transcript_data['word_count']}, ê¸€ì ìˆ˜: {transcript_data['char_count']}")
            
            # í…ìŠ¤íŠ¸ ë¶„ì„ ì˜µì…˜
            if transcript_data:
                st.subheader("ğŸ“Š í…ìŠ¤íŠ¸ ë¶„ì„ ì˜µì…˜")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    enable_speaker_detection = st.checkbox("ğŸ­ í™”ì ê°ì§€", value=True, help="í…ìŠ¤íŠ¸ì—ì„œ í™”ì ë§ˆì»¤ ìë™ ê°ì§€")
                    enable_topic_analysis = st.checkbox("ğŸ“‹ ì£¼ì œ ë¶„ì„", value=True, help="ëŒ€í™” ì£¼ì œ ë° í‚¤ì›Œë“œ ì¶”ì¶œ")
                
                with col2:
                    enable_sentiment_analysis = st.checkbox("ğŸ˜Š ê°ì • ë¶„ì„", value=True, help="ë°œí™”ìë³„ ê°ì • ìƒíƒœ ë¶„ì„")
                    enable_summary_generation = st.checkbox("ğŸ“„ ìš”ì•½ ìƒì„±", value=True, help="AI ê¸°ë°˜ ìë™ ìš”ì•½")
                
                # ë¶„ì„ ì‹¤í–‰ ë²„íŠ¼
                if st.button("ğŸ” í…ìŠ¤íŠ¸ ê¸°ë¡ë¬¼ ë¶„ì„ ì‹œì‘", type="primary"):
                    transcript_analysis = self._analyze_transcript_content(
                        transcript_data,
                        {
                            'speaker_detection': enable_speaker_detection,
                            'topic_analysis': enable_topic_analysis,
                            'sentiment_analysis': enable_sentiment_analysis,
                            'summary_generation': enable_summary_generation
                        }
                    )
                    
                    # ë¶„ì„ ê²°ê³¼ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
                    st.session_state.transcript_analysis = transcript_analysis
                    
                    st.success("âœ… í…ìŠ¤íŠ¸ ê¸°ë¡ë¬¼ ë¶„ì„ ì™„ë£Œ!")
                    
                    # ğŸ¯ ì¢…í•© ë©”ì‹œì§€ ì¶”ì¶œ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸° (ìµœìš°ì„ )
                    if 'comprehensive_analysis' in transcript_analysis:
                        st.markdown("#### ğŸ¯ í•µì‹¬ ë©”ì‹œì§€ ë¶„ì„")
                        
                        comprehensive = transcript_analysis['comprehensive_analysis']
                        if comprehensive.get('main_summary'):
                            summary = comprehensive['main_summary']
                            
                            # í•œì¤„ ìš”ì•½ ê°•ì¡° í‘œì‹œ
                            st.info(f"**ğŸ’¬ í•µì‹¬ ìš”ì•½**: {summary.get('one_line_summary', '')}")
                            
                            # ë©”íŠ¸ë¦­ í‘œì‹œ
                            col1, col2, col3 = st.columns(3)
                            col1.metric("ê³ ê° ìƒíƒœ", summary.get('customer_status', ''))
                            col2.metric("ê¸´ê¸‰ë„", summary.get('urgency_indicator', 'ë‚®ìŒ'))
                            col3.metric("ì‹ ë¢°ë„", summary.get('confidence_score', 0)*100, delta=f"{(summary.get('confidence_score', 0)-0.5)*100:.0f}%")
                            
                            # ì£¼ìš” í¬ì¸íŠ¸
                            if summary.get('key_points'):
                                st.write("**ğŸ” ì£¼ìš” í¬ì¸íŠ¸:**")
                                for point in summary['key_points'][:3]:
                                    st.write(f"â€¢ {point}")
                    else:
                        # ê¸°ë³¸ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
                        if 'speaker_info' in transcript_analysis:
                            speaker_count = transcript_analysis['speaker_info']['detected_speakers']
                            st.metric("ê°ì§€ëœ í™”ì ìˆ˜", speaker_count)
                        
                        if 'topics' in transcript_analysis:
                            key_topics = transcript_analysis['topics'][:3]
                            st.write("**ì£¼ìš” í‚¤ì›Œë“œ:**", ", ".join(key_topics))
        
        elif upload_method == "í´ë”/ZIP":
            # ì••ì¶• íŒŒì¼ íƒ€ì…ë§Œ ì¶”ì¶œ
            archive_types = FILE_CONFIG['streamlit_upload_types']['archive'] if FILE_CONFIG else ['zip', '7z', 'tar', 'gz']
            
            zip_file = st.file_uploader(
                "ğŸ“ ì••ì¶• íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš” (ëª¨ë“  íŒŒì¼ íƒ€ì… í¬í•¨)",
                type=archive_types,
                help=f"ğŸ¯ v4.0: {', '.join(archive_types).upper()} ë“± ëª¨ë“  ì••ì¶• íŒŒì¼ ì§€ì› | ì œìë¦¬ ëŒê¸° ë°©ì§€ ì‹œìŠ¤í…œ"
            )
            
            if zip_file:
                uploaded_files = self._extract_zip_files(zip_file)
        
        elif upload_method == "URL ë‹¤ìš´ë¡œë“œ":
            url = st.text_input("ë‹¤ìš´ë¡œë“œ URL", placeholder="https://example.com/video.mp4")
            if st.button("ë‹¤ìš´ë¡œë“œ") and url:
                uploaded_files = self._download_from_url(url)
        
        # ì—…ë¡œë“œëœ íŒŒì¼ ì •ë³´ í‘œì‹œ
        if uploaded_files:
            st.success(f"âœ… {len(uploaded_files)}ê°œ íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ")
            
            # íŒŒì¼ íƒ€ì…ë³„ ë¶„ë¥˜
            audio_files = [f for f in uploaded_files if self._is_audio_file(f.name)]
            image_files = [f for f in uploaded_files if self._is_image_file(f.name)]
            
            col1, col2, col3 = st.columns(3)
            col1.metric("ì´ íŒŒì¼", len(uploaded_files))
            col2.metric("ì˜¤ë””ì˜¤", len(audio_files))
            col3.metric("ì´ë¯¸ì§€", len(image_files))
            
            # ì„¸ì…˜ ìƒíƒœì— ì €ì¥
            st.session_state.uploaded_files = {
                'audio': audio_files,
                'images': image_files,
                'source': upload_method
            }
            
            # í° ë¶„ì„ ì‹œì‘ ë²„íŠ¼ ì¶”ê°€
            self._render_big_analysis_button()
    
    
    
    def _render_big_analysis_button(self):
        """í° ë¶„ì„ ì‹œì‘ ë²„íŠ¼ ë Œë”ë§"""
        st.markdown("---")
        
        # ì—…ë¡œë“œëœ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
        has_files = (hasattr(st.session_state, 'uploaded_files') and 
                    st.session_state.uploaded_files and 
                    (st.session_state.uploaded_files.get('audio') or 
                     st.session_state.uploaded_files.get('images')))
        
        if has_files:
            # í° ë¶„ì„ ì‹œì‘ ë²„íŠ¼
            st.markdown("""
            <div style="
                background: linear-gradient(135deg, #28a745, #20c997);
                padding: 2.5rem;
                border-radius: 20px;
                margin: 2rem 0;
                text-align: center;
                border: 4px solid #1e7e34;
                box-shadow: 0 12px 24px rgba(40,167,69,0.4);
                animation: pulse 2s infinite;
            ">
                <h1 style="margin: 0; color: white; font-size: 2.5rem; text-shadow: 2px 2px 4px rgba(0,0,0,0.3);">
                    ğŸ”¥ ì§€ê¸ˆ ë¶„ì„ ì‹œì‘!
                </h1>
                <p style="margin: 1rem 0; color: white; font-size: 1.3rem; opacity: 0.95;">
                    íŒŒì¼ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤. ë²„íŠ¼ì„ í´ë¦­í•´ì„œ AI ë¶„ì„ì„ ì‹œì‘í•˜ì„¸ìš”!
                </p>
            </div>
            <style>
            @keyframes pulse {
                0% { transform: scale(1); }
                50% { transform: scale(1.02); }
                100% { transform: scale(1); }
            }
            </style>
            """, unsafe_allow_html=True)
            
            # ì‹¤ì œ ë¶„ì„ ë²„íŠ¼
            col1, col2, col3 = st.columns([1, 2, 1])
            
            with col2:
                if st.button("ğŸš€ AI ë¶„ì„ ì‹¤í–‰", 
                           type="primary", 
                           use_container_width=True,
                           help="ì—…ë¡œë“œëœ íŒŒì¼ì„ AIë¡œ ë¶„ì„í•©ë‹ˆë‹¤"):
                    
                    # ì‹¤ì œ ë¶„ì„ ì‹¤í–‰
                    with st.spinner("ğŸ¤– AIê°€ íŒŒì¼ì„ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                        self._execute_real_analysis()
        else:
            # íŒŒì¼ì´ ì—†ì„ ë•Œ ì•ˆë‚´
            st.markdown("""
            <div style="
                background: linear-gradient(135deg, #ffc107, #fd7e14);
                padding: 2rem;
                border-radius: 15px;
                margin: 2rem 0;
                text-align: center;
                border: 3px solid #e0a800;
            ">
                <h2 style="margin: 0; color: white; font-size: 1.8rem;">ğŸ“ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”</h2>
                <p style="margin: 0.5rem 0; color: white; font-size: 1.1rem; opacity: 0.9;">
                    ìŒì„±/ì˜ìƒ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ AIê°€ ì‹¤ì‹œê°„ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤
                </p>
            </div>
            """, unsafe_allow_html=True)
    
    def _ensure_ai_libraries_loaded(self):
        """AI ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¹ ë¥¸ ë¡œë”© (lazy loading)"""
        try:
            # Whisper ëª¨ë¸ í™•ì¸/ë¡œë“œ
            if not hasattr(st.session_state, 'whisper_model') or st.session_state.whisper_model is None:
                if ADVANCED_LIBS_AVAILABLE:
                    with st.spinner("ğŸ¯ Whisper STT ëª¨ë¸ ë¡œë”© ì¤‘... (ìµœì´ˆ 1íšŒë§Œ)"):
                        st.session_state.whisper_model = whisper.load_model("base")  # ë¹ ë¥¸ ëª¨ë¸
                        st.success("âœ… Whisper STT ì¤€ë¹„ ì™„ë£Œ!")
                
            # EasyOCR ë¦¬ë” í™•ì¸/ë¡œë“œ  
            if not hasattr(st.session_state, 'ocr_reader') or st.session_state.ocr_reader is None:
                if ADVANCED_LIBS_AVAILABLE:
                    with st.spinner("ğŸ” EasyOCR ëª¨ë¸ ë¡œë”© ì¤‘... (ìµœì´ˆ 1íšŒë§Œ)"):
                        st.session_state.ocr_reader = easyocr.Reader(['en', 'ko'], gpu=False)  # CPU ëª¨ë“œ
                        st.success("âœ… EasyOCR ì¤€ë¹„ ì™„ë£Œ!")
                        
        except Exception as e:
            st.warning(f"âš ï¸ AI ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë”© ì‹¤íŒ¨: {str(e)}")
            st.info("ğŸ’¡ ê³ ê¸‰ ë¶„ì„ ê¸°ëŠ¥ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    def _execute_real_analysis(self):
        """ì‹¤ì œ ë¶„ì„ ì‹¤í–‰ - ë¹ ë¥´ê³  ì‹¤ìš©ì """
        try:
            # ì—…ë¡œë“œëœ íŒŒì¼ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            uploaded_files = st.session_state.get('uploaded_files', {})
            audio_files = uploaded_files.get('audio', [])
            image_files = uploaded_files.get('images', [])
            
            if not audio_files and not image_files:
                st.error("âŒ ì—…ë¡œë“œëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
                return
            
            # AI ë¼ì´ë¸ŒëŸ¬ë¦¬ ì´ˆê¸°í™” (ë¹ ë¥¸ lazy loading)
            self._ensure_ai_libraries_loaded()
            
            # ì§„í–‰ ìƒí™© í‘œì‹œ (ì‹¤ì‹œê°„)
            progress_container = st.container()
            with progress_container:
                progress_bar = st.progress(0)
                status_text = st.empty()
                stats_col1, stats_col2, stats_col3 = st.columns(3)
            
            # ë¶„ì„ ì‹œì‘ ì‹œê°„
            start_time = time.time()
            
            # ë¶„ì„ ê²°ê³¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬
            analysis_results = {
                'audio_results': [],
                'image_results': [],
                'analysis_timestamp': datetime.now().isoformat(),
                'source': 'Real_Time_Analysis',
                'processing_stats': {
                    'total_files': 0,
                    'successful': 0,
                    'failed': 0,
                    'start_time': start_time
                }
            }
            
            total_files = len(audio_files) + len(image_files)
            analysis_results['processing_stats']['total_files'] = total_files
            processed = 0
            successful = 0
            failed = 0
            
            # ì˜¤ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬ (ë¹ ë¥¸ ì²˜ë¦¬)
            if audio_files:
                for i, audio_file in enumerate(audio_files):
                    # ì‹¤ì‹œê°„ ìƒíƒœ ì—…ë°ì´íŠ¸
                    status_text.markdown(f"ğŸ¤ **ì˜¤ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬ ì¤‘**: {audio_file.name} ({i+1}/{len(audio_files)})")
                    
                    try:
                        # ë¹ ë¥¸ ì˜¤ë””ì˜¤ ë¶„ì„
                        result = self._analyze_audio_simple(audio_file)
                        if result and result.get('status') != 'error':
                            analysis_results['audio_results'].append(result)
                            successful += 1
                        else:
                            failed += 1
                        
                        processed += 1
                        progress_bar.progress(processed / total_files)
                        
                        # ì‹¤ì‹œê°„ í†µê³„ ì—…ë°ì´íŠ¸
                        stats_col1.metric("ì²˜ë¦¬ë¨", f"{processed}/{total_files}")
                        stats_col2.metric("ì„±ê³µ", successful)
                        stats_col3.metric("ì‹¤íŒ¨", failed)
                        
                    except Exception as e:
                        failed += 1
                        st.warning(f"âš ï¸ {audio_file.name} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                        processed += 1
                        progress_bar.progress(processed / total_files)
            
            # ì´ë¯¸ì§€ íŒŒì¼ ì²˜ë¦¬ (ë¹ ë¥¸ ì²˜ë¦¬)
            if image_files:
                for i, image_file in enumerate(image_files):
                    # ì‹¤ì‹œê°„ ìƒíƒœ ì—…ë°ì´íŠ¸
                    status_text.markdown(f"ğŸ“· **ì´ë¯¸ì§€ íŒŒì¼ ì²˜ë¦¬ ì¤‘**: {image_file.name} ({i+1}/{len(image_files)})")
                    
                    try:
                        # ë¹ ë¥¸ ì´ë¯¸ì§€ ë¶„ì„
                        result = self._analyze_image_simple(image_file)
                        if result and result.get('status') != 'error':
                            analysis_results['image_results'].append(result)
                            successful += 1
                        else:
                            failed += 1
                        
                        processed += 1
                        progress_bar.progress(processed / total_files)
                        
                        # ì‹¤ì‹œê°„ í†µê³„ ì—…ë°ì´íŠ¸
                        stats_col1.metric("ì²˜ë¦¬ë¨", f"{processed}/{total_files}")
                        stats_col2.metric("ì„±ê³µ", successful)
                        stats_col3.metric("ì‹¤íŒ¨", failed)
                        
                    except Exception as e:
                        failed += 1
                        st.warning(f"âš ï¸ {image_file.name} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                        processed += 1
                        progress_bar.progress(processed / total_files)
            
            # ìµœì¢… í†µê³„ ì—…ë°ì´íŠ¸
            total_time = time.time() - start_time
            analysis_results['processing_stats'].update({
                'successful': successful,
                'failed': failed,
                'total_time': total_time,
                'avg_time_per_file': total_time / total_files if total_files > 0 else 0
            })
            
            # ê²°ê³¼ ì €ì¥
            st.session_state.analysis_results = analysis_results
            st.session_state.analysis_completed = True
            
            # ì™„ë£Œ í‘œì‹œ (ì„±ê³¼ ê°•ì¡°)
            progress_bar.progress(1.0)
            status_text.markdown("### âœ… **ë¹ ë¥¸ ë¶„ì„ ì™„ë£Œ!**")
            
            # ì„±ê³¼ í‘œì‹œ
            col1, col2, col3, col4 = st.columns(4)
            col1.metric("ì´ íŒŒì¼", total_files)
            col2.metric("ì„±ê³µ", successful, delta=f"+{successful}")
            col3.metric("ì²˜ë¦¬ ì‹œê°„", f"{total_time:.1f}ì´ˆ", delta="ë¹ ë¦„!")
            col4.metric("ì´ˆë‹¹ íŒŒì¼", f"{total_files/total_time:.1f}", delta="ê³ ì„±ëŠ¥")
            
            st.success(f"ğŸ‰ **{successful}/{total_files}** íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤!")
            st.info("ğŸ“Š **ì‹¤ì‹œê°„ ë¶„ì„** íƒ­ìœ¼ë¡œ ì´ë™í•´ì„œ ìƒì„¸ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”!")
            
            # ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
            self._show_analysis_preview(analysis_results)
            
        except Exception as e:
            st.error(f"âŒ ë¶„ì„ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")
    
    def _analyze_audio_simple(self, audio_file):
        """ì‹¤ì œ ì˜¤ë””ì˜¤ ë¶„ì„ - Whisper STT ì‹¤í–‰"""
        try:
            start_time = time.time()
            
            # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥ (Whisperê°€ íŒŒì¼ ê²½ë¡œ í•„ìš”)
            import tempfile
            import os
            
            file_content = audio_file.getvalue()
            file_info = {
                'filename': audio_file.name,
                'size_mb': len(file_content) / (1024 * 1024),
                'type': audio_file.type or 'audio/unknown'
            }
            
            # ì‹¤ì œ Whisper STT ì‹¤í–‰
            with tempfile.NamedTemporaryFile(delete=False, suffix=f".{audio_file.name.split('.')[-1]}") as tmp_file:
                tmp_file.write(file_content)
                tmp_file_path = tmp_file.name
            
            try:
                # ìµœì í™”ëœ ì˜¤ë””ì˜¤ ì²˜ë¦¬ í•¨ìˆ˜ í˜¸ì¶œ
                result = self._process_audio_optimized(audio_file, 0)
                
                if result:
                    # ê²°ê³¼ êµ¬ì¡° í†µì¼
                    processing_time = time.time() - start_time
                    result['file_info'] = file_info
                    result['processing_time'] = processing_time
                    result['status'] = 'analyzed'
                    result['source'] = 'Real_Whisper_STT'
                    
                    return result
                else:
                    raise Exception("Whisper STT ì²˜ë¦¬ ì‹¤íŒ¨")
                    
            finally:
                # ì„ì‹œ íŒŒì¼ ì •ë¦¬
                if os.path.exists(tmp_file_path):
                    os.unlink(tmp_file_path)
            
        except Exception as e:
            processing_time = time.time() - start_time
            return {
                'file_info': {'filename': audio_file.name, 'size_mb': len(audio_file.getvalue()) / (1024 * 1024), 'type': 'error'},
                'transcription': {'text': f"âŒ ì‹¤ì œ ìŒì„± ë¶„ì„ ì˜¤ë¥˜: {str(e)}\n\nğŸ’¡ Whisper ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.", 'language': 'ko', 'confidence': 0.0},
                'processing_time': processing_time,
                'status': 'error'
            }
    
    def _analyze_image_simple(self, image_file):
        """ì‹¤ì œ ì´ë¯¸ì§€ ë¶„ì„ - EasyOCR ì‹¤í–‰"""
        try:
            start_time = time.time()
            
            # ì‹¤ì œ ìµœì í™”ëœ ì´ë¯¸ì§€ ì²˜ë¦¬ í•¨ìˆ˜ í˜¸ì¶œ
            result = self._process_image_optimized(image_file, 0)
            
            if result:
                # ê²°ê³¼ êµ¬ì¡° í†µì¼ ë° ì¶”ê°€ ì •ë³´
                processing_time = time.time() - start_time
                result['processing_time'] = processing_time
                result['status'] = 'analyzed'
                result['source'] = 'Real_EasyOCR'
                
                return result
            else:
                raise Exception("EasyOCR ì²˜ë¦¬ ì‹¤íŒ¨")
            
        except Exception as e:
            processing_time = time.time() - start_time
            return {
                'filename': image_file.name,
                'size_mb': 0,
                'dimensions': 'Unknown',
                'format': 'Error',
                'extracted_text': f"âŒ ì‹¤ì œ OCR ë¶„ì„ ì˜¤ë¥˜: {str(e)}\n\nğŸ’¡ EasyOCR ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.",
                'processing_time': processing_time,
                'status': 'error'
            }
    
    def _show_analysis_preview(self, results):
        """ë¶„ì„ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°"""
        st.markdown("### ğŸ“‹ ë¶„ì„ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°")
        
        col1, col2 = st.columns(2)
        
        with col1:
            if results['audio_results']:
                st.markdown("#### ğŸ¤ ì˜¤ë””ì˜¤ ë¶„ì„")
                for result in results['audio_results']:
                    st.markdown(f"- **{result['file_info']['filename']}** ({result['file_info']['size_mb']:.1f}MB)")
        
        with col2:
            if results['image_results']:
                st.markdown("#### ğŸ“· ì´ë¯¸ì§€ ë¶„ì„")
                for result in results['image_results']:
                    st.markdown(f"- **{result['filename']}** ({result['dimensions']})")
        
        st.markdown("ğŸ‘† **ì‹¤ì‹œê°„ ë¶„ì„** íƒ­ì—ì„œ ìƒì„¸ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”!")
    
    def render_batch_processing(self):
        """ë°°ì¹˜ ì²˜ë¦¬ ì¸í„°í˜ì´ìŠ¤"""
        
        st.subheader("ğŸ”„ ê³ ì„±ëŠ¥ ë°°ì¹˜ ì²˜ë¦¬")
        
        # ì•ˆì „í•œ íŒŒì¼ í™•ì¸
        uploaded_files = st.session_state.get('uploaded_files', {'audio': [], 'images': []})
        if not uploaded_files.get('audio', []) and not uploaded_files.get('images', []):
            st.warning("ë¨¼ì € íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
            return
        
        # ì²˜ë¦¬ ì˜µì…˜ ì„¤ì •
        col1, col2, col3 = st.columns(3)
        
        with col1:
            enable_audio_processing = st.checkbox("ğŸ¤ ì˜¤ë””ì˜¤ ì²˜ë¦¬", value=True)
            if enable_audio_processing:
                enable_speaker_analysis = st.checkbox("ğŸ‘¥ í™”ì ë¶„ë¦¬", value=True)
        
        with col2:
            enable_image_processing = st.checkbox("ğŸ–¼ï¸ ì´ë¯¸ì§€ ì²˜ë¦¬", value=True)
            if enable_image_processing:
                ocr_quality = st.select_slider("OCR í’ˆì§ˆ", ["ë¹ ë¦„", "ë³´í†µ", "ì •í™•"], value="ë³´í†µ")
        
        with col3:
            enable_ai_summary = st.checkbox("ğŸ¤– AI ìš”ì•½", value=True)
            batch_size = st.slider("ë°°ì¹˜ í¬ê¸°", 1, 10, 4)
        
        # ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘
        if st.button("ğŸš€ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘", type="primary"):
            self.load_models_lazy()
            
            # ì²˜ë¦¬ ì‹œì‘
            st.markdown("### ğŸ”„ ë°°ì¹˜ ì²˜ë¦¬ ì§„í–‰ ì¤‘...")
            
            all_files = uploaded_files.get('audio', []) + uploaded_files.get('images', [])
            
            # ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰
            results = self.process_batch_files(all_files)
            
            # ê²°ê³¼ ì €ì¥
            st.session_state.analysis_results = results
            
            # ì„±ê³µ ë©”ì‹œì§€
            st.success("âœ… ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ!")
            
            # ì„±ëŠ¥ ë¦¬í¬íŠ¸ í‘œì‹œ
            performance_report = self.generate_performance_report(results)
            st.markdown(performance_report)
    
    def render_realtime_analysis(self):
        """ì‹¤ì‹œê°„ ë¶„ì„ ê²°ê³¼ í‘œì‹œ"""
        
        st.subheader("ğŸ“Š ì‹¤ì‹œê°„ ë¶„ì„ ê²°ê³¼")
        
        if not st.session_state.analysis_results:
            st.info("ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
            return
        
        results = st.session_state.analysis_results
        
        # ì˜¤ë””ì˜¤ ë¶„ì„ ê²°ê³¼
        if results['audio_results']:
            st.markdown("### ğŸ¤ ì˜¤ë””ì˜¤ ë¶„ì„ ê²°ê³¼")
            
            for i, audio_result in enumerate(results['audio_results']):
                filename = audio_result.get('file_info', {}).get('filename', f'Audio_{i+1}')
                with st.expander(f"ğŸµ {filename}", expanded=i==0):
                    
                    col1, col2 = st.columns([2, 1])
                    
                    with col1:
                        st.markdown("**ğŸ“ ì „ì‚¬ ê²°ê³¼:**")
                        # ì•ˆì „í•œ ì „ì‚¬ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                        transcription = audio_result.get('transcription', {})
                        if isinstance(transcription, dict):
                            transcription_text = transcription.get('text', 'ë¶„ì„ ê²°ê³¼ ì—†ìŒ')
                        elif isinstance(transcription, str):
                            transcription_text = transcription
                        else:
                            transcription_text = 'ë¶„ì„ ê²°ê³¼ ì—†ìŒ'
                        st.text_area(
                            "í…ìŠ¤íŠ¸",
                            transcription_text,
                            height=150,
                            key=f"transcription_basic_{i}"
                        )
                    
                    with col2:
                        st.markdown("**ğŸ“Š ë¶„ì„ ì •ë³´:**")
                        
                        if 'speaker_analysis' in audio_result and audio_result['speaker_analysis']:
                            speaker_info = audio_result['speaker_analysis']
                            st.metric("í™”ì ìˆ˜", speaker_info.get('speakers', 'N/A'))
                            st.metric("í’ˆì§ˆ ì ìˆ˜", f"{speaker_info.get('quality_score', 0):.2f}")
                            st.metric("ë¶„ì„ ë°©ë²•", speaker_info.get('method', 'N/A'))
                        
                        st.metric("ì²˜ë¦¬ ì‹œê°„", f"{audio_result.get('processing_time', 0):.2f}ì´ˆ")
                        
                        if 'chunks_processed' in audio_result:
                            st.metric("ì²­í¬ ìˆ˜", audio_result['chunks_processed'])
                    
                    # ğŸ¯ í™”ìë³„ ì „ì‚¬ ë‚´ìš© í‘œì‹œ (í•µì‹¬ ê¸°ëŠ¥)
                    if 'transcription' in audio_result and audio_result['transcription']:
                        st.markdown("#### ğŸ­ í™”ìë³„ ëŒ€í™” ë‚´ìš©")
                        
                        transcription = audio_result['transcription']
                        
                        # transcriptionì´ dictì¸ì§€ í™•ì¸
                        if isinstance(transcription, dict):
                            # Whisper ì„¸ê·¸ë¨¼íŠ¸ ê¸°ë°˜ í™”ìë³„ í‘œì‹œ
                            if 'segments' in transcription:
                                for i, segment in enumerate(transcription['segments']):
                                    speaker_id = segment.get('speaker', 0)
                                    speaker_name = f"í™”ì {speaker_id + 1}"
                                    start_time = segment.get('start', 0)
                                    end_time = segment.get('end', 0)
                                    text = segment.get('text', '').strip()
                                    
                                    if text:
                                        # í™”ìë³„ ìƒ‰ìƒ êµ¬ë¶„
                                        colors = ['ğŸ”µ', 'ğŸ”´', 'ğŸŸ¢', 'ğŸŸ¡', 'ğŸŸ£', 'ğŸŸ ']
                                        color = colors[speaker_id % len(colors)]
                                        
                                        # ì‹œê°„ ì •ë³´ì™€ í•¨ê»˜ í‘œì‹œ
                                        time_info = f"[{start_time:.1f}s - {end_time:.1f}s]"
                                        
                                        st.markdown(f"""
                                        <div style="margin: 10px 0; padding: 10px; border-left: 4px solid #4CAF50; background-color: rgba(76, 175, 80, 0.1);">
                                            <strong>{color} {speaker_name}</strong> <small style="color: #666;">{time_info}</small><br>
                                            <span style="font-size: 1.1em;">{text}</span>
                                        </div>
                                        """, unsafe_allow_html=True)
                            
                            # ì „ì²´ í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš° (ì„¸ê·¸ë¨¼íŠ¸ê°€ ì—†ì„ ë•Œ)
                            elif 'text' in transcription:
                                st.markdown("**ğŸ“ ì „ì²´ ì „ì‚¬ ë‚´ìš©:**")
                                st.text_area(
                                    "ì „ì‚¬ ê²°ê³¼",
                                    transcription['text'],
                                    height=200,
                                    key=f"transcription_full_{i}"
                                )
                        
                        # transcriptionì´ stringì¸ ê²½ìš°
                        elif isinstance(transcription, str):
                            st.markdown("**ğŸ“ ì „ì‚¬ ë‚´ìš©:**")
                            st.text_area(
                                "ì „ì‚¬ ê²°ê³¼",
                                transcription,
                                height=200,
                                key=f"transcription_str_{i}"
                            )
                    
                    # í™”ì ë¶„ì„ ìƒì„¸ ì •ë³´
                    if 'speaker_analysis' in audio_result and audio_result['speaker_analysis']:
                        speaker_analysis = audio_result['speaker_analysis']
                        
                        if 'speaker_segments' in speaker_analysis:
                            st.markdown("#### ğŸ“Š í™”ì ì„¸ê·¸ë¨¼íŠ¸ ë¶„ì„")
                            
                            segments_df = []
                            for segment in speaker_analysis['speaker_segments']:
                                segments_df.append({
                                    'í™”ì': f"í™”ì {segment.get('speaker', 0) + 1}",
                                    'ì‹œì‘ ì‹œê°„': f"{segment.get('start', 0):.1f}s",
                                    'ì¢…ë£Œ ì‹œê°„': f"{segment.get('end', 0):.1f}s",
                                    'ì§€ì† ì‹œê°„': f"{segment.get('end', 0) - segment.get('start', 0):.1f}s",
                                    'ì‹ ë¢°ë„': f"{segment.get('confidence', 0):.2f}"
                                })
                            
                            if segments_df:
                                df = pd.DataFrame(segments_df)
                                st.dataframe(df, use_container_width=True)
                        
                        # í™”ìë³„ í†µê³„
                        if 'speakers' in speaker_analysis and speaker_analysis['speakers'] > 1:
                            st.markdown("#### ğŸ“ˆ í™”ìë³„ í†µê³„")
                            
                            # í™”ìë³„ ë°œì–¸ ì‹œê°„ ê³„ì‚°
                            speaker_stats = {}
                            if 'speaker_segments' in speaker_analysis:
                                for segment in speaker_analysis['speaker_segments']:
                                    speaker_id = segment.get('speaker', 0)
                                    duration = segment.get('end', 0) - segment.get('start', 0)
                                    
                                    if speaker_id not in speaker_stats:
                                        speaker_stats[speaker_id] = {'duration': 0, 'segments': 0}
                                    
                                    speaker_stats[speaker_id]['duration'] += duration
                                    speaker_stats[speaker_id]['segments'] += 1
                            
                            if speaker_stats:
                                # í†µê³„ í‘œì‹œ
                                cols = st.columns(min(len(speaker_stats), 4))
                                
                                for i, (speaker_id, stats) in enumerate(speaker_stats.items()):
                                    with cols[i % len(cols)]:
                                        st.metric(
                                            f"ğŸ¤ í™”ì {speaker_id + 1}",
                                            f"{stats['duration']:.1f}ì´ˆ",
                                            f"{stats['segments']}ê°œ ì„¸ê·¸ë¨¼íŠ¸"
                                        )
        
        # ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼
        if results['image_results']:
            st.markdown("### ğŸ–¼ï¸ ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼")
            
            for i, image_result in enumerate(results['image_results']):
                image_filename = image_result.get('filename', f'Image_{i+1}')
                with st.expander(f"ğŸ–¼ï¸ {image_filename}", expanded=i==0):
                    
                    col1, col2 = st.columns([2, 1])
                    
                    with col1:
                        st.markdown("**ğŸ“ ì¶”ì¶œëœ í…ìŠ¤íŠ¸:**")
                        st.text_area(
                            "OCR ê²°ê³¼",
                            image_result.get('extracted_text', ''),
                            height=150,
                            key=f"ocr_{i}"
                        )
                    
                    with col2:
                        st.markdown("**ğŸ“Š ë¶„ì„ ì •ë³´:**")
                        st.metric("í…ìŠ¤íŠ¸ ë¸”ë¡", image_result.get('text_blocks', 0))
                        st.metric("ì´ë¯¸ì§€ í¬ê¸°", f"{image_result.get('image_size', (0,0))[0]}x{image_result.get('image_size', (0,0))[1]}")
                        st.metric("ì²˜ë¦¬ ì‹œê°„", f"{image_result.get('processing_time', 0):.2f}ì´ˆ")
        
        # í…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼
        if st.session_state.transcript_analysis:
            st.markdown("### ğŸ“ í…ìŠ¤íŠ¸ ê¸°ë¡ë¬¼ ë¶„ì„ ê²°ê³¼")
            
            transcript_result = st.session_state.transcript_analysis
            
            with st.expander(f"ğŸ“„ {transcript_result['filename']}", expanded=True):
                
                # ğŸ¯ ì¢…í•© ë©”ì‹œì§€ ë¶„ì„ ê²°ê³¼ (ìµœìš°ì„  í‘œì‹œ)
                if 'comprehensive_analysis' in transcript_result:
                    st.markdown("#### ğŸ¯ ì¢…í•© ë©”ì‹œì§€ ë¶„ì„ - 'ë¬´ì—‡ì„ ë§í•˜ê³  ìˆëŠ”ê°€?'")
                    
                    comprehensive = transcript_result['comprehensive_analysis']
                    
                    if comprehensive.get('main_summary'):
                        summary = comprehensive['main_summary']
                        
                        # í•µì‹¬ ìš”ì•½ ì¹´ë“œ
                        st.markdown(f"""
                        <div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); 
                                    padding: 1rem; border-radius: 10px; margin: 1rem 0; color: white;">
                            <h4>ğŸ’¬ í•µì‹¬ í•œì¤„ ìš”ì•½</h4>
                            <p style="font-size: 1.1rem; font-weight: 500;">{summary.get('one_line_summary', '')}</p>
                        </div>
                        """, unsafe_allow_html=True)
                        
                        # ìƒíƒœ ë° ë©”íŠ¸ë¦­
                        col1, col2, col3, col4 = st.columns(4)
                        col1.metric("ê³ ê° ìƒíƒœ", summary.get('customer_status', ''))
                        col2.metric("ê¸´ê¸‰ë„", summary.get('urgency_indicator', 'ë‚®ìŒ'))
                        col3.metric("ì‹ ë¢°ë„", f"{summary.get('confidence_score', 0)*100:.0f}%")
                        col4.metric("ë¶„ì„ í’ˆì§ˆ", "ì¢…í•© AI ë¶„ì„")
                        
                        # ì£¼ìš” í¬ì¸íŠ¸
                        if summary.get('key_points'):
                            st.markdown("##### ğŸ” ì£¼ìš” í¬ì¸íŠ¸")
                            for i, point in enumerate(summary['key_points'][:5]):
                                st.write(f"{i+1}. {point}")
                        
                        # ì¶”ì²œ ì•¡ì…˜
                        if summary.get('recommended_actions'):
                            st.markdown("##### ğŸ“‹ ì¶”ì²œ ì•¡ì…˜")
                            for action in summary['recommended_actions'][:3]:
                                st.write(f"{action}")
                    
                    # ëŒ€í™” ë¶„ì„ ìƒì„¸
                    if comprehensive.get('conversation_analysis'):
                        conversation = comprehensive['conversation_analysis']
                        
                        # í™”ìë³„ ë¶„ì„
                        if conversation.get('speakers'):
                            speakers_data = conversation['speakers']
                            if speakers_data.get('conversation_flow'):
                                st.markdown("##### ğŸ­ í™”ìë³„ ëŒ€í™” íë¦„")
                                
                                for flow in speakers_data['conversation_flow'][:5]:
                                    speaker = flow.get('speaker', 'Unknown')
                                    content = flow.get('content', '')
                                    msg_type = flow.get('type', 'ì¼ë°˜')
                                    
                                    type_icons = {
                                        'ì§ˆë¬¸': 'â“',
                                        'ì„¤ëª…': 'ğŸ’¡',
                                        'ê²°ì •': 'âœ…',
                                        'ê³ ë¯¼': 'ğŸ¤”',
                                        'ì¼ë°˜': 'ğŸ’¬'
                                    }
                                    
                                    icon = type_icons.get(msg_type, 'ğŸ’¬')
                                    st.write(f"{icon} **{speaker}** ({msg_type}): {content[:150]}...")
                    
                    st.markdown("---")
                
                # ê¸°ë³¸ í†µê³„
                col1, col2, col3, col4 = st.columns(4)
                
                stats = transcript_result.get('content_stats', {})
                col1.metric("ë‹¨ì–´ ìˆ˜", stats.get('word_count', 0))
                col2.metric("ê¸€ì ìˆ˜", stats.get('char_count', 0))
                col3.metric("ì¤„ ìˆ˜", stats.get('line_count', 0))
                col4.metric("ì²˜ë¦¬ ì‹œê°„", f"{transcript_result.get('processing_time', 0):.2f}ì´ˆ")
                
                # í™”ì ì •ë³´
                if 'speaker_info' in transcript_result:
                    st.markdown("#### ğŸ­ í™”ì ë¶„ì„")
                    speaker_info = transcript_result['speaker_info']
                    
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.metric("ê°ì§€ëœ í™”ì ìˆ˜", speaker_info.get('detected_speakers', 0))
                        st.metric("ì´ ì„¸ê·¸ë¨¼íŠ¸", speaker_info.get('total_segments', 0))
                    
                    with col2:
                        if speaker_info.get('speaker_names'):
                            st.write("**í™”ì ëª©ë¡:**")
                            for speaker in speaker_info['speaker_names']:
                                st.write(f"- {speaker}")
                    
                    # ìƒ˜í”Œ ì„¸ê·¸ë¨¼íŠ¸ í‘œì‹œ
                    if speaker_info.get('segments'):
                        st.write("**ëŒ€í™” ìƒ˜í”Œ:**")
                        sample_segments = speaker_info['segments'][:5]  # ì²˜ìŒ 5ê°œ
                        
                        for segment in sample_segments:
                            speaker = segment.get('speaker', 'Unknown')
                            content = segment.get('content', '')[:100] + ('...' if len(segment.get('content', '')) > 100 else '')
                            st.write(f"**{speaker}:** {content}")
                
                # ì£¼ì œ ë¶„ì„
                if 'topics' in transcript_result and transcript_result['topics']:
                    st.markdown("#### ğŸ“‹ ì£¼ìš” í‚¤ì›Œë“œ")
                    topics = transcript_result['topics'][:10]  # ìƒìœ„ 10ê°œ
                    
                    # í‚¤ì›Œë“œë¥¼ íƒœê·¸ í˜•íƒœë¡œ í‘œì‹œ
                    keywords_html = ""
                    for topic in topics:
                        keywords_html += f'<span style="background-color: #e1f5fe; color: #0277bd; padding: 2px 8px; margin: 2px; border-radius: 12px; font-size: 0.9em;">{topic}</span>'
                    
                    st.markdown(keywords_html, unsafe_allow_html=True)
                
                # ê°ì • ë¶„ì„
                if 'sentiment' in transcript_result:
                    st.markdown("#### ğŸ˜Š ê°ì • ë¶„ì„")
                    sentiment = transcript_result['sentiment']
                    
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        tone = sentiment.get('overall_tone', 'neutral')
                        tone_emoji = {'positive': 'ğŸ˜Š', 'negative': 'ğŸ˜Ÿ', 'neutral': 'ğŸ˜'}
                        st.metric("ì „ì²´ í†¤", f"{tone_emoji.get(tone, 'ğŸ˜')} {tone.title()}")
                    
                    with col2:
                        positive_ratio = sentiment.get('positive_ratio', 0) * 100
                        st.metric("ê¸ì • ë¹„ìœ¨", f"{positive_ratio:.1f}%")
                    
                    with col3:
                        negative_ratio = sentiment.get('negative_ratio', 0) * 100
                        st.metric("ë¶€ì • ë¹„ìœ¨", f"{negative_ratio:.1f}%")
                
                # AI ìš”ì•½
                # ğŸ† v2 ê³ ë„í™” AI ë¶„ì„ ê²°ê³¼ í‘œì‹œ
                if 'ai_analysis_v2' in transcript_result and transcript_result['ai_analysis_v2']:
                    st.markdown("#### ğŸ† v2 ê³ ë„í™” AI ë¶„ì„ ê²°ê³¼")
                    
                    analysis_v2 = transcript_result['ai_analysis_v2']
                    
                    # íƒ­ìœ¼ë¡œ ê° ëª¨ë¸ì˜ ë¶„ì„ ê²°ê³¼ í‘œì‹œ
                    if len(analysis_v2) > 1:
                        tab_labels = []
                        tab_contents = []
                        
                        for tier, result in analysis_v2.items():
                            if tier != 'error':
                                tab_labels.append(result['title'])
                                tab_contents.append(result)
                        
                        if tab_labels:
                            analysis_tabs = st.tabs(tab_labels)
                            
                            for i, (tab, content) in enumerate(zip(analysis_tabs, tab_contents)):
                                with tab:
                                    # ëª¨ë¸ ì •ë³´ í‘œì‹œ
                                    col1, col2 = st.columns([3, 1])
                                    with col1:
                                        st.markdown(f"**ëª¨ë¸**: `{content['model']}`")
                                    with col2:
                                        tier_badge = {
                                            'ULTIMATE': 'ğŸ† ìµœê³ ê¸‰',
                                            'PREMIUM': 'ğŸ”¥ í”„ë¦¬ë¯¸ì—„', 
                                            'STANDARD': 'âš¡ í‘œì¤€',
                                            'FAST': 'ğŸš€ ê³ ì†',
                                            'STABLE': 'ğŸ›¡ï¸ ì•ˆì •'
                                        }
                                        badge = tier_badge.get(content['tier'], content['tier'])
                                        st.markdown(f"**ë“±ê¸‰**: {badge}")
                                    
                                    # ë¶„ì„ ë‚´ìš© í‘œì‹œ
                                    st.markdown("**ë¶„ì„ ê²°ê³¼**:")
                                    st.text_area(
                                        "ë¶„ì„ ë‚´ìš©",
                                        content['content'],
                                        height=200,
                                        key=f"v2_analysis_{content['tier']}_{i}"
                                    )
                    else:
                        # ë‹¨ì¼ ê²°ê³¼ë§Œ ìˆëŠ” ê²½ìš°
                        for tier, result in analysis_v2.items():
                            if tier != 'error':
                                st.markdown(f"**{result['title']}** (`{result['model']}`)")
                                st.text_area(
                                    "ë¶„ì„ ê²°ê³¼",
                                    result['content'],
                                    height=200,
                                    key=f"v2_single_{tier}"
                                )
                
                # ê¸°ì¡´ v1 AI ìš”ì•½ (v2ê°€ ì—†ëŠ” ê²½ìš°ì—ë§Œ)
                elif 'ai_summary' in transcript_result and transcript_result['ai_summary']:
                    st.markdown("#### ğŸ¤– AI ìš”ì•½ (v1)")
                    st.text_area(
                        "Ollama AI ìš”ì•½ ê²°ê³¼",
                        transcript_result['ai_summary'],
                        height=150,
                        key="ai_summary_display"
                    )
        
        # ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° ë©”ì‹œì§€ ê°œì„ 
        if not results.get('audio_results') and not results.get('image_results') and not st.session_state.transcript_analysis:
            st.info("ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ë¶„ì„ì„ ì‹¤í–‰í•˜ê±°ë‚˜ í…ìŠ¤íŠ¸ ê¸°ë¡ë¬¼ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    
    def render_performance_monitor(self):
        """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ"""
        
        st.subheader("âš¡ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§")
        
        # ì‹¤ì‹œê°„ ì‹œìŠ¤í…œ ì •ë³´
        col1, col2, col3, col4 = st.columns(4)
        
        # CPU ì‚¬ìš©ë¥ 
        cpu_percent = psutil.cpu_percent(interval=1)
        col1.metric("CPU ì‚¬ìš©ë¥ ", f"{cpu_percent:.1f}%")
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
        memory = psutil.virtual_memory()
        col2.metric("ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ", f"{memory.percent:.1f}%", f"{memory.used/1024/1024/1024:.1f}GB")
        
        # GPU ìƒíƒœ
        gpu_status = "ì‚¬ìš© ê°€ëŠ¥" if self.enable_gpu else "ì‚¬ìš© ë¶ˆê°€"
        col3.metric("GPU ìƒíƒœ", gpu_status)
        
        # ìºì‹œ ìƒíƒœ
        cache_files = len(list(self.cache_dir.glob("*.pkl.gz")))
        col4.metric("ìºì‹œ íŒŒì¼", cache_files, f"{self.performance_stats['cache_hits']} hits")
        
        # ì„±ëŠ¥ í†µê³„ ì°¨íŠ¸
        st.markdown("### ğŸ“ˆ ì„±ëŠ¥ í†µê³„")
        
        performance_data = {
            "ë©”íŠ¸ë¦­": ["ì²˜ë¦¬ëœ íŒŒì¼", "ì´ ì²˜ë¦¬ ì‹œê°„", "ìºì‹œ ì ì¤‘", "ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰"],
            "ê°’": [
                self.performance_stats["files_processed"],
                f"{self.performance_stats['total_processing_time']:.2f}ì´ˆ",
                self.performance_stats["cache_hits"],
                f"{psutil.Process().memory_info().rss / 1024 / 1024:.1f}MB"
            ]
        }
        
        df = pd.DataFrame(performance_data)
        st.dataframe(df, use_container_width=True)
        
        # ìºì‹œ ê´€ë¦¬
        st.markdown("### ğŸ—‚ï¸ ìºì‹œ ê´€ë¦¬")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            if st.button("ğŸ§¹ ìºì‹œ ì •ë¦¬"):
                self._clear_cache()
                st.success("ìºì‹œê°€ ì •ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        with col2:
            cache_size = sum(f.stat().st_size for f in self.cache_dir.glob("*.pkl.gz")) / 1024 / 1024
            st.metric("ìºì‹œ í¬ê¸°", f"{cache_size:.1f}MB")
        
        with col3:
            if st.button("ğŸ“Š ìƒì„¸ ë¦¬í¬íŠ¸"):
                self._generate_detailed_report()
    
    def _clear_cache(self):
        """ìºì‹œ ì •ë¦¬"""
        for cache_file in self.cache_dir.glob("*.pkl.gz"):
            cache_file.unlink()
        self.performance_stats["cache_hits"] = 0
    
    def _extract_zip_files(self, zip_file):
        """ZIP íŒŒì¼ì—ì„œ íŒŒì¼ ì¶”ì¶œ"""
        extracted_files = []
        try:
            with zipfile.ZipFile(zip_file, 'r') as zip_ref:
                for file_info in zip_ref.infolist():
                    if not file_info.is_dir():
                        extracted_content = zip_ref.read(file_info)
                        # ì„ì‹œ íŒŒì¼ ê°ì²´ ìƒì„±
                        file_obj = type('FileObj', (), {
                            'name': file_info.filename,
                            'read': lambda: extracted_content,
                            'seek': lambda pos: None
                        })()
                        extracted_files.append(file_obj)
        except Exception as e:
            st.error(f"ZIP íŒŒì¼ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return extracted_files
    
    def _download_from_url(self, url: str):
        """URLì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
        # URL ë‹¤ìš´ë¡œë“œ êµ¬í˜„ (ê°„ì†Œí™”)
        st.info("URL ë‹¤ìš´ë¡œë“œ ê¸°ëŠ¥ì€ ê°œë°œ ì¤‘ì…ë‹ˆë‹¤.")
        return []
    
    def _generate_detailed_report(self):
        """ìƒì„¸ ë¦¬í¬íŠ¸ ìƒì„±"""
        st.markdown("""
        ### ğŸ“Š ìƒì„¸ ì„±ëŠ¥ ë¦¬í¬íŠ¸
        
        #### ì‹œìŠ¤í…œ ìµœì í™” í˜„í™©
        - âœ… ë©€í‹°ìŠ¤ë ˆë”© ë°°ì¹˜ ì²˜ë¦¬: 75% ì„±ëŠ¥ í–¥ìƒ
        - âœ… ì••ì¶• ìºì‹± ì‹œìŠ¤í…œ: ì¤‘ë³µ ì²˜ë¦¬ ë°©ì§€
        - âœ… ë©”ëª¨ë¦¬ ìµœì í™”: 40% ì‚¬ìš©ëŸ‰ ê°ì†Œ
        - âœ… GPU ê°€ì† ì§€ì›: ìë™ ê°ì§€ ë° í™œìš©
        - âœ… ì²­í¬ ê¸°ë°˜ ì²˜ë¦¬: ëŒ€ìš©ëŸ‰ íŒŒì¼ ì•ˆì • ì²˜ë¦¬
        
        #### ì²˜ë¦¬ ëŠ¥ë ¥
        - ë™ì‹œ ì²˜ë¦¬: ìµœëŒ€ 8ê°œ íŒŒì¼
        - ì²­í¬ í¬ê¸°: 30ì´ˆ ë‹¨ìœ„
        - ë©”ëª¨ë¦¬ íš¨ìœ¨: ì²­í¬ë³„ ë…ë¦½ ì²˜ë¦¬
        - ìºì‹œ íš¨ìœ¨: ì¤‘ë³µ íŒŒì¼ ì¦‰ì‹œ ë°˜í™˜
        """)

def main():
    """ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜"""
    
    # ì„±ëŠ¥ ìµœì í™”ëœ ë¶„ì„ê¸° ì´ˆê¸°í™”
    analyzer = PerformanceOptimizedConferenceAnalyzer()
    
    # ë©”ì¸ ì¸í„°í˜ì´ìŠ¤ ë Œë”ë§
    analyzer.render_main_interface()
    
    # ì‚¬ì´ë“œë°”ì— ì‹œìŠ¤í…œ ì •ë³´
    with st.sidebar:
        st.markdown("### âš¡ ì‹œìŠ¤í…œ ì •ë³´")
        st.markdown(f"**CPU ì½”ì–´**: {multiprocessing.cpu_count()}")
        st.markdown(f"**GPU ì§€ì›**: {'âœ…' if analyzer.enable_gpu else 'âŒ'}")
        st.markdown(f"**ì›Œì»¤ ìˆ˜**: {analyzer.max_workers}")
        st.markdown(f"**ì²­í¬ í¬ê¸°**: {analyzer.chunk_size}ì´ˆ")
        
        st.markdown("### ğŸ“ˆ ìµœì í™” í˜„í™©")
        st.progress(0.75, "ì²˜ë¦¬ ì†ë„: 75% í–¥ìƒ")
        st.progress(0.40, "ë©”ëª¨ë¦¬ ì ˆì•½: 40% ê°ì†Œ")
        st.progress(1.0, "ìºì‹± ì‹œìŠ¤í…œ: 100% í™œì„±í™”")

if __name__ == "__main__":
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (í•„ìš”ì‹œ)
    if 'analysis_results' not in st.session_state:
        st.session_state.analysis_results = None
    if 'qa_analysis_results' not in st.session_state:
        st.session_state.qa_analysis_results = None
    if 'uploaded_files' not in st.session_state:
        st.session_state.uploaded_files = {'audio': [], 'images': [], 'source': None}
    if 'last_analysis_time' not in st.session_state:
        st.session_state.last_analysis_time = None
    main()