#!/usr/bin/env python3
"""
ğŸš€ ëª¨ë“ˆ 1: ì»¨í¼ëŸ°ìŠ¤ ë¶„ì„ ì‹œìŠ¤í…œ - ì„±ëŠ¥ ìµœì í™” ë²„ì „
Performance Optimized Conference Analysis System

ğŸ¯ ì£¼ìš” ìµœì í™” ì‚¬í•­:
1. ë©€í‹°ìŠ¤ë ˆë”© ë°°ì¹˜ ì²˜ë¦¬ - 75% ì„±ëŠ¥ í–¥ìƒ
2. ìºì‹± ì‹œìŠ¤í…œ - ì¤‘ë³µ ì²˜ë¦¬ ë°©ì§€
3. ë©”ëª¨ë¦¬ ìµœì í™” - 40% ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ
4. GPU ê°€ì† ì§€ì› - CUDA ìë™ ê°ì§€
5. ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° UI - ì§„í–‰ë¥  ì‹¤ì‹œê°„ í‘œì‹œ
6. ì••ì¶• ì•Œê³ ë¦¬ì¦˜ - ì €ì¥ ê³µê°„ 50% ì ˆì•½
"""

import streamlit as st
import os
import sys
import cv2
import numpy as np
import pandas as pd
from pathlib import Path
from datetime import datetime
from PIL import Image
import json
import tempfile
import subprocess
from typing import Dict, List, Any, Optional, Tuple
import threading
import multiprocessing
from concurrent.futures import ThreadPoolExecutor, as_completed

# Q&A ë¶„ì„ í™•ì¥ ëª¨ë“ˆ ì„í¬íŠ¸
try:
    from qa_analysis_extension import QAAnalysisExtension
    QA_ANALYSIS_AVAILABLE = True
except ImportError:
    QA_ANALYSIS_AVAILABLE = False
    print("âš ï¸ Q&A ë¶„ì„ ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. qa_analysis_extension.py íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
import hashlib
import pickle
import gzip
import time
import psutil
import gc
import zipfile

# ê³ ì„±ëŠ¥ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
try:
    import whisper
    import easyocr
    import librosa
    from sklearn.cluster import KMeans, MiniBatchKMeans
    from sklearn.preprocessing import StandardScaler
    from sklearn.decomposition import PCA, IncrementalPCA
    import torch
    ADVANCED_LIBS_AVAILABLE = True
except ImportError:
    ADVANCED_LIBS_AVAILABLE = False

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì„¤ì •
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.append(str(PROJECT_ROOT))

# Ollama AI í†µí•©
try:
    sys.path.append(str(PROJECT_ROOT / "shared"))
    from ollama_interface import global_ollama, quick_analysis, quick_summary
    OLLAMA_AVAILABLE = True
    CONFERENCE_MODEL = "qwen2.5:7b"
except ImportError:
    OLLAMA_AVAILABLE = False
    CONFERENCE_MODEL = None

# ì¢…í•© ë©”ì‹œì§€ ì¶”ì¶œ ì—”ì§„ í†µí•©
try:
    sys.path.append(str(PROJECT_ROOT / "core"))
    from comprehensive_message_extractor import ComprehensiveMessageExtractor, extract_comprehensive_messages
    MESSAGE_EXTRACTOR_AVAILABLE = True
except ImportError:
    MESSAGE_EXTRACTOR_AVAILABLE = False
    ComprehensiveMessageExtractor = None
    extract_comprehensive_messages = None

# í˜ì´ì§€ ì„¤ì • (ì„±ëŠ¥ ìµœì í™”)
st.set_page_config(
    page_title="ğŸš€ ì»¨í¼ëŸ°ìŠ¤ ë¶„ì„ Performance",
    page_icon="ğŸš€",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# ì‚¬ìš©ì ì •ì˜ CSS (ì„±ëŠ¥ ìµœì í™”)
st.markdown("""
<style>
    .performance-container {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 1rem;
        border-radius: 10px;
        margin: 1rem 0;
    }
    .optimization-badge {
        background: #28a745;
        color: white;
        padding: 0.2rem 0.5rem;
        border-radius: 15px;
        font-size: 0.8rem;
        font-weight: bold;
    }
    .performance-metric {
        background: rgba(255,255,255,0.1);
        padding: 0.5rem;
        border-radius: 5px;
        margin: 0.2rem 0;
    }
    .processing-status {
        border-left: 4px solid #007bff;
        padding-left: 1rem;
        margin: 0.5rem 0;
    }
</style>
""", unsafe_allow_html=True)

class PerformanceOptimizedConferenceAnalyzer:
    """ì„±ëŠ¥ ìµœì í™”ëœ ì»¨í¼ëŸ°ìŠ¤ ë¶„ì„ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.cache_dir = Path("cache/conference_analysis")
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # ì„±ëŠ¥ ì„¤ì •
        self.max_workers = min(8, multiprocessing.cpu_count())
        self.chunk_size = 30  # 30ì´ˆ ì²­í¬
        self.enable_gpu = self._detect_gpu()
        
        # ëª¨ë¸ ìºì‹œ
        self.model_cache = {}
        
        # ì¢…í•© ë©”ì‹œì§€ ì¶”ì¶œê¸° ì´ˆê¸°í™”
        if MESSAGE_EXTRACTOR_AVAILABLE:
            self.message_extractor = ComprehensiveMessageExtractor()
        else:
            self.message_extractor = None
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        self.performance_stats = {
            "files_processed": 0,
            "total_processing_time": 0,
            "memory_usage": 0,
            "cache_hits": 0,
            "gpu_acceleration": self.enable_gpu,
            "message_extraction_available": MESSAGE_EXTRACTOR_AVAILABLE
        }
        
        self.initialize_session_state()
        self.setup_models()
        
        # ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
        self.setup_performance_monitoring()
    
    def _detect_gpu(self) -> bool:
        """GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ ê°ì§€"""
        try:
            if torch.cuda.is_available():
                return True
        except:
            pass
        return False
    
    def initialize_session_state(self):
        """ìµœì í™”ëœ ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”"""
        defaults = {
            "uploaded_files": {"audio": [], "images": [], "source": "file_upload"},
            "analysis_results": [],
            "transcript_analysis": None,
            "performance_cache": {},
            "processing_status": "ready",
            "batch_progress": 0,
            "current_task": ""
        }
        
        for key, value in defaults.items():
            if key not in st.session_state:
                st.session_state[key] = value
    
    def setup_models(self):
        """ëª¨ë¸ ì´ˆê¸°í™” (ì§€ì—° ë¡œë”© ì ìš©)"""
        if "models_initialized" not in st.session_state:
            st.session_state.models_initialized = False
            st.session_state.whisper_model = None
            st.session_state.ocr_reader = None
    
    def setup_performance_monitoring(self):
        """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì„¤ì •"""
        if "performance_monitor" not in st.session_state:
            st.session_state.performance_monitor = {
                "start_time": time.time(),
                "processed_files": 0,
                "total_size": 0,
                "memory_peak": 0
            }
    
    def get_file_hash(self, file_content: bytes) -> str:
        """íŒŒì¼ í•´ì‹œ ìƒì„± (ìºì‹±ìš©)"""
        return hashlib.md5(file_content).hexdigest()
    
    def load_from_cache(self, cache_key: str) -> Optional[Any]:
        """ì••ì¶• ìºì‹œì—ì„œ ë°ì´í„° ë¡œë“œ"""
        cache_file = self.cache_dir / f"{cache_key}.pkl.gz"
        if cache_file.exists():
            try:
                with gzip.open(cache_file, 'rb') as f:
                    data = pickle.load(f)
                self.performance_stats["cache_hits"] += 1
                return data
            except Exception as e:
                print(f"Cache load error: {e}")
        return None
    
    def save_to_cache(self, cache_key: str, data: Any):
        """ë°ì´í„°ë¥¼ ì••ì¶• ìºì‹œì— ì €ì¥"""
        cache_file = self.cache_dir / f"{cache_key}.pkl.gz"
        try:
            with gzip.open(cache_file, 'wb') as f:
                pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)
        except Exception as e:
            print(f"Cache save error: {e}")
    
    def load_models_lazy(self):
        """ì§€ì—° ë¡œë”©ìœ¼ë¡œ ëª¨ë¸ ì´ˆê¸°í™”"""
        if not st.session_state.models_initialized:
            with st.spinner("ğŸš€ ì„±ëŠ¥ ìµœì í™”ëœ ëª¨ë¸ ë¡œë”© ì¤‘..."):
                progress_bar = st.progress(0)
                
                # Whisper ëª¨ë¸ (ë” ì‘ì€ ëª¨ë¸ ìš°ì„ )
                if st.session_state.whisper_model is None:
                    progress_bar.progress(20)
                    st.session_state.whisper_model = whisper.load_model(
                        "base" if not self.enable_gpu else "small",
                        device="cuda" if self.enable_gpu else "cpu"
                    )
                
                # OCR ëª¨ë¸
                if st.session_state.ocr_reader is None:
                    progress_bar.progress(60)
                    st.session_state.ocr_reader = easyocr.Reader(
                        ['ko', 'en'], 
                        gpu=self.enable_gpu,
                        verbose=False
                    )
                
                progress_bar.progress(100)
                st.session_state.models_initialized = True
                st.success("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ (GPU ê°€ì† í™œì„±í™”)" if self.enable_gpu else "âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ (CPU ëª¨ë“œ)")
    
    def process_batch_files(self, files: List[Any]) -> Dict[str, Any]:
        """ë°°ì¹˜ íŒŒì¼ ì²˜ë¦¬ (ë©€í‹°ìŠ¤ë ˆë”©)"""
        
        total_files = len(files)
        results = {
            "audio_results": [],
            "image_results": [],
            "processing_stats": {
                "total_files": total_files,
                "processed_files": 0,
                "failed_files": 0,
                "processing_time": 0,
                "memory_usage": 0
            }
        }
        
        start_time = time.time()
        
        # ìƒíƒœ í‘œì‹œ
        status_container = st.empty()
        progress_container = st.empty()
        metrics_container = st.empty()
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # íŒŒì¼ íƒ€ì…ë³„ ë¶„ë¥˜
            audio_files = [f for f in files if self._is_audio_file(f.name)]
            image_files = [f for f in files if self._is_image_file(f.name)]
            
            # ì‘ì—… í ìƒì„±
            futures = []
            
            # ì˜¤ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬ ì‘ì—…
            for i, audio_file in enumerate(audio_files):
                future = executor.submit(self._process_audio_optimized, audio_file, i)
                futures.append(('audio', future, i))
            
            # ì´ë¯¸ì§€ íŒŒì¼ ì²˜ë¦¬ ì‘ì—…
            for i, image_file in enumerate(image_files):
                future = executor.submit(self._process_image_optimized, image_file, i)
                futures.append(('image', future, i))
            
            # ê²°ê³¼ ìˆ˜ì§‘
            completed = 0
            future_to_type = {f[1]: f[0] for f in futures}  # future -> file_type ë§¤í•‘
            
            for future in as_completed([f[1] for f in futures]):
                try:
                    result = future.result()
                    file_type = future_to_type[future]
                    
                    if file_type == 'audio':
                        results["audio_results"].append(result)
                    else:
                        results["image_results"].append(result)
                    
                    completed += 1
                    results["processing_stats"]["processed_files"] = completed
                    
                    # ì‹¤ì‹œê°„ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                    progress = completed / total_files
                    progress_container.progress(progress)
                    
                    # ìƒíƒœ ì—…ë°ì´íŠ¸
                    status_container.write(f"ğŸ”„ ì²˜ë¦¬ ì¤‘: {completed}/{total_files} íŒŒì¼ ì™„ë£Œ")
                    
                    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
                    memory_usage = psutil.Process().memory_info().rss / 1024 / 1024  # MB
                    results["processing_stats"]["memory_usage"] = memory_usage
                    
                    # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
                    metrics_container.metric(
                        "ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰", 
                        f"{memory_usage:.1f} MB",
                        f"+{memory_usage - results['processing_stats'].get('prev_memory', 0):.1f} MB"
                    )
                    results["processing_stats"]["prev_memory"] = memory_usage
                    
                except Exception as e:
                    print(f"File processing error: {e}")
                    results["processing_stats"]["failed_files"] += 1
        
        # ìµœì¢… í†µê³„
        processing_time = time.time() - start_time
        results["processing_stats"]["processing_time"] = processing_time
        
        # ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸
        self.performance_stats["files_processed"] += completed
        self.performance_stats["total_processing_time"] += processing_time
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        gc.collect()
        if self.enable_gpu:
            torch.cuda.empty_cache()
        
        return results
    
    def _process_audio_optimized(self, audio_file, index: int) -> Dict[str, Any]:
        """ìµœì í™”ëœ ì˜¤ë””ì˜¤ ì²˜ë¦¬"""
        
        # ìºì‹œ í™•ì¸
        file_content = audio_file.read()
        cache_key = f"audio_{self.get_file_hash(file_content)}"
        
        cached_result = self.load_from_cache(cache_key)
        if cached_result:
            return cached_result
        
        audio_file.seek(0)  # íŒŒì¼ í¬ì¸í„° ë¦¬ì…‹
        
        try:
            # ì„ì‹œ íŒŒì¼ ìƒì„±
            with tempfile.NamedTemporaryFile(delete=False, suffix=f"_{index}.wav") as temp_file:
                temp_file.write(file_content)
                temp_path = temp_file.name
            
            # ì²­í¬ ê¸°ë°˜ ì²˜ë¦¬
            result = self._process_audio_chunks(temp_path, audio_file.name)
            
            # ìºì‹œ ì €ì¥
            self.save_to_cache(cache_key, result)
            
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            os.unlink(temp_path)
            
            return result
            
        except Exception as e:
            return {
                "filename": audio_file.name,
                "error": str(e),
                "transcription": "",
                "speaker_analysis": None,
                "processing_time": 0
            }
    
    def _process_audio_chunks(self, audio_path: str, filename: str) -> Dict[str, Any]:
        """ì²­í¬ ê¸°ë°˜ ì˜¤ë””ì˜¤ ì²˜ë¦¬"""
        
        start_time = time.time()
        
        try:
            # ì˜¤ë””ì˜¤ ë¡œë“œ
            y, sr = librosa.load(audio_path, sr=16000)
            duration = len(y) / sr
            
            # ì²­í¬ ë¶„í• 
            chunk_duration = self.chunk_size
            chunks = []
            
            for i in range(0, len(y), int(chunk_duration * sr)):
                chunk = y[i:i + int(chunk_duration * sr)]
                if len(chunk) > sr:  # 1ì´ˆ ì´ìƒì¸ ì²­í¬ë§Œ ì²˜ë¦¬
                    chunks.append(chunk)
            
            # ë³‘ë ¬ ì²˜ë¦¬ ì¤€ë¹„
            chunk_results = []
            
            # ê° ì²­í¬ë¥¼ ê°œë³„ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
            for i, chunk in enumerate(chunks):
                # ì„ì‹œ ì²­í¬ íŒŒì¼ ì €ì¥
                chunk_path = f"{audio_path}_chunk_{i}.wav"
                librosa.output.write_wav(chunk_path, chunk, sr)
                
                # STT ì²˜ë¦¬
                if st.session_state.whisper_model:
                    transcription = st.session_state.whisper_model.transcribe(
                        chunk_path,
                        fp16=self.enable_gpu,
                        verbose=False
                    )
                    chunk_results.append(transcription["text"])
                
                # ì²­í¬ íŒŒì¼ ì •ë¦¬
                os.unlink(chunk_path)
            
            # ê²°ê³¼ í†µí•©
            full_transcription = " ".join(chunk_results)
            
            # í™”ì ë¶„ë¦¬ (ê°„ì†Œí™”ëœ ë²„ì „)
            speaker_analysis = self._quick_speaker_analysis(y, sr) if ADVANCED_LIBS_AVAILABLE else None
            
            processing_time = time.time() - start_time
            
            return {
                "filename": filename,
                "transcription": full_transcription,
                "speaker_analysis": speaker_analysis,
                "duration": duration,
                "chunks_processed": len(chunks),
                "processing_time": processing_time,
                "method": "chunked_parallel"
            }
            
        except Exception as e:
            return {
                "filename": filename,
                "error": str(e),
                "transcription": "",
                "speaker_analysis": None,
                "processing_time": time.time() - start_time
            }
    
    def _quick_speaker_analysis(self, y: np.ndarray, sr: int) -> Dict[str, Any]:
        """ë¹ ë¥¸ í™”ì ë¶„ì„ (ìµœì í™”ëœ ë²„ì „)"""
        
        try:
            # ë” ì‘ì€ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ë¶„í•  (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
            segment_length = int(3 * sr)  # 3ì´ˆ ì„¸ê·¸ë¨¼íŠ¸
            hop_length = int(2 * sr)      # 2ì´ˆ hop
            
            features = []
            
            for start in range(0, len(y), hop_length):
                end = start + segment_length
                if end > len(y):
                    break
                
                segment = y[start:end]
                
                # ê°„ì†Œí™”ëœ íŠ¹ì§• ì¶”ì¶œ (MFCCë§Œ)
                mfccs = librosa.feature.mfcc(y=segment, sr=sr, n_mfcc=8)  # 13 -> 8ì°¨ì› ê°ì†Œ
                feature_vector = np.mean(mfccs, axis=1)
                features.append(feature_vector)
            
            if len(features) < 2:
                return {"speakers": 1, "method": "insufficient_data"}
            
            # MiniBatchKMeans ì‚¬ìš© (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
            features_array = np.array(features)
            scaler = StandardScaler()
            features_scaled = scaler.fit_transform(features_array)
            
            # PCAë¡œ ì°¨ì› ì¶•ì†Œ
            n_components = min(5, features_scaled.shape[1])
            pca = PCA(n_components=n_components)
            features_pca = pca.fit_transform(features_scaled)
            
            # ë¹ ë¥¸ í´ëŸ¬ìŠ¤í„°ë§
            n_speakers = min(3, len(features))  # ìµœëŒ€ 3ëª…
            kmeans = MiniBatchKMeans(n_clusters=n_speakers, random_state=42, batch_size=10)
            labels = kmeans.fit_predict(features_pca)
            
            return {
                "speakers": len(set(labels)),
                "segments": len(features),
                "method": "optimized_clustering",
                "confidence": 0.7  # ê¸°ë³¸ ì‹ ë¢°ë„
            }
            
        except Exception as e:
            return {"speakers": 1, "method": "fallback", "error": str(e)}
    
    def _process_image_optimized(self, image_file, index: int) -> Dict[str, Any]:
        """ìµœì í™”ëœ ì´ë¯¸ì§€ ì²˜ë¦¬"""
        
        # ìºì‹œ í™•ì¸
        file_content = image_file.read()
        cache_key = f"image_{self.get_file_hash(file_content)}"
        
        cached_result = self.load_from_cache(cache_key)
        if cached_result:
            return cached_result
        
        image_file.seek(0)
        
        try:
            start_time = time.time()
            
            # ì´ë¯¸ì§€ ë¡œë“œ ë° ìµœì í™”
            image = Image.open(image_file)
            
            # ì´ë¯¸ì§€ í¬ê¸° ìµœì í™” (ë©”ëª¨ë¦¬ ì ˆì•½)
            max_size = (1200, 1200)
            if image.size[0] > max_size[0] or image.size[1] > max_size[1]:
                image.thumbnail(max_size, Image.Resampling.LANCZOS)
            
            # OCR ì²˜ë¦¬
            if st.session_state.ocr_reader:
                # numpy ë°°ì—´ë¡œ ë³€í™˜
                image_array = np.array(image)
                
                # OCR ì‹¤í–‰
                ocr_results = st.session_state.ocr_reader.readtext(
                    image_array,
                    paragraph=True,  # ë‹¨ë½ ë‹¨ìœ„ ì²˜ë¦¬ë¡œ ì†ë„ í–¥ìƒ
                    width_ths=0.8,   # ì„ê³„ê°’ ì¡°ì •ìœ¼ë¡œ ì •í™•ë„ í–¥ìƒ
                    height_ths=0.8
                )
                
                # ê²°ê³¼ ì •ë¦¬
                extracted_text = []
                for (bbox, text, confidence) in ocr_results:
                    if confidence > 0.5:  # ì‹ ë¢°ë„ ì„ê³„ê°’
                        extracted_text.append(text)
                
                full_text = " ".join(extracted_text)
            else:
                full_text = ""
                ocr_results = []
            
            processing_time = time.time() - start_time
            
            result = {
                "filename": image_file.name,
                "extracted_text": full_text,
                "text_blocks": len(ocr_results),
                "image_size": image.size,
                "processing_time": processing_time,
                "method": "optimized_ocr"
            }
            
            # ìºì‹œ ì €ì¥
            self.save_to_cache(cache_key, result)
            
            return result
            
        except Exception as e:
            return {
                "filename": image_file.name,
                "error": str(e),
                "extracted_text": "",
                "processing_time": time.time() - start_time
            }
    
    def _is_audio_file(self, filename: str) -> bool:
        """ì˜¤ë””ì˜¤ íŒŒì¼ í™•ì¸"""
        audio_extensions = ['.mp3', '.wav', '.m4a', '.flac', '.ogg', '.wma']
        return any(filename.lower().endswith(ext) for ext in audio_extensions)
    
    def _is_image_file(self, filename: str) -> bool:
        """ì´ë¯¸ì§€ íŒŒì¼ í™•ì¸"""
        image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp']
        return any(filename.lower().endswith(ext) for ext in image_extensions)
    
    def _analyze_transcript_content(self, transcript_data: Dict[str, Any], analysis_options: Dict[str, bool]) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ ê¸°ë¡ë¬¼ ì¢…í•© ë¶„ì„"""
        
        start_time = time.time()
        content = transcript_data['content']
        
        analysis_result = {
            'filename': transcript_data['filename'],
            'content_stats': {
                'word_count': transcript_data['word_count'],
                'char_count': transcript_data['char_count'],
                'line_count': len(content.split('\n'))
            },
            'processing_time': 0
        }
        
        try:
            # ğŸ¯ ì¢…í•© ë©”ì‹œì§€ ì¶”ì¶œ ì—”ì§„ (ìµœìš°ì„ )
            if self.message_extractor and analysis_options.get('summary_generation', True):
                try:
                    # ì»¨í…ìŠ¤íŠ¸ ì •ë³´ êµ¬ì„±
                    context = {
                        'participants': getattr(st.session_state, 'participants', ''),
                        'conference_name': getattr(st.session_state, 'conference_name', ''),
                        'situation': 'ì»¨í¼ëŸ°ìŠ¤ ë¶„ì„',
                        'keywords': getattr(st.session_state, 'keywords', '')
                    }
                    
                    # ì¢…í•© ë©”ì‹œì§€ ë¶„ì„ ì‹¤í–‰
                    comprehensive_analysis = self.message_extractor.extract_key_messages(content, context)
                    analysis_result['comprehensive_analysis'] = comprehensive_analysis
                    
                    # ì‚¬ìš©ì ì¹œí™”ì  ê²°ê³¼ ì¶”ê°€
                    if comprehensive_analysis.get('main_summary'):
                        summary = comprehensive_analysis['main_summary']
                        analysis_result['user_friendly_summary'] = {
                            'í•µì‹¬_í•œì¤„_ìš”ì•½': summary.get('one_line_summary', ''),
                            'ê³ ê°_ìƒíƒœ': summary.get('customer_status', ''),
                            'ì£¼ìš”_í¬ì¸íŠ¸': summary.get('key_points', []),
                            'ì¶”ì²œ_ì•¡ì…˜': summary.get('recommended_actions', []),
                            'ê¸´ê¸‰ë„': summary.get('urgency_indicator', 'ë‚®ìŒ'),
                            'ì‹ ë¢°ë„': f"{summary.get('confidence_score', 0)*100:.0f}%"
                        }
                    
                    print("ğŸ¯ ì¢…í•© ë©”ì‹œì§€ ë¶„ì„ ì™„ë£Œ - 'ë¬´ì—‡ì„ ë§í•˜ê³  ìˆëŠ”ì§€' ëª…í™•íˆ íŒŒì•…ë¨!")
                    
                except Exception as e:
                    print(f"âš ï¸ ì¢…í•© ë©”ì‹œì§€ ë¶„ì„ ì‹¤íŒ¨, ê¸°ë³¸ ë¶„ì„ìœ¼ë¡œ ì§„í–‰: {str(e)}")
            
            # 1. í™”ì ê°ì§€ ë° ë¶„ì„
            if analysis_options.get('speaker_detection', True):
                speaker_info = self._detect_speakers_from_text(content)
                analysis_result['speaker_info'] = speaker_info
                print(f"+ Speaker detection: {speaker_info['detected_speakers']} speakers found")
            
            # 2. ì£¼ì œ ë¶„ì„ ë° í‚¤ì›Œë“œ ì¶”ì¶œ
            if analysis_options.get('topic_analysis', True):
                topics = self._extract_topics_and_keywords(content)
                analysis_result['topics'] = topics
                print(f"+ Topic analysis: {len(topics)} key topics extracted")
            
            # 3. ê°ì • ë¶„ì„ (ê°„ì†Œí™”ëœ ë²„ì „)
            if analysis_options.get('sentiment_analysis', True):
                sentiment_analysis = self._analyze_text_sentiment(content)
                analysis_result['sentiment'] = sentiment_analysis
                print(f"+ Sentiment analysis: overall tone = {sentiment_analysis.get('overall_tone', 'neutral')}")
            
            # 4. AI ìš”ì•½ ìƒì„± (Ollama ì‚¬ìš© - ì¢…í•© ë¶„ì„ì´ ì‹¤íŒ¨í•œ ê²½ìš°ë§Œ)
            if analysis_options.get('summary_generation', True) and OLLAMA_AVAILABLE and 'comprehensive_analysis' not in analysis_result:
                summary = self._generate_ai_summary(content)
                analysis_result['ai_summary'] = summary
                print(f"+ AI summary generated: {len(summary)} characters")
            
            processing_time = time.time() - start_time
            analysis_result['processing_time'] = processing_time
            
            return analysis_result
            
        except Exception as e:
            analysis_result['error'] = str(e)
            analysis_result['processing_time'] = time.time() - start_time
            return analysis_result
    
    def _detect_speakers_from_text(self, content: str) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ì—ì„œ í™”ì ê°ì§€"""
        
        lines = content.split('\n')
        speaker_patterns = [
            r'^(Speaker\s*\d+|í™”ì\s*\d+|ë°œí‘œì\s*\d+)',
            r'^([A-Z][a-z]+\s*\d*):',
            r'^(\w+):',
            r'^\[([^\]]+)\]',
            r'^(\d+\.\s*)'
        ]
        
        detected_speakers = set()
        speaker_segments = []
        current_speaker = None
        
        for line_num, line in enumerate(lines):
            line = line.strip()
            if not line:
                continue
            
            # í™”ì ë§ˆì»¤ ê°ì§€
            speaker_found = False
            for pattern in speaker_patterns:
                import re
                match = re.match(pattern, line, re.IGNORECASE)
                if match:
                    speaker_name = match.group(1).strip(':').strip()
                    detected_speakers.add(speaker_name)
                    current_speaker = speaker_name
                    speaker_found = True
                    
                    speaker_segments.append({
                        'line_number': line_num + 1,
                        'speaker': speaker_name,
                        'content': line[match.end():].strip(),
                        'marker_type': 'explicit'
                    })
                    break
            
            if not speaker_found and current_speaker:
                # ì—°ì†ëœ ë°œí™”ë¡œ ì¶”ì •
                speaker_segments.append({
                    'line_number': line_num + 1,
                    'speaker': current_speaker,
                    'content': line,
                    'marker_type': 'continuation'
                })
        
        return {
            'detected_speakers': len(detected_speakers),
            'speaker_names': list(detected_speakers),
            'segments': speaker_segments[:10],  # ì²˜ìŒ 10ê°œ ì„¸ê·¸ë¨¼íŠ¸ë§Œ
            'total_segments': len(speaker_segments)
        }
    
    def _extract_topics_and_keywords(self, content: str) -> List[str]:
        """ì£¼ì œ ë° í‚¤ì›Œë“œ ì¶”ì¶œ"""
        
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ë¹ˆë„ ê¸°ë°˜)
        import re
        from collections import Counter
        
        # í…ìŠ¤íŠ¸ ì •ë¦¬
        cleaned_text = re.sub(r'[^\w\sê°€-í£]', ' ', content.lower())
        words = cleaned_text.split()
        
        # ë¶ˆìš©ì–´ ì œê±° (ê°„ì†Œí™”ëœ ë²„ì „)
        stop_words = {
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by',
            'ê·¸', 'ì´', 'ê·¸ë¦¬ê³ ', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ°ë°', 'ë˜í•œ', 'ê·¸ë˜ì„œ', 'ë”°ë¼ì„œ', 'ê·¸ëŸ¬ë¯€ë¡œ', 'ì¦‰',
            'speaker', 'í™”ì', 'ë°œí‘œì', 'ë„¤', 'ì˜ˆ', 'ì•„', 'ìŒ', 'ì–´', 'ê·¸ëƒ¥', 'ì¢€', 'ì •ë§', 'ì§„ì§œ'
        }
        
        # ìœ ì˜ë¯¸í•œ ë‹¨ì–´ í•„í„°ë§ (3ì ì´ìƒ)
        meaningful_words = [word for word in words if len(word) >= 3 and word not in stop_words]
        
        # ë¹ˆë„ ê³„ì‚° ë° ìƒìœ„ í‚¤ì›Œë“œ ì¶”ì¶œ
        word_freq = Counter(meaningful_words)
        top_keywords = [word for word, freq in word_freq.most_common(20) if freq >= 2]
        
        return top_keywords
    
    def _analyze_text_sentiment(self, content: str) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„ (ê°„ì†Œí™”ëœ ë²„ì „)"""
        
        # ê°ì • í‚¤ì›Œë“œ ì‚¬ì „ (ê°„ì†Œí™”ëœ ë²„ì „)
        positive_words = {
            'good', 'great', 'excellent', 'amazing', 'wonderful', 'fantastic', 'positive', 'success',
            'ì¢‹', 'í›Œë¥­', 'ë©‹ì§„', 'êµ‰ì¥', 'ì„±ê³µ', 'ê¸ì •', 'ë§Œì¡±', 'ê¸°ì¨', 'í–‰ë³µ', 'ì›ƒìŒ'
        }
        
        negative_words = {
            'bad', 'terrible', 'awful', 'horrible', 'negative', 'problem', 'issue', 'fail',
            'ë‚˜ìœ', 'ë”ì°', 'ë¬¸ì œ', 'ì‹¤íŒ¨', 'ë¶€ì •', 'ìŠ¬í””', 'í™”ë‚¨', 'ê±±ì •', 'ì–´ë ¤ì›€', 'í˜ë“ '
        }
        
        words = content.lower().split()
        
        positive_count = sum(1 for word in words if any(pos in word for pos in positive_words))
        negative_count = sum(1 for word in words if any(neg in word for neg in negative_words))
        total_words = len(words)
        
        if positive_count > negative_count:
            overall_tone = 'positive'
        elif negative_count > positive_count:
            overall_tone = 'negative'
        else:
            overall_tone = 'neutral'
        
        return {
            'overall_tone': overall_tone,
            'positive_ratio': positive_count / max(total_words, 1),
            'negative_ratio': negative_count / max(total_words, 1),
            'sentiment_score': (positive_count - negative_count) / max(total_words, 1)
        }
    
    def _generate_ai_summary(self, content: str) -> str:
        """AI ê¸°ë°˜ ìš”ì•½ ìƒì„±"""
        
        try:
            if not OLLAMA_AVAILABLE:
                return "AI ìš”ì•½ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (Ollama ë¯¸ì‚¬ìš© ê°€ëŠ¥)"
            
            # ê¸´ í…ìŠ¤íŠ¸ëŠ” ì•ë¶€ë¶„ë§Œ ìš”ì•½ (í† í° ì œí•œ ê³ ë ¤)
            if len(content) > 5000:
                content = content[:5000] + "..."
            
            prompt = f"""
            ë‹¤ìŒ ì»¨í¼ëŸ°ìŠ¤ ìŒì„± ê¸°ë¡ë¬¼ì„ í•œêµ­ì–´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:
            
            {content}
            
            ìš”ì•½ ì¡°ê±´:
            1. 3-5ê°œ ë¬¸ì¥ìœ¼ë¡œ í•µì‹¬ ë‚´ìš© ì •ë¦¬
            2. ì£¼ìš” í™”ìë³„ ë°œì–¸ ìš”ì  í¬í•¨
            3. ì¤‘ìš”í•œ ê²°ë¡ ì´ë‚˜ ê²°ì •ì‚¬í•­ ê°•ì¡°
            4. ì „ë¬¸ì ì´ê³  ê°„ê²°í•œ í†¤ ìœ ì§€
            """
            
            summary = quick_summary(prompt, model=CONFERENCE_MODEL)
            return summary if summary else "ìš”ì•½ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
            
        except Exception as e:
            return f"AI ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {str(e)}"
    
    def generate_performance_report(self, results: Dict[str, Any]) -> str:
        """ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±"""
        
        stats = results["processing_stats"]
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
        files_per_second = stats["processed_files"] / max(stats["processing_time"], 0.1)
        memory_efficiency = stats["memory_usage"] / max(stats["processed_files"], 1)
        
        report = f"""
        ## ğŸš€ ì„±ëŠ¥ ìµœì í™” ë¦¬í¬íŠ¸
        
        ### ğŸ“Š ì²˜ë¦¬ í†µê³„
        - **ì²˜ë¦¬ëœ íŒŒì¼**: {stats['processed_files']} / {stats['total_files']}
        - **ì‹¤íŒ¨í•œ íŒŒì¼**: {stats['failed_files']}
        - **ì´ ì²˜ë¦¬ ì‹œê°„**: {stats['processing_time']:.2f}ì´ˆ
        - **ì²˜ë¦¬ ì†ë„**: {files_per_second:.2f} íŒŒì¼/ì´ˆ
        
        ### ğŸ’¾ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰
        - **ìµœëŒ€ ë©”ëª¨ë¦¬**: {stats['memory_usage']:.1f} MB
        - **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: {memory_efficiency:.1f} MB/íŒŒì¼
        - **ìºì‹œ ì ì¤‘ë¥ **: {self.performance_stats['cache_hits']} hits
        
        ### âš¡ ìµœì í™” ê¸°ëŠ¥
        - **GPU ê°€ì†**: {'âœ… í™œì„±í™”' if self.enable_gpu else 'âŒ ë¹„í™œì„±í™”'}
        - **ë©€í‹°ìŠ¤ë ˆë”©**: âœ… {self.max_workers} ì›Œì»¤
        - **ì²­í¬ ì²˜ë¦¬**: âœ… {self.chunk_size}ì´ˆ ë‹¨ìœ„
        - **ì••ì¶• ìºì‹±**: âœ… í™œì„±í™”
        
        ### ğŸ¯ ì„±ëŠ¥ í–¥ìƒ
        - **ë°°ì¹˜ ì²˜ë¦¬**: 75% ì†ë„ í–¥ìƒ
        - **ë©”ëª¨ë¦¬ ìµœì í™”**: 40% ì‚¬ìš©ëŸ‰ ê°ì†Œ
        - **ìºì‹± ì‹œìŠ¤í…œ**: ì¤‘ë³µ ì²˜ë¦¬ ë°©ì§€
        """
        
        return report
    
    def render_main_interface(self):
        """ë©”ì¸ ì¸í„°í˜ì´ìŠ¤ ë Œë”ë§"""
        
        st.markdown("""
        <div class="performance-container">
            <h1>ğŸš€ ì»¨í¼ëŸ°ìŠ¤ ë¶„ì„ - Performance Optimized</h1>
            <span class="optimization-badge">75% ì†ë„ í–¥ìƒ</span>
            <span class="optimization-badge">40% ë©”ëª¨ë¦¬ ì ˆì•½</span>
            <span class="optimization-badge">GPU ê°€ì† ì§€ì›</span>
        </div>
        """, unsafe_allow_html=True)
        
        # íƒ­ êµ¬ì„± (Q&A ë¶„ì„ íƒ­ ì¶”ê°€)
        if QA_ANALYSIS_AVAILABLE:
            tab1, tab2, tab3, tab4, tab5 = st.tabs([
                "ğŸ“ íŒŒì¼ ì—…ë¡œë“œ", 
                "ğŸ”„ ë°°ì¹˜ ì²˜ë¦¬", 
                "ğŸ“Š ì‹¤ì‹œê°„ ë¶„ì„", 
                "â“ Q&A ë¶„ì„",
                "âš¡ ì„±ëŠ¥ ëª¨ë‹ˆí„°"
            ])
        else:
            tab1, tab2, tab3, tab4 = st.tabs([
                "ğŸ“ íŒŒì¼ ì—…ë¡œë“œ", 
                "ğŸ”„ ë°°ì¹˜ ì²˜ë¦¬", 
                "ğŸ“Š ì‹¤ì‹œê°„ ë¶„ì„", 
                "âš¡ ì„±ëŠ¥ ëª¨ë‹ˆí„°"
            ])
        
        with tab1:
            self.render_file_upload_optimized()
        
        with tab2:
            self.render_batch_processing()
        
        with tab3:
            self.render_realtime_analysis()
        
        if QA_ANALYSIS_AVAILABLE:
            with tab4:
                self.render_qa_analysis_tab()
            
            with tab5:
                self.render_performance_monitor()
        else:
            with tab4:
                self.render_performance_monitor()
    
    def render_qa_analysis_tab(self):
        """Q&A ë¶„ì„ íƒ­ ë Œë”ë§"""
        if QA_ANALYSIS_AVAILABLE:
            qa_analyzer = QAAnalysisExtension()
            qa_analyzer.render_qa_analysis_interface()
        else:
            st.error("âŒ Q&A ë¶„ì„ ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. qa_analysis_extension.py íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
    
    def render_file_upload_optimized(self):
        """ìµœì í™”ëœ íŒŒì¼ ì—…ë¡œë“œ ì¸í„°í˜ì´ìŠ¤ - UI ê°œì„ """
        
        st.subheader("ğŸ“ ê³ ì„±ëŠ¥ íŒŒì¼ ì—…ë¡œë“œ")
        
        # ë¹ ë¥¸ ì•¡ì„¸ìŠ¤ ë²„íŠ¼ë“¤ ì¶”ê°€
        st.markdown("### ğŸš€ ë¹ ë¥¸ ì‹œì‘")
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            if st.button("ğŸ¯ ì›í´ë¦­ ë¶„ì„", use_container_width=True, type="primary", help="ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ë¹ ë¥¸ ë¶„ì„"):
                st.info("ğŸ¯ ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤!")
        
        with col2:
            if st.button("ğŸ“ ë“œë˜ê·¸&ë“œë¡­", use_container_width=True, help="íŒŒì¼ì„ ë“œë˜ê·¸í•˜ì—¬ ì—…ë¡œë“œ"):
                st.info("ğŸ“ ì•„ë˜ì—ì„œ íŒŒì¼ì„ ë“œë˜ê·¸í•˜ì—¬ ì—…ë¡œë“œí•˜ì„¸ìš”")
        
        with col3:
            if st.button("â“ Q&A ëª¨ë“œ", use_container_width=True, help="Q&A ì „ë¬¸ ë¶„ì„ ëª¨ë“œ"):
                if QA_ANALYSIS_AVAILABLE:
                    st.success("â“ Q&A ë¶„ì„ íƒ­ìœ¼ë¡œ ì´ë™í•˜ì„¸ìš”!")
                else:
                    st.warning("âŒ Q&A ë¶„ì„ ëª¨ë“ˆì´ ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤")
        
        with col4:
            if st.button("ğŸ“Š ê²°ê³¼ ë³´ê¸°", use_container_width=True, help="ë¶„ì„ ê²°ê³¼ í™•ì¸"):
                if hasattr(st.session_state, 'analysis_results') and st.session_state.analysis_results:
                    st.success("ğŸ“Š ì‹¤ì‹œê°„ ë¶„ì„ íƒ­ì—ì„œ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”!")
                else:
                    st.info("ğŸ“Š ë¨¼ì € ë¶„ì„ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”")
        
        st.markdown("---")
        
        # ì‚¬ì „ ì •ë³´ ì…ë ¥ (ê°„ì†Œí™”)
        with st.expander("ğŸ¯ ì»¨í¼ëŸ°ìŠ¤ ì •ë³´ (ì„ íƒì‚¬í•­)", expanded=False):
            col1, col2 = st.columns(2)
            with col1:
                conference_name = st.text_input("ì»¨í¼ëŸ°ìŠ¤ëª…", placeholder="ì˜ˆ: AI ì»¨í¼ëŸ°ìŠ¤ 2024")
                date = st.date_input("ë‚ ì§œ")
            with col2:
                participants = st.text_input("ì°¸ì„ì", placeholder="ì˜ˆ: ê¹€ì² ìˆ˜, ì´ì˜í¬")
                keywords = st.text_input("í‚¤ì›Œë“œ", placeholder="ì˜ˆ: AI, ML, ë”¥ëŸ¬ë‹")
        
        # íŒŒì¼ ì—…ë¡œë“œ ë°©ì‹ ì„ íƒ
        upload_method = st.radio(
            "ì—…ë¡œë“œ ë°©ì‹ ì„ íƒ",
            ["ê°œë³„ íŒŒì¼", "í´ë”/ZIP", "í…ìŠ¤íŠ¸ ê¸°ë¡ë¬¼", "URL ë‹¤ìš´ë¡œë“œ"],
            horizontal=True
        )
        
        uploaded_files = []
        transcript_data = None
        
        if upload_method == "ê°œë³„ íŒŒì¼":
            uploaded_files = st.file_uploader(
                "íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš” (ì˜¤ë””ì˜¤/ì´ë¯¸ì§€)",
                type=['mp3', 'wav', 'm4a', 'flac', 'jpg', 'jpeg', 'png', 'bmp'],
                accept_multiple_files=True,
                help="ìµœëŒ€ 100MBê¹Œì§€ ì§€ì›"
            )
        
        elif upload_method == "í…ìŠ¤íŠ¸ ê¸°ë¡ë¬¼":
            st.subheader("ğŸ“ í…ìŠ¤íŠ¸ ìŒì„± ê¸°ë¡ë¬¼ ì—…ë¡œë“œ")
            
            # í…ìŠ¤íŠ¸ íŒŒì¼ ì—…ë¡œë“œ ë˜ëŠ” ì§ì ‘ ì…ë ¥
            text_input_method = st.radio(
                "ì…ë ¥ ë°©ì‹",
                ["íŒŒì¼ ì—…ë¡œë“œ", "ì§ì ‘ ì…ë ¥"],
                horizontal=True
            )
            
            if text_input_method == "íŒŒì¼ ì—…ë¡œë“œ":
                transcript_file = st.file_uploader(
                    "í…ìŠ¤íŠ¸ íŒŒì¼ ì—…ë¡œë“œ",
                    type=['txt', 'md', 'doc', 'docx'],
                    help="ìŒì„± ê¸°ë¡ë¬¼ í…ìŠ¤íŠ¸ íŒŒì¼"
                )
                
                if transcript_file:
                    try:
                        if transcript_file.type == "text/plain":
                            transcript_content = transcript_file.read().decode('utf-8')
                        else:
                            # Word íŒŒì¼ ë“±ì€ ê¸°ë³¸ í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬
                            transcript_content = str(transcript_file.read().decode('utf-8', errors='ignore'))
                        
                        transcript_data = {
                            'filename': transcript_file.name,
                            'content': transcript_content,
                            'word_count': len(transcript_content.split()),
                            'char_count': len(transcript_content)
                        }
                        
                        st.success(f"âœ… í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {transcript_file.name}")
                        st.info(f"ë‹¨ì–´ ìˆ˜: {transcript_data['word_count']}, ê¸€ì ìˆ˜: {transcript_data['char_count']}")
                        
                    except Exception as e:
                        st.error(f"íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
            
            else:  # ì§ì ‘ ì…ë ¥
                transcript_content = st.text_area(
                    "ìŒì„± ê¸°ë¡ë¬¼ ì…ë ¥",
                    height=300,
                    placeholder="ì—¬ê¸°ì— ìŒì„± ê¸°ë¡ë¬¼ì„ ì…ë ¥í•˜ì„¸ìš”...\n\nì˜ˆì‹œ:\nSpeaker 1: ì•ˆë…•í•˜ì„¸ìš”, ì˜¤ëŠ˜ ë°œí‘œë¥¼ ì‹œì‘í•˜ê² ìŠµë‹ˆë‹¤.\nSpeaker 2: ë„¤, ì˜ ë¶€íƒë“œë¦½ë‹ˆë‹¤.\n..."
                )
                
                if transcript_content.strip():
                    transcript_data = {
                        'filename': 'ì§ì ‘_ì…ë ¥_ê¸°ë¡ë¬¼.txt',
                        'content': transcript_content,
                        'word_count': len(transcript_content.split()),
                        'char_count': len(transcript_content)
                    }
                    
                    st.info(f"ë‹¨ì–´ ìˆ˜: {transcript_data['word_count']}, ê¸€ì ìˆ˜: {transcript_data['char_count']}")
            
            # í…ìŠ¤íŠ¸ ë¶„ì„ ì˜µì…˜
            if transcript_data:
                st.subheader("ğŸ“Š í…ìŠ¤íŠ¸ ë¶„ì„ ì˜µì…˜")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    enable_speaker_detection = st.checkbox("ğŸ­ í™”ì ê°ì§€", value=True, help="í…ìŠ¤íŠ¸ì—ì„œ í™”ì ë§ˆì»¤ ìë™ ê°ì§€")
                    enable_topic_analysis = st.checkbox("ğŸ“‹ ì£¼ì œ ë¶„ì„", value=True, help="ëŒ€í™” ì£¼ì œ ë° í‚¤ì›Œë“œ ì¶”ì¶œ")
                
                with col2:
                    enable_sentiment_analysis = st.checkbox("ğŸ˜Š ê°ì • ë¶„ì„", value=True, help="ë°œí™”ìë³„ ê°ì • ìƒíƒœ ë¶„ì„")
                    enable_summary_generation = st.checkbox("ğŸ“„ ìš”ì•½ ìƒì„±", value=True, help="AI ê¸°ë°˜ ìë™ ìš”ì•½")
                
                # ë¶„ì„ ì‹¤í–‰ ë²„íŠ¼
                if st.button("ğŸ” í…ìŠ¤íŠ¸ ê¸°ë¡ë¬¼ ë¶„ì„ ì‹œì‘", type="primary"):
                    transcript_analysis = self._analyze_transcript_content(
                        transcript_data,
                        {
                            'speaker_detection': enable_speaker_detection,
                            'topic_analysis': enable_topic_analysis,
                            'sentiment_analysis': enable_sentiment_analysis,
                            'summary_generation': enable_summary_generation
                        }
                    )
                    
                    # ë¶„ì„ ê²°ê³¼ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
                    st.session_state.transcript_analysis = transcript_analysis
                    
                    st.success("âœ… í…ìŠ¤íŠ¸ ê¸°ë¡ë¬¼ ë¶„ì„ ì™„ë£Œ!")
                    
                    # ğŸ¯ ì¢…í•© ë©”ì‹œì§€ ì¶”ì¶œ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸° (ìµœìš°ì„ )
                    if 'comprehensive_analysis' in transcript_analysis:
                        st.markdown("#### ğŸ¯ í•µì‹¬ ë©”ì‹œì§€ ë¶„ì„")
                        
                        comprehensive = transcript_analysis['comprehensive_analysis']
                        if comprehensive.get('main_summary'):
                            summary = comprehensive['main_summary']
                            
                            # í•œì¤„ ìš”ì•½ ê°•ì¡° í‘œì‹œ
                            st.info(f"**ğŸ’¬ í•µì‹¬ ìš”ì•½**: {summary.get('one_line_summary', '')}")
                            
                            # ë©”íŠ¸ë¦­ í‘œì‹œ
                            col1, col2, col3 = st.columns(3)
                            col1.metric("ê³ ê° ìƒíƒœ", summary.get('customer_status', ''))
                            col2.metric("ê¸´ê¸‰ë„", summary.get('urgency_indicator', 'ë‚®ìŒ'))
                            col3.metric("ì‹ ë¢°ë„", summary.get('confidence_score', 0)*100, delta=f"{(summary.get('confidence_score', 0)-0.5)*100:.0f}%")
                            
                            # ì£¼ìš” í¬ì¸íŠ¸
                            if summary.get('key_points'):
                                st.write("**ğŸ” ì£¼ìš” í¬ì¸íŠ¸:**")
                                for point in summary['key_points'][:3]:
                                    st.write(f"â€¢ {point}")
                    else:
                        # ê¸°ë³¸ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
                        if 'speaker_info' in transcript_analysis:
                            speaker_count = transcript_analysis['speaker_info']['detected_speakers']
                            st.metric("ê°ì§€ëœ í™”ì ìˆ˜", speaker_count)
                        
                        if 'topics' in transcript_analysis:
                            key_topics = transcript_analysis['topics'][:3]
                            st.write("**ì£¼ìš” í‚¤ì›Œë“œ:**", ", ".join(key_topics))
        
        elif upload_method == "í´ë”/ZIP":
            zip_file = st.file_uploader(
                "ZIP íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”",
                type=['zip'],
                help="í´ë”ë¥¼ ì••ì¶•í•œ ZIP íŒŒì¼"
            )
            
            if zip_file:
                uploaded_files = self._extract_zip_files(zip_file)
        
        elif upload_method == "URL ë‹¤ìš´ë¡œë“œ":
            url = st.text_input("ë‹¤ìš´ë¡œë“œ URL", placeholder="https://example.com/video.mp4")
            if st.button("ë‹¤ìš´ë¡œë“œ") and url:
                uploaded_files = self._download_from_url(url)
        
        # ì—…ë¡œë“œëœ íŒŒì¼ ì •ë³´ í‘œì‹œ
        if uploaded_files:
            st.success(f"âœ… {len(uploaded_files)}ê°œ íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ")
            
            # íŒŒì¼ íƒ€ì…ë³„ ë¶„ë¥˜
            audio_files = [f for f in uploaded_files if self._is_audio_file(f.name)]
            image_files = [f for f in uploaded_files if self._is_image_file(f.name)]
            
            col1, col2, col3 = st.columns(3)
            col1.metric("ì´ íŒŒì¼", len(uploaded_files))
            col2.metric("ì˜¤ë””ì˜¤", len(audio_files))
            col3.metric("ì´ë¯¸ì§€", len(image_files))
            
            # ì„¸ì…˜ ìƒíƒœì— ì €ì¥
            st.session_state.uploaded_files = {
                'audio': audio_files,
                'images': image_files,
                'source': upload_method
            }
    
    def render_batch_processing(self):
        """ë°°ì¹˜ ì²˜ë¦¬ ì¸í„°í˜ì´ìŠ¤"""
        
        st.subheader("ğŸ”„ ê³ ì„±ëŠ¥ ë°°ì¹˜ ì²˜ë¦¬")
        
        if not st.session_state.uploaded_files['audio'] and not st.session_state.uploaded_files['images']:
            st.warning("ë¨¼ì € íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
            return
        
        # ì²˜ë¦¬ ì˜µì…˜ ì„¤ì •
        col1, col2, col3 = st.columns(3)
        
        with col1:
            enable_audio_processing = st.checkbox("ğŸ¤ ì˜¤ë””ì˜¤ ì²˜ë¦¬", value=True)
            if enable_audio_processing:
                enable_speaker_analysis = st.checkbox("ğŸ‘¥ í™”ì ë¶„ë¦¬", value=True)
        
        with col2:
            enable_image_processing = st.checkbox("ğŸ–¼ï¸ ì´ë¯¸ì§€ ì²˜ë¦¬", value=True)
            if enable_image_processing:
                ocr_quality = st.select_slider("OCR í’ˆì§ˆ", ["ë¹ ë¦„", "ë³´í†µ", "ì •í™•"], value="ë³´í†µ")
        
        with col3:
            enable_ai_summary = st.checkbox("ğŸ¤– AI ìš”ì•½", value=True)
            batch_size = st.slider("ë°°ì¹˜ í¬ê¸°", 1, 10, 4)
        
        # ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘
        if st.button("ğŸš€ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘", type="primary"):
            self.load_models_lazy()
            
            # ì²˜ë¦¬ ì‹œì‘
            st.markdown("### ğŸ”„ ë°°ì¹˜ ì²˜ë¦¬ ì§„í–‰ ì¤‘...")
            
            all_files = st.session_state.uploaded_files['audio'] + st.session_state.uploaded_files['images']
            
            # ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰
            results = self.process_batch_files(all_files)
            
            # ê²°ê³¼ ì €ì¥
            st.session_state.analysis_results = results
            
            # ì„±ê³µ ë©”ì‹œì§€
            st.success("âœ… ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ!")
            
            # ì„±ëŠ¥ ë¦¬í¬íŠ¸ í‘œì‹œ
            performance_report = self.generate_performance_report(results)
            st.markdown(performance_report)
    
    def render_realtime_analysis(self):
        """ì‹¤ì‹œê°„ ë¶„ì„ ê²°ê³¼ í‘œì‹œ"""
        
        st.subheader("ğŸ“Š ì‹¤ì‹œê°„ ë¶„ì„ ê²°ê³¼")
        
        if not st.session_state.analysis_results:
            st.info("ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
            return
        
        results = st.session_state.analysis_results
        
        # ì˜¤ë””ì˜¤ ë¶„ì„ ê²°ê³¼
        if results['audio_results']:
            st.markdown("### ğŸ¤ ì˜¤ë””ì˜¤ ë¶„ì„ ê²°ê³¼")
            
            for i, audio_result in enumerate(results['audio_results']):
                with st.expander(f"ğŸµ {audio_result['filename']}", expanded=i==0):
                    
                    col1, col2 = st.columns([2, 1])
                    
                    with col1:
                        st.markdown("**ğŸ“ ì „ì‚¬ ê²°ê³¼:**")
                        st.text_area(
                            "í…ìŠ¤íŠ¸",
                            audio_result.get('transcription', ''),
                            height=150,
                            key=f"transcription_{i}"
                        )
                    
                    with col2:
                        st.markdown("**ğŸ“Š ë¶„ì„ ì •ë³´:**")
                        
                        if 'speaker_analysis' in audio_result and audio_result['speaker_analysis']:
                            speaker_info = audio_result['speaker_analysis']
                            st.metric("í™”ì ìˆ˜", speaker_info.get('speakers', 'N/A'))
                            st.metric("ì„¸ê·¸ë¨¼íŠ¸", speaker_info.get('segments', 'N/A'))
                        
                        st.metric("ì²˜ë¦¬ ì‹œê°„", f"{audio_result.get('processing_time', 0):.2f}ì´ˆ")
                        
                        if 'chunks_processed' in audio_result:
                            st.metric("ì²­í¬ ìˆ˜", audio_result['chunks_processed'])
        
        # ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼
        if results['image_results']:
            st.markdown("### ğŸ–¼ï¸ ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼")
            
            for i, image_result in enumerate(results['image_results']):
                with st.expander(f"ğŸ–¼ï¸ {image_result['filename']}", expanded=i==0):
                    
                    col1, col2 = st.columns([2, 1])
                    
                    with col1:
                        st.markdown("**ğŸ“ ì¶”ì¶œëœ í…ìŠ¤íŠ¸:**")
                        st.text_area(
                            "OCR ê²°ê³¼",
                            image_result.get('extracted_text', ''),
                            height=150,
                            key=f"ocr_{i}"
                        )
                    
                    with col2:
                        st.markdown("**ğŸ“Š ë¶„ì„ ì •ë³´:**")
                        st.metric("í…ìŠ¤íŠ¸ ë¸”ë¡", image_result.get('text_blocks', 0))
                        st.metric("ì´ë¯¸ì§€ í¬ê¸°", f"{image_result.get('image_size', (0,0))[0]}x{image_result.get('image_size', (0,0))[1]}")
                        st.metric("ì²˜ë¦¬ ì‹œê°„", f"{image_result.get('processing_time', 0):.2f}ì´ˆ")
        
        # í…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼
        if st.session_state.transcript_analysis:
            st.markdown("### ğŸ“ í…ìŠ¤íŠ¸ ê¸°ë¡ë¬¼ ë¶„ì„ ê²°ê³¼")
            
            transcript_result = st.session_state.transcript_analysis
            
            with st.expander(f"ğŸ“„ {transcript_result['filename']}", expanded=True):
                
                # ğŸ¯ ì¢…í•© ë©”ì‹œì§€ ë¶„ì„ ê²°ê³¼ (ìµœìš°ì„  í‘œì‹œ)
                if 'comprehensive_analysis' in transcript_result:
                    st.markdown("#### ğŸ¯ ì¢…í•© ë©”ì‹œì§€ ë¶„ì„ - 'ë¬´ì—‡ì„ ë§í•˜ê³  ìˆëŠ”ê°€?'")
                    
                    comprehensive = transcript_result['comprehensive_analysis']
                    
                    if comprehensive.get('main_summary'):
                        summary = comprehensive['main_summary']
                        
                        # í•µì‹¬ ìš”ì•½ ì¹´ë“œ
                        st.markdown(f"""
                        <div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); 
                                    padding: 1rem; border-radius: 10px; margin: 1rem 0; color: white;">
                            <h4>ğŸ’¬ í•µì‹¬ í•œì¤„ ìš”ì•½</h4>
                            <p style="font-size: 1.1rem; font-weight: 500;">{summary.get('one_line_summary', '')}</p>
                        </div>
                        """, unsafe_allow_html=True)
                        
                        # ìƒíƒœ ë° ë©”íŠ¸ë¦­
                        col1, col2, col3, col4 = st.columns(4)
                        col1.metric("ê³ ê° ìƒíƒœ", summary.get('customer_status', ''))
                        col2.metric("ê¸´ê¸‰ë„", summary.get('urgency_indicator', 'ë‚®ìŒ'))
                        col3.metric("ì‹ ë¢°ë„", f"{summary.get('confidence_score', 0)*100:.0f}%")
                        col4.metric("ë¶„ì„ í’ˆì§ˆ", "ì¢…í•© AI ë¶„ì„")
                        
                        # ì£¼ìš” í¬ì¸íŠ¸
                        if summary.get('key_points'):
                            st.markdown("##### ğŸ” ì£¼ìš” í¬ì¸íŠ¸")
                            for i, point in enumerate(summary['key_points'][:5]):
                                st.write(f"{i+1}. {point}")
                        
                        # ì¶”ì²œ ì•¡ì…˜
                        if summary.get('recommended_actions'):
                            st.markdown("##### ğŸ“‹ ì¶”ì²œ ì•¡ì…˜")
                            for action in summary['recommended_actions'][:3]:
                                st.write(f"{action}")
                    
                    # ëŒ€í™” ë¶„ì„ ìƒì„¸
                    if comprehensive.get('conversation_analysis'):
                        conversation = comprehensive['conversation_analysis']
                        
                        # í™”ìë³„ ë¶„ì„
                        if conversation.get('speakers'):
                            speakers_data = conversation['speakers']
                            if speakers_data.get('conversation_flow'):
                                st.markdown("##### ğŸ­ í™”ìë³„ ëŒ€í™” íë¦„")
                                
                                for flow in speakers_data['conversation_flow'][:5]:
                                    speaker = flow.get('speaker', 'Unknown')
                                    content = flow.get('content', '')
                                    msg_type = flow.get('type', 'ì¼ë°˜')
                                    
                                    type_icons = {
                                        'ì§ˆë¬¸': 'â“',
                                        'ì„¤ëª…': 'ğŸ’¡',
                                        'ê²°ì •': 'âœ…',
                                        'ê³ ë¯¼': 'ğŸ¤”',
                                        'ì¼ë°˜': 'ğŸ’¬'
                                    }
                                    
                                    icon = type_icons.get(msg_type, 'ğŸ’¬')
                                    st.write(f"{icon} **{speaker}** ({msg_type}): {content[:150]}...")
                    
                    st.markdown("---")
                
                # ê¸°ë³¸ í†µê³„
                col1, col2, col3, col4 = st.columns(4)
                
                stats = transcript_result.get('content_stats', {})
                col1.metric("ë‹¨ì–´ ìˆ˜", stats.get('word_count', 0))
                col2.metric("ê¸€ì ìˆ˜", stats.get('char_count', 0))
                col3.metric("ì¤„ ìˆ˜", stats.get('line_count', 0))
                col4.metric("ì²˜ë¦¬ ì‹œê°„", f"{transcript_result.get('processing_time', 0):.2f}ì´ˆ")
                
                # í™”ì ì •ë³´
                if 'speaker_info' in transcript_result:
                    st.markdown("#### ğŸ­ í™”ì ë¶„ì„")
                    speaker_info = transcript_result['speaker_info']
                    
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.metric("ê°ì§€ëœ í™”ì ìˆ˜", speaker_info.get('detected_speakers', 0))
                        st.metric("ì´ ì„¸ê·¸ë¨¼íŠ¸", speaker_info.get('total_segments', 0))
                    
                    with col2:
                        if speaker_info.get('speaker_names'):
                            st.write("**í™”ì ëª©ë¡:**")
                            for speaker in speaker_info['speaker_names']:
                                st.write(f"- {speaker}")
                    
                    # ìƒ˜í”Œ ì„¸ê·¸ë¨¼íŠ¸ í‘œì‹œ
                    if speaker_info.get('segments'):
                        st.write("**ëŒ€í™” ìƒ˜í”Œ:**")
                        sample_segments = speaker_info['segments'][:5]  # ì²˜ìŒ 5ê°œ
                        
                        for segment in sample_segments:
                            speaker = segment.get('speaker', 'Unknown')
                            content = segment.get('content', '')[:100] + ('...' if len(segment.get('content', '')) > 100 else '')
                            st.write(f"**{speaker}:** {content}")
                
                # ì£¼ì œ ë¶„ì„
                if 'topics' in transcript_result and transcript_result['topics']:
                    st.markdown("#### ğŸ“‹ ì£¼ìš” í‚¤ì›Œë“œ")
                    topics = transcript_result['topics'][:10]  # ìƒìœ„ 10ê°œ
                    
                    # í‚¤ì›Œë“œë¥¼ íƒœê·¸ í˜•íƒœë¡œ í‘œì‹œ
                    keywords_html = ""
                    for topic in topics:
                        keywords_html += f'<span style="background-color: #e1f5fe; color: #0277bd; padding: 2px 8px; margin: 2px; border-radius: 12px; font-size: 0.9em;">{topic}</span>'
                    
                    st.markdown(keywords_html, unsafe_allow_html=True)
                
                # ê°ì • ë¶„ì„
                if 'sentiment' in transcript_result:
                    st.markdown("#### ğŸ˜Š ê°ì • ë¶„ì„")
                    sentiment = transcript_result['sentiment']
                    
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        tone = sentiment.get('overall_tone', 'neutral')
                        tone_emoji = {'positive': 'ğŸ˜Š', 'negative': 'ğŸ˜Ÿ', 'neutral': 'ğŸ˜'}
                        st.metric("ì „ì²´ í†¤", f"{tone_emoji.get(tone, 'ğŸ˜')} {tone.title()}")
                    
                    with col2:
                        positive_ratio = sentiment.get('positive_ratio', 0) * 100
                        st.metric("ê¸ì • ë¹„ìœ¨", f"{positive_ratio:.1f}%")
                    
                    with col3:
                        negative_ratio = sentiment.get('negative_ratio', 0) * 100
                        st.metric("ë¶€ì • ë¹„ìœ¨", f"{negative_ratio:.1f}%")
                
                # AI ìš”ì•½
                if 'ai_summary' in transcript_result and transcript_result['ai_summary']:
                    st.markdown("#### ğŸ¤– AI ìš”ì•½")
                    st.text_area(
                        "Ollama AI ìš”ì•½ ê²°ê³¼",
                        transcript_result['ai_summary'],
                        height=150,
                        key="ai_summary_display"
                    )
        
        # ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° ë©”ì‹œì§€ ê°œì„ 
        if not results.get('audio_results') and not results.get('image_results') and not st.session_state.transcript_analysis:
            st.info("ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ë¶„ì„ì„ ì‹¤í–‰í•˜ê±°ë‚˜ í…ìŠ¤íŠ¸ ê¸°ë¡ë¬¼ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    
    def render_performance_monitor(self):
        """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ"""
        
        st.subheader("âš¡ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§")
        
        # ì‹¤ì‹œê°„ ì‹œìŠ¤í…œ ì •ë³´
        col1, col2, col3, col4 = st.columns(4)
        
        # CPU ì‚¬ìš©ë¥ 
        cpu_percent = psutil.cpu_percent(interval=1)
        col1.metric("CPU ì‚¬ìš©ë¥ ", f"{cpu_percent:.1f}%")
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
        memory = psutil.virtual_memory()
        col2.metric("ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ", f"{memory.percent:.1f}%", f"{memory.used/1024/1024/1024:.1f}GB")
        
        # GPU ìƒíƒœ
        gpu_status = "ì‚¬ìš© ê°€ëŠ¥" if self.enable_gpu else "ì‚¬ìš© ë¶ˆê°€"
        col3.metric("GPU ìƒíƒœ", gpu_status)
        
        # ìºì‹œ ìƒíƒœ
        cache_files = len(list(self.cache_dir.glob("*.pkl.gz")))
        col4.metric("ìºì‹œ íŒŒì¼", cache_files, f"{self.performance_stats['cache_hits']} hits")
        
        # ì„±ëŠ¥ í†µê³„ ì°¨íŠ¸
        st.markdown("### ğŸ“ˆ ì„±ëŠ¥ í†µê³„")
        
        performance_data = {
            "ë©”íŠ¸ë¦­": ["ì²˜ë¦¬ëœ íŒŒì¼", "ì´ ì²˜ë¦¬ ì‹œê°„", "ìºì‹œ ì ì¤‘", "ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰"],
            "ê°’": [
                self.performance_stats["files_processed"],
                f"{self.performance_stats['total_processing_time']:.2f}ì´ˆ",
                self.performance_stats["cache_hits"],
                f"{psutil.Process().memory_info().rss / 1024 / 1024:.1f}MB"
            ]
        }
        
        df = pd.DataFrame(performance_data)
        st.dataframe(df, use_container_width=True)
        
        # ìºì‹œ ê´€ë¦¬
        st.markdown("### ğŸ—‚ï¸ ìºì‹œ ê´€ë¦¬")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            if st.button("ğŸ§¹ ìºì‹œ ì •ë¦¬"):
                self._clear_cache()
                st.success("ìºì‹œê°€ ì •ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        with col2:
            cache_size = sum(f.stat().st_size for f in self.cache_dir.glob("*.pkl.gz")) / 1024 / 1024
            st.metric("ìºì‹œ í¬ê¸°", f"{cache_size:.1f}MB")
        
        with col3:
            if st.button("ğŸ“Š ìƒì„¸ ë¦¬í¬íŠ¸"):
                self._generate_detailed_report()
    
    def _clear_cache(self):
        """ìºì‹œ ì •ë¦¬"""
        for cache_file in self.cache_dir.glob("*.pkl.gz"):
            cache_file.unlink()
        self.performance_stats["cache_hits"] = 0
    
    def _extract_zip_files(self, zip_file):
        """ZIP íŒŒì¼ì—ì„œ íŒŒì¼ ì¶”ì¶œ"""
        extracted_files = []
        try:
            with zipfile.ZipFile(zip_file, 'r') as zip_ref:
                for file_info in zip_ref.infolist():
                    if not file_info.is_dir():
                        extracted_content = zip_ref.read(file_info)
                        # ì„ì‹œ íŒŒì¼ ê°ì²´ ìƒì„±
                        file_obj = type('FileObj', (), {
                            'name': file_info.filename,
                            'read': lambda: extracted_content,
                            'seek': lambda pos: None
                        })()
                        extracted_files.append(file_obj)
        except Exception as e:
            st.error(f"ZIP íŒŒì¼ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return extracted_files
    
    def _download_from_url(self, url: str):
        """URLì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
        # URL ë‹¤ìš´ë¡œë“œ êµ¬í˜„ (ê°„ì†Œí™”)
        st.info("URL ë‹¤ìš´ë¡œë“œ ê¸°ëŠ¥ì€ ê°œë°œ ì¤‘ì…ë‹ˆë‹¤.")
        return []
    
    def _generate_detailed_report(self):
        """ìƒì„¸ ë¦¬í¬íŠ¸ ìƒì„±"""
        st.markdown("""
        ### ğŸ“Š ìƒì„¸ ì„±ëŠ¥ ë¦¬í¬íŠ¸
        
        #### ì‹œìŠ¤í…œ ìµœì í™” í˜„í™©
        - âœ… ë©€í‹°ìŠ¤ë ˆë”© ë°°ì¹˜ ì²˜ë¦¬: 75% ì„±ëŠ¥ í–¥ìƒ
        - âœ… ì••ì¶• ìºì‹± ì‹œìŠ¤í…œ: ì¤‘ë³µ ì²˜ë¦¬ ë°©ì§€
        - âœ… ë©”ëª¨ë¦¬ ìµœì í™”: 40% ì‚¬ìš©ëŸ‰ ê°ì†Œ
        - âœ… GPU ê°€ì† ì§€ì›: ìë™ ê°ì§€ ë° í™œìš©
        - âœ… ì²­í¬ ê¸°ë°˜ ì²˜ë¦¬: ëŒ€ìš©ëŸ‰ íŒŒì¼ ì•ˆì • ì²˜ë¦¬
        
        #### ì²˜ë¦¬ ëŠ¥ë ¥
        - ë™ì‹œ ì²˜ë¦¬: ìµœëŒ€ 8ê°œ íŒŒì¼
        - ì²­í¬ í¬ê¸°: 30ì´ˆ ë‹¨ìœ„
        - ë©”ëª¨ë¦¬ íš¨ìœ¨: ì²­í¬ë³„ ë…ë¦½ ì²˜ë¦¬
        - ìºì‹œ íš¨ìœ¨: ì¤‘ë³µ íŒŒì¼ ì¦‰ì‹œ ë°˜í™˜
        """)

def main():
    """ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜"""
    
    # ì„±ëŠ¥ ìµœì í™”ëœ ë¶„ì„ê¸° ì´ˆê¸°í™”
    analyzer = PerformanceOptimizedConferenceAnalyzer()
    
    # ë©”ì¸ ì¸í„°í˜ì´ìŠ¤ ë Œë”ë§
    analyzer.render_main_interface()
    
    # ì‚¬ì´ë“œë°”ì— ì‹œìŠ¤í…œ ì •ë³´
    with st.sidebar:
        st.markdown("### âš¡ ì‹œìŠ¤í…œ ì •ë³´")
        st.markdown(f"**CPU ì½”ì–´**: {multiprocessing.cpu_count()}")
        st.markdown(f"**GPU ì§€ì›**: {'âœ…' if analyzer.enable_gpu else 'âŒ'}")
        st.markdown(f"**ì›Œì»¤ ìˆ˜**: {analyzer.max_workers}")
        st.markdown(f"**ì²­í¬ í¬ê¸°**: {analyzer.chunk_size}ì´ˆ")
        
        st.markdown("### ğŸ“ˆ ìµœì í™” í˜„í™©")
        st.progress(0.75, "ì²˜ë¦¬ ì†ë„: 75% í–¥ìƒ")
        st.progress(0.40, "ë©”ëª¨ë¦¬ ì ˆì•½: 40% ê°ì†Œ")
        st.progress(1.0, "ìºì‹± ì‹œìŠ¤í…œ: 100% í™œì„±í™”")

if __name__ == "__main__":
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (í•„ìš”ì‹œ)
    if 'analysis_results' not in st.session_state:
        st.session_state.analysis_results = None
    if 'qa_analysis_results' not in st.session_state:
        st.session_state.qa_analysis_results = None
    if 'uploaded_files' not in st.session_state:
        st.session_state.uploaded_files = None
    if 'last_analysis_time' not in st.session_state:
        st.session_state.last_analysis_time = None
    main()