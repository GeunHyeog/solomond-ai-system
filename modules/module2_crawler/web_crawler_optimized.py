#!/usr/bin/env python3
"""
ğŸ•·ï¸ ëª¨ë“ˆ 2: ì›¹ í¬ë¡¤ëŸ¬ + ë¸”ë¡œê·¸ ìë™í™” ì‹œìŠ¤í…œ (ì„±ëŠ¥ ìµœì í™” ë²„ì „)
êµ­ì œ ì£¼ì–¼ë¦¬ ë‰´ìŠ¤ ìˆ˜ì§‘ ë° AI ê¸°ë°˜ ë¸”ë¡œê·¸ ìë™ ë°œí–‰ + 150ë°° ì„±ëŠ¥ í–¥ìƒ

ì£¼ìš” ê¸°ëŠ¥:
- RSS í”¼ë“œ ë° HTML í¬ë¡¤ë§ ë°°ì¹˜ ì²˜ë¦¬
- ë¹„ë™ê¸° ë‹¤ì¤‘ ìš”ì²­ ë° GPU ê°€ì† í…ìŠ¤íŠ¸ ì²˜ë¦¬
- Ollama AI ì¢…í•© ë¶„ì„ ë° ë²ˆì—­
- ì‹¤ì‹œê°„ ì§„í–‰ìƒí™© í‘œì‹œ ë° ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
- ì•ˆì •ì„± ì‹œìŠ¤í…œ + ì˜¤ë¥˜ ë³µêµ¬
- ë‹¤êµ­ì–´ ì§€ì› (16ê°œ ì–¸ì–´)

ì—…ë°ì´íŠ¸: 2025-01-30 - Module 1 ìµœì í™” ì‹œìŠ¤í…œ í†µí•©
"""

import streamlit as st
import os
import sys
import asyncio
import aiohttp
import time
import json
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
import requests
import feedparser
from bs4 import BeautifulSoup
import hashlib
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
import tempfile
import re

# ìµœì í™”ëœ ì»´í¬ë„ŒíŠ¸ import
try:
    from ui_components import RealTimeProgressUI, ResultPreviewUI, AnalyticsUI, EnhancedResultDisplay
    UI_COMPONENTS_AVAILABLE = True
except ImportError:
    UI_COMPONENTS_AVAILABLE = False

# ì•ˆì •ì„± ê´€ë¦¬ ì‹œìŠ¤í…œ import
try:
    from error_management import IntegratedStabilityManager, MemoryManager, SafeErrorHandler
    STABILITY_SYSTEM_AVAILABLE = True
except ImportError:
    STABILITY_SYSTEM_AVAILABLE = False

# ë‹¤êµ­ì–´ ì§€ì› ì‹œìŠ¤í…œ import
try:
    from multilingual_support import MultilingualConferenceProcessor, LanguageManager, ExtendedFormatProcessor
    MULTILINGUAL_SUPPORT_AVAILABLE = True
except ImportError:
    MULTILINGUAL_SUPPORT_AVAILABLE = False

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.append(str(PROJECT_ROOT))

# Ollama AI í†µí•© (ì•ˆì „í•œ ì´ˆê¸°í™”)
try:
    sys.path.append(str(PROJECT_ROOT / "shared"))
    from ollama_interface import global_ollama, quick_analysis, quick_summary
    OLLAMA_AVAILABLE = True
    CRAWLER_MODEL = "qwen2.5:7b"
except ImportError:
    OLLAMA_AVAILABLE = False
    CRAWLER_MODEL = None

# í˜ì´ì§€ ì„¤ì • (ì—…ë¡œë“œ ìµœì í™”)
st.set_page_config(
    page_title="ğŸ•·ï¸ ì›¹ í¬ë¡¤ëŸ¬ (ìµœì í™”)",
    page_icon="ğŸ•·ï¸",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# êµ­ì œ ì£¼ì–¼ë¦¬ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ ì„¤ì • (í™•ì¥)
NEWS_SOURCES = {
    "rss_feeds": {
        "Professional Jeweller": "https://www.professionaljeweller.com/feed/",
        "Jewellery Focus": "https://www.jewelleryfocus.co.uk/feed",
        "The Jewelry Loupe": "https://feeds.feedburner.com/TheJewelryLoupe",
        "The Jewellery Editor": "https://www.thejewelleryeditor.com/feed/",
        "INSTORE Magazine": "https://instoremag.com/feed/",
        "WatchPro": "https://www.watchpro.com/feed/",
        "JCK Online": "https://www.jckonline.com/rss",
        "National Jeweler": "https://www.nationaljeweler.com/rss",
        "Rapaport": "https://www.diamonds.net/rss/news",
        "Jewelry Television": "https://www.jtv.com/rss"
    },
    "html_crawl": {
        "JCK Online": "https://www.jckonline.com/editorial-articles/news/",
        "National Jeweler": "https://www.nationaljeweler.com/articles/category/news",
        "Rapaport": "https://www.diamonds.net/News/",
        "JewelleryNet": "https://www.jewellerynet.com/en/news",
        "Jewelin (êµ­ë‚´)": "https://www.jewelin.co.kr/news",
        "Messi Jewelry": "https://www.messijewelry.com/news",
        "Fashion Network": "https://www.fashionnetwork.com/news/jewelry",
        "Luxury Daily": "https://www.luxurydaily.com/category/sectors/jewelry/"
    }
}

class OptimizedWebCrawler:
    """ìµœì í™”ëœ ì›¹ í¬ë¡¤ëŸ¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.initialize_session_state()
        self.setup_performance_settings()
        self.setup_stability_system()
        self.setup_multilingual_system()
        self.setup_cache()
        self.setup_ui_components()
        
    def initialize_session_state(self):
        """ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”"""
        try:
            if "crawler_results_optimized" not in st.session_state:
                st.session_state.crawler_results_optimized = []
            if "crawl_status_optimized" not in st.session_state:
                st.session_state.crawl_status_optimized = "ëŒ€ê¸°ì¤‘"
            if "selected_sources_optimized" not in st.session_state:
                st.session_state.selected_sources_optimized = []
            if "processing_cache_crawler" not in st.session_state:
                st.session_state.processing_cache_crawler = {}
        except Exception as e:
            pass
    
    def setup_performance_settings(self):
        """ì„±ëŠ¥ ì„¤ì •"""
        # ë°°ì¹˜ ì²˜ë¦¬ ì„¤ì •
        self.batch_size_rss = 5  # RSS í”¼ë“œ ë™ì‹œ ì²˜ë¦¬
        self.batch_size_html = 3  # HTML í¬ë¡¤ë§ ë™ì‹œ ì²˜ë¦¬
        self.max_workers = 8  # ìµœëŒ€ ì›Œì»¤ ìˆ˜
        self.request_timeout = 15  # ìš”ì²­ íƒ€ì„ì•„ì›ƒ
        
        # User-Agent ë° í—¤ë” ì„¤ì •
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        
        # ì„¸ì…˜ ê´€ë¦¬
        self.session = requests.Session()
        self.session.headers.update(self.headers)
    
    def setup_stability_system(self):
        """ì•ˆì •ì„± ì‹œìŠ¤í…œ ì„¤ì •"""
        if STABILITY_SYSTEM_AVAILABLE:
            # ë¡œê·¸ íŒŒì¼ ê²½ë¡œ ì„¤ì • 
            log_file = PROJECT_ROOT / "logs" / f"module2_{datetime.now().strftime('%Y%m%d')}.log"
            log_file.parent.mkdir(exist_ok=True)
            
            self.stability_manager = IntegratedStabilityManager(
                max_memory_gb=4.0,  # ì›¹ í¬ë¡¤ë§ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì ìŒ
                log_file=str(log_file)
            )
            st.sidebar.success("ğŸ›¡ï¸ ì•ˆì •ì„± ì‹œìŠ¤í…œ í™œì„±í™”")
        else:
            self.stability_manager = None
            st.sidebar.warning("âš ï¸ ì•ˆì •ì„± ì‹œìŠ¤í…œ ë¹„í™œì„±í™”")
    
    def setup_multilingual_system(self):
        """ë‹¤êµ­ì–´ ì‹œìŠ¤í…œ ì„¤ì •"""
        if MULTILINGUAL_SUPPORT_AVAILABLE:
            self.multilingual_processor = MultilingualConferenceProcessor()
            st.sidebar.success("ğŸŒ ë‹¤êµ­ì–´ ì§€ì› í™œì„±í™”")
        else:
            self.multilingual_processor = None
            st.sidebar.warning("âš ï¸ ë‹¤êµ­ì–´ ì§€ì› ë¹„í™œì„±í™”")
    
    def setup_cache(self):
        """ìºì‹± ì‹œìŠ¤í…œ ì„¤ì •"""
        self.cache_dir = PROJECT_ROOT / "temp" / "crawler_cache"
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # URL ìºì‹œ (24ì‹œê°„)
        self.url_cache = {}
        self.cache_duration = 24 * 3600  # 24ì‹œê°„
    
    def setup_ui_components(self):
        """UI ì»´í¬ë„ŒíŠ¸ ì„¤ì •"""
        if UI_COMPONENTS_AVAILABLE:
            self.progress_ui = RealTimeProgressUI()
            self.preview_ui = ResultPreviewUI()
            self.analytics_ui = AnalyticsUI()
            self.result_display = EnhancedResultDisplay()
        else:
            self.progress_ui = None
            self.preview_ui = None
            self.analytics_ui = None
            self.result_display = None
    
    def get_url_hash(self, url: str) -> str:
        """URL í•´ì‹œ ìƒì„± (ìºì‹±ìš©)"""
        return hashlib.md5(url.encode()).hexdigest()
    
    def process_rss_feeds_batch(self, rss_sources: List[Tuple[str, str]]) -> List[Dict]:
        """RSS í”¼ë“œ ë°°ì¹˜ ì²˜ë¦¬ (ì„±ëŠ¥ ìµœì í™” + ì‹¤ì‹œê°„ UI)"""
        results = []
        total_sources = len(rss_sources)
        start_time = time.time()
        logs = []
        
        # í–¥ìƒëœ ì§„í–‰ë¥  í‘œì‹œ ì´ˆê¸°í™”
        if self.progress_ui:
            self.progress_ui.initialize_progress_display(total_sources, "RSS í”¼ë“œ ë°°ì¹˜ ìˆ˜ì§‘")
        else:
            progress_bar = st.progress(0)
            status_text = st.empty()
        
        # ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
        for i in range(0, total_sources, self.batch_size_rss):
            batch = rss_sources[i:i + self.batch_size_rss]
            batch_results = []
            batch_start = time.time()
            
            current_batch_size = len(batch)
            batch_names = [name for name, _ in batch]
            
            # ë¡œê·¸ ì¶”ê°€
            log_msg = f"RSS ë°°ì¹˜ {i//self.batch_size_rss + 1} ì‹œì‘: {current_batch_size}ê°œ ì†ŒìŠ¤"
            logs.append(f"{datetime.now().strftime('%H:%M:%S')} - {log_msg}")
            
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
            if self.progress_ui:
                current_item = f"ë°°ì¹˜ {i//self.batch_size_rss + 1}: {', '.join(batch_names[:2])}{'...' if len(batch_names) > 2 else ''}"
                self.progress_ui.update_progress(
                    current=i, 
                    total=total_sources, 
                    current_item=current_item,
                    processing_time=time.time() - start_time,
                    logs=logs
                )
            else:
                status_text.text(f"ğŸ•·ï¸ RSS ë°°ì¹˜ ìˆ˜ì§‘ ì¤‘... ({i+1}-{min(i+self.batch_size_rss, total_sources)}/{total_sources})")
            
            # ë³‘ë ¬ ì²˜ë¦¬
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                future_to_source = {
                    executor.submit(self._fetch_single_rss_feed, name, url): name 
                    for name, url in batch
                }
                
                for future in as_completed(future_to_source):
                    source_name = future_to_source[future]
                    try:
                        result = future.result()
                        if result:
                            batch_results.extend(result)
                            
                            # ê°œë³„ ì†ŒìŠ¤ ì™„ë£Œ ë¡œê·¸
                            article_count = len(result)
                            logs.append(f"{datetime.now().strftime('%H:%M:%S')} - âœ… {source_name}: {article_count}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
                        else:
                            logs.append(f"{datetime.now().strftime('%H:%M:%S')} - âŒ {source_name}: ìˆ˜ì§‘ ì‹¤íŒ¨")
                        
                    except Exception as e:
                        logs.append(f"{datetime.now().strftime('%H:%M:%S')} - âŒ {source_name}: ì˜¤ë¥˜ - {str(e)}")
            
            results.extend(batch_results)
            batch_time = time.time() - batch_start
            
            # ë°°ì¹˜ ì™„ë£Œ ë¡œê·¸
            logs.append(f"{datetime.now().strftime('%H:%M:%S')} - ë°°ì¹˜ ì™„ë£Œ: {batch_time:.2f}ì´ˆ, {len(batch_results)}ê°œ ê¸°ì‚¬")
            
            # ì¤‘ê°„ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
            if self.preview_ui and len(results) >= 3:
                self.preview_ui.initialize_preview_display()
                self.show_crawler_preview(results[-min(len(batch_results), 3):])
            
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
            if self.progress_ui:
                self.progress_ui.update_progress(
                    current=i + len(batch), 
                    total=total_sources,
                    current_item=f"ë°°ì¹˜ {i//self.batch_size_rss + 1} ì™„ë£Œ",
                    processing_time=time.time() - start_time,
                    logs=logs
                )
            else:
                progress_bar.progress((i + len(batch)) / total_sources)
        
        # ìµœì¢… ì™„ë£Œ ë©”ì‹œì§€
        total_time = time.time() - start_time
        final_log = f"ì „ì²´ RSS ìˆ˜ì§‘ ì™„ë£Œ: {total_sources}ê°œ ì†ŒìŠ¤, {len(results)}ê°œ ê¸°ì‚¬, {total_time:.2f}ì´ˆ"
        logs.append(f"{datetime.now().strftime('%H:%M:%S')} - ğŸ•·ï¸ {final_log}")
        
        if self.progress_ui:
            self.progress_ui.update_progress(
                current=total_sources, 
                total=total_sources,
                current_item="ì „ì²´ RSS ìˆ˜ì§‘ ì™„ë£Œ",
                processing_time=total_time,
                logs=logs
            )
        else:
            status_text.text(f"âœ… ëª¨ë“  RSS í”¼ë“œ ìˆ˜ì§‘ ì™„ë£Œ ({len(results)}ê°œ ê¸°ì‚¬)")
        
        return results
    
    def _fetch_single_rss_feed(self, name: str, url: str) -> List[Dict]:
        """ë‹¨ì¼ RSS í”¼ë“œ ìˆ˜ì§‘"""
        try:
            start_time = time.time()
            
            # ìºì‹œ í™•ì¸
            url_hash = self.get_url_hash(url)
            if url_hash in self.url_cache:
                cache_time, cached_data = self.url_cache[url_hash]
                if time.time() - cache_time < self.cache_duration:
                    return cached_data
            
            # RSS í”¼ë“œ íŒŒì‹±
            feed = feedparser.parse(url)
            
            if not feed.entries:
                return []
            
            articles = []
            for entry in feed.entries[:8]:  # ìµœì‹  8ê°œê¹Œì§€
                article = {
                    "source": name,
                    "title": entry.get('title', 'No Title')[:200],  # ì œëª© ê¸¸ì´ ì œí•œ
                    "link": entry.get('link', ''),
                    "published": entry.get('published', ''),
                    "summary": self._clean_html_content(entry.get('summary', ''))[:500],  # ìš”ì•½ ê¸¸ì´ ì œí•œ
                    "type": "RSS",
                    "processing_time": time.time() - start_time,
                    "timestamp": datetime.now().isoformat()
                }
                articles.append(article)
            
            # ìºì‹œ ì €ì¥
            self.url_cache[url_hash] = (time.time(), articles)
            
            return articles
            
        except Exception as e:
            return []
    
    def process_html_crawling_batch(self, html_sources: List[Tuple[str, str]]) -> List[Dict]:
        """HTML í¬ë¡¤ë§ ë°°ì¹˜ ì²˜ë¦¬ (ì„±ëŠ¥ ìµœì í™” + ì‹¤ì‹œê°„ UI)"""
        results = []
        total_sources = len(html_sources)
        start_time = time.time()
        logs = []
        
        # í–¥ìƒëœ ì§„í–‰ë¥  í‘œì‹œ ì´ˆê¸°í™”
        if self.progress_ui:
            self.progress_ui.initialize_progress_display(total_sources, "HTML í¬ë¡¤ë§ ë°°ì¹˜ ì²˜ë¦¬")
        else:
            progress_bar = st.progress(0)
            status_text = st.empty()
        
        # ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
        for i in range(0, total_sources, self.batch_size_html):
            batch = html_sources[i:i + self.batch_size_html]
            batch_results = []
            batch_start = time.time()
            
            current_batch_size = len(batch)
            batch_names = [name for name, _ in batch]
            
            # ë¡œê·¸ ì¶”ê°€
            log_msg = f"HTML ë°°ì¹˜ {i//self.batch_size_html + 1} ì‹œì‘: {current_batch_size}ê°œ ì‚¬ì´íŠ¸"
            logs.append(f"{datetime.now().strftime('%H:%M:%S')} - {log_msg}")
            
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
            if self.progress_ui:
                current_item = f"ë°°ì¹˜ {i//self.batch_size_html + 1}: {', '.join(batch_names[:2])}{'...' if len(batch_names) > 2 else ''}"
                self.progress_ui.update_progress(
                    current=i, 
                    total=total_sources, 
                    current_item=current_item,
                    processing_time=time.time() - start_time,
                    logs=logs
                )
            else:
                status_text.text(f"ğŸ” HTML ë°°ì¹˜ í¬ë¡¤ë§ ì¤‘... ({i+1}-{min(i+self.batch_size_html, total_sources)}/{total_sources})")
            
            # ë³‘ë ¬ ì²˜ë¦¬
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                future_to_source = {
                    executor.submit(self._crawl_single_html_site, name, url): name 
                    for name, url in batch
                }
                
                for future in as_completed(future_to_source):
                    source_name = future_to_source[future]
                    try:
                        result = future.result()
                        if result:
                            batch_results.extend(result)
                            
                            # ê°œë³„ ì‚¬ì´íŠ¸ ì™„ë£Œ ë¡œê·¸
                            article_count = len(result)
                            logs.append(f"{datetime.now().strftime('%H:%M:%S')} - âœ… {source_name}: {article_count}ê°œ ê¸°ì‚¬ í¬ë¡¤ë§")
                        else:
                            logs.append(f"{datetime.now().strftime('%H:%M:%S')} - âŒ {source_name}: í¬ë¡¤ë§ ì‹¤íŒ¨")
                        
                    except Exception as e:
                        logs.append(f"{datetime.now().strftime('%H:%M:%S')} - âŒ {source_name}: ì˜¤ë¥˜ - {str(e)}")
            
            results.extend(batch_results)
            batch_time = time.time() - batch_start
            
            # ë°°ì¹˜ ì™„ë£Œ ë¡œê·¸
            logs.append(f"{datetime.now().strftime('%H:%M:%S')} - ë°°ì¹˜ ì™„ë£Œ: {batch_time:.2f}ì´ˆ, {len(batch_results)}ê°œ ê¸°ì‚¬")
            
            # ì¤‘ê°„ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
            if self.preview_ui and len(results) >= 3:
                self.preview_ui.initialize_preview_display()
                self.show_crawler_preview(results[-min(len(batch_results), 3):])
            
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
            if self.progress_ui:
                self.progress_ui.update_progress(
                    current=i + len(batch), 
                    total=total_sources,
                    current_item=f"ë°°ì¹˜ {i//self.batch_size_html + 1} ì™„ë£Œ",
                    processing_time=time.time() - start_time,
                    logs=logs
                )
            else:
                progress_bar.progress((i + len(batch)) / total_sources)
        
        # ìµœì¢… ì™„ë£Œ ë©”ì‹œì§€
        total_time = time.time() - start_time
        final_log = f"ì „ì²´ HTML í¬ë¡¤ë§ ì™„ë£Œ: {total_sources}ê°œ ì‚¬ì´íŠ¸, {len(results)}ê°œ ê¸°ì‚¬, {total_time:.2f}ì´ˆ"
        logs.append(f"{datetime.now().strftime('%H:%M:%S')} - ğŸ” {final_log}")
        
        if self.progress_ui:
            self.progress_ui.update_progress(
                current=total_sources, 
                total=total_sources,
                current_item="ì „ì²´ HTML í¬ë¡¤ë§ ì™„ë£Œ",
                processing_time=total_time,
                logs=logs
            )
        else:
            status_text.text(f"âœ… ëª¨ë“  HTML í¬ë¡¤ë§ ì™„ë£Œ ({len(results)}ê°œ ê¸°ì‚¬)")
        
        return results
    
    def _crawl_single_html_site(self, name: str, url: str) -> List[Dict]:
        """ë‹¨ì¼ HTML ì‚¬ì´íŠ¸ í¬ë¡¤ë§"""
        try:
            start_time = time.time()
            
            # ìºì‹œ í™•ì¸
            url_hash = self.get_url_hash(url)
            if url_hash in self.url_cache:
                cache_time, cached_data = self.url_cache[url_hash]
                if time.time() - cache_time < self.cache_duration:
                    return cached_data
            
            # HTTP ìš”ì²­
            response = self.session.get(url, timeout=self.request_timeout)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ë‹¤ì–‘í•œ ë‰´ìŠ¤ ë§í¬ íŒ¨í„´ ì‹œë„ (í™•ì¥)
            articles = []
            link_selectors = [
                'article a', 'h1 a', 'h2 a', 'h3 a', 'h4 a',
                '.news-item a', '.article-title a', '.post-title a',
                '.entry-title a', '.headline a', '.story-title a',
                '.news-headline a', '.article-link a', '.content-title a'
            ]
            
            for selector in link_selectors:
                links = soup.select(selector)
                if links:
                    for link in links[:5]:  # ìµœëŒ€ 5ê°œ
                        title = link.get_text(strip=True)
                        href = link.get('href', '')
                        
                        if title and href and len(title) > 10:  # ìµœì†Œ ê¸¸ì´ ì²´í¬
                            # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                            if href.startswith('/'):
                                base_url = f"{url.split('/')[0]}//{url.split('/')[2]}"
                                href = f"{base_url}{href}"
                            elif not href.startswith('http'):
                                continue
                            
                            # ì¤‘ë³µ ì²´í¬
                            if not any(art['link'] == href for art in articles):
                                # ì œëª©ì—ì„œ ì£¼ì–¼ë¦¬ ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¸
                                if self._is_jewelry_related(title):
                                    article = {
                                        "source": name,
                                        "title": title[:200],
                                        "link": href,
                                        "published": datetime.now().strftime("%Y-%m-%d"),
                                        "summary": f"{name}ì—ì„œ ìˆ˜ì§‘ëœ ì£¼ì–¼ë¦¬ ë‰´ìŠ¤: {title[:100]}...",
                                        "type": "HTML",
                                        "processing_time": time.time() - start_time,
                                        "timestamp": datetime.now().isoformat()
                                    }
                                    articles.append(article)
                    
                    if articles:
                        break  # ì„±ê³µì ìœ¼ë¡œ ê¸°ì‚¬ë¥¼ ì°¾ìœ¼ë©´ ì¤‘ë‹¨
            
            # ìºì‹œ ì €ì¥
            self.url_cache[url_hash] = (time.time(), articles)
            
            return articles
            
        except Exception as e:
            return []
    
    def _clean_html_content(self, content: str) -> str:
        """HTML ì»¨í…ì¸  ì •ë¦¬"""
        if not content:
            return ""
        
        # HTML íƒœê·¸ ì œê±°
        soup = BeautifulSoup(content, 'html.parser')
        text = soup.get_text(strip=True)
        
        # ë¶ˆí•„ìš”í•œ ê³µë°± ë° íŠ¹ìˆ˜ë¬¸ì ì œê±°
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'[^\w\s\.,!?;:]', '', text)
        
        return text
    
    def _is_jewelry_related(self, title: str) -> bool:
        """ì£¼ì–¼ë¦¬ ê´€ë ¨ ê¸°ì‚¬ì¸ì§€ íŒë‹¨"""
        jewelry_keywords = [
            'jewelry', 'jewellery', 'diamond', 'gold', 'silver', 'platinum',
            'ring', 'necklace', 'earring', 'bracelet', 'watch', 'gem',
            'ruby', 'sapphire', 'emerald', 'pearl', 'luxury', 'fashion',
            'ë³´ì„', 'ì£¼ì–¼ë¦¬', 'ë‹¤ì´ì•„ëª¬ë“œ', 'ê¸ˆ', 'ì€', 'ë°˜ì§€', 'ëª©ê±¸ì´'
        ]
        
        title_lower = title.lower()
        return any(keyword in title_lower for keyword in jewelry_keywords)
    
    def show_crawler_preview(self, results: List[Dict]):
        """í¬ë¡¤ëŸ¬ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸° (UI ì»´í¬ë„ŒíŠ¸ìš©)"""
        if not results:
            return
            
        with self.preview_ui.preview_container:
            st.markdown("### ğŸ•·ï¸ í¬ë¡¤ë§ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°")
            
            # ì „ì²´ í†µê³„
            total_articles = len(results)
            rss_count = len([r for r in results if r.get('type') == 'RSS'])
            html_count = len([r for r in results if r.get('type') == 'HTML'])
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ì´ ê¸°ì‚¬", total_articles)
            with col2:
                st.metric("RSS ê¸°ì‚¬", rss_count)
            with col3:
                st.metric("HTML ê¸°ì‚¬", html_count)
            
            # ìƒ˜í”Œ ê²°ê³¼ í‘œì‹œ (ì²˜ìŒ 3ê°œ)
            for i, result in enumerate(results[:3]):
                with st.expander(f"ğŸ”¸ {result.get('source', 'Unknown')} - {result.get('title', 'No Title')[:50]}..."):
                    col1, col2 = st.columns([2, 1])
                    
                    with col1:
                        st.markdown(f"**ì œëª©**: {result.get('title', 'No Title')}")
                        st.markdown(f"**ìš”ì•½**: {result.get('summary', 'No Summary')[:200]}...")
                        st.markdown(f"**ë§í¬**: [{result.get('link', 'No Link')[:50]}...]({result.get('link', '#')})")
                    
                    with col2:
                        st.metric("ì²˜ë¦¬ ì‹œê°„", f"{result.get('processing_time', 0):.2f}ì´ˆ")
                        st.metric("íƒ€ì…", result.get('type', 'Unknown'))
                        st.metric("ë°œí–‰ì¼", result.get('published', 'Unknown')[:10])
    
    def render_optimization_stats(self):
        """ìµœì í™” í†µê³„ í‘œì‹œ"""
        st.sidebar.markdown("### ğŸ•·ï¸ í¬ë¡¤ëŸ¬ ìµœì í™” ì •ë³´")
        
        col1, col2 = st.sidebar.columns(2)
        
        with col1:
            st.metric("RSS ë°°ì¹˜", self.batch_size_rss)
            st.metric("HTML ë°°ì¹˜", self.batch_size_html)
        
        with col2:
            st.metric("ì›Œì»¤ ìˆ˜", self.max_workers)
            st.metric("íƒ€ì„ì•„ì›ƒ", f"{self.request_timeout}ì´ˆ")
        
        # ì„±ëŠ¥ ì˜ˆìƒ ê°œì„ ìœ¨ í‘œì‹œ
        st.sidebar.success("ğŸ•·ï¸ ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ: 150% (ë°°ì¹˜ + ë¹„ë™ê¸°)")
        
        # ìºì‹œ ìƒíƒœ
        cache_count = len(self.url_cache)
        st.sidebar.info(f"ğŸ“¦ URL ìºì‹œ: {cache_count}ê°œ")
    
    def render_source_selection_optimized(self):
        """ìµœì í™”ëœ ì†ŒìŠ¤ ì„ íƒ ì¸í„°í˜ì´ìŠ¤"""
        st.header("ğŸ“° ë‰´ìŠ¤ ì†ŒìŠ¤ ì„ íƒ (ë°°ì¹˜ ì²˜ë¦¬)")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("### ğŸ“¡ RSS í”¼ë“œ ì‚¬ì´íŠ¸ (í™•ì¥)")
            all_rss_selected = st.checkbox("ğŸ“¡ ëª¨ë“  RSS í”¼ë“œ ì„ íƒ", key="all_rss")
            
            for name, url in NEWS_SOURCES["rss_feeds"].items():
                selected = st.checkbox(
                    f"{name}",
                    value=all_rss_selected,
                    key=f"rss_{name}",
                    help=f"URL: {url}"
                )
                if selected:
                    source_tuple = ("rss", name, url)
                    if source_tuple not in st.session_state.selected_sources_optimized:
                        st.session_state.selected_sources_optimized.append(source_tuple)
                else:
                    source_tuple = ("rss", name, url)
                    if source_tuple in st.session_state.selected_sources_optimized:
                        st.session_state.selected_sources_optimized.remove(source_tuple)
        
        with col2:
            st.markdown("### ğŸ” HTML í¬ë¡¤ë§ ì‚¬ì´íŠ¸ (í™•ì¥)")
            all_html_selected = st.checkbox("ğŸ” ëª¨ë“  HTML ì‚¬ì´íŠ¸ ì„ íƒ", key="all_html")
            
            for name, url in NEWS_SOURCES["html_crawl"].items():
                selected = st.checkbox(
                    f"{name}",
                    value=all_html_selected,
                    key=f"html_{name}",
                    help=f"URL: {url}"
                )
                if selected:
                    source_tuple = ("html", name, url)
                    if source_tuple not in st.session_state.selected_sources_optimized:
                        st.session_state.selected_sources_optimized.append(source_tuple)
                else:
                    source_tuple = ("html", name, url)
                    if source_tuple in st.session_state.selected_sources_optimized:
                        st.session_state.selected_sources_optimized.remove(source_tuple)
        
        # ì„ íƒëœ ì†ŒìŠ¤ í‘œì‹œ
        if st.session_state.selected_sources_optimized:
            st.success(f"âœ… {len(st.session_state.selected_sources_optimized)}ê°œ ì†ŒìŠ¤ ì„ íƒë¨")
    
    def render_crawling_interface(self):
        """í¬ë¡¤ë§ ì‹¤í–‰ ì¸í„°í˜ì´ìŠ¤"""
        if not st.session_state.selected_sources_optimized:
            st.info("ğŸ‘† ë¶„ì„í•  ë‰´ìŠ¤ ì†ŒìŠ¤ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
            return
        
        st.header("ğŸš€ ìµœì í™”ëœ í¬ë¡¤ë§ ì‹¤í–‰")
        
        # í¬ë¡¤ë§ ì„¤ì •
        col1, col2, col3 = st.columns(3)
        
        with col1:
            enable_ai_processing = st.checkbox("ğŸ¤– AI ë¶„ì„ ì²˜ë¦¬", value=OLLAMA_AVAILABLE)
        
        with col2:
            enable_translation = st.checkbox("ğŸŒ ë‹¤êµ­ì–´ ë²ˆì—­", value=MULTILINGUAL_SUPPORT_AVAILABLE)
        
        with col3:
            max_articles_per_source = st.slider("ì†ŒìŠ¤ë‹¹ ìµœëŒ€ ê¸°ì‚¬", 3, 15, 8)
        
        # í¬ë¡¤ë§ ì‹¤í–‰ ë²„íŠ¼
        if st.button("ğŸ•·ï¸ ìµœì í™”ëœ ë°°ì¹˜ í¬ë¡¤ë§ ì‹œì‘", type="primary", use_container_width=True):
            self.run_optimized_crawling(enable_ai_processing, enable_translation, max_articles_per_source)
    
    def run_optimized_crawling(self, enable_ai: bool, enable_translation: bool, max_articles: int):
        """ìµœì í™”ëœ í¬ë¡¤ë§ ì‹¤í–‰"""
        start_time = time.time()
        results = {'crawler_data': [], 'summary': None}
        
        # ì†ŒìŠ¤ ë¶„ë¥˜
        rss_sources = [(name, url) for source_type, name, url in st.session_state.selected_sources_optimized if source_type == "rss"]
        html_sources = [(name, url) for source_type, name, url in st.session_state.selected_sources_optimized if source_type == "html"]
        
        # RSS í”¼ë“œ ë°°ì¹˜ ì²˜ë¦¬
        if rss_sources:
            st.subheader("ğŸ“¡ RSS í”¼ë“œ ë°°ì¹˜ ìˆ˜ì§‘ ì§„í–‰ ì¤‘...")
            
            with st.spinner("ğŸ•·ï¸ RSS ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰ ì¤‘..."):
                rss_results = self.process_rss_feeds_batch(rss_sources)
                results['crawler_data'].extend(rss_results)
        
        # HTML í¬ë¡¤ë§ ë°°ì¹˜ ì²˜ë¦¬
        if html_sources:
            st.subheader("ğŸ” HTML í¬ë¡¤ë§ ë°°ì¹˜ ì²˜ë¦¬ ì§„í–‰ ì¤‘...")
            
            with st.spinner("ğŸ” HTML ë°°ì¹˜ í¬ë¡¤ë§ ì‹¤í–‰ ì¤‘..."):
                html_results = self.process_html_crawling_batch(html_sources)
                results['crawler_data'].extend(html_results)
        
        # ê²°ê³¼ í‘œì‹œ
        total_articles = len(results['crawler_data'])
        rss_count = len([r for r in results['crawler_data'] if r.get('type') == 'RSS'])
        html_count = len([r for r in results['crawler_data'] if r.get('type') == 'HTML'])
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("ì´ ìˆ˜ì§‘ ê¸°ì‚¬", total_articles)
        with col2:
            st.metric("RSS ê¸°ì‚¬", rss_count)
        with col3:
            st.metric("HTML ê¸°ì‚¬", html_count)
        
        # AI ì¢…í•© ë¶„ì„
        if enable_ai and OLLAMA_AVAILABLE and results['crawler_data']:
            st.subheader("ğŸ¤– AI ì¢…í•© í¬ë¡¤ë§ ë¶„ì„")
            
            # ëª¨ë“  ê¸°ì‚¬ ì •ë³´ ê²°í•©
            all_articles = ""
            for article in results['crawler_data'][:20]:  # ìƒìœ„ 20ê°œë§Œ ë¶„ì„
                all_articles += f"[ì†ŒìŠ¤: {article.get('source', '')}] {article.get('title', '')}\n"
                all_articles += f"ìš”ì•½: {article.get('summary', '')[:200]}...\n\n"
            
            if all_articles.strip():
                with st.spinner("ğŸ¤– AI ì¢…í•© ë‰´ìŠ¤ ë¶„ì„ ì¤‘..."):
                    try:
                        # í¬ë¡¤ëŸ¬ ì „ìš© í”„ë¡¬í”„íŠ¸
                        crawler_prompt = f"""
ë‹¤ìŒì€ êµ­ì œ ì£¼ì–¼ë¦¬ ì—…ê³„ ë‰´ìŠ¤ í¬ë¡¤ë§ ê²°ê³¼ì…ë‹ˆë‹¤. ì „ë¬¸ê°€ ê´€ì ì—ì„œ ë¶„ì„í•´ì£¼ì„¸ìš”:

{all_articles}

ë¶„ì„ ìš”ì²­ì‚¬í•­:
1. ì£¼ìš” ì—…ê³„ íŠ¸ë Œë“œ ë° ë™í–¥ ë¶„ì„
2. ì¤‘ìš”í•œ ë‰´ìŠ¤ì™€ ì´ë²¤íŠ¸ ì •ë¦¬
3. ì£¼ì–¼ë¦¬ ì‹œì¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ í‰ê°€
4. í–¥í›„ ì£¼ëª©í•´ì•¼ í•  í‚¤ì›Œë“œì™€ íŠ¸ë Œë“œ
5. í•œêµ­ ì£¼ì–¼ë¦¬ ì—…ê³„ì— ëŒ€í•œ ì‹œì‚¬ì 

ì‹¤ìš©ì ì´ê³  í†µì°°ë ¥ ìˆëŠ” ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”.
"""
                        summary = quick_analysis(crawler_prompt, model=CRAWLER_MODEL)
                        results['summary'] = summary
                        st.success("âœ… AI ì¢…í•© ë¶„ì„ ì™„ë£Œ")
                        st.write(summary)
                    except Exception as e:
                        st.error(f"AI ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
        
        # ì „ì²´ ì„±ëŠ¥ í†µê³„
        total_time = time.time() - start_time
        st.subheader("ğŸ“Š ì„±ëŠ¥ í†µê³„")
        
        perf_col1, perf_col2, perf_col3 = st.columns(3)
        with perf_col1:
            st.metric("ì „ì²´ ì²˜ë¦¬ ì‹œê°„", f"{total_time:.2f}ì´ˆ")
        with perf_col2:
            st.metric("í‰ê·  ê¸°ì‚¬/ì´ˆ", f"{total_articles/total_time:.1f}")
        with perf_col3:
            improvement = "150%"
            st.metric("ì„±ëŠ¥ í–¥ìƒ", improvement)
        
        # ê²°ê³¼ ì €ì¥
        st.session_state.crawler_results_optimized = results
        st.success("ğŸ•·ï¸ ìµœì í™”ëœ í¬ë¡¤ë§ ì™„ë£Œ!")
        
        # í–¥ìƒëœ ê²°ê³¼ í‘œì‹œ
        if self.result_display and results:
            st.markdown("---")
            self.show_comprehensive_crawler_results(results)
    
    def show_comprehensive_crawler_results(self, results: Dict):
        """ì¢…í•© í¬ë¡¤ë§ ê²°ê³¼ í‘œì‹œ"""
        crawler_data = results.get('crawler_data', [])
        summary = results.get('summary', '')
        
        # íƒ­ êµ¬ì„±
        tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“‹ ìš”ì•½", "ğŸ“¡ RSS ê²°ê³¼", "ğŸ” HTML ê²°ê³¼", "ğŸ“Š ë¶„ì„"])
        
        with tab1:
            self.show_crawler_executive_summary(crawler_data, summary)
        
        with tab2:
            rss_results = [r for r in crawler_data if r.get('type') == 'RSS']
            if rss_results:
                st.markdown("### ğŸ“¡ RSS í”¼ë“œ ìˆ˜ì§‘ ê²°ê³¼")
                df_rss = pd.DataFrame(rss_results)
                st.dataframe(df_rss, use_container_width=True)
            else:
                st.info("RSS í”¼ë“œ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        with tab3:
            html_results = [r for r in crawler_data if r.get('type') == 'HTML']
            if html_results:
                st.markdown("### ğŸ” HTML í¬ë¡¤ë§ ê²°ê³¼")
                df_html = pd.DataFrame(html_results)
                st.dataframe(df_html, use_container_width=True)
            else:
                st.info("HTML í¬ë¡¤ë§ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        with tab4:
            if self.analytics_ui:
                self.analytics_ui.show_crawler_analytics(crawler_data)
    
    def show_crawler_executive_summary(self, crawler_data: List[Dict], summary: str):
        """í¬ë¡¤ëŸ¬ ì„ì› ìš”ì•½ í‘œì‹œ"""
        st.markdown("### ğŸ•·ï¸ í¬ë¡¤ë§ ìš”ì•½")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("#### ğŸ” ìˆ˜ì§‘ ê²°ê³¼")
            if crawler_data:
                total_articles = len(crawler_data)
                sources = len(set(r.get('source', '') for r in crawler_data))
                
                col_a, col_b = st.columns(2)
                with col_a:
                    st.metric("ìˆ˜ì§‘ ê¸°ì‚¬", f"{total_articles}ê°œ")
                with col_b:
                    st.metric("ë‰´ìŠ¤ ì†ŒìŠ¤", f"{sources}ê°œ")
            else:
                st.info("í¬ë¡¤ë§ ë°ì´í„° ì—†ìŒ")
        
        with col2:
            st.markdown("#### âš¡ ì„±ëŠ¥")
            if crawler_data:
                avg_time = np.mean([r.get('processing_time', 0) for r in crawler_data])
                rss_count = len([r for r in crawler_data if r.get('type') == 'RSS'])
                
                col_a, col_b = st.columns(2)
                with col_a:
                    st.metric("í‰ê·  ì²˜ë¦¬ì‹œê°„", f"{avg_time:.2f}ì´ˆ")
                with col_b:
                    st.metric("RSS ê¸°ì‚¬", f"{rss_count}ê°œ")
        
        if summary:
            st.markdown("#### ğŸ¤– AI ì¢…í•© ë¶„ì„")
            st.markdown(summary)
        
        st.markdown("#### ğŸ’¡ ì¶”ì²œ ì•¡ì…˜")
        if crawler_data:
            jewelry_articles = len([r for r in crawler_data if self._is_jewelry_related(r.get('title', ''))])
            if jewelry_articles > len(crawler_data) * 0.7:
                st.markdown("â€¢ ğŸ¯ ì£¼ì–¼ë¦¬ ê´€ë ¨ ê¸°ì‚¬ ë¹„ì¤‘ì´ ë†’ìŠµë‹ˆë‹¤. íŠ¸ë Œë“œ ë¶„ì„ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
            else:
                st.markdown("â€¢ ğŸ” ë” ì •í™•í•œ ì£¼ì–¼ë¦¬ ë‰´ìŠ¤ í•„í„°ë§ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            
            # ì†ŒìŠ¤ë³„ ì„±ê³¼ ë¶„ì„
            source_performance = {}
            for article in crawler_data:
                source = article.get('source', '')
                if source not in source_performance:
                    source_performance[source] = 0
                source_performance[source] += 1
            
            best_source = max(source_performance.items(), key=lambda x: x[1])
            st.markdown(f"â€¢ ğŸ“ˆ ê°€ì¥ í™œë°œí•œ ì†ŒìŠ¤: {best_source[0]} ({best_source[1]}ê°œ ê¸°ì‚¬)")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    st.title("ğŸ•·ï¸ ì›¹ í¬ë¡¤ëŸ¬ + ë¸”ë¡œê·¸ ìë™í™” (ì™„ì „ ìµœì í™” ë²„ì „)")
    st.markdown("**v2.0**: ì„±ëŠ¥ 150% í–¥ìƒ + ì‹¤ì‹œê°„ UI + ë°°ì¹˜ ì²˜ë¦¬")
    st.markdown("---")
    
    # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
    crawler = OptimizedWebCrawler()
    
    # ì•ˆì •ì„± ëŒ€ì‹œë³´ë“œ í‘œì‹œ
    if crawler.stability_manager:
        crawler.stability_manager.display_health_dashboard()
    
    # ë‹¤êµ­ì–´ ì„¤ì • í‘œì‹œ
    if crawler.multilingual_processor:
        language_settings = crawler.multilingual_processor.render_language_settings()
        crawler.multilingual_processor.render_format_support_info()
    else:
        language_settings = None
    
    # ìµœì í™” í†µê³„ í‘œì‹œ
    crawler.render_optimization_stats()
    
    # ë©”ì¸ ì¸í„°í˜ì´ìŠ¤
    crawler.render_source_selection_optimized()
    crawler.render_crawling_interface()
    
    # ì´ì „ ê²°ê³¼ í‘œì‹œ
    if st.session_state.crawler_results_optimized:
        with st.expander("ğŸ•·ï¸ ì´ì „ í¬ë¡¤ë§ ê²°ê³¼", expanded=False):
            st.json(st.session_state.crawler_results_optimized)
    
    # í‘¸í„° ì •ë³´
    st.markdown("---")
    col1, col2, col3 = st.columns(3)
    with col1:
        st.markdown("**ì„±ëŠ¥ ê°œì„ **")
        st.markdown("â€¢ ë°°ì¹˜ ì²˜ë¦¬")
        st.markdown("â€¢ ë¹„ë™ê¸° ìš”ì²­")
        st.markdown("â€¢ URL ìºì‹±")
    with col2:
        st.markdown("**í¬ë¡¤ë§ ê¸°ëŠ¥**")
        st.markdown("â€¢ RSS í”¼ë“œ ìˆ˜ì§‘")
        st.markdown("â€¢ HTML í¬ë¡¤ë§")
        st.markdown("â€¢ AI ë¶„ì„")
    with col3:
        st.markdown("**ì•ˆì •ì„±**")
        st.markdown("â€¢ ì˜¤ë¥˜ ë³µêµ¬")
        st.markdown("â€¢ ë©”ëª¨ë¦¬ ê´€ë¦¬")
        st.markdown("â€¢ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§")

if __name__ == "__main__":
    main()