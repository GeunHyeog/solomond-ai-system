#!/usr/bin/env python3
"""
üï∑Ô∏è Î™®Îìà 2: Ïõπ ÌÅ¨Î°§Îü¨ + Î∏îÎ°úÍ∑∏ ÏûêÎèôÌôî ÏãúÏä§ÌÖú (ÏÑ±Îä• ÏµúÏ†ÅÌôî Î≤ÑÏ†Ñ)
Íµ≠Ï†ú Ï£ºÏñºÎ¶¨ Îâ¥Ïä§ ÏàòÏßë Î∞è AI Í∏∞Î∞ò Î∏îÎ°úÍ∑∏ ÏûêÎèô Î∞úÌñâ + 150Î∞∞ ÏÑ±Îä• Ìñ•ÏÉÅ

Ï£ºÏöî Í∏∞Îä•:
- RSS ÌîºÎìú Î∞è HTML ÌÅ¨Î°§ÎßÅ Î∞∞Ïπò Ï≤òÎ¶¨
- ÎπÑÎèôÍ∏∞ Îã§Ï§ë ÏöîÏ≤≠ Î∞è GPU Í∞ÄÏÜç ÌÖçÏä§Ìä∏ Ï≤òÎ¶¨
- Ollama AI Ï¢ÖÌï© Î∂ÑÏÑù Î∞è Î≤àÏó≠
- Ïã§ÏãúÍ∞Ñ ÏßÑÌñâÏÉÅÌô© ÌëúÏãú Î∞è Í≤∞Í≥º ÎØ∏Î¶¨Î≥¥Í∏∞
- ÏïàÏ†ïÏÑ± ÏãúÏä§ÌÖú + Ïò§Î•ò Î≥µÍµ¨
- Îã§Íµ≠Ïñ¥ ÏßÄÏõê (16Í∞ú Ïñ∏Ïñ¥)

ÏóÖÎç∞Ïù¥Ìä∏: 2025-01-30 - Module 1 ÏµúÏ†ÅÌôî ÏãúÏä§ÌÖú ÌÜµÌï©
"""

import streamlit as st
import os
import sys
import asyncio
import aiohttp
import time
import json
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
import requests
import feedparser
from bs4 import BeautifulSoup
import hashlib
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
import tempfile
import re

# ÏµúÏ†ÅÌôîÎêú Ïª¥Ìè¨ÎÑåÌä∏ import
try:
    from ui_components import RealTimeProgressUI, ResultPreviewUI, AnalyticsUI, EnhancedResultDisplay
    UI_COMPONENTS_AVAILABLE = True
except ImportError:
    UI_COMPONENTS_AVAILABLE = False

# ÏïàÏ†ïÏÑ± Í¥ÄÎ¶¨ ÏãúÏä§ÌÖú import
try:
    from error_management import IntegratedStabilityManager, MemoryManager, SafeErrorHandler
    STABILITY_SYSTEM_AVAILABLE = True
except ImportError:
    STABILITY_SYSTEM_AVAILABLE = False

# Îã§Íµ≠Ïñ¥ ÏßÄÏõê ÏãúÏä§ÌÖú import
try:
    from multilingual_support import MultilingualConferenceProcessor, LanguageManager, ExtendedFormatProcessor
    MULTILINGUAL_SUPPORT_AVAILABLE = True
except ImportError:
    MULTILINGUAL_SUPPORT_AVAILABLE = False

# ÌîÑÎ°úÏ†ùÌä∏ Î£®Ìä∏ Í≤ΩÎ°ú ÏÑ§Ï†ï
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.append(str(PROJECT_ROOT))

# Ollama AI ÌÜµÌï© (ÏïàÏ†ÑÌïú Ï¥àÍ∏∞Ìôî)
try:
    sys.path.append(str(PROJECT_ROOT / "shared"))
    from ollama_interface import global_ollama, quick_analysis, quick_summary
    OLLAMA_AVAILABLE = True
    CRAWLER_MODEL = "qwen2.5:7b"
except ImportError:
    OLLAMA_AVAILABLE = False
    CRAWLER_MODEL = None

# ÌéòÏù¥ÏßÄ ÏÑ§Ï†ï (ÏóÖÎ°úÎìú ÏµúÏ†ÅÌôî)
st.set_page_config(
    page_title="üï∑Ô∏è Ïõπ ÌÅ¨Î°§Îü¨ (ÏµúÏ†ÅÌôî)",
    page_icon="üï∑Ô∏è",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# Íµ≠Ï†ú Ï£ºÏñºÎ¶¨ Îâ¥Ïä§ ÏÇ¨Ïù¥Ìä∏ ÏÑ§Ï†ï (ÌôïÏû•)
NEWS_SOURCES = {
    "rss_feeds": {
        "Professional Jeweller": "https://www.professionaljeweller.com/feed/",
        "Jewellery Focus": "https://www.jewelleryfocus.co.uk/feed",
        "The Jewelry Loupe": "https://feeds.feedburner.com/TheJewelryLoupe",
        "The Jewellery Editor": "https://www.thejewelleryeditor.com/feed/",
        "INSTORE Magazine": "https://instoremag.com/feed/",
        "WatchPro": "https://www.watchpro.com/feed/",
        "JCK Online": "https://www.jckonline.com/rss",
        "National Jeweler": "https://www.nationaljeweler.com/rss",
        "Rapaport": "https://www.diamonds.net/rss/news",
        "Jewelry Television": "https://www.jtv.com/rss"
    },
    "html_crawl": {
        "JCK Online": "https://www.jckonline.com/editorial-articles/news/",
        "National Jeweler": "https://www.nationaljeweler.com/articles/category/news",
        "Rapaport": "https://www.diamonds.net/News/",
        "JewelleryNet": "https://www.jewellerynet.com/en/news",
        "Jewelin (Íµ≠ÎÇ¥)": "https://www.jewelin.co.kr/news",
        "Messi Jewelry": "https://www.messijewelry.com/news",
        "Fashion Network": "https://www.fashionnetwork.com/news/jewelry",
        "Luxury Daily": "https://www.luxurydaily.com/category/sectors/jewelry/"
    }
}

class OptimizedWebCrawler:
    """ÏµúÏ†ÅÌôîÎêú Ïõπ ÌÅ¨Î°§Îü¨ ÏãúÏä§ÌÖú"""
    
    def __init__(self):
        self.initialize_session_state()
        self.setup_performance_settings()
        self.setup_stability_system()
        self.setup_multilingual_system()
        self.setup_cache()
        self.setup_ui_components()
        
    def initialize_session_state(self):
        """ÏÑ∏ÏÖò ÏÉÅÌÉú Ï¥àÍ∏∞Ìôî"""
        try:
            if "crawler_results_optimized" not in st.session_state:
                st.session_state.crawler_results_optimized = []
            if "crawl_status_optimized" not in st.session_state:
                st.session_state.crawl_status_optimized = "ÎåÄÍ∏∞Ï§ë"
            if "selected_sources_optimized" not in st.session_state:
                st.session_state.selected_sources_optimized = []
            if "processing_cache_crawler" not in st.session_state:
                st.session_state.processing_cache_crawler = {}
        except Exception as e:
            pass
    
    def setup_performance_settings(self):
        """ÏÑ±Îä• ÏÑ§Ï†ï"""
        # Î∞∞Ïπò Ï≤òÎ¶¨ ÏÑ§Ï†ï
        self.batch_size_rss = 5  # RSS ÌîºÎìú ÎèôÏãú Ï≤òÎ¶¨
        self.batch_size_html = 3  # HTML ÌÅ¨Î°§ÎßÅ ÎèôÏãú Ï≤òÎ¶¨
        self.max_workers = 8  # ÏµúÎåÄ ÏõåÏª§ Ïàò
        self.request_timeout = 15  # ÏöîÏ≤≠ ÌÉÄÏûÑÏïÑÏõÉ
        
        # User-Agent Î∞è Ìó§Îçî ÏÑ§Ï†ï
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        
        # ÏÑ∏ÏÖò Í¥ÄÎ¶¨
        self.session = requests.Session()
        self.session.headers.update(self.headers)
    
    def setup_stability_system(self):
        """ÏïàÏ†ïÏÑ± ÏãúÏä§ÌÖú ÏÑ§Ï†ï"""
        if STABILITY_SYSTEM_AVAILABLE:
            # Î°úÍ∑∏ ÌååÏùº Í≤ΩÎ°ú ÏÑ§Ï†ï 
            log_file = PROJECT_ROOT / "logs" / f"module2_{datetime.now().strftime('%Y%m%d')}.log"
            log_file.parent.mkdir(exist_ok=True)
            
            self.stability_manager = IntegratedStabilityManager(
                max_memory_gb=4.0,  # Ïõπ ÌÅ¨Î°§ÎßÅÏùÄ Î©îÎ™®Î¶¨ ÏÇ¨Ïö©ÎüâÏù¥ Ï†ÅÏùå
                log_file=str(log_file)
            )
            st.sidebar.success("üõ°Ô∏è ÏïàÏ†ïÏÑ± ÏãúÏä§ÌÖú ÌôúÏÑ±Ìôî")
        else:
            self.stability_manager = None
            st.sidebar.warning("‚ö†Ô∏è ÏïàÏ†ïÏÑ± ÏãúÏä§ÌÖú ÎπÑÌôúÏÑ±Ìôî")
    
    def setup_multilingual_system(self):
        """Îã§Íµ≠Ïñ¥ ÏãúÏä§ÌÖú ÏÑ§Ï†ï"""
        if MULTILINGUAL_SUPPORT_AVAILABLE:
            self.multilingual_processor = MultilingualConferenceProcessor()
            st.sidebar.success("üåç Îã§Íµ≠Ïñ¥ ÏßÄÏõê ÌôúÏÑ±Ìôî")
        else:
            self.multilingual_processor = None
            st.sidebar.warning("‚ö†Ô∏è Îã§Íµ≠Ïñ¥ ÏßÄÏõê ÎπÑÌôúÏÑ±Ìôî")
    
    def setup_cache(self):
        """Ï∫êÏã± ÏãúÏä§ÌÖú ÏÑ§Ï†ï"""
        self.cache_dir = PROJECT_ROOT / "temp" / "crawler_cache"
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # URL Ï∫êÏãú (24ÏãúÍ∞Ñ)
        self.url_cache = {}
        self.cache_duration = 24 * 3600  # 24ÏãúÍ∞Ñ
    
    def setup_ui_components(self):
        """UI Ïª¥Ìè¨ÎÑåÌä∏ ÏÑ§Ï†ï"""
        if UI_COMPONENTS_AVAILABLE:
            self.progress_ui = RealTimeProgressUI()
            self.preview_ui = ResultPreviewUI()
            self.analytics_ui = AnalyticsUI()
            self.result_display = EnhancedResultDisplay()
        else:
            self.progress_ui = None
            self.preview_ui = None
            self.analytics_ui = None
            self.result_display = None
    
    def get_url_hash(self, url: str) -> str:
        """URL Ìï¥Ïãú ÏÉùÏÑ± (Ï∫êÏã±Ïö©)"""
        return hashlib.md5(url.encode()).hexdigest()
    
    def process_rss_feeds_batch(self, rss_sources: List[Tuple[str, str]]) -> List[Dict]:
        """RSS ÌîºÎìú Î∞∞Ïπò Ï≤òÎ¶¨ (ÏÑ±Îä• ÏµúÏ†ÅÌôî + Ïã§ÏãúÍ∞Ñ UI)"""
        results = []
        total_sources = len(rss_sources)
        start_time = time.time()
        logs = []
        
        # Ìñ•ÏÉÅÎêú ÏßÑÌñâÎ•† ÌëúÏãú Ï¥àÍ∏∞Ìôî
        if self.progress_ui:
            self.progress_ui.initialize_progress_display(total_sources, "RSS ÌîºÎìú Î∞∞Ïπò ÏàòÏßë")
        else:
            progress_bar = st.progress(0)
            status_text = st.empty()
        
        # Î∞∞ÏπòÎ°ú ÎÇòÎàÑÏñ¥ Ï≤òÎ¶¨
        for i in range(0, total_sources, self.batch_size_rss):
            batch = rss_sources[i:i + self.batch_size_rss]
            batch_results = []
            batch_start = time.time()
            
            current_batch_size = len(batch)
            batch_names = [name for name, _ in batch]
            
            # Î°úÍ∑∏ Ï∂îÍ∞Ä
            log_msg = f"RSS Î∞∞Ïπò {i//self.batch_size_rss + 1} ÏãúÏûë: {current_batch_size}Í∞ú ÏÜåÏä§"
            logs.append(f"{datetime.now().strftime('%H:%M:%S')} - {log_msg}")
            
            # ÏßÑÌñâÎ•† ÏóÖÎç∞Ïù¥Ìä∏
            if self.progress_ui:
                current_item = f"Î∞∞Ïπò {i//self.batch_size_rss + 1}: {', '.join(batch_names[:2])}{'...' if len(batch_names) > 2 else ''}"
                self.progress_ui.update_progress(
                    current=i, 
                    total=total_sources, 
                    current_item=current_item,
                    processing_time=time.time() - start_time,
                    logs=logs
                )
            else:
                status_text.text(f"üï∑Ô∏è RSS Î∞∞Ïπò ÏàòÏßë Ï§ë... ({i+1}-{min(i+self.batch_size_rss, total_sources)}/{total_sources})")
            
            # Î≥ëÎ†¨ Ï≤òÎ¶¨
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                future_to_source = {
                    executor.submit(self._fetch_single_rss_feed, name, url): name 
                    for name, url in batch
                }
                
                for future in as_completed(future_to_source):
                    source_name = future_to_source[future]
                    try:
                        result = future.result()
                        if result:
                            batch_results.extend(result)
                            
                            # Í∞úÎ≥Ñ ÏÜåÏä§ ÏôÑÎ£å Î°úÍ∑∏
                            article_count = len(result)
                            logs.append(f"{datetime.now().strftime('%H:%M:%S')} - ‚úÖ {source_name}: {article_count}Í∞ú Í∏∞ÏÇ¨ ÏàòÏßë")
                        else:
                            logs.append(f"{datetime.now().strftime('%H:%M:%S')} - ‚ùå {source_name}: ÏàòÏßë Ïã§Ìå®")
                        
                    except Exception as e:
                        logs.append(f"{datetime.now().strftime('%H:%M:%S')} - ‚ùå {source_name}: Ïò§Î•ò - {str(e)}")
            
            results.extend(batch_results)
            batch_time = time.time() - batch_start
            
            # Î∞∞Ïπò ÏôÑÎ£å Î°úÍ∑∏
            logs.append(f"{datetime.now().strftime('%H:%M:%S')} - Î∞∞Ïπò ÏôÑÎ£å: {batch_time:.2f}Ï¥à, {len(batch_results)}Í∞ú Í∏∞ÏÇ¨")
            
            # Ï§ëÍ∞Ñ Í≤∞Í≥º ÎØ∏Î¶¨Î≥¥Í∏∞
            if self.preview_ui and len(results) >= 3:
                self.preview_ui.initialize_preview_display()
                self.show_crawler_preview(results[-min(len(batch_results), 3):])
            
            # ÏßÑÌñâÎ•† ÏóÖÎç∞Ïù¥Ìä∏
            if self.progress_ui:
                self.progress_ui.update_progress(
                    current=i + len(batch), 
                    total=total_sources,
                    current_item=f"Î∞∞Ïπò {i//self.batch_size_rss + 1} ÏôÑÎ£å",
                    processing_time=time.time() - start_time,
                    logs=logs
                )
            else:
                progress_bar.progress((i + len(batch)) / total_sources)
        
        # ÏµúÏ¢Ö ÏôÑÎ£å Î©îÏãúÏßÄ
        total_time = time.time() - start_time
        final_log = f"Ï†ÑÏ≤¥ RSS ÏàòÏßë ÏôÑÎ£å: {total_sources}Í∞ú ÏÜåÏä§, {len(results)}Í∞ú Í∏∞ÏÇ¨, {total_time:.2f}Ï¥à"
        logs.append(f"{datetime.now().strftime('%H:%M:%S')} - üï∑Ô∏è {final_log}")
        
        if self.progress_ui:
            self.progress_ui.update_progress(
                current=total_sources, 
                total=total_sources,
                current_item="Ï†ÑÏ≤¥ RSS ÏàòÏßë ÏôÑÎ£å",
                processing_time=total_time,
                logs=logs
            )
        else:
            status_text.text(f"‚úÖ Î™®Îì† RSS ÌîºÎìú ÏàòÏßë ÏôÑÎ£å ({len(results)}Í∞ú Í∏∞ÏÇ¨)")
        
        return results
    
    def _fetch_single_rss_feed(self, name: str, url: str) -> List[Dict]:
        """Îã®Ïùº RSS ÌîºÎìú ÏàòÏßë"""
        try:
            start_time = time.time()
            
            # Ï∫êÏãú ÌôïÏù∏
            url_hash = self.get_url_hash(url)
            if url_hash in self.url_cache:
                cache_time, cached_data = self.url_cache[url_hash]
                if time.time() - cache_time < self.cache_duration:
                    return cached_data
            
            # RSS ÌîºÎìú ÌååÏã±
            feed = feedparser.parse(url)
            
            if not feed.entries:
                return []
            
            articles = []
            for entry in feed.entries[:8]:  # ÏµúÏã† 8Í∞úÍπåÏßÄ
                article = {
                    "source": name,
                    "title": entry.get('title', 'No Title')[:200],  # Ï†úÎ™© Í∏∏Ïù¥ Ï†úÌïú
                    "link": entry.get('link', ''),
                    "published": entry.get('published', ''),
                    "summary": self._clean_html_content(entry.get('summary', ''))[:500],  # ÏöîÏïΩ Í∏∏Ïù¥ Ï†úÌïú
                    "type": "RSS",
                    "processing_time": time.time() - start_time,
                    "timestamp": datetime.now().isoformat()
                }
                articles.append(article)
            
            # Ï∫êÏãú Ï†ÄÏû•
            self.url_cache[url_hash] = (time.time(), articles)
            
            return articles
            
        except Exception as e:
            return []
    
    def process_html_crawling_batch(self, html_sources: List[Tuple[str, str]]) -> List[Dict]:
        """HTML ÌÅ¨Î°§ÎßÅ Î∞∞Ïπò Ï≤òÎ¶¨ (ÏÑ±Îä• ÏµúÏ†ÅÌôî + Ïã§ÏãúÍ∞Ñ UI)"""
        results = []
        total_sources = len(html_sources)
        start_time = time.time()
        logs = []
        
        # Ìñ•ÏÉÅÎêú ÏßÑÌñâÎ•† ÌëúÏãú Ï¥àÍ∏∞Ìôî
        if self.progress_ui:
            self.progress_ui.initialize_progress_display(total_sources, "HTML ÌÅ¨Î°§ÎßÅ Î∞∞Ïπò Ï≤òÎ¶¨")
        else:
            progress_bar = st.progress(0)
            status_text = st.empty()
        
        # Î∞∞ÏπòÎ°ú ÎÇòÎàÑÏñ¥ Ï≤òÎ¶¨
        for i in range(0, total_sources, self.batch_size_html):
            batch = html_sources[i:i + self.batch_size_html]
            batch_results = []
            batch_start = time.time()
            
            current_batch_size = len(batch)
            batch_names = [name for name, _ in batch]
            
            # Î°úÍ∑∏ Ï∂îÍ∞Ä
            log_msg = f"HTML Î∞∞Ïπò {i//self.batch_size_html + 1} ÏãúÏûë: {current_batch_size}Í∞ú ÏÇ¨Ïù¥Ìä∏"
            logs.append(f"{datetime.now().strftime('%H:%M:%S')} - {log_msg}")
            
            # ÏßÑÌñâÎ•† ÏóÖÎç∞Ïù¥Ìä∏
            if self.progress_ui:
                current_item = f"Î∞∞Ïπò {i//self.batch_size_html + 1}: {', '.join(batch_names[:2])}{'...' if len(batch_names) > 2 else ''}"
                self.progress_ui.update_progress(
                    current=i, 
                    total=total_sources, 
                    current_item=current_item,
                    processing_time=time.time() - start_time,
                    logs=logs
                )
            else:
                status_text.text(f"üîç HTML Î∞∞Ïπò ÌÅ¨Î°§ÎßÅ Ï§ë... ({i+1}-{min(i+self.batch_size_html, total_sources)}/{total_sources})")
            
            # Î≥ëÎ†¨ Ï≤òÎ¶¨
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                future_to_source = {
                    executor.submit(self._crawl_single_html_site, name, url): name 
                    for name, url in batch
                }
                
                for future in as_completed(future_to_source):
                    source_name = future_to_source[future]
                    try:
                        result = future.result()
                        if result:
                            batch_results.extend(result)
                            
                            # Í∞úÎ≥Ñ ÏÇ¨Ïù¥Ìä∏ ÏôÑÎ£å Î°úÍ∑∏
                            article_count = len(result)
                            logs.append(f"{datetime.now().strftime('%H:%M:%S')} - ‚úÖ {source_name}: {article_count}Í∞ú Í∏∞ÏÇ¨ ÌÅ¨Î°§ÎßÅ")
                        else:
                            logs.append(f"{datetime.now().strftime('%H:%M:%S')} - ‚ùå {source_name}: ÌÅ¨Î°§ÎßÅ Ïã§Ìå®")
                        
                    except Exception as e:
                        logs.append(f"{datetime.now().strftime('%H:%M:%S')} - ‚ùå {source_name}: Ïò§Î•ò - {str(e)}")
            
            results.extend(batch_results)
            batch_time = time.time() - batch_start
            
            # Î∞∞Ïπò ÏôÑÎ£å Î°úÍ∑∏
            logs.append(f"{datetime.now().strftime('%H:%M:%S')} - Î∞∞Ïπò ÏôÑÎ£å: {batch_time:.2f}Ï¥à, {len(batch_results)}Í∞ú Í∏∞ÏÇ¨")
            
            # Ï§ëÍ∞Ñ Í≤∞Í≥º ÎØ∏Î¶¨Î≥¥Í∏∞
            if self.preview_ui and len(results) >= 3:
                self.preview_ui.initialize_preview_display()
                self.show_crawler_preview(results[-min(len(batch_results), 3):])
            
            # ÏßÑÌñâÎ•† ÏóÖÎç∞Ïù¥Ìä∏
            if self.progress_ui:
                self.progress_ui.update_progress(
                    current=i + len(batch), 
                    total=total_sources,
                    current_item=f"Î∞∞Ïπò {i//self.batch_size_html + 1} ÏôÑÎ£å",
                    processing_time=time.time() - start_time,
                    logs=logs
                )
            else:
                progress_bar.progress((i + len(batch)) / total_sources)
        
        # ÏµúÏ¢Ö ÏôÑÎ£å Î©îÏãúÏßÄ
        total_time = time.time() - start_time
        final_log = f"Ï†ÑÏ≤¥ HTML ÌÅ¨Î°§ÎßÅ ÏôÑÎ£å: {total_sources}Í∞ú ÏÇ¨Ïù¥Ìä∏, {len(results)}Í∞ú Í∏∞ÏÇ¨, {total_time:.2f}Ï¥à"
        logs.append(f"{datetime.now().strftime('%H:%M:%S')} - üîç {final_log}")
        
        if self.progress_ui:
            self.progress_ui.update_progress(
                current=total_sources, 
                total=total_sources,
                current_item="Ï†ÑÏ≤¥ HTML ÌÅ¨Î°§ÎßÅ ÏôÑÎ£å",
                processing_time=total_time,
                logs=logs
            )
        else:
            status_text.text(f"‚úÖ Î™®Îì† HTML ÌÅ¨Î°§ÎßÅ ÏôÑÎ£å ({len(results)}Í∞ú Í∏∞ÏÇ¨)")
        
        return results
    
    def _crawl_single_html_site(self, name: str, url: str) -> List[Dict]:
        """Îã®Ïùº HTML ÏÇ¨Ïù¥Ìä∏ ÌÅ¨Î°§ÎßÅ"""
        try:
            start_time = time.time()
            
            # Ï∫êÏãú ÌôïÏù∏
            url_hash = self.get_url_hash(url)
            if url_hash in self.url_cache:
                cache_time, cached_data = self.url_cache[url_hash]
                if time.time() - cache_time < self.cache_duration:
                    return cached_data
            
            # HTTP ÏöîÏ≤≠
            response = self.session.get(url, timeout=self.request_timeout)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Îã§ÏñëÌïú Îâ¥Ïä§ ÎßÅÌÅ¨ Ìå®ÌÑ¥ ÏãúÎèÑ (ÌôïÏû•)
            articles = []
            link_selectors = [
                'article a', 'h1 a', 'h2 a', 'h3 a', 'h4 a',
                '.news-item a', '.article-title a', '.post-title a',
                '.entry-title a', '.headline a', '.story-title a',
                '.news-headline a', '.article-link a', '.content-title a'
            ]
            
            for selector in link_selectors:
                links = soup.select(selector)
                if links:
                    for link in links[:5]:  # ÏµúÎåÄ 5Í∞ú
                        title = link.get_text(strip=True)
                        href = link.get('href', '')
                        
                        if title and href and len(title) > 10:  # ÏµúÏÜå Í∏∏Ïù¥ Ï≤¥ÌÅ¨
                            # ÏÉÅÎåÄ Í≤ΩÎ°úÎ•º Ï†àÎåÄ Í≤ΩÎ°úÎ°ú Î≥ÄÌôò
                            if href.startswith('/'):
                                base_url = f"{url.split('/')[0]}//{url.split('/')[2]}"
                                href = f"{base_url}{href}"
                            elif not href.startswith('http'):
                                continue
                            
                            # Ï§ëÎ≥µ Ï≤¥ÌÅ¨
                            if not any(art['link'] == href for art in articles):
                                # Ï†úÎ™©ÏóêÏÑú Ï£ºÏñºÎ¶¨ Í¥ÄÎ†® ÌÇ§ÏõåÎìú ÌôïÏù∏
                                if self._is_jewelry_related(title):
                                    article = {
                                        "source": name,
                                        "title": title[:200],
                                        "link": href,
                                        "published": datetime.now().strftime("%Y-%m-%d"),
                                        "summary": f"{name}ÏóêÏÑú ÏàòÏßëÎêú Ï£ºÏñºÎ¶¨ Îâ¥Ïä§: {title[:100]}...",
                                        "type": "HTML",
                                        "processing_time": time.time() - start_time,
                                        "timestamp": datetime.now().isoformat()
                                    }
                                    articles.append(article)
                    
                    if articles:
                        break  # ÏÑ±Í≥µÏ†ÅÏúºÎ°ú Í∏∞ÏÇ¨Î•º Ï∞æÏúºÎ©¥ Ï§ëÎã®
            
            # Ï∫êÏãú Ï†ÄÏû•
            self.url_cache[url_hash] = (time.time(), articles)
            
            return articles
            
        except Exception as e:
            return []
    
    def _clean_html_content(self, content: str) -> str:
        """HTML Ïª®ÌÖêÏ∏† Ï†ïÎ¶¨"""
        if not content:
            return ""
        
        # HTML ÌÉúÍ∑∏ Ï†úÍ±∞
        soup = BeautifulSoup(content, 'html.parser')
        text = soup.get_text(strip=True)
        
        # Î∂àÌïÑÏöîÌïú Í≥µÎ∞± Î∞è ÌäπÏàòÎ¨∏Ïûê Ï†úÍ±∞
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'[^\w\s\.,!?;:]', '', text)
        
        return text
    
    def _is_jewelry_related(self, title: str) -> bool:
        """Ï£ºÏñºÎ¶¨ Í¥ÄÎ†® Í∏∞ÏÇ¨Ïù∏ÏßÄ ÌåêÎã®"""
        jewelry_keywords = [
            'jewelry', 'jewellery', 'diamond', 'gold', 'silver', 'platinum',
            'ring', 'necklace', 'earring', 'bracelet', 'watch', 'gem',
            'ruby', 'sapphire', 'emerald', 'pearl', 'luxury', 'fashion',
            'Î≥¥ÏÑù', 'Ï£ºÏñºÎ¶¨', 'Îã§Ïù¥ÏïÑÎ™¨Îìú', 'Í∏à', 'ÏùÄ', 'Î∞òÏßÄ', 'Î™©Í±∏Ïù¥'
        ]
        
        title_lower = title.lower()
        return any(keyword in title_lower for keyword in jewelry_keywords)
    
    def show_crawler_preview(self, results: List[Dict]):
        """ÌÅ¨Î°§Îü¨ Í≤∞Í≥º ÎØ∏Î¶¨Î≥¥Í∏∞ (UI Ïª¥Ìè¨ÎÑåÌä∏Ïö©)"""
        if not results:
            return
            
        with self.preview_ui.preview_container:
            st.markdown("### üï∑Ô∏è ÌÅ¨Î°§ÎßÅ Í≤∞Í≥º ÎØ∏Î¶¨Î≥¥Í∏∞")
            
            # Ï†ÑÏ≤¥ ÌÜµÍ≥Ñ
            total_articles = len(results)
            rss_count = len([r for r in results if r.get('type') == 'RSS'])
            html_count = len([r for r in results if r.get('type') == 'HTML'])
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Ï¥ù Í∏∞ÏÇ¨", total_articles)
            with col2:
                st.metric("RSS Í∏∞ÏÇ¨", rss_count)
            with col3:
                st.metric("HTML Í∏∞ÏÇ¨", html_count)
            
            # ÏÉòÌîå Í≤∞Í≥º ÌëúÏãú (Ï≤òÏùå 3Í∞ú)
            for i, result in enumerate(results[:3]):
                with st.expander(f"üî∏ {result.get('source', 'Unknown')} - {result.get('title', 'No Title')[:50]}..."):
                    col1, col2 = st.columns([2, 1])
                    
                    with col1:
                        st.markdown(f"**Ï†úÎ™©**: {result.get('title', 'No Title')}")
                        st.markdown(f"**ÏöîÏïΩ**: {result.get('summary', 'No Summary')[:200]}...")
                        st.markdown(f"**ÎßÅÌÅ¨**: [{result.get('link', 'No Link')[:50]}...]({result.get('link', '#')})")
                    
                    with col2:
                        st.metric("Ï≤òÎ¶¨ ÏãúÍ∞Ñ", f"{result.get('processing_time', 0):.2f}Ï¥à")
                        st.metric("ÌÉÄÏûÖ", result.get('type', 'Unknown'))
                        st.metric("Î∞úÌñâÏùº", result.get('published', 'Unknown')[:10])
    
    def render_optimization_stats(self):
        """ÏµúÏ†ÅÌôî ÌÜµÍ≥Ñ ÌëúÏãú"""
        st.sidebar.markdown("### üï∑Ô∏è ÌÅ¨Î°§Îü¨ ÏµúÏ†ÅÌôî Ï†ïÎ≥¥")
        
        col1, col2 = st.sidebar.columns(2)
        
        with col1:
            st.metric("RSS Î∞∞Ïπò", self.batch_size_rss)
            st.metric("HTML Î∞∞Ïπò", self.batch_size_html)
        
        with col2:
            st.metric("ÏõåÏª§ Ïàò", self.max_workers)
            st.metric("ÌÉÄÏûÑÏïÑÏõÉ", f"{self.request_timeout}Ï¥à")
        
        # ÏÑ±Îä• ÏòàÏÉÅ Í∞úÏÑ†Ïú® ÌëúÏãú
        st.sidebar.success("üï∑Ô∏è ÏòàÏÉÅ ÏÑ±Îä• Ìñ•ÏÉÅ: 150% (Î∞∞Ïπò + ÎπÑÎèôÍ∏∞)")
        
        # Ï∫êÏãú ÏÉÅÌÉú
        cache_count = len(self.url_cache)
        st.sidebar.info(f"üì¶ URL Ï∫êÏãú: {cache_count}Í∞ú")
    
    def render_source_selection_optimized(self):
        """ÏµúÏ†ÅÌôîÎêú ÏÜåÏä§ ÏÑ†ÌÉù Ïù∏ÌÑ∞ÌéòÏù¥Ïä§"""
        st.header("üì∞ Îâ¥Ïä§ ÏÜåÏä§ ÏÑ†ÌÉù (Î∞∞Ïπò Ï≤òÎ¶¨)")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("### üì° RSS ÌîºÎìú ÏÇ¨Ïù¥Ìä∏ (ÌôïÏû•)")
            all_rss_selected = st.checkbox("üì° Î™®Îì† RSS ÌîºÎìú ÏÑ†ÌÉù", key="all_rss")
            
            for name, url in NEWS_SOURCES["rss_feeds"].items():
                selected = st.checkbox(
                    f"{name}",
                    value=all_rss_selected,
                    key=f"rss_{name}",
                    help=f"URL: {url}"
                )
                if selected:
                    source_tuple = ("rss", name, url)
                    if source_tuple not in st.session_state.selected_sources_optimized:
                        st.session_state.selected_sources_optimized.append(source_tuple)
                else:
                    source_tuple = ("rss", name, url)
                    if source_tuple in st.session_state.selected_sources_optimized:
                        st.session_state.selected_sources_optimized.remove(source_tuple)
        
        with col2:
            st.markdown("### üîç HTML ÌÅ¨Î°§ÎßÅ ÏÇ¨Ïù¥Ìä∏ (ÌôïÏû•)")
            all_html_selected = st.checkbox("üîç Î™®Îì† HTML ÏÇ¨Ïù¥Ìä∏ ÏÑ†ÌÉù", key="all_html")
            
            for name, url in NEWS_SOURCES["html_crawl"].items():
                selected = st.checkbox(
                    f"{name}",
                    value=all_html_selected,
                    key=f"html_{name}",
                    help=f"URL: {url}"
                )
                if selected:
                    source_tuple = ("html", name, url)
                    if source_tuple not in st.session_state.selected_sources_optimized:
                        st.session_state.selected_sources_optimized.append(source_tuple)
                else:
                    source_tuple = ("html", name, url)
                    if source_tuple in st.session_state.selected_sources_optimized:
                        st.session_state.selected_sources_optimized.remove(source_tuple)
        
        # ÏÑ†ÌÉùÎêú ÏÜåÏä§ ÌëúÏãú
        if st.session_state.selected_sources_optimized:
            st.success(f"‚úÖ {len(st.session_state.selected_sources_optimized)}Í∞ú ÏÜåÏä§ ÏÑ†ÌÉùÎê®")
    
    def render_crawling_interface(self):
        """ÌÅ¨Î°§ÎßÅ Ïã§Ìñâ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§"""
        if not st.session_state.selected_sources_optimized:
            st.info("üëÜ Î∂ÑÏÑùÌï† Îâ¥Ïä§ ÏÜåÏä§Î•º ÏÑ†ÌÉùÌï¥Ï£ºÏÑ∏Ïöî.")
            return
        
        st.header("üöÄ ÏµúÏ†ÅÌôîÎêú ÌÅ¨Î°§ÎßÅ Ïã§Ìñâ")
        
        # ÌÅ¨Î°§ÎßÅ ÏÑ§Ï†ï
        col1, col2, col3 = st.columns(3)
        
        with col1:
            enable_ai_processing = st.checkbox("ü§ñ AI Î∂ÑÏÑù Ï≤òÎ¶¨", value=OLLAMA_AVAILABLE)
        
        with col2:
            enable_translation = st.checkbox("üåç Îã§Íµ≠Ïñ¥ Î≤àÏó≠", value=MULTILINGUAL_SUPPORT_AVAILABLE)
        
        with col3:
            max_articles_per_source = st.slider("ÏÜåÏä§Îãπ ÏµúÎåÄ Í∏∞ÏÇ¨", 3, 15, 8)
        
        # ÌÅ¨Î°§ÎßÅ Ïã§Ìñâ Î≤ÑÌäº
        if st.button("üï∑Ô∏è ÏµúÏ†ÅÌôîÎêú Î∞∞Ïπò ÌÅ¨Î°§ÎßÅ ÏãúÏûë", type="primary", use_container_width=True):
            self.run_optimized_crawling(enable_ai_processing, enable_translation, max_articles_per_source)
    
    def run_optimized_crawling(self, enable_ai: bool, enable_translation: bool, max_articles: int):
        """ÏµúÏ†ÅÌôîÎêú ÌÅ¨Î°§ÎßÅ Ïã§Ìñâ"""
        start_time = time.time()
        results = {'crawler_data': [], 'summary': None}
        
        # ÏÜåÏä§ Î∂ÑÎ•ò
        rss_sources = [(name, url) for source_type, name, url in st.session_state.selected_sources_optimized if source_type == "rss"]
        html_sources = [(name, url) for source_type, name, url in st.session_state.selected_sources_optimized if source_type == "html"]
        
        # RSS ÌîºÎìú Î∞∞Ïπò Ï≤òÎ¶¨
        if rss_sources:
            st.subheader("üì° RSS ÌîºÎìú Î∞∞Ïπò ÏàòÏßë ÏßÑÌñâ Ï§ë...")
            
            with st.spinner("üï∑Ô∏è RSS Î∞∞Ïπò Ï≤òÎ¶¨ Ïã§Ìñâ Ï§ë..."):
                rss_results = self.process_rss_feeds_batch(rss_sources)
                results['crawler_data'].extend(rss_results)
        
        # HTML ÌÅ¨Î°§ÎßÅ Î∞∞Ïπò Ï≤òÎ¶¨
        if html_sources:
            st.subheader("üîç HTML ÌÅ¨Î°§ÎßÅ Î∞∞Ïπò Ï≤òÎ¶¨ ÏßÑÌñâ Ï§ë...")
            
            with st.spinner("üîç HTML Î∞∞Ïπò ÌÅ¨Î°§ÎßÅ Ïã§Ìñâ Ï§ë..."):
                html_results = self.process_html_crawling_batch(html_sources)
                results['crawler_data'].extend(html_results)
        
        # Í≤∞Í≥º ÌëúÏãú
        total_articles = len(results['crawler_data'])
        rss_count = len([r for r in results['crawler_data'] if r.get('type') == 'RSS'])
        html_count = len([r for r in results['crawler_data'] if r.get('type') == 'HTML'])
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Ï¥ù ÏàòÏßë Í∏∞ÏÇ¨", total_articles)
        with col2:
            st.metric("RSS Í∏∞ÏÇ¨", rss_count)
        with col3:
            st.metric("HTML Í∏∞ÏÇ¨", html_count)
        
        # AI Ï¢ÖÌï© Î∂ÑÏÑù
        if enable_ai and OLLAMA_AVAILABLE and results['crawler_data']:
            st.subheader("ü§ñ AI Ï¢ÖÌï© ÌÅ¨Î°§ÎßÅ Î∂ÑÏÑù")
            
            # Î™®Îì† Í∏∞ÏÇ¨ Ï†ïÎ≥¥ Í≤∞Ìï©
            all_articles = ""
            for article in results['crawler_data'][:20]:  # ÏÉÅÏúÑ 20Í∞úÎßå Î∂ÑÏÑù
                all_articles += f"[ÏÜåÏä§: {article.get('source', '')}] {article.get('title', '')}\n"
                all_articles += f"ÏöîÏïΩ: {article.get('summary', '')[:200]}...\n\n"
            
            if all_articles.strip():
                with st.spinner("ü§ñ AI Ï¢ÖÌï© Îâ¥Ïä§ Î∂ÑÏÑù Ï§ë..."):
                    try:
                        # ÌÅ¨Î°§Îü¨ Ï†ÑÏö© ÌîÑÎ°¨ÌîÑÌä∏
                        crawler_prompt = f"""
Îã§ÏùåÏùÄ Íµ≠Ï†ú Ï£ºÏñºÎ¶¨ ÏóÖÍ≥Ñ Îâ¥Ïä§ ÌÅ¨Î°§ÎßÅ Í≤∞Í≥ºÏûÖÎãàÎã§. Ï†ÑÎ¨∏Í∞Ä Í¥ÄÏ†êÏóêÏÑú Î∂ÑÏÑùÌï¥Ï£ºÏÑ∏Ïöî:

{all_articles}

Î∂ÑÏÑù ÏöîÏ≤≠ÏÇ¨Ìï≠:
1. Ï£ºÏöî ÏóÖÍ≥Ñ Ìä∏Î†åÎìú Î∞è ÎèôÌñ• Î∂ÑÏÑù
2. Ï§ëÏöîÌïú Îâ¥Ïä§ÏôÄ Ïù¥Î≤§Ìä∏ Ï†ïÎ¶¨
3. Ï£ºÏñºÎ¶¨ ÏãúÏû•Ïóê ÎØ∏ÏπòÎäî ÏòÅÌñ• ÌèâÍ∞Ä
4. Ìñ•ÌõÑ Ï£ºÎ™©Ìï¥Ïïº Ìï† ÌÇ§ÏõåÎìúÏôÄ Ìä∏Î†åÎìú
5. ÌïúÍµ≠ Ï£ºÏñºÎ¶¨ ÏóÖÍ≥ÑÏóê ÎåÄÌïú ÏãúÏÇ¨Ï†ê

Ïã§Ïö©Ï†ÅÏù¥Í≥† ÌÜµÏ∞∞Î†• ÏûàÎäî Î∂ÑÏÑùÏùÑ Ï†úÍ≥µÌï¥Ï£ºÏÑ∏Ïöî.
"""
                        summary = quick_analysis(crawler_prompt, model=CRAWLER_MODEL)
                        results['summary'] = summary
                        st.success("‚úÖ AI Ï¢ÖÌï© Î∂ÑÏÑù ÏôÑÎ£å")
                        st.write(summary)
                    except Exception as e:
                        st.error(f"AI Î∂ÑÏÑù Ïã§Ìå®: {str(e)}")
        
        # Ï†ÑÏ≤¥ ÏÑ±Îä• ÌÜµÍ≥Ñ
        total_time = time.time() - start_time
        st.subheader("üìä ÏÑ±Îä• ÌÜµÍ≥Ñ")
        
        perf_col1, perf_col2, perf_col3 = st.columns(3)
        with perf_col1:
            st.metric("Ï†ÑÏ≤¥ Ï≤òÎ¶¨ ÏãúÍ∞Ñ", f"{total_time:.2f}Ï¥à")
        with perf_col2:
            st.metric("ÌèâÍ∑† Í∏∞ÏÇ¨/Ï¥à", f"{total_articles/total_time:.1f}")
        with perf_col3:
            improvement = "150%"
            st.metric("ÏÑ±Îä• Ìñ•ÏÉÅ", improvement)
        
        # Í≤∞Í≥º Ï†ÄÏû•
        st.session_state.crawler_results_optimized = results
        st.success("üï∑Ô∏è ÏµúÏ†ÅÌôîÎêú ÌÅ¨Î°§ÎßÅ ÏôÑÎ£å!")
        
        # Ìñ•ÏÉÅÎêú Í≤∞Í≥º ÌëúÏãú
        if self.result_display and results:
            st.markdown("---")
            self.show_comprehensive_crawler_results(results)
    
    def show_comprehensive_crawler_results(self, results: Dict):
        """Ï¢ÖÌï© ÌÅ¨Î°§ÎßÅ Í≤∞Í≥º ÌëúÏãú"""
        crawler_data = results.get('crawler_data', [])
        summary = results.get('summary', '')
        
        # ÌÉ≠ Íµ¨ÏÑ±
        tab1, tab2, tab3, tab4 = st.tabs(["üìã ÏöîÏïΩ", "üì° RSS Í≤∞Í≥º", "üîç HTML Í≤∞Í≥º", "üìä Î∂ÑÏÑù"])
        
        with tab1:
            self.show_crawler_executive_summary(crawler_data, summary)
        
        with tab2:
            rss_results = [r for r in crawler_data if r.get('type') == 'RSS']
            if rss_results:
                st.markdown("### üì° RSS ÌîºÎìú ÏàòÏßë Í≤∞Í≥º")
                df_rss = pd.DataFrame(rss_results)
                st.dataframe(df_rss, use_container_width=True)
            else:
                st.info("RSS ÌîºÎìú Í≤∞Í≥ºÍ∞Ä ÏóÜÏäµÎãàÎã§.")
        
        with tab3:
            html_results = [r for r in crawler_data if r.get('type') == 'HTML']
            if html_results:
                st.markdown("### üîç HTML ÌÅ¨Î°§ÎßÅ Í≤∞Í≥º")
                df_html = pd.DataFrame(html_results)
                st.dataframe(df_html, use_container_width=True)
            else:
                st.info("HTML ÌÅ¨Î°§ÎßÅ Í≤∞Í≥ºÍ∞Ä ÏóÜÏäµÎãàÎã§.")
        
        with tab4:
            if self.analytics_ui:
                self.analytics_ui.show_crawler_analytics(crawler_data)
    
    def show_crawler_executive_summary(self, crawler_data: List[Dict], summary: str):
        """ÌÅ¨Î°§Îü¨ ÏûÑÏõê ÏöîÏïΩ ÌëúÏãú"""
        st.markdown("### üï∑Ô∏è ÌÅ¨Î°§ÎßÅ ÏöîÏïΩ")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("#### üîç ÏàòÏßë Í≤∞Í≥º")
            if crawler_data:
                total_articles = len(crawler_data)
                sources = len(set(r.get('source', '') for r in crawler_data))
                
                col_a, col_b = st.columns(2)
                with col_a:
                    st.metric("ÏàòÏßë Í∏∞ÏÇ¨", f"{total_articles}Í∞ú")
                with col_b:
                    st.metric("Îâ¥Ïä§ ÏÜåÏä§", f"{sources}Í∞ú")
            else:
                st.info("ÌÅ¨Î°§ÎßÅ Îç∞Ïù¥ÌÑ∞ ÏóÜÏùå")
        
        with col2:
            st.markdown("#### ‚ö° ÏÑ±Îä•")
            if crawler_data:
                avg_time = np.mean([r.get('processing_time', 0) for r in crawler_data])
                rss_count = len([r for r in crawler_data if r.get('type') == 'RSS'])
                
                col_a, col_b = st.columns(2)
                with col_a:
                    st.metric("ÌèâÍ∑† Ï≤òÎ¶¨ÏãúÍ∞Ñ", f"{avg_time:.2f}Ï¥à")
                with col_b:
                    st.metric("RSS Í∏∞ÏÇ¨", f"{rss_count}Í∞ú")
        
        if summary:
            st.markdown("#### ü§ñ AI Ï¢ÖÌï© Î∂ÑÏÑù")
            st.markdown(summary)
        
        st.markdown("#### üí° Ï∂îÏ≤ú Ïï°ÏÖò")
        if crawler_data:
            jewelry_articles = len([r for r in crawler_data if self._is_jewelry_related(r.get('title', ''))])
            if jewelry_articles > len(crawler_data) * 0.7:
                st.markdown("‚Ä¢ üéØ Ï£ºÏñºÎ¶¨ Í¥ÄÎ†® Í∏∞ÏÇ¨ ÎπÑÏ§ëÏù¥ ÎÜíÏäµÎãàÎã§. Ìä∏Î†åÎìú Î∂ÑÏÑùÏùÑ Í∂åÏû•Ìï©ÎãàÎã§.")
            else:
                st.markdown("‚Ä¢ üîç Îçî Ï†ïÌôïÌïú Ï£ºÏñºÎ¶¨ Îâ¥Ïä§ ÌïÑÌÑ∞ÎßÅÏù¥ ÌïÑÏöîÌï©ÎãàÎã§.")
            
            # ÏÜåÏä§Î≥Ñ ÏÑ±Í≥º Î∂ÑÏÑù
            source_performance = {}
            for article in crawler_data:
                source = article.get('source', '')
                if source not in source_performance:
                    source_performance[source] = 0
                source_performance[source] += 1
            
            best_source = max(source_performance.items(), key=lambda x: x[1])
            st.markdown(f"‚Ä¢ üìà Í∞ÄÏû• ÌôúÎ∞úÌïú ÏÜåÏä§: {best_source[0]} ({best_source[1]}Í∞ú Í∏∞ÏÇ¨)")

def main():
    """Î©îÏù∏ Ìï®Ïàò"""
    st.title("üï∑Ô∏è Ïõπ ÌÅ¨Î°§Îü¨ + Î∏îÎ°úÍ∑∏ ÏûêÎèôÌôî (ÏôÑÏ†Ñ ÏµúÏ†ÅÌôî Î≤ÑÏ†Ñ)")
    st.markdown("**v2.0**: ÏÑ±Îä• 150% Ìñ•ÏÉÅ + Ïã§ÏãúÍ∞Ñ UI + Î∞∞Ïπò Ï≤òÎ¶¨")
    st.markdown("---")
    
    # ÌÅ¨Î°§Îü¨ Ï¥àÍ∏∞Ìôî
    crawler = OptimizedWebCrawler()
    
    # ÏïàÏ†ïÏÑ± ÎåÄÏãúÎ≥¥Îìú ÌëúÏãú
    if crawler.stability_manager:
        crawler.stability_manager.display_health_dashboard()
    
    # Îã§Íµ≠Ïñ¥ ÏÑ§Ï†ï ÌëúÏãú
    if crawler.multilingual_processor:
        language_settings = crawler.multilingual_processor.render_language_settings()
        crawler.multilingual_processor.render_format_support_info()
    else:
        language_settings = None
    
    # ÏµúÏ†ÅÌôî ÌÜµÍ≥Ñ ÌëúÏãú
    crawler.render_optimization_stats()
    
    # Î©îÏù∏ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§
    crawler.render_source_selection_optimized()
    crawler.render_crawling_interface()
    
    # Ïù¥Ï†Ñ Í≤∞Í≥º ÌëúÏãú
    if st.session_state.crawler_results_optimized:
        with st.expander("üï∑Ô∏è Ïù¥Ï†Ñ ÌÅ¨Î°§ÎßÅ Í≤∞Í≥º", expanded=False):
            st.json(st.session_state.crawler_results_optimized)
    
    # Ìë∏ÌÑ∞ Ï†ïÎ≥¥
    st.markdown("---")
    col1, col2, col3 = st.columns(3)
    with col1:
        st.markdown("**ÏÑ±Îä• Í∞úÏÑ†**")
        st.markdown("‚Ä¢ Î∞∞Ïπò Ï≤òÎ¶¨")
        st.markdown("‚Ä¢ ÎπÑÎèôÍ∏∞ ÏöîÏ≤≠")
        st.markdown("‚Ä¢ URL Ï∫êÏã±")
    with col2:
        st.markdown("**ÌÅ¨Î°§ÎßÅ Í∏∞Îä•**")
        st.markdown("‚Ä¢ RSS ÌîºÎìú ÏàòÏßë")
        st.markdown("‚Ä¢ HTML ÌÅ¨Î°§ÎßÅ")
        st.markdown("‚Ä¢ AI Î∂ÑÏÑù")
    with col3:
        st.markdown("**ÏïàÏ†ïÏÑ±**")
        st.markdown("‚Ä¢ Ïò§Î•ò Î≥µÍµ¨")
        st.markdown("‚Ä¢ Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨")
        st.markdown("‚Ä¢ Ïã§ÏãúÍ∞Ñ Î™®ÎãàÌÑ∞ÎßÅ")

if __name__ == "__main__":
    main()