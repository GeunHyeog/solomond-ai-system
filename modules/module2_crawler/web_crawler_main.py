#!/usr/bin/env python3
"""
ğŸ•·ï¸ ëª¨ë“ˆ 2: ì›¹ í¬ë¡¤ëŸ¬ + ë¸”ë¡œê·¸ ìë™í™” ì‹œìŠ¤í…œ
êµ­ì œ ì£¼ì–¼ë¦¬ ë‰´ìŠ¤ ìˆ˜ì§‘ ë° AI ê¸°ë°˜ ë¸”ë¡œê·¸ ìë™ ë°œí–‰

ì£¼ìš” ê¸°ëŠ¥:
- RSS í”¼ë“œ ìë™ ìˆ˜ì§‘
- HTML í¬ë¡¤ë§ (RSS ë¯¸ì œê³µ ì‚¬ì´íŠ¸)
- AI ìš”ì•½ ë° ë²ˆì—­ (MCP Perplexity)
- Notion ë¸”ë¡œê·¸ ìë™ ë°œí–‰ (MCP Notion)
- Supabase ë°ì´í„° ì €ì¥
"""

import streamlit as st
import os
import sys
import time
import json
import asyncio
from datetime import datetime, timedelta
from pathlib import Path
import requests
import feedparser
from bs4 import BeautifulSoup
import pandas as pd

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.append(str(PROJECT_ROOT))

# Ollama AI í†µí•© (v2.0 ê³ ë„í™”)
try:
    sys.path.append(str(PROJECT_ROOT / "shared"))
    from ollama_interface import global_ollama, quick_summary, quick_translate
    # v2 ê³ ë„í™”ëœ ì¸í„°í˜ì´ìŠ¤ ì¶”ê°€
    from ollama_interface_v2 import advanced_ollama, premium_insight, quick_summary as v2_summary, smart_translate
    OLLAMA_AVAILABLE = global_ollama.health_check()
    OLLAMA_V2_AVAILABLE = True
    print("âœ… ì›¹ í¬ë¡¤ëŸ¬ v2 Ollama ì¸í„°í˜ì´ìŠ¤ ë¡œë“œ ì™„ë£Œ!")
except ImportError as e:
    try:
        # v1 ì¸í„°í˜ì´ìŠ¤ë§Œ ì‹œë„
        from ollama_interface import global_ollama, quick_summary, quick_translate
        OLLAMA_AVAILABLE = global_ollama.health_check()
        OLLAMA_V2_AVAILABLE = False
        print("âš ï¸ ì›¹ í¬ë¡¤ëŸ¬ v1 Ollama ì¸í„°í˜ì´ìŠ¤ë§Œ ì‚¬ìš© ê°€ëŠ¥")
    except ImportError:
        OLLAMA_AVAILABLE = False
        OLLAMA_V2_AVAILABLE = False
        print(f"âŒ ì›¹ í¬ë¡¤ëŸ¬ Ollama ì¸í„°í˜ì´ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")

# MCP ë° ì™¸ë¶€ ë„êµ¬ import ì‹œë„
try:
    # MCP ë„êµ¬ë“¤ì´ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸
    MCP_AVAILABLE = True
except ImportError:
    MCP_AVAILABLE = False

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ğŸ•·ï¸ ì›¹ í¬ë¡¤ëŸ¬ + ë¸”ë¡œê·¸",
    page_icon="ğŸ•·ï¸",
    layout="wide"
)

# êµ­ì œ ì£¼ì–¼ë¦¬ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ ì„¤ì •
NEWS_SOURCES = {
    "rss_feeds": {
        "Professional Jeweller": "https://www.professionaljeweller.com/feed/",
        "Jewellery Focus": "https://www.jewelleryfocus.co.uk/feed",
        "The Jewelry Loupe": "https://feeds.feedburner.com/TheJewelryLoupe",
        "The Jewellery Editor": "https://www.thejewelleryeditor.com/feed/",
        "INSTORE Magazine": "https://instoremag.com/feed/",
        "WatchPro": "https://www.watchpro.com/feed/"
    },
    "html_crawl": {
        "JCK Online": "https://www.jckonline.com/editorial-articles/news/",
        "National Jeweler": "https://www.nationaljeweler.com/articles/category/news",
        "Rapaport": "https://www.diamonds.net/News/",
        "JewelleryNet": "https://www.jewellerynet.com/en/news",
        "Jewelin (êµ­ë‚´)": "https://www.jewelin.co.kr/news",
        "Messi Jewelry": "https://www.messijewelry.com/news"
    }
}

class WebCrawlerModule:
    """ì›¹ í¬ë¡¤ëŸ¬ ëª¨ë“ˆ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.initialize_session_state()
        self.setup_user_agent()
    
    def initialize_session_state(self):
        """ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”"""
        if "crawler_results" not in st.session_state:
            st.session_state.crawler_results = []
        if "crawl_status" not in st.session_state:
            st.session_state.crawl_status = "ëŒ€ê¸°ì¤‘"
        if "selected_sources" not in st.session_state:
            st.session_state.selected_sources = []
    
    def setup_user_agent(self):
        """User-Agent ì„¤ì •"""
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
    
    def fetch_rss_feed(self, name, url):
        """RSS í”¼ë“œ ìˆ˜ì§‘"""
        try:
            st.info(f"ğŸ“¡ {name} RSS í”¼ë“œ ìˆ˜ì§‘ ì¤‘...")
            
            # feedparserë¡œ RSS íŒŒì‹±
            feed = feedparser.parse(url)
            
            articles = []
            for entry in feed.entries[:5]:  # ìµœì‹  5ê°œë§Œ
                article = {
                    "source": name,
                    "title": entry.get('title', 'No Title'),
                    "link": entry.get('link', ''),
                    "published": entry.get('published', ''),
                    "summary": entry.get('summary', '')[:300] + "..." if len(entry.get('summary', '')) > 300 else entry.get('summary', ''),
                    "type": "RSS"
                }
                articles.append(article)
            
            st.success(f"âœ… {name}: {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
            return articles
            
        except Exception as e:
            st.error(f"âŒ {name} RSS ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def crawl_html_news(self, name, url):
        """HTML í¬ë¡¤ë§ (RSS ë¯¸ì œê³µ ì‚¬ì´íŠ¸)"""
        try:
            st.info(f"ğŸ” {name} HTML í¬ë¡¤ë§ ì¤‘...")
            
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ê¸°ë³¸ì ì¸ ë‰´ìŠ¤ ë§í¬ ì¶”ì¶œ ë¡œì§
            articles = []
            
            # ì¼ë°˜ì ì¸ ë‰´ìŠ¤ ë§í¬ íŒ¨í„´ë“¤ ì‹œë„
            link_selectors = [
                'article a', 'h2 a', 'h3 a', '.news-item a', 
                '.article-title a', '.post-title a'
            ]
            
            for selector in link_selectors:
                links = soup.select(selector)
                if links:
                    for link in links[:3]:  # ìµœëŒ€ 3ê°œ
                        title = link.get_text(strip=True)
                        href = link.get('href', '')
                        
                        if title and href:
                            # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                            if href.startswith('/'):
                                href = f"{url.split('/')[0]}//{url.split('/')[2]}{href}"
                            
                            article = {
                                "source": name,
                                "title": title,
                                "link": href,
                                "published": datetime.now().strftime("%Y-%m-%d"),
                                "summary": f"{name}ì—ì„œ ìˆ˜ì§‘ëœ ë‰´ìŠ¤",
                                "type": "HTML"
                            }
                            articles.append(article)
                    
                    if articles:
                        break
            
            st.success(f"âœ… {name}: {len(articles)}ê°œ ê¸°ì‚¬ í¬ë¡¤ë§ ì™„ë£Œ")
            return articles
            
        except Exception as e:
            st.error(f"âŒ {name} HTML í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def process_with_ai(self, articles):
        """Ollama AI ê¸°ë°˜ ê¸°ì‚¬ ìš”ì•½ ë° ì²˜ë¦¬"""
        if not OLLAMA_AVAILABLE:
            st.warning("âš ï¸ Ollama AIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì²˜ë¦¬ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
            return articles
        
        st.info("ğŸ¤– Ollama AI ê¸°ë°˜ ê¸°ì‚¬ ìš”ì•½ ì¤‘...")
        
        # ì§„í–‰ë¥  í‘œì‹œ
        progress_bar = st.progress(0)
        processed_articles = []
        
        for i, article in enumerate(articles):
            try:
                st.text(f"ì²˜ë¦¬ ì¤‘: {article['title'][:50]}...")
                
                # ğŸ† v2 ê³ ë„í™” ë‰´ìŠ¤ ë¶„ì„
                full_content = f"ì œëª©: {article['title']}\n\në‚´ìš©: {article['summary']}"
                
                if OLLAMA_V2_AVAILABLE:
                    # v2 ì¸í„°í˜ì´ìŠ¤ë¡œ ë‹¤ì–‘í•œ ë ˆë²¨ ë¶„ì„
                    v2_analysis = self.process_news_v2(full_content, article['title'])
                    ai_summary_ko = v2_analysis['best_summary']
                    # ì¶”ê°€ ë¶„ì„ ë°ì´í„°ë„ ì €ì¥
                    processed_article = article.copy()
                    processed_article["v2_analysis"] = v2_analysis
                else:
                    # ê¸°ì¡´ v1 ë¶„ì„
                    ai_summary = quick_summary(full_content)
                    
                    # ë²ˆì—­ (ì˜ì–´ ê¸°ì‚¬ì˜ ê²½ìš°)
                    if self.is_english_content(article['title']):
                        ai_summary_ko = quick_translate(ai_summary, "í•œêµ­ì–´")
                    else:
                        ai_summary_ko = ai_summary
                    
                    processed_article = article.copy()
                
                processed_article["ai_summary"] = ai_summary_ko
                processed_article["original_summary"] = article["summary"]
                processed_article["tags"] = self.extract_tags(ai_summary_ko)
                processed_article["importance"] = self.assess_importance(ai_summary_ko)
                processed_articles.append(processed_article)
                
                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                progress_bar.progress((i + 1) / len(articles))
                time.sleep(0.5)  # AI ì²˜ë¦¬ ê°„ê²©
                
            except Exception as e:
                st.warning(f"ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                processed_articles.append(article)
        
        st.success(f"âœ… {len(processed_articles)}ê°œ ê¸°ì‚¬ AI ì²˜ë¦¬ ì™„ë£Œ!")
        return processed_articles
    
    def is_english_content(self, text: str) -> bool:
        """ì˜ì–´ ì»¨í…ì¸  ê°ì§€"""
        try:
            text.encode('ascii')
            return len([c for c in text if c.isalpha() and ord(c) < 128]) > len(text) * 0.7
        except UnicodeEncodeError:
            return False
    
    def process_news_v2(self, content: str, title: str) -> dict:
        """ğŸ† v2 ê³ ë„í™” ë‰´ìŠ¤ ì²˜ë¦¬ - 5ê°œ ëª¨ë¸ ì „ëµ í™œìš©"""
        
        try:
            is_english = self.is_english_content(title)
            
            # ğŸš€ ë¹ ë¥¸ ìš”ì•½ (Gemma3-4B) - ê¸°ë³¸ ì²˜ë¦¬
            fast_result = v2_summary(content)
            if is_english and fast_result:
                fast_result_ko = smart_translate(fast_result, "í•œêµ­ì–´")
            else:
                fast_result_ko = fast_result
            
            # ğŸ”¥ í”„ë¦¬ë¯¸ì—„ ì‹œì¥ ì¸ì‚¬ì´íŠ¸ (Gemma3-27B) - ì‹¬í™” ë¶„ì„
            try:
                premium_result = premium_insight(content)
                if is_english and premium_result:
                    premium_result_ko = smart_translate(premium_result, "í•œêµ­ì–´") 
                else:
                    premium_result_ko = premium_result
            except:
                premium_result_ko = fast_result_ko
            
            # âš¡ í‘œì¤€ ë‰´ìŠ¤ ë¶„ì„ (Qwen3-8B)
            try:
                standard_result = advanced_ollama.advanced_generate(
                    task_type="news_analysis",
                    content=content,
                    task_goal="ì£¼ì–¼ë¦¬ ì—…ê³„ ë‰´ìŠ¤ íŠ¸ë Œë“œ ë¶„ì„",
                    quality_priority=False,
                    speed_priority=False
                )
                if is_english and standard_result:
                    standard_result_ko = smart_translate(standard_result, "í•œêµ­ì–´")
                else:
                    standard_result_ko = standard_result
            except:
                standard_result_ko = fast_result_ko
            
            # ìµœê³  í’ˆì§ˆ ê²°ê³¼ ì„ íƒ (ê¸¸ì´ì™€ í’ˆì§ˆì„ ê¸°ì¤€ìœ¼ë¡œ)
            candidates = [
                ('premium', premium_result_ko),
                ('standard', standard_result_ko), 
                ('fast', fast_result_ko)
            ]
            
            # ê°€ì¥ ì¢‹ì€ ê²°ê³¼ ì„ íƒ (ê¸¸ì´ + í’ˆì§ˆ ê¸°ì¤€)
            best_summary = fast_result_ko
            best_tier = 'fast'
            
            for tier, result in candidates:
                if result and len(result) > len(best_summary) and len(result) < 1000:
                    best_summary = result
                    best_tier = tier
            
            return {
                'best_summary': best_summary,
                'best_tier': best_tier,
                'fast_summary': fast_result_ko,
                'premium_insight': premium_result_ko,
                'standard_analysis': standard_result_ko,
                'is_english': is_english,
                'v2_processed': True
            }
            
        except Exception as e:
            # í´ë°±: v1 ë°©ì‹
            try:
                fallback_summary = quick_summary(content)
                if self.is_english_content(title):
                    fallback_summary = quick_translate(fallback_summary, "í•œêµ­ì–´")
                
                return {
                    'best_summary': fallback_summary,
                    'best_tier': 'fallback',
                    'v2_processed': False,
                    'error': str(e)
                }
            except:
                return {
                    'best_summary': "AI ë¶„ì„ ì‹¤íŒ¨",
                    'best_tier': 'error',
                    'v2_processed': False,
                    'error': str(e)
                }
    
    def extract_tags(self, content: str) -> list:
        """AI ìš”ì•½ì—ì„œ íƒœê·¸ ì¶”ì¶œ"""
        # ê¸°ë³¸ ì£¼ì–¼ë¦¬ ê´€ë ¨ í‚¤ì›Œë“œ
        jewelry_keywords = [
            "ë‹¤ì´ì•„ëª¬ë“œ", "ë³´ì„", "ì£¼ì–¼ë¦¬", "ë°˜ì§€", "ëª©ê±¸ì´", "ê·€ê±¸ì´", "íŒ”ì°Œ",
            "ë£¨ë¹„", "ì‚¬íŒŒì´ì–´", "ì—ë©”ë„ë“œ", "ì§„ì£¼", "ê¸ˆ", "ì€", "í”Œë˜í‹°ë„˜",
            "ë¸Œëœë“œ", "ì „ì‹œíšŒ", "íŠ¸ë Œë“œ", "ë””ìì¸", "ì»¬ë ‰ì…˜", "ëŸ­ì…”ë¦¬"
        ]
        
        tags = []
        content_lower = content.lower()
        
        for keyword in jewelry_keywords:
            if keyword in content_lower:
                tags.append(keyword)
        
        # ìµœëŒ€ 5ê°œê¹Œì§€
        return tags[:5] if tags else ["ì£¼ì–¼ë¦¬", "ë‰´ìŠ¤"]
    
    def assess_importance(self, content: str) -> str:
        """ì¤‘ìš”ë„ í‰ê°€"""
        high_importance_words = ["ì¤‘ìš”", "í˜ì‹ ", "ë³€í™”", "ì„±ì¥", "ë°œí‘œ", "ì‹ ì œí’ˆ", "íŠ¸ë Œë“œ"]
        medium_importance_words = ["ì—…ë°ì´íŠ¸", "ì†Œì‹", "ì •ë³´", "ë°œê²¬"]
        
        content_lower = content.lower()
        
        high_count = sum(1 for word in high_importance_words if word in content_lower)
        medium_count = sum(1 for word in medium_importance_words if word in content_lower)
        
        if high_count >= 2:
            return "ìƒ"
        elif high_count >= 1 or medium_count >= 2:
            return "ì¤‘"
        else:
            return "í•˜"
    
    def save_to_notion(self, articles):
        """Notionì— ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìë™ ë°œí–‰"""
        if not MCP_AVAILABLE:
            st.warning("âš ï¸ MCP Notionì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        st.info("ğŸ“ Notion ë¸”ë¡œê·¸ ë°œí–‰ ì¤‘...")
        
        try:
            # ì‹¤ì œë¡œëŠ” MCP Notion í•¨ìˆ˜ ì‚¬ìš©
            for article in articles:
                # notion_create_page() ë“±ì˜ MCP í•¨ìˆ˜ í˜¸ì¶œ
                pass
            
            st.success(f"âœ… {len(articles)}ê°œ ê¸°ì‚¬ê°€ Notionì— ë°œí–‰ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return True
            
        except Exception as e:
            st.error(f"âŒ Notion ë°œí–‰ ì‹¤íŒ¨: {str(e)}")
            return False
    
    def render_source_selection(self):
        """ë‰´ìŠ¤ ì†ŒìŠ¤ ì„ íƒ UI"""
        st.markdown("## ğŸ“° ë‰´ìŠ¤ ì†ŒìŠ¤ ì„ íƒ")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("### ğŸ“¡ RSS í”¼ë“œ ì‚¬ì´íŠ¸")
            for name, url in NEWS_SOURCES["rss_feeds"].items():
                selected = st.checkbox(
                    f"{name}",
                    key=f"rss_{name}",
                    help=url
                )
                if selected and name not in st.session_state.selected_sources:
                    st.session_state.selected_sources.append(("rss", name, url))
        
        with col2:
            st.markdown("### ğŸ” HTML í¬ë¡¤ë§ ì‚¬ì´íŠ¸")
            for name, url in NEWS_SOURCES["html_crawl"].items():
                selected = st.checkbox(
                    f"{name}",
                    key=f"html_{name}",
                    help=url
                )
                if selected and name not in st.session_state.selected_sources:
                    st.session_state.selected_sources.append(("html", name, url))
    
    def render_crawl_controls(self):
        """í¬ë¡¤ë§ ì œì–´ UI"""
        st.markdown("## âš¡ í¬ë¡¤ë§ ì‹¤í–‰")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            if st.button("ğŸš€ ì„ íƒëœ ì†ŒìŠ¤ í¬ë¡¤ë§ ì‹œì‘", type="primary"):
                if st.session_state.selected_sources:
                    self.start_crawling()
                else:
                    st.warning("âš ï¸ ë¨¼ì € ë‰´ìŠ¤ ì†ŒìŠ¤ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
        
        with col2:
            if st.button("ğŸ”„ ì „ì²´ ì†ŒìŠ¤ í¬ë¡¤ë§"):
                st.session_state.selected_sources = []
                # ëª¨ë“  ì†ŒìŠ¤ ì¶”ê°€
                for name, url in NEWS_SOURCES["rss_feeds"].items():
                    st.session_state.selected_sources.append(("rss", name, url))
                for name, url in NEWS_SOURCES["html_crawl"].items():
                    st.session_state.selected_sources.append(("html", name, url))
                
                self.start_crawling()
        
        with col3:
            if st.button("ğŸ§¹ ê²°ê³¼ ì´ˆê¸°í™”"):
                st.session_state.crawler_results = []
                st.session_state.selected_sources = []
                st.rerun()
    
    def start_crawling(self):
        """í¬ë¡¤ë§ ì‹œì‘"""
        st.session_state.crawl_status = "ì‹¤í–‰ì¤‘"
        st.session_state.crawler_results = []
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        total_sources = len(st.session_state.selected_sources)
        
        for i, (source_type, name, url) in enumerate(st.session_state.selected_sources):
            status_text.text(f"ì²˜ë¦¬ ì¤‘: {name}")
            
            if source_type == "rss":
                articles = self.fetch_rss_feed(name, url)
            else:
                articles = self.crawl_html_news(name, url)
            
            st.session_state.crawler_results.extend(articles)
            
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
            progress_bar.progress((i + 1) / total_sources)
            time.sleep(1)  # ì„œë²„ ë¶€í•˜ ë°©ì§€
        
        status_text.text("âœ… í¬ë¡¤ë§ ì™„ë£Œ!")
        st.session_state.crawl_status = "ì™„ë£Œ"
        
        # AI ì²˜ë¦¬
        if st.session_state.crawler_results:
            st.session_state.crawler_results = self.process_with_ai(st.session_state.crawler_results)
    
    def render_results(self):
        """í¬ë¡¤ë§ ê²°ê³¼ í‘œì‹œ"""
        if not st.session_state.crawler_results:
            st.info("ì•„ì§ í¬ë¡¤ë§ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        st.markdown(f"## ğŸ“Š í¬ë¡¤ë§ ê²°ê³¼ ({len(st.session_state.crawler_results)}ê°œ)")
        
        # í†µê³„
        col1, col2, col3 = st.columns(3)
        
        rss_count = len([r for r in st.session_state.crawler_results if r["type"] == "RSS"])
        html_count = len([r for r in st.session_state.crawler_results if r["type"] == "HTML"])
        
        with col1:
            st.metric("RSS ê¸°ì‚¬", rss_count)
        with col2:
            st.metric("HTML ê¸°ì‚¬", html_count)
        with col3:
            st.metric("ì´ ê¸°ì‚¬", len(st.session_state.crawler_results))
        
        # ê²°ê³¼ í…Œì´ë¸”
        df = pd.DataFrame(st.session_state.crawler_results)
        st.dataframe(df, use_container_width=True)
        
        # Notion ë°œí–‰ ë²„íŠ¼
        if st.button("ğŸ“ Notion ë¸”ë¡œê·¸ ë°œí–‰"):
            self.save_to_notion(st.session_state.crawler_results)
    
    def render_sidebar(self):
        """ì‚¬ì´ë“œë°”"""
        with st.sidebar:
            st.markdown("## âš™ï¸ ì„¤ì •")
            
            st.markdown("### ğŸ”§ í¬ë¡¤ë§ ì„¤ì •")
            crawl_interval = st.selectbox(
                "í¬ë¡¤ë§ ì£¼ê¸°",
                ["ìˆ˜ë™", "1ì‹œê°„", "6ì‹œê°„", "12ì‹œê°„", "24ì‹œê°„"]
            )
            
            max_articles = st.slider("ìµœëŒ€ ê¸°ì‚¬ ìˆ˜", 1, 50, 10)
            
            st.markdown("### ğŸ“Š í†µê³„")
            st.info(f"""
            **ìƒíƒœ**: {st.session_state.crawl_status}
            **ìˆ˜ì§‘ëœ ê¸°ì‚¬**: {len(st.session_state.crawler_results)}ê°œ
            **ì„ íƒëœ ì†ŒìŠ¤**: {len(st.session_state.selected_sources)}ê°œ
            **Ollama AI**: {'âœ… ì—°ê²°ë¨' if OLLAMA_AVAILABLE else 'âŒ ë¶ˆê°€ëŠ¥'}
            **ì‚¬ìš© ëª¨ë¸**: {global_ollama.select_model('web_crawler') if OLLAMA_AVAILABLE else 'N/A'}
            """)
            
            if st.button("ğŸ  ë©”ì¸ ëŒ€ì‹œë³´ë“œë¡œ"):
                st.markdown("ë©”ì¸ ëŒ€ì‹œë³´ë“œ: http://localhost:8505")
    
    def run(self):
        """ëª¨ë“ˆ ì‹¤í–‰"""
        st.markdown("# ğŸ•·ï¸ ì›¹ í¬ë¡¤ëŸ¬ + ë¸”ë¡œê·¸ ìë™í™”")
        st.markdown("êµ­ì œ ì£¼ì–¼ë¦¬ ë‰´ìŠ¤ ìˆ˜ì§‘ ë° AI ê¸°ë°˜ ë¸”ë¡œê·¸ ìë™ ë°œí–‰ ì‹œìŠ¤í…œ")
        
        self.render_sidebar()
        
        st.markdown("---")
        
        # íƒ­ êµ¬ì„±
        tab1, tab2, tab3 = st.tabs(["ğŸ“° ì†ŒìŠ¤ ì„ íƒ", "ğŸš€ ì‹¤í–‰", "ğŸ“Š ê²°ê³¼"])
        
        with tab1:
            self.render_source_selection()
        
        with tab2:
            self.render_crawl_controls()
        
        with tab3:
            self.render_results()

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    crawler = WebCrawlerModule()
    crawler.run()

if __name__ == "__main__":
    main()