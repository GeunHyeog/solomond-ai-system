#!/usr/bin/env python3
"""
ğŸ¤– Ollama Integration Module for SOLOMOND AI
HTML í™˜ê²½ì—ì„œ Ollama API í˜¸ì¶œì„ ìœ„í•œ Python ë°±ì—”ë“œ ì„œë²„
"""

from flask import Flask, request, jsonify, send_from_directory
from flask_cors import CORS
import requests
import json
import os
import logging
from pathlib import Path
import threading
import time

app = Flask(__name__)
CORS(app)  # HTMLì—ì„œ AJAX í˜¸ì¶œ í—ˆìš©

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class OllamaInterface:
    def __init__(self, base_url="http://localhost:11434"):
        self.base_url = base_url
        self.available_models = []
        self.check_connection()
    
    def check_connection(self):
        """Ollama ì—°ê²° ìƒíƒœ í™•ì¸"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            if response.status_code == 200:
                models_data = response.json()
                self.available_models = [model['name'] for model in models_data.get('models', [])]
                logger.info(f"Ollama ì—°ê²° ì„±ê³µ! ì‚¬ìš©ê°€ëŠ¥ ëª¨ë¸: {self.available_models}")
                return True
        except Exception as e:
            logger.error(f"Ollama ì—°ê²° ì‹¤íŒ¨: {e}")
            return False
    
    def generate_response(self, prompt, model="gemma2:2b", stream=False):
        """Ollama ëª¨ë¸ë¡œ ì‘ë‹µ ìƒì„±"""
        try:
            payload = {
                "model": model,
                "prompt": prompt,
                "stream": stream,
                "options": {
                    "temperature": 0.7,
                    "top_p": 0.9,
                    "max_tokens": 2000
                }
            }
            
            response = requests.post(
                f"{self.base_url}/api/generate",
                json=payload,
                timeout=120  # 2ë¶„ìœ¼ë¡œ ì¦ê°€
            )
            
            if response.status_code == 200:
                result = response.json()
                return {
                    "success": True,
                    "response": result.get("response", ""),
                    "model": model,
                    "done": result.get("done", True)
                }
            else:
                return {
                    "success": False,
                    "error": f"HTTP {response.status_code}: {response.text}"
                }
                
        except Exception as e:
            logger.error(f"Ollama ìƒì„± ì˜¤ë¥˜: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def analyze_extracted_content(self, image_texts, audio_texts, context="ì£¼ì–¼ë¦¬ ì»¨í¼ëŸ°ìŠ¤"):
        """ì¶”ì¶œëœ í…ìŠ¤íŠ¸ë“¤ì„ ì¢…í•© ë¶„ì„"""
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""
You are a professional analyst specializing in {context}. Please analyze the following extracted texts:

=== Texts from Images ===
{chr(10).join([f"Image {i+1}: {text}" for i, text in enumerate(image_texts)])}

=== Texts from Audio ===
{chr(10).join([f"Audio {i+1}: {text}" for i, text in enumerate(audio_texts)])}

Please provide comprehensive analysis in the following format:

## Core Message Summary
(What are these people talking about? Clear summary)

## Key Content Analysis
1. Main Topics:
2. Important Keywords:
3. Speaker Intentions:

## Business/Jewelry Industry Insights
(Analysis from industry perspective)

## Action Items
(Future tasks or important points to note)

Please provide detailed and professional analysis in Korean language.
"""
        
        return self.generate_response(prompt)

# Ollama ì¸í„°í˜ì´ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
ollama = OllamaInterface()

@app.route('/')
def index():
    """ê¸°ë³¸ ìƒíƒœ í˜ì´ì§€"""
    return jsonify({
        "service": "SOLOMOND AI Ollama Integration",
        "status": "running",
        "available_models": ollama.available_models,
        "endpoints": [
            "/health - ìƒíƒœ í™•ì¸",
            "/models - ì‚¬ìš©ê°€ëŠ¥ ëª¨ë¸ ëª©ë¡", 
            "/generate - í…ìŠ¤íŠ¸ ìƒì„±",
            "/analyze - ì¢…í•© ë¶„ì„"
        ]
    })

@app.route('/health')
def health():
    """ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
    connection_ok = ollama.check_connection()
    return jsonify({
        "ollama_connected": connection_ok,
        "available_models": ollama.available_models,
        "status": "healthy" if connection_ok else "ollama_disconnected"
    })

@app.route('/models')
def get_models():
    """ì‚¬ìš©ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡"""
    return jsonify({
        "models": ollama.available_models
    })

@app.route('/generate', methods=['POST'])
def generate():
    """Ollama í…ìŠ¤íŠ¸ ìƒì„±"""
    try:
        data = request.json
        prompt = data.get('prompt', '')
        model = data.get('model', 'qwen2.5:7b')
        
        if not prompt:
            return jsonify({"success": False, "error": "promptê°€ í•„ìš”í•©ë‹ˆë‹¤"}), 400
        
        result = ollama.generate_response(prompt, model)
        return jsonify(result)
        
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500

@app.route('/analyze', methods=['POST'])
def analyze():
    """ì¢…í•© ë¶„ì„ ìˆ˜í–‰"""
    try:
        data = request.json
        image_texts = data.get('image_texts', [])
        audio_texts = data.get('audio_texts', [])
        context = data.get('context', 'ì£¼ì–¼ë¦¬ ì»¨í¼ëŸ°ìŠ¤')
        
        if not image_texts and not audio_texts:
            return jsonify({
                "success": False, 
                "error": "ë¶„ì„í•  í…ìŠ¤íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤"
            }), 400
        
        result = ollama.analyze_extracted_content(image_texts, audio_texts, context)
        return jsonify(result)
        
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500

@app.route('/test_analysis')
def test_analysis():
    """í…ŒìŠ¤íŠ¸ìš© ë¶„ì„ ì‹¤í–‰"""
    # ìƒ˜í”Œ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
    sample_image_texts = [
        "JGA 2025 ì£¼ì–¼ë¦¬ ë°•ëŒíšŒ",
        "ë‹¤ì´ì•„ëª¬ë“œ íŠ¸ë Œë“œ ë°œí‘œ",
        "2025ë…„ ì‹œì¥ ì „ë§"
    ]
    
    sample_audio_texts = [
        "ì•ˆë…•í•˜ì„¸ìš”. ì˜¤ëŠ˜ì€ ì£¼ì–¼ë¦¬ ì‹œì¥ì˜ ë¯¸ë˜ì— ëŒ€í•´ ë§ì”€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
        "ë‹¤ì´ì•„ëª¬ë“œ ê°€ê²©ì´ ì‘ë…„ ëŒ€ë¹„ 15% ìƒìŠ¹í–ˆìŠµë‹ˆë‹¤.",
        "ìƒˆë¡œìš´ ë””ìì¸ íŠ¸ë Œë“œê°€ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤."
    ]
    
    result = ollama.analyze_extracted_content(sample_image_texts, sample_audio_texts)
    return jsonify(result)

def run_server():
    """ì„œë²„ ì‹¤í–‰"""
    port = 8888
    print(f"Ollama Integration Server Starting...")
    print(f"URL: http://localhost:{port}")
    print(f"Available Models: {ollama.available_models}")
    
    app.run(host='0.0.0.0', port=port, debug=False)

if __name__ == "__main__":
    run_server()