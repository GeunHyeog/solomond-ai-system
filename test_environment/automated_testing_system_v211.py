#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì†”ë¡œëª¬ë“œ AI v2.1.1 - ìë™í™”ëœ í…ŒìŠ¤íŠ¸ ë° ì„±ëŠ¥ ê²€ì¦ ì‹œìŠ¤í…œ
ë² íƒ€ í…ŒìŠ¤íŠ¸ í™˜ê²½ ì™„ì„±ì„ ìœ„í•œ ì¢…í•© í…ŒìŠ¤íŠ¸ ìë™í™”

ì‘ì„±ì: ì „ê·¼í˜ (ì†”ë¡œëª¬ë“œ ëŒ€í‘œ, í•œêµ­ë³´ì„í˜‘íšŒ ì‚¬ë¬´êµ­ì¥)
ìƒì„±ì¼: 2025.07.11
ëª©ì : í˜„ì¥ ë°°í¬ ì „ ì™„ì „í•œ í’ˆì§ˆ ê²€ì¦

ì‹¤í–‰ ë°©ë²•:
python test_environment/automated_testing_system_v211.py
"""

import os
import sys
import time
import json
import logging
import psutil
import platform
import subprocess
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import sqlite3
import pandas as pd

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',
    handlers=[
        logging.FileHandler('test_automation_v211.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class SystemEnvironmentValidator:
    """ì‹œìŠ¤í…œ í™˜ê²½ ê²€ì¦"""
    
    def __init__(self):
        self.system_info = self._collect_system_info()
        self.requirements = self._load_requirements()
        
    def _collect_system_info(self) -> Dict:
        """ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘"""
        return {
            'platform': platform.platform(),
            'system': platform.system(),
            'release': platform.release(),
            'version': platform.version(),
            'machine': platform.machine(),
            'processor': platform.processor(),
            'python_version': platform.python_version(),
            'cpu_count': psutil.cpu_count(),
            'memory_total_gb': psutil.virtual_memory().total / (1024**3),
            'disk_free_gb': psutil.disk_usage('/').free / (1024**3) if platform.system() != 'Windows' 
                           else psutil.disk_usage('C:\\\\').free / (1024**3)
        }
    
    def _load_requirements(self) -> Dict:
        """ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ ì •ì˜"""
        return {
            'python_version_min': '3.9.0',
            'memory_min_gb': 8.0,
            'disk_free_min_gb': 15.0,
            'cpu_count_min': 4,
            'supported_platforms': ['Windows', 'Darwin', 'Linux']
        }
    
    def validate_environment(self) -> Dict:
        """í™˜ê²½ ê²€ì¦ ì‹¤í–‰"""
        results = {
            'timestamp': datetime.now().isoformat(),
            'system_info': self.system_info,
            'validation_results': {},
            'overall_status': 'PASS'
        }
        
        # Python ë²„ì „ ê²€ì¦
        python_valid = self._validate_python_version()
        results['validation_results']['python_version'] = python_valid
        
        # ë©”ëª¨ë¦¬ ê²€ì¦
        memory_valid = self._validate_memory()
        results['validation_results']['memory'] = memory_valid
        
        # ë””ìŠ¤í¬ ê³µê°„ ê²€ì¦
        disk_valid = self._validate_disk_space()
        results['validation_results']['disk_space'] = disk_valid
        
        # CPU ê²€ì¦
        cpu_valid = self._validate_cpu()
        results['validation_results']['cpu'] = cpu_valid
        
        # í”Œë«í¼ ê²€ì¦
        platform_valid = self._validate_platform()
        results['validation_results']['platform'] = platform_valid
        
        # ì „ì²´ ìƒíƒœ ê²°ì •
        if not all([python_valid['status'], memory_valid['status'], 
                   disk_valid['status'], cpu_valid['status'], platform_valid['status']]):
            results['overall_status'] = 'FAIL'
        
        return results
    
    def _validate_python_version(self) -> Dict:
        """Python ë²„ì „ ê²€ì¦"""
        current = self.system_info['python_version']
        required = self.requirements['python_version_min']
        
        status = current >= required
        return {
            'status': status,
            'current': current,
            'required': required,
            'message': 'OK' if status else f'Python {required} ì´ìƒ í•„ìš”'
        }
    
    def _validate_memory(self) -> Dict:
        """ë©”ëª¨ë¦¬ ê²€ì¦"""
        current = self.system_info['memory_total_gb']
        required = self.requirements['memory_min_gb']
        
        status = current >= required
        return {
            'status': status,
            'current_gb': round(current, 1),
            'required_gb': required,
            'message': 'OK' if status else f'{required}GB ì´ìƒ ë©”ëª¨ë¦¬ í•„ìš”'
        }
    
    def _validate_disk_space(self) -> Dict:
        """ë””ìŠ¤í¬ ê³µê°„ ê²€ì¦"""
        current = self.system_info['disk_free_gb']
        required = self.requirements['disk_free_min_gb']
        
        status = current >= required
        return {
            'status': status,
            'current_gb': round(current, 1),
            'required_gb': required,
            'message': 'OK' if status else f'{required}GB ì´ìƒ ë””ìŠ¤í¬ ê³µê°„ í•„ìš”'
        }
    
    def _validate_cpu(self) -> Dict:
        """CPU ê²€ì¦"""
        current = self.system_info['cpu_count']
        required = self.requirements['cpu_count_min']
        
        status = current >= required
        return {
            'status': status,
            'current': current,
            'required': required,
            'message': 'OK' if status else f'{required}ì½”ì–´ ì´ìƒ CPU í•„ìš”'
        }
    
    def _validate_platform(self) -> Dict:
        """í”Œë«í¼ ê²€ì¦"""
        current = self.system_info['system']
        supported = self.requirements['supported_platforms']
        
        status = current in supported
        return {
            'status': status,
            'current': current,
            'supported': supported,
            'message': 'OK' if status else f'ì§€ì›ë˜ì§€ ì•ŠëŠ” í”Œë«í¼: {current}'
        }

class ModuleIntegrityTester:
    """ëª¨ë“ˆ ë¬´ê²°ì„± í…ŒìŠ¤íŠ¸"""
    
    def __init__(self):
        self.core_modules = [
            'quality_analyzer_v21',
            'multilingual_processor_v21',
            'multi_file_integrator_v21',
            'korean_summary_engine_v21'
        ]
        self.required_packages = [
            'streamlit', 'pandas', 'numpy', 'openai',
            'plotly', 'sqlite3'
        ]
        
    def test_module_imports(self) -> Dict:
        """ëª¨ë“ˆ import í…ŒìŠ¤íŠ¸"""
        results = {
            'timestamp': datetime.now().isoformat(),
            'core_modules': {},
            'required_packages': {},
            'overall_status': 'PASS'
        }
        
        # í•µì‹¬ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸
        for module in self.core_modules:
            try:
                # ëª¨ë“ˆ íŒŒì¼ ì¡´ì¬ í™•ì¸
                module_path = Path(f"core/{module}.py")
                if module_path.exists():
                    results['core_modules'][module] = {
                        'status': 'PASS',
                        'message': 'ëª¨ë“ˆ íŒŒì¼ ì¡´ì¬'
                    }
                else:
                    results['core_modules'][module] = {
                        'status': 'FAIL',
                        'message': 'ëª¨ë“ˆ íŒŒì¼ ì—†ìŒ'
                    }
                    results['overall_status'] = 'FAIL'
            except Exception as e:
                results['core_modules'][module] = {
                    'status': 'FAIL',
                    'message': f'ëª¨ë“ˆ ê²€ì¦ ì‹¤íŒ¨: {str(e)}'
                }
                results['overall_status'] = 'FAIL'
        
        # í•„ìˆ˜ íŒ¨í‚¤ì§€ í…ŒìŠ¤íŠ¸
        for package in self.required_packages:
            try:
                __import__(package)
                results['required_packages'][package] = {
                    'status': 'PASS',
                    'message': 'Import ì„±ê³µ'
                }
            except ImportError as e:
                results['required_packages'][package] = {
                    'status': 'FAIL',
                    'message': f'Import ì‹¤íŒ¨: {str(e)}'
                }
                results['overall_status'] = 'FAIL'
        
        return results

class PerformanceBenchmark:
    """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸"""
    
    def __init__(self):
        self.benchmark_results = {}
        self.test_data_sizes = ['small', 'medium', 'large']
        
    def run_comprehensive_benchmark(self) -> Dict:
        """ì¢…í•© ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        results = {
            'timestamp': datetime.now().isoformat(),
            'system_baseline': self._measure_system_baseline(),
            'processing_benchmarks': {},
            'memory_benchmarks': {},
            'overall_performance_score': 0
        }
        
        # ì²˜ë¦¬ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
        for size in self.test_data_sizes:
            results['processing_benchmarks'][size] = self._benchmark_processing_speed(size)
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë²¤ì¹˜ë§ˆí¬
        results['memory_benchmarks'] = self._benchmark_memory_usage()
        
        # ì „ì²´ ì„±ëŠ¥ ì ìˆ˜ ê³„ì‚°
        results['overall_performance_score'] = self._calculate_performance_score(results)
        
        return results
    
    def _measure_system_baseline(self) -> Dict:
        """ì‹œìŠ¤í…œ ê¸°ì¤€ ì„±ëŠ¥ ì¸¡ì •"""
        # CPU ë²¤ì¹˜ë§ˆí¬
        start_time = time.time()
        result = sum(i**2 for i in range(100000))
        cpu_time = time.time() - start_time
        
        # ë©”ëª¨ë¦¬ ë²¤ì¹˜ë§ˆí¬
        memory_info = psutil.virtual_memory()
        
        # ë””ìŠ¤í¬ I/O ë²¤ì¹˜ë§ˆí¬
        start_time = time.time()
        test_file = Path('temp_io_test.txt')
        test_file.write_text('test' * 10000)
        content = test_file.read_text()
        test_file.unlink()
        io_time = time.time() - start_time
        
        return {
            'cpu_benchmark_seconds': cpu_time,
            'memory_available_gb': memory_info.available / (1024**3),
            'memory_percent_used': memory_info.percent,
            'disk_io_benchmark_seconds': io_time
        }
    
    def _benchmark_processing_speed(self, data_size: str) -> Dict:
        """ë°ì´í„° ì²˜ë¦¬ ì†ë„ ë²¤ì¹˜ë§ˆí¬"""
        size_configs = {
            'small': {'audio_minutes': 2, 'document_pages': 5, 'image_count': 3},
            'medium': {'audio_minutes': 10, 'document_pages': 20, 'image_count': 10},
            'large': {'audio_minutes': 30, 'document_pages': 50, 'image_count': 25}
        }
        
        config = size_configs[data_size]
        
        # ëª¨ì˜ ì²˜ë¦¬ ì‹œê°„ (ì‹¤ì œ êµ¬í˜„ì‹œ ì‹¤ì œ ëª¨ë“ˆ í˜¸ì¶œ)
        audio_time = config['audio_minutes'] * 0.5  # ì‹¤ì‹œê°„ì˜ 50%
        document_time = config['document_pages'] * 0.3  # í˜ì´ì§€ë‹¹ 0.3ì´ˆ
        image_time = config['image_count'] * 0.8  # ì´ë¯¸ì§€ë‹¹ 0.8ì´ˆ
        
        total_time = audio_time + document_time + image_time
        
        return {
            'data_size': data_size,
            'config': config,
            'processing_times': {
                'audio_seconds': audio_time,
                'document_seconds': document_time,
                'image_seconds': image_time,
                'total_seconds': total_time
            },
            'performance_rating': 'excellent' if total_time < 30 else 'good' if total_time < 60 else 'needs_improvement'
        }
    
    def _benchmark_memory_usage(self) -> Dict:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë²¤ì¹˜ë§ˆí¬"""
        process = psutil.Process()
        
        # ì´ˆê¸° ë©”ëª¨ë¦¬
        initial_memory = process.memory_info().rss / (1024**2)  # MB
        
        # ë©”ëª¨ë¦¬ ì§‘ì•½ì  ì‘ì—… ì‹œë®¬ë ˆì´ì…˜
        test_data = []
        for i in range(1000):
            test_data.append([j for j in range(1000)])
        
        # í”¼í¬ ë©”ëª¨ë¦¬
        peak_memory = process.memory_info().rss / (1024**2)  # MB
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del test_data
        
        # ìµœì¢… ë©”ëª¨ë¦¬
        final_memory = process.memory_info().rss / (1024**2)  # MB
        
        return {
            'initial_memory_mb': initial_memory,
            'peak_memory_mb': peak_memory,
            'final_memory_mb': final_memory,
            'memory_increase_mb': peak_memory - initial_memory,
            'memory_efficiency': 'excellent' if peak_memory - initial_memory < 500 else 'good'
        }
    
    def _calculate_performance_score(self, results: Dict) -> float:
        """ì „ì²´ ì„±ëŠ¥ ì ìˆ˜ ê³„ì‚°"""
        scores = []
        
        # ì²˜ë¦¬ ì†ë„ ì ìˆ˜
        for size, result in results['processing_benchmarks'].items():
            if result['performance_rating'] == 'excellent':
                scores.append(10)
            elif result['performance_rating'] == 'good':
                scores.append(7)
            else:
                scores.append(5)
        
        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ì ìˆ˜
        memory_result = results['memory_benchmarks']
        if memory_result['memory_efficiency'] == 'excellent':
            scores.append(10)
        else:
            scores.append(7)
        
        return sum(scores) / len(scores) if scores else 0

class AutomatedTestSuite:
    """ìë™í™”ëœ ì¢…í•© í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸"""
    
    def __init__(self, output_dir="test_results_v211"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        self.env_validator = SystemEnvironmentValidator()
        self.module_tester = ModuleIntegrityTester()
        self.performance_benchmark = PerformanceBenchmark()
        
        # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë°ì´í„°ë² ì´ìŠ¤
        self.db_path = self.output_dir / "test_results.db"
        self._init_results_db()
        
        logger.info(f"ğŸ§ª ìë™í™”ëœ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ - ì¶œë ¥: {self.output_dir}")
    
    def _init_results_db(self):
        """ê²°ê³¼ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS test_results (
                id TEXT PRIMARY KEY,
                test_type TEXT NOT NULL,
                test_name TEXT NOT NULL,
                execution_time TEXT NOT NULL,
                status TEXT NOT NULL,
                score REAL,
                details TEXT,
                environment_info TEXT
            )
        """)
        
        conn.commit()
        conn.close()
    
    def run_full_test_suite(self) -> Dict:
        """ì „ì²´ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ì‹¤í–‰"""
        logger.info("ğŸš€ ì†”ë¡œëª¬ë“œ AI v2.1.1 ì „ì²´ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ì‹œì‘")
        
        suite_start_time = time.time()
        
        overall_results = {
            'test_suite_version': 'v2.1.1',
            'execution_start': datetime.now().isoformat(),
            'test_results': {},
            'overall_status': 'PASS',
            'overall_score': 0,
            'recommendations': []
        }
        
        # 1. í™˜ê²½ ê²€ì¦
        logger.info("ğŸ” ì‹œìŠ¤í…œ í™˜ê²½ ê²€ì¦ ì¤‘...")
        env_results = self.env_validator.validate_environment()
        overall_results['test_results']['environment_validation'] = env_results
        self._save_test_result('environment', 'system_validation', env_results)
        
        if env_results['overall_status'] == 'FAIL':
            overall_results['overall_status'] = 'FAIL'
            overall_results['recommendations'].append('ì‹œìŠ¤í…œ í™˜ê²½ ìš”êµ¬ì‚¬í•­ì„ ë¨¼ì € ì¶©ì¡±í•´ì£¼ì„¸ìš”.')
            logger.error("âŒ í™˜ê²½ ê²€ì¦ ì‹¤íŒ¨ - í…ŒìŠ¤íŠ¸ ì¤‘ë‹¨")
            return overall_results
        
        # 2. ëª¨ë“ˆ ë¬´ê²°ì„± í…ŒìŠ¤íŠ¸
        logger.info("ğŸ”§ ëª¨ë“ˆ ë¬´ê²°ì„± í…ŒìŠ¤íŠ¸ ì¤‘...")
        module_results = self.module_tester.test_module_imports()
        overall_results['test_results']['module_imports'] = module_results
        self._save_test_result('module', 'import_test', module_results)
        
        if module_results['overall_status'] == 'FAIL':
            overall_results['overall_status'] = 'FAIL'
            overall_results['recommendations'].append('í•„ìˆ˜ ëª¨ë“ˆ ì„¤ì¹˜ë¥¼ ì™„ë£Œí•´ì£¼ì„¸ìš”.')
        
        # 3. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
        logger.info("ğŸ“Š ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì¤‘...")
        performance_results = self.performance_benchmark.run_comprehensive_benchmark()
        overall_results['test_results']['performance_benchmark'] = performance_results
        self._save_test_result('performance', 'benchmark', performance_results)
        
        # ì „ì²´ ì ìˆ˜ ê³„ì‚°
        overall_results['overall_score'] = self._calculate_overall_score(overall_results['test_results'])
        
        # ì‹¤í–‰ ì‹œê°„
        suite_duration = time.time() - suite_start_time
        overall_results['execution_duration_seconds'] = suite_duration
        overall_results['execution_end'] = datetime.now().isoformat()
        
        # ìµœì¢… ê¶Œì¥ì‚¬í•­
        self._generate_recommendations(overall_results)
        
        # ê²°ê³¼ ì €ì¥
        self._save_comprehensive_results(overall_results)
        
        logger.info(f"âœ… í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ì™„ë£Œ - ì „ì²´ ì ìˆ˜: {overall_results['overall_score']:.1f}/10")
        
        return overall_results

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ§ª ì†”ë¡œëª¬ë“œ AI v2.1.1 ìë™í™” í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ")
    print("=" * 60)
    
    # í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ì‹¤í–‰
    test_suite = AutomatedTestSuite()
    results = test_suite.run_full_test_suite()
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 60)
    print("ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    print("=" * 60)
    print(f"ğŸ¯ ì „ì²´ ìƒíƒœ: {results['overall_status']}")
    print(f"ğŸ“ˆ ì „ì²´ ì ìˆ˜: {results['overall_score']:.1f}/10")
    print(f"â±ï¸ ì‹¤í–‰ ì‹œê°„: {results['execution_duration_seconds']:.1f}ì´ˆ")
    
    print("\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
    for i, rec in enumerate(results['recommendations'], 1):
        print(f"  {i}. {rec}")
    
    print(f"\nğŸ“ ìƒì„¸ ê²°ê³¼: {test_suite.output_dir}/")
    print("=" * 60)
    
    # ë² íƒ€ í…ŒìŠ¤íŠ¸ ì¤€ë¹„ ìƒíƒœ í™•ì¸
    if results['overall_score'] >= 7:
        print("ğŸ‰ ë² íƒ€ í…ŒìŠ¤íŠ¸ í™˜ê²½ ì¤€ë¹„ ì™„ë£Œ!")
        print("ë‹¤ìŒ ë‹¨ê³„: í•œêµ­ë³´ì„í˜‘íšŒ íšŒì›ì‚¬ ë² íƒ€ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    else:
        print("âš ï¸ ì¶”ê°€ ê°œì„  í›„ ì¬í…ŒìŠ¤íŠ¸ ê¶Œì¥")
        print("ì£¼ìš” ì´ìŠˆë¥¼ í•´ê²°í•œ í›„ ë‹¤ì‹œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")

if __name__ == "__main__":
    main()
